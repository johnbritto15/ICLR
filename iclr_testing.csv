Text,Rating
"I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter ""drop rate"" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. 

======
The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.

Major concerns:
An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.

Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.

Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.",4
"This paper proposes a surprisingly simple technique for improving the robustness of neural networks against black-box attacks. The proposed method creates a *fixed* random mask to zero out lower layer activations during training and test. Extensive experiments show that the proposed method without adversarial training is competitive with a state-of-the-art defense method under blackbox attacks.

Pros:
 -- simplicity and effectiveness of the method
 -- extensive experimental results under different settings

Cons:
 -- it's not clear why the method works besides some not-yet-validated hypotheses.
 -- graybox results seem to suggest that the effectiveness of the method is due to the baseline CNNs and the proposed CNNs learning very different functions; source models within the same family still produce strong transferable attacks. It would have been much more impressive if different randomness could result in very different functions, leading to strong defense in the graybox setting.",6
"The authors propose a simple method for increasing the robustness of convolutional neural networks against adversarial examples. This method is simple but seems to achieve surprisingly good results. It consist in randomly remove neurons from the network architecture. The deleted neurons are selected before training and remain deleted during the training and test phase.  The authors also study the adversarial examples that still fool the network after applying their method and find than those examples also fool human. This finding raises the question of what is an adversarial example if both humans and networks are fooled by the same example. 

Using Random Masks in neural network is not a new idea since it was already proposed for DropOut or DropConnect (Regularization of Neural Networks using DropConnect, ICML2013) and in the context of adversarial attacks (Dhillon et al. 2018)  as reported by the authors. The discussion (Section 2) about the impact of random masks on what convolution layers capture in the spatial organisation of the input is interesting: whereas standard CNNs focus on detecting the presence of a feature in the output, random mask could force the CNN layers to learn how a specific feature distributes on the whole input maps. This limitation of the CNN has already been pointed up and solutions have been proposed for example Capsule Networks (Dynamic Routing Between Capsules, NIPS 2017). This intuition is experimentally supported by a simple random shuffle by block of the input image  (Appendix A).

In Section 3, the authors present a large number of experiments to demonstrate the robustness of their method. Most of the details are given in the 13 (!) pages of appendix. Experiments against black-box attack, random noise, white-box attack, grey-box are presented. Most of the experiments are on CIFAR10 but one experiment is also presented on MNIST. One could regret that only one architecture of CNN is tested (ResNet18) except for gray-box attack, for which DenseNet121 and VG19 are tested. One could ask why the type of models tested is not consistent across the different experiments.  For black-box attack, random masks compare favourably to Madry’s defence. For white box defence, Random Mask is not compared to another defence method, which seems a weakness to me but I am not familiar enough with papers in this area to estimate if this is a common practice. In most of the experiments, the drop ratio is between 0.5 and 0.9, which seems to indicate that the size the initial network could be reduced by more than 50% to increase the robustness to attack. This ratio is larger than what is usually used for dropout (0.5 at most).  

In section 3.3, different strategies for random masks are explored : where to apply random masks, random mask versus random channels, random masks versus same masks. Results are given in table 2. The caption of Table 2 could be more explicit : what are the presented percent ?

Experiments on masking shallow versus deep layers are interesting. Best results for robustness are obtained with masking shallow layers at quite a high ratio (0.9). One could ask if this result could be due to the type or the parameters of adversarial attacks which are not adapted to such a high sparseness on shallow layers or to the specific kind of sparseness induced by the masks. A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect. 

pros : simple to implement, good robustness shown agains a variety of attack types
cons : mainly tested on a single architecture (ResNet) and on a single datatbase CIFAR. Maybe not robust against the latest techniques of adversarial attack.",7
"This paper presents new variants of ADAM and AMSGrad that bound the gradients above and below to avoid potential negative effects on generalization of excessively large and small gradients; and the paper demonstrates the effectiveness on a few commonly used machine learning test cases.  The paper also presents detailed proofs that there exists a convex optimization problem for which the ADAM regret does not converge to zero.

This paper is very well written and easy to read.  For that I thank the authors for their hard word.  I also believe that their approach to bound is well structured in that it converges to SGD in the infinite limit and allows the algorithm to get teh best of both worlds - faster convergence and better generalization.  The authors' experimental results support the value of their proposed algorithms.  In sum, this is an important result that I believe will be of interest to a wide audience at ICLR.

The proofs in the paper, although impressive, are not very compelling for the point that the authors want to get across.  That fact that such cases of poor performance can exists, says nothing about the average performance of the algorithms, which is practice is what really matters.

The paper could be improved by including more and larger data sets.  For example, the authors ran on CIFAR-10.  They could have done CIFAR-100, for example, to get more believable results.

The authors add a useful section on notation, but go on to abuse it a bit.  This could be improved.  Specifically, they use an ""i"" subscript to indicate the i-th coordinate of a vector and then in the Table 1 sum over t using i as a subscript.  Also, superscript on vectors are said to element-wise powers.  If so, why is a diag() operation required?  Either make the outproduct explicit, or get rid of the diag().",7
"The authors introduce AdaBound, a method that starts off as Adam but eventually transitions to SGD. The motivation is to benefit from the rapid training process of Adam in the beginning and the improved convergence of SGD at the end. The authors do so by clipping the weight updates of Adam in a dynamic way. They show numerical results and theoretical guarantees. The numerical results are presented on CIFAR-10 and PTB while the theoretical results are shown on assumptions similar to AMSGrad (& using similar proof strategies). As it stands, I have some foundational concerns about the paper and believe that it needs significant improvement before it can be published. I request the authors to please let me know if I misunderstood any aspect of the algorithm, I will adjust my rating promptly. I detail my key criticisms below:

- I'm somewhat confused by the formulation of \eta_u and \eta_l. The way it is set up (end of Section 4), the final learning rate for the algorithm converges to 0.1 as t goes to infinity. In the Appendix, the authors show results also with final convergence to 1. Are the results coincidental with the fact that SGD works well with those learning rates? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \eta s. 

- Am I correct in saying that with t=100 (i.e., the 100th iteration), the \eta s constrain the learning rates to be in a tight bound around 0.1? If beta=0.9, then \eta_l(1) = 0.1 - 0.1 / (0.1*100+1) = 0.091. After t=1000 iterations, \eta_l becomes 0.099. Again, are the good results coincidental with the fact that SGD with learning rate 0.1 works well for this setup? In the scheme of the 200 epochs of training (equaling almost 100-150k iterations), if \eta s are almost 0.099 / 0.10099, for over 99% of the training, we're only doing SGD with learning rate 0.1. 

- Along the same lines, what learning rates on the grid were chosen for each of the problems? Does the setup still work if SGD needs a small step size and we still have \eta converge to 1? A VGG-11 without batch normalization typically needs a smaller learning rate than usual; could you try the algorithms on that? 

- Can the authors plot the evolution of learning rate of the algorithm over time? You could pick the min/median/max of the learning rates and plot them against epochs in the same way as accuracy.This would be a good meta-result to show how gradual the transition from Adam to SGD is. 
 
- The core observation of extreme learning rates and the proposal of clipping the updates is not novel; Keskar and Socher (which the authors cite for other claims) motivates their setup with the same idea (Section 2 of their paper). I feel that the authors should clarify what they are proposing as novel. Is it correct that a careful theoretical analysis of this framework is what stands as the authors' major contribution?

- Can you try experimenting with/suggesting trajectories for \eta which converge to SGD stepsize more slower? 

- Similarly, can you suggest ways to automate the choice for the \eta^\star? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning? 
",4
"*Summary :
The paper explores variants of popular adaptive optimization methods.
The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates.
The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments.


*Significance:
-There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.

-Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound.
Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well.

-The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea.

*Clarity:
The idea and motivation are very clear and so are the experiments.


*Presentation:
The presentation is mostly good.

Summary of review:
The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method.

",6
"The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of ""missing"" (to the generative model) features.

Novelty:
Generative models have a long history of being used to impute missing data. e.g. http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2011.pdf, https://arxiv.org/pdf/1610.04167.pdf,
https://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F]
It is a little difficult to guage what the novelty of this work is.

Clarity
This is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use ""p"").

Experimental Results
The model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot?
Beyond that:
(a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?
(b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?

The qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned).

Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?


Priors on b:
What kind of priors on b did you experiment with? ",6
"This paper introduces the VAEAC model, inspired by CVAEs, it allows conditioning on any subset
of the latent features. This provides a model able to achieve good results on image inpainting
and feature imputation tasks.

The paper appears to be technically sound, and the experiments are
thoughtfully designed. The writing is clear and the model is easy to
understand. The closest work to this of the Universal Marginalizer is
compared to well, with more compelling examples in the appendix. I
would have preferred if more of the experimental results were in the
main paper instead of in the appendix especially as the authors state
they chose to highlight their better results in the main paper.

While not the first model to try to handle modeling data with missing features, it is
still a fairly original and elegant formulation.

Minor details:

In equation (8) should x be x_b?
",7
"The paper presents a model for learning conditional distribution when arbitrary partitioning the input to observed and masked parts. The idea is to extend the conditional VAE framework such that the posterior is a function of an arbitrary subset of observed variables. Accordingly, reconstruction loss only penalizes the error in the reconstruction of masked (unobserved) variables. The method is compared against 1) classical approaches in missing data imputation on UCI benchmarks; 2) image inpainting against recently proposed GANS for the similar task, as well as; 3) against universal marginalizer, which learns conditional densities using a feedforward / autoregressive architecture.

My concern about the experimental results on missing data imputation is that strong competition such as Gondra et al’17 and Yoon et al’18 that report better results on UCI than classical approaches are not included. Could you please comment? See also [1,2] for other autoencoding architectures for this task.

While the derivation of the method is principled, it assumes that either the mask is known during the training OR one could efficiently sample a distribution of masks to learn arbitrary conditional densities. Given the exponential number of valid masks in a general setting, one only subsamples a small portion during the training. The question is whether the model can generalize well in this regime? The experimental results in this setting is not very encouraging, suggesting the proposed approach is effective only when the limitted mask patterns are known in advance. 

[1] Gondara, Lovedeep, and Ke Wang. ""Multiple imputation using deep denoising autoencoders."" arXiv preprint arXiv:1705.02737 (2017).

[2] Zhang, Hongbao, Pengtao Xie, and Eric Xing. ""Missing Value Imputation Based on Deep Generative Models."" arXiv preprint arXiv:1808.01684 (2018).
",6
"The authors come up with a surprisingly elegant algorithm (""minimal random coding"") which encodes samples from a posterior distribution, only using a number of bits that approximates the KL divergence between posterior and prior, while Shannon-type algorithms can only do this if the posterior is deterministic (a delta distribution). It can also be directly used to sample from continuous distributions, while Shannon-type algorithms require quantization. In my opinion, this is the main contribution of the paper.

The other part of the paper that is specifically concerned with weight compression (""MIRACLE"") turns out to be a lot less elegant. It is somewhat ironic that the authors specifically call attention to the their algorithm sending random samples, as opposed to existing algorithms, which quantize and then send deterministic variables. This is clearly true for the basic algorithm, but, if I understand correctly, not for MIRACLE. It seems clear that neural networks are sensitive to random resampling of their weights -- otherwise, the authors would not have to fix the weights in each block and then do further gradient descent for the following blocks. What would happen if the distributions were held constant, and the algorithm would be run again, just with a different (but identical) random seed in both sender and receiver? It seems this would lead to a performance drop, demonstrating that (albeit indirectly), MIRACLE also makes a deterministic choice of weights.

Overall, I find the paper somewhat lacking in terms of evaluation. MIRACLE consists of a lot of parts. It is hard to assess how much of the final coding gain presented in table 1 is due to the basic algorithm. What is the effect of selecting other probability models, possibly different ones than Gaussians? Choosing appropriate distributions can have a significant impact on the value of the KL divergence. Exactly how much is gained by applying the hashing trick? Are the standard deviations of the priors included in the size, and how are they encoded?

This could be assessed more clearly by directly evaluating the basic algorithm. Theorem 3.2 predicts that the approximation error of algorithm 1 asymptotically zero, i.e. one can gain an arbitrarily good approximation to the posterior by spending more bits. But how many more are practically necessary? It would be fantastic to actually see some empirical data quantifying how large the error is for different distributions (even simple toy distributions). What are the worst vs. best cases?
",7
"In this paper the authors propose to use MLD principle to encode the weights of NNs and still preserve the performance of the original network. The main comparison is from Han 2016, in which the authors use ad-hoc techniques to zero some coefficient and prune some connection + Huffman coding. In this case , the authors uses as a regularizer (See equation 3) a constraints that the weights are easy to compress. The results seem significant improvement with respect to the state of the art. 

",6
"This paper considers the compression of the model parameters in deep neural networks. The authors propose minimal random code learning (MIRACLE), which uses a random sample of weights and the variational framework interpreted by the bits-back argument. The authors introduce two theorems characterizing the properties of MIRACLE, and demonstrate its compression performance through the experiments.

The proposed approach is interesting and the performance on the benchmarks is good enough to demonstrate its effectiveness. However, since the two main theorems are based on the existing results by Harsha et al. (2010) and Chatterjee & Diaconis (2018), the main technical contribution of this paper is the sampling scheme in Algorithm 1.

Although the authors compare the performance trade-offs of MIRACLE with that of the baseline methods quoted from source materials, isn't it possible or desirable to include other competitors or other results for the baseline methods? Are there any other methods, in particular, achieving low error rate (with high compression size)? Little is discussed on why the baseline results are only a few. 

minor comment: 
- Eq.(4) lacks p(D) in front of dD.    

Pros:
- Interesting approach based-on the bits back argument
- Good performance trade off demonstrated through experiments
Cons:
- Only a few baseline results, in particular, at high compression size
",7
"The authors present some theoretical results on the loss surface of neural networks. Their main results are:

(1) They consider a 1 layer hidden neural network where the single nonlinearity is ReLU / ReLU-like. Here they prove that as long as a linear model cannot fit the data, then there exits a local minimum strictly inferior to the global one (They can then scale the parameters to get infinitely many local optima).

The key idea is to construct a local minima whose risk value is the same as the local least squares solution. Then to construct a set of parameters that has smaller risk value than this local optima. The proof technique is interesting.

(2) They construct a particular dataset for which a one hidden layer neural net with other nonlinear activations (sigmoid, tanh, etc.) also has local optima.

I think this theorem is a bit less interesting since the dataset given has only two data points. I think it is less interesting to prove suboptimality of neural nets in small sample size settings. 

(3) Global optimailty of linear networks. The authors show that deep linear networks (i.e. y = W1 W2 W3...W5 x) have only global minima or saddle points.

I'm not familiar enough with the field to know the significant of this result. The deep linear network  just seems like an artificial construction (i.e. in practice one would simply condense W1...W5 to one W) to study nonconvexity / local optima, no one would use it in practice.",7
"Paper represents theoretical analysis of the loss surface of neural networks. The authors supplied interesting results about local minima properties of neural networks. The paper is written quite well and easy to follow. Furthermore, authors made a comprehensive literature survey and connected their paper to already existing literature. The proofs seem correct (please note that paper is quite long and it requires more time then conference review period, hence, I stated “seem correct”). 
Although, paper provides novel theorems, I have several concerns, and these are:
•	Isn’t it clear that (generally speaking) a non-convex problem will have many local minima? Previous paper in neural network community is (in my opinion) not theorems. I believe one should read those statements such that in practice (please note here practice means that architectures, e.g. resnet50, inceptionv3, used in day to day life) researchers don’t not observe those “really bad” local minima.
•	In ML literature, we have convex machines which are guaranteed to converge to global optimum. Given image dataset, or text datasets supervised deep learning in in general much better than convex methods e.g. SVMs. The paper clearly shows that there exist, in some cases, exponentially many local minima however  current training methods are able to find better solutions than convex methods (I am completely aware that functions classes are different however success metric, accuracy, is the same). Hence, how relevant are the results without taking in to account architectural choices or optimization methods for deep learning? May be structural risk minimisation is a better approach than empirical risk minimization for quantifying the performance of deep neural networks,
In conclusion, paper is interesting however I believe it need to be improved. 
",7
"
The authors provide a clean and easily understood sufficient
condition for spurious local minima to exist in networks with
a hidden layer using ReLUs or leaky ReLUs.  This condition,
that there is not linear transformation with zero loss,
is satisfied for almost all inputs with more examples than
input variables.

The construction is elegant.  The mathematical writing in the paper,
especially describing the proof of Theorem 1, is very nice -- they
expose the main ideas effectively.

I do not know of another paper using a similar proof, but I have not
studied the proofs of the most closely related papers prior to doing
this review, so I have limited ability to vouch for this paper's
technical novelty.

The authors also show that networks using many other popular
activation functions have spurious local minima for a very
simple dataset.  All of these analysis are unified using a
simple, if technical, set of conditions on activation function.

Finally, the authors prove a somewhat technical theorem about
optima in deep linear networks, which generalizes some
earlier treatments of this topic, providing an checkable
condition for global minimality.

There is extensive discussion of related work.  I am not aware of
related work not covered by the authors.

In some cases, when the authors discuss previous work, they write as
if restriction to the realizable case is an assumption, when it seems
to me to be more of a constraint.  In other words, it seems harder to
prove the existence of spurious minima in the realizable case.
They seem to acknowledge this after their statement of their Theorem 2,
which also uses a realizable dataset.

Also, a few papers, including the Venturi, et al paper cited by
the authors, have analyzed whether spurious local minima exist
in subsets of the parameter space, including those likely to
be reached during training with different sorts of initializations.
In light of this work, the authors might want to tone down claims
about how their work shows that results about linear networks do
not generalize to the non-linear case.  In particular, to make
their construction work in the case of wide networks, they
need an overwhelming majority of the hidden units to be ""dead"",
which seems as it is unlikely to arise from training with
commonly used initializations.

Overall, I think that this paper makes an interesting and
non-obvious contribution on a hot topic.",8
"﻿This paper applies a rotation-equivariant convolutional neural network model to a dataset of neural responses from mouse primary visual cortex. This submission follows a series of recent papers using deep convolutional neural networks to model visual responses, either in the retina (Batty et al., 2016; McIntosh et al., 2016) or V1 (Cadena et al., 2017; Kindel et al., 2017; Klindt et al., 2017). The authors show that adding rotation equivariance improves the explanatory power of the model compared to non-rotation-equivariant models with similar numbers of parameters, but the performance is not better than other CNN-based models (e.g. Klindt et al., 2017). The main potential contributions of the paper are therefore the neuroscientific insights obtained from the model. However, I have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below). Together with the fact that the model does not provide better explanatory power than other models, I cannot recommend acceptance. I am open to discussions with the authors, but do not anticipate a major change in the rating.

Update after revisions: The authors performed extensive work to address my concerns. This showed that some concerns (RF appearance) were valid, and the authors removed them from the final manuscript. I raised my score accordingly.

1. As noted by the authors, the finding that “Feature weights are sparse” (page 6) could be due to the sparsity-inducing L1 penalty. The fact that a model without L1 penalty performs worse does not mean that there is sparsity in the underlying data. For example, the unregularized model could be overfitting. A more careful model selection analysis is necessary to show that the data is better fit by a sparse than a dense model. 

2. The finding that there are center-surround or asymmetric (non-gabor) RFs in mouse V1 is not novel and not specific to this model (e.g. Antolik et al., 2016). 

3. Many of the receptive fields in Figure 6 look pathological (overfitted?) compared to typical V1 receptive fields in the literature. I understand that sensitivity to previously undetected RF features is a goal of the present work. However, given how unusual the RFs look, more controls are necessary to ensure they are not an artefact of the method, e.g. the activation maximization approach with gradient preconditioning, the sparsity constraints, or overfitting. Perhaps a comparison of RFs learned on two disjoint subsets of the training set would help to determine which features are reproducible.

4. Should orientation be treated as a nuisance variable? Natural image statistics are not rotation-invariant. In the visual system, especially in mice, it is not clear whether orientation is completely disentangled from other RF properties. The orientation space is not uniformly covered, and some directions have special meaning (e.g. cardinal directions), such that it might be invalid to assume that the visual system is equivariant to rotation. (The same concern applies to the translation equivariance assumed when modeling visual RFs with standard CNNs.) Of course, there is a tradeoff between model expressiveness and the need to make assumptions to fit the model with realistic amounts of data. However, this concern should at least be discussed.

5. Some more details about the neural recordings would be good. What calcium indicator? How was the recording targeted to V1? Perhaps some example traces.",5
"In this interesting study, the authors show that incorporating rotation-equivariant filters  (i.e. enforcing weight sharing across filters with different orientations) in a CNN model of the visual system is a useful prior to predict responses in V1. After fitting this model to data, they find that the RFs of model V1 cells do not resemble the simple Gabor filters of textbooks, and they present other quantitative results about V1 receptive fields. The article is clearly written and the claims are supported by their analyses. It is the first time to my knowledge that a rotation-equivariant CNN is used to model V1 cells.

The article would benefit from the following clarifications:

1. The first paragraph of the introduction discusses functional cell types in V1, but the article does not seem to reach any new conclusion about the existence of well-defined clusters of functional cell types in V1. If this last statement is correct, I believe it is misleading to begin the article with considerations about functional cell types in V1. Please clarify.

2. For clarity, it would help the reader to mention in the abstract, introduction and/or methods that the CNN is trained on reproducing V1 neuron activations, not on an image classification task as in many other studies (Yamins 2014, etc). 

3. “As a first step, we simply assume that each of the 16 features corresponds to one functional cell type and classify all neurons into one of these types based on their strongest feature weight.” and “The resulting preferred stimuli of each functional type are shown in Fig. 6.“
Again, I think these statements are misleading because they suggest that V1 cells indeed cluster in distinct functional cell types rather than form a continuum. However, from the data shown, it is unclear whether the V1 cells recorded form a continuum or distinct clusters. Unless this question is clarified and the authors show the existence of functionally distinct clusters in their data, they should preferably not mention ""cell types"" in the text.

Suggestions for improvement and questions (may not necessarily be addressed in this paper):

4. “we apply batch normalization”
What is the importance of batch normalization for successfully training the model? Do you think that a sort of batch normalization is implemented by the visual system? 

5. “The second interesting aspect is that many of the resulting preferred stimuli do not look typical standard textbook V1 neurons which are Gabor filters. ”
OK but the analysis consists of iteratively ascending the gradient of activation of the neuron from an initial image. This cannot be compared directly to the linear approximation of the V1 filter that is computed experimentally from doing a spike-triggered average (STA) from white noise. A better comparison would be to do a single-step gradient ascent from a blank image. In this case, do the filters look like Gabors?

6. Did you find any evidence that individual V1 neurons are themselves invariant to a rotation?

7. The article could be more self-contained. There are a lot of references to Klindt et al. (2017) on which this work is based, but it would be nice to make the article understandable without having to read this other article.

Typo: Number of fearture maps in last layer 

Conclusion:
I believe this work is significant and of interest for the rest of the community studying the visual system with deep networks, in particular because it finds an interesting prior for modeling V1 neurons, that can probably be extended to the rest of the visual system. However, it would benefit from the clarifications mentioned above.",7
"The paper analyses the data collected from 6005 neurons in a mouse brain. Visual stimuli are presented and the responses of the neurons recorded. In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli. Results show a decent correlation between neuron responses and trained network. Moreover, the rotational equivariant architecture beats a standard CNN with similar number of feature maps. The analysis and discussion of the results is interesting. Overall, the methodological approach is good.

I have trouble understanding the plot in Figure 4, it also does not print well and is barely readable on paper.

I have a small problem Figure 6 where ""optimal"" response-maps are presented. From my understanding, many of those feature maps are not looking similar to feature maps that are usually considered. Given the limited data available and the non-perfect modeling of neurons, the computed optimal response-map might include features that are not present in the dataset. Therefore, it would be interesting to compare those results with the stimuli used to gather the data. E.g. for a subset of neurons, one could pick the stimulus that created the maximum response and compare that to what the stimulus with the maximum response of the trained neuron was. It might be useful to include the average correlation of the neurons belong to each of the 16 groups(if there are any meaningful differences), especially as the cut-off of ""correlation 0.2 on the validation set"" is rather low.

Note: I am not an expert in the neural-computation literature, I am adapting the confidence rating accordingly.",8
"The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set. This setting is further generalized to multiple sets. The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors. Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.

The paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical. This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.

Overall, I enjoyed reading the paper. My only concern is the experiments:

1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.

2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)  Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?
",8
"Given a graph G of n vertices, the activations at each level of a graph neural network (G-NN) for G 
can be arranged in an n^k tensor T for some k. A fundamental criterion is that this tensor must be equivariant 
to permutations of the vertices of G in the sense of each index of of T being permuted simultaneously. 

This paper enumerates the set of all linear maps that satisfy this criterion, i.e., all linear maps 
which (the authors claim) can serve as the analog of convolution in equivariant G-NNs. 
The authors find that for invariant neural networks such maps span a space of dimension just b(k), whereas 
for equivariant neural networks they span a space of dimension b(2k).

The proof of this result is simple, but elegant. It hinges on the fact that the set of tensor elements of 
the same equality type is both closed and transitive under the permutation action. Therefore, the 
dimensionality of the subspace in question is just the number of different identity types, i.e., 
partitions of either {1,...,k} or {1,...,2k}, depending on whether we are talking about invariance or 
equivariance.

My problem with the paper is that the authors' model of G-NNs doesn't actually map to what is used 
in practice or what is interesting and useful. Let me list my reservations in increasing order of significance.

1. The authors claim that they give a ``full characterization'' of equivariant layers. This is not true. 
Equivariance means that there is *some* action of the symmetric group S_n on each layer, and wrt these actions 
the network is equivariant. Collecting all the activations of a given layer together into a single object L, 
this means that L is transformed according to some representation of S_n. Such a representation can always be 
reduced into a direct sum of the irreducible representations of S_n. The authors only consider the case then 
the representation is the k'th power of the permutation representation (technically called the defining 
representation of the S_n). This corresponds to a specific choice of irreducibles and is not the most general case. 
In fact, this is not an unnatural choice, and all G-NNs that I know follow this route. 
Nonetheless, technically, saying that they consider all possible equivariant networks is not correct.

2. The paper does not discuss what happens when the input tensor is symmetric. On the surface this might seem 
like a strength, since it just means that they can consider the more general case of undirected graphs (although 
they should really say so). In reality, when considering higher order activations it is very misleading because 
it leads to a massive overcounting of the dimensionality of the space of convolutions. In the case of k=2, for 
example, the dimensionality for undirected graphs is probably closer to 5 than 15 for example (I didn't count).

3. Finally, and critically, in actual G-NNs, the aggregation operation in each layer is *not* 
linear, in the sense that it involves a product of the activations of the previous layer with the adjacency 
matrix (messages might be linear but they are only propagated along the edges of the graph). 
In most cases this is motivated by making some reference to the geometric meaning of convolution,  
the Weisfeiler-Lehman algorithm or message passing in graphical models. In any case, it is critical that the 
graph topology be reintroduced into the network at each layer. The algebraic way to see it is that each layer 
must mix the information from the vertices, edges, hyperedges, etc.. The model in this paper could only aggregated 
edge information at the vertices. Vertex information could not be broadcast to neighboring vertices again. 
The elemenary step of ``collecting vertex information from the neighbors but only the neighbors'' cannot be 
realized in this model.

Therefore, I feel that the model used in this paper is rather uninteresting and irrelevant for practical 
purposes. If the authors disagree, I would encourage them to explicitly write down how they think the model 
can replicate one of the standard message passing networks. It is apparent from the 15 operations listed on 
page 11 that they have nothing to do with the graph topology at all.

Minor gripes:

- I wouldn't call (3) and (4) fixed point equations, that's usually used in dynamical systems. Here there is 
an entire subspace fixed by *all* permutations.

- Below (1), they probably mean that ``up to permutation vec(L)=vec(L^T)''. 

",4
"This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model. I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data. The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below). So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.). 

I only have a couple of concerns: 
1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from. The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges). Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used). Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.  The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting). 
2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work. I would have also liked to see a comparison to these methods in the the classification results.
3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input. This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0]. So the two results coincide for the exchangeable case. Might be worth pointing this out. 
",9
"This paper proposes Leap, a meta-learning procedure that finds better initialization for new tasks. Leap is based on past training/optimization trajectories and updates the initialization to minimize the total trajectory lengths. Experiments show that Leap outperforms popular alternatives like MAML and Reptile.

Pros
- Novel idea
- Relatively well-written
- Sufficient experiment evidence

Cons
- There exist several gaps between the theory and the algorithm

I have several concerns.
1. The idea is clearly delivered, but there are several practical treatments that are questionable. The first special treatment is that on page 5, when the objective is increased instead of decreased, the sign of the f part is flipped, which is not theoretically sound. It is basically saying that when we move from psi^i to psi^{i+1} with increased objective, we lie to the meta-learner that it is decreasing. The optimization trajectory is what it is. It would be beneficial to see the effect of removing this trick, at least in the experiments. Second, replacing the Jacobian with the identity matrix is also questionable. Suppose we use a very small but constant learning rate alpha for a convex problem. Then J^i=(I-G)^i goes to the zero matrix as i increases (G is small positive). However, instead, the paper uses J^i=I for all i. This means that the contributions for all i are the same, which is unsubstantiated.

2. The proof of Thm1 in Appendix A is not complete. For example, ""By assumption, beta is sufficiently small to satisfy F"", which I do not understand the inequality. Is there a missing i superscript? Isn't this the exact inequality we are trying to prove for i=0? As another example, ""if the right-most term is positive in expectation, we are done"", how so? BTW, the right-most term is a vector so there must be something missing. It would be more understandable if the proof includes a high-level proof roadmap, and frequently reminds the reader where we are in the overall proof now.

3. The set \Theta is not very well-defined, and sometimes misleading. Above Eq.(6), \Theta is mathematically defined as the intersection of points whose final solutions are within a tolerance of the *global* optimum, which is in fact unknown. As a result, finding a good initialization in \Theta for all the tasks as in Eq.(5) is not well-defined.

4. About the experiments. What is the ""Finetuning"" in Table 1? Presumably it is multi-headed but it should be made explicit. What is the standard deviation for Fig.4? The claim that ""Leap learns faster than a random initialization"" for Breakout is not convincing at all.

Minors
- In Eq.(4), f is a scalar so abs should suffice. This also applies to subsequent formulations.
- \mu is introduced above Eq.(8) but never used in the gradient formula.
- On p6, there is a missing norm notation when introducing the Reptile algorithm.",8
"\documentclass[10pt]{article}
\usepackage{geometry}[1in]
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{indentfirst}

\begin{document}
	
	\section*{SUMMARY}
	
	The article proposes Leap, a novel meta-learning objective aimed at outperforming state-of-the-art approaches when dealing with collections of tasks that exhibit substantial between-task diversity.
	
	Similarly to prior work such as MAML [1] or Reptile [2], the goal of Leap is to learn an initialization $\theta_{0}$ for the model parameters, shared across tasks, which leads to good and data-efficient generalization performance when fine-tuning the model on a set of held-out tasks. In a nutshell, what sets Leap apart from MAML or Reptile is its cost function, which explicitly accounts for the entire path traversed by the model parameters during task-specific fine-tuning -- i.e., ``inner loop'' optimization --, rather than mainly focusing on the final value attained by the model parameters after fine-tuning. More precisely, Leap looks for an initialization $\theta_{0}$ of the model parameters such that the energy of the path traversed by $\gamma_{\tau}(\theta) = (\theta, f_{\tau}(\theta))$ while fine-tuning $\theta$ to optimize the loss $f_{\tau}(\theta)$ of a task $\tau$ is minimized, on average, across $\tau \sim p(\tau)$. Thus, it could be argued that Leap extends Reptile, which can be informally understood as seeking an initialization $\theta_{0}$ that minimizes the average squared Euclidean distance between $\theta_{0}$ and the model parameters after fine-tuning on each task $\tau \sim p(\tau)$ [2, Section 5.2], by using a distance function between initial and final model parameters that accounts for the geometry of the loss surface of each task during optimization.  
	
	The final algorithm introduced in the paper considers however a variant of the aforementioned cost function, motivated by its authors on the basis of stabilising learning and eliminating the need for Hessian-vector products. The resulting approach is then evaluated on image recognition tasks (Omniglot plus a set of six additional computer vision datasets) as well as reinforcement learning tasks (Atari games).
	
	\section*{HIGH-LEVEL ASSESSMENT}
	
	The article proposes an interesting extension of existing work in meta-learning. In a slightly different context (meta-optimization), recent work [3] pointed out the existence of a ``short-horizon bias'' which could arise when using meta-learning objectives that apply only a small number of updates during ``inner-loop'' optimization. This observation is well-aligned with the motivation of this article, in which the authors attempt to complement successful methods like MAML or Reptile to perform well also in situations where a large number of gradient descent-based updates are applied during task-specific fine-tuning. Consequently, I believe the article is timely and relevant.
	
	Unfortunately, I have some concerns with the current version of the manuscript regarding (i) the proposed approach and the way it is motivated, (ii) the underlying theoretical results and, perhaps most importantly, (iii) the experimental evaluation. In my opinion, these should ideally be tackled prior to publication. Nonetheless, I believe that the proposed approach is promising and that these concerns can be either addressed or clarified. Thus I look forward to the rebuttal.
	
	\section*{MAJOR POINTS}
	
	\subsection*{1. Issues regarding proposed approach and its motivation/derivation}
	
	\textbf{1.a} Section 2.1 argues in favour of studying the path traversed by $\gamma_{\tau}(\theta) = (\theta, f_{\tau}(\theta))$ rather than the path traversed by the model parameters $\theta$ alone. However, this could in turn exacerbate the difficulty in dealing with collections of tasks for which the loss functions have highly diverse scales. For instance, taking the situation to the extreme, one could define an equivalence class of tasks $[\tau] = \left\{\tau \mid f_{\tau}(\theta) = g(\theta) + \mathrm{constant} \right\}$ such that any two tasks $\tau_{1}, \tau_{2} \in [\tau]$ would essentially represent the same underlying task, but could lead to arbitrarily different values of the Leap cost function. 
	
	Given that Leap is a model-agnostic approach, like MAML or Reptile, and thus could be potentially applied in many different settings and domains, I believe the authors should study and discuss (theoretically or experimentally) the robustness of Leap with respect to between-task variation in the scale of the loss functions and, in case the method is indeed sensitive to those, propose an effective scheme to normalize them.
	
	\textbf{1.b} The current version of the manuscript motivates defining the cost function in terms of $\gamma_{\tau}(\theta) = (\theta, f_{\tau}(\theta))$ rather than the model parameters $\theta$ alone in order to ``avoid information loss'', making it seem that this modification is ``optional'' or, at least, not critical. Nevertheless, taking a closer look at the Leap objective and the meta-updates it induces, I believe it might actually be essential for the correctness of the approach. I elaborate this view in what follows. Let us write the Leap objective for a task $\tau$ as
	\[
	F_{\tau}(\theta_{0},\widetilde{\theta}_{0}) = \underbrace{\sum_{i=0}^{K_{\tau} - 1}{\left\vert\left\vert u^{(i+1)}_{\tau}(\widetilde{\theta}_{0}) - u^{(i)}_{\tau}(\theta_{0}) \right\vert\right\vert^{2}}}_{C_{\tau, 1}(\theta_{0},\widetilde{\theta}_{0})} + \underbrace{\sum_{i=0}^{K_{\tau} - 1}{\left( f_{\tau}\left(u^{(i+1)}_{\tau}(\widetilde{\theta}_{0})\right) - f_{\tau}\left(u^{(i)}_{\tau}(\theta_{0})\right) \right)^{2}}}_{C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0})},
	\]
	where $\widetilde{\theta}_{0}$ denotes a ``frozen'' or ``detached'' copy of $\theta_{0}$ and $u^{(i)}_{\tau}$ maps $\theta_{0}$ to $\theta_{i}$, the model parameters after applying $i$ gradient descent updates to $f_{\tau}$ according to Equation (1) in the manuscript. Then, differentiating $C_{\tau, 1}(\theta_{0},\widetilde{\theta}_{0})$ and $C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0})$ with respect to $\theta_{0}$ separately yields:
	\begin{align*}
	\nabla_{\theta_{0}} C_{\tau, 1}(\theta_{0},\widetilde{\theta}_{0}) &= -2 \sum_{i=0}^{K_{\tau} - 1}{J_{i}^{T}\left(\theta_{i+1} - \theta_{i} \right)} = -2 \alpha \sum_{i=0}^{K_{\tau} - 1}{J_{i}^{T} g_{i}} \\
	\nabla_{\theta_{0}} C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0}) &= -2 \sum_{i=0}^{K_{\tau} - 1}{\left(f_{\tau}(\theta_{i+1}) -  f_{\tau}(\theta_{i})\right) J_{i}^{T}g_{i}} = -2 \sum_{i=0}^{K_{\tau} - 1}{\Delta f^{i}_{\tau} J_{i}^{T}g_{i}}
	\end{align*}
	where $J_{i} = J_{\theta_{0}}u^{(i)}_{\tau}(\theta_{0})$ denotes the Jacobian of $u^{(i)}_{\tau}$ with respect to $\theta_{0}$, $g_{i} = \left. \nabla_{\theta} f_{\tau}(\theta)\right\rvert_{\theta=\theta_{i}}$ denotes the gradient of the loss function $f_{\tau}$ evaluated at $\theta_{i}$ and $\Delta f^{i}_{\tau} = f_{\tau}(\theta_{i+1}) -  f_{\tau}(\theta_{i})$ stands for the change in the loss function after the $i$-th update. To simplify the exposition, a constant ``inner-loop'' learning rate and no preconditioning were assumed, i.e., $\alpha_{i} = \alpha$ and $S_{i} = I$.
	
	Furthermore, the article claims that all Jacobian terms are approximated by identity matrices (i.e., $J_{i} = I$) as suggested in Section 5.2 of [1], leading to the following approximations:
	\begin{align*}
		\nabla_{\theta_{0}} C_{\tau, 1}(\theta_{0},\widetilde{\theta}_{0}) \approx -2 \alpha \sum_{i=0}^{K_{\tau} - 1}{ g_{i}} \\
		\nabla_{\theta_{0}} C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0}) \approx -2 \sum_{i=0}^{K_{\tau} - 1}{\Delta f^{i}_{\tau} g_{i}}
	\end{align*}
	
	Interestingly, it can be seen that the contribution to the meta-update of the energy of the path traversed by the model parameters $\theta$, $g_{\mathrm{Leap},1} =\nabla_{\theta_{0}} C_{\tau, 1}(\theta_{0},\widetilde{\theta}_{0})$, actually points in exactly the opposite direction than the meta-update of Reptile, given by $g_{\mathrm{Reptile}} = \sum_{i=0}^{K_{\tau} - 1}{g_{i}}$ (e.g. Equation (27) in [2]). In summary, if the Leap objective was defined in terms of $\theta$ rather than $(\theta, f_{\tau}(\theta))$, minimising the Leap cost function should maximise Reptile's cost function and viceversa. It is only the term $g_{\mathrm{Leap},2} =\nabla_{\theta_{0}} C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0})$ that presumably ``re-aligns'' $g_{\mathrm{Reptile}}$ and $g_{\mathrm{Leap}} = g_{\mathrm{Leap},1} + g_{\mathrm{Leap},2}$. Indeed, 
	\[
	g_{\mathrm{Leap}} = 2 \sum_{i=0}^{K_{\tau} - 1}{\left(-\Delta f^{i}_{\tau} - \alpha \right) g_{i}}
	\]
	will have positive inner product with $g_{\mathrm{Reptile}}$ if each gradient update yields a sufficient decrease in the loss $f_{\tau}$, that is, $\Delta f^{i}_{\tau} < -\alpha$.
	
	Moreover, I also wonder if this is the reason why the authors introduce the ``regularization'' term $\mu_{\tau}^{i}$, which as it currently stands in the manuscript, does not seem to relate in a particularly intuitive manner to the original objective of minimising the energy of $\gamma(t)$. By introducing $\mu_{\tau}^{i}$, the term $C_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0})$ becomes
	\[
		C^{\prime}_{\tau, 2}(\theta_{0},\widetilde{\theta}_{0}) = \sum_{i=0}^{K_{\tau} - 1}{-\mathrm{sign}  \left( f_{\tau}\left(u^{(i+1)}_{\tau}(\widetilde{\theta}_{0})\right) - f_{\tau}\left(u^{(i)}_{\tau}(\theta_{0})\right) \right) \left( f_{\tau}\left(u^{(i+1)}_{\tau}(\widetilde{\theta}_{0})\right) - f_{\tau}\left(u^{(i)}_{\tau}(\theta_{0})\right) \right)^{2}},
	\]
	leading to $g^{\prime}_{\mathrm{Leap},2} = 2 \sum_{i=0}^{K_{\tau} - 1}{\vert \Delta f^{i}_{\tau} \vert g_{i}}$ and 
	\[
	g^{\prime}_{\mathrm{Leap}} = 2 \sum_{i=0}^{K_{\tau} - 1}{\left(\vert \Delta f^{i}_{\tau} \vert - \alpha \right) g_{i}}.
	\]
	In turn, this relaxes the sufficient condition under which Leap and Reptile lead to meta-updates with positive inner product, namely, it changes the condition $\Delta f^{i}_{\tau} < -\alpha$ by a less restrictive counterpart $\vert \Delta f^{i}_{\tau} \vert \ge \alpha$.
	
	If these derivations happen to be correct, then I believe the way Leap is currently motivated in the article could be argued to be slightly misleading. What seems to be its main inspiration, accounting for the path that the model parameters traverse during fine-tuning, does not seem to be what drives the meta-updates towards the ``correct'' direction. Instead, the component of the objective due to the path traversed by the loss function values appears to be more important or, at least, not optional. Furthermore, I believe the regularization term $\mu_{\tau}^{i}$ should be better motivated, as the current version of the manuscript does not seem to justify its need clearly enough.
	
	Finally, under the assumption that the above is not mistaken, I wonder whether further tweaks to the meta-update, such as $g^{\prime\prime}_{\mathrm{Leap}} = 2 \sum_{i=0}^{K_{\tau} - 1}{\mathrm{max}\left(\vert \Delta f^{i}_{\tau} \vert - \alpha, 0 \right) g_{i}}$, could perhaps turn out to be helpful as well.

	\subsection*{2. Theoretical results}
	
	\textbf{2.a} Theorem 1 currently claims that the Pull-Forward algorithm converges to a local minimum of Equation (5). However, due to the non-convexity of the objective function, only convergence to a stationary point is established.
	
	\textbf{2.b} Most importantly, I am not entirely certain that the proof of Theorem 1 is complete in its current form. As I understand it, using the notation introduced by the authors in Appendix A, the following identities hold:
	\begin{align*}
		F(\psi_{s};\Psi_{s}) &= \mathbb{E}_{\tau,i} \vert\vert h_{\tau}^{i} - z_{\tau}^{i} \vert\vert^{2} \\
		F(\psi_{s+1};\Psi_{s}) &= \mathbb{E}_{\tau,i} \vert\vert h_{\tau}^{i} - x_{\tau}^{i} \vert\vert^{2} \\
		F(\psi_{s};\Psi_{s+1}) &= \mathbb{E}_{\tau,i} \vert\vert y_{\tau}^{i} - z_{\tau}^{i} \vert\vert^{2} \\
		F(\psi_{s+1};\Psi_{s+1}) &= \mathbb{E}_{\tau,i} \vert\vert y_{\tau}^{i} - x_{\tau}^{i} \vert\vert^{2}.
	\end{align*}
	
	The bulk of the proof is then devoted to show that $\mathbb{E}_{\tau,i} \vert\vert y_{\tau}^{i} - z_{\tau}^{i} \vert\vert^{2} = F(\psi_{s};\Psi_{s+1}) \ge \mathbb{E}_{\tau,i} \vert\vert y_{\tau}^{i} - x_{\tau}^{i} \vert\vert^{2} = F(\psi_{s+1};\Psi_{s+1})$. However, I do not immediately see how to make the final ``leap'' from $F(\psi_{s+1};\Psi_{s+1}) \le F(\psi_{s};\Psi_{s+1})$ to the actual claim of the Theorem, $F(\psi_{s+1};\Psi_{s+1}) \le F(\psi_{s};\Psi_{s})$.
	
	\subsection*{3. Experimental evaluation}
	
	\textbf{3.a} The experimental setup of Section 4.1 closely resembles experiments described in articles that introduced continual learning approaches, such as [4]. However, rather than including [4] as a baseline, the current manuscript compares against meta-learning approaches typically used for few-shot learning, such as MAML and Reptile. Consequently, I would argue the combination of experimental setup and selection of baselines is not entirely fair or, at least, it is incomplete.
	
	To this end, I would suggest to (i) include [4] (or a related continual learning approach) as an additional baseline in the experiments currently described in Section 4.1 as well as (ii) perform a new experiment to compare the performance of Leap to that of MAML and Reptile in few-shot classification tasks using OmniGlot and/or Mini-ImageNet as datasets.
	
	\textbf{3.b} The Multi-CV experiment described in Section 4.2 currently does not have strong baselines other than Leap. If possible, I would suggest including [5] in the comparison, as it is the article which inspired this particular experiment.
	
	\textbf{3.b} Likewise, the same holds for the experiment described in Section 4.3. In this case, I would suggest comparing to [4] for the same reason described above.
	
	\section*{MINOR POINTS}
	
	\begin{enumerate}
	
	\item In Section 2.1, it is claimed that ""gradients that largely point in the same direction indicate a convex loss surface, whereas gradients with frequently opposing directions indicate an ill-conditioned loss landscape"". Nevertheless, convex loss surfaces can in principle be ill-conditioned as well.
	
	\item Introducing a mathematical definition for the metric ""area under the training curve"" could make the experiment in Section 4.1 more self-contained.
	
	\item Several references are outdated, as they cite preprints that have since been accepted at peer-reviewed venues.
	
	\item The reinforcement learning experiments in Section 4.3 would benefit from additional runs with multiple seeds, and the subsequent inclusion of confidence intervals.
	
	\item I believe certain additional experiments could be insightful. For example, (i) studying how sensitive the performance of Leap is to parameter of the ``inner-loop'' optimizer (e.g. choice of 
	optimizer, learning rate, batch size) or (ii) describing how the introduction of $\mu_{\tau}^{i}$ affects the performance of Leap.
	
	\end{enumerate}
	
	\section*{TYPOS}
	
	\begin{enumerate}
	
	\item The first sentence entirely in page 6 appears to have a superfluous word.
	
	\item The Taylor series expansion in the proof of Theorem 1 is missing the $O(\bullet)$ terms (or a $\approx$ sign).
	
	\item Also in the proof of Theorem 1, if $c_{\tau}^{i} = (\delta_{\tau}^{i})^{2} - \alpha_{\tau}^{i}\xi_{\tau}^{i}\delta_{\tau}^{i}$, wouldn't $\omega = \underset{\tau, i}{\mathrm{sup}} \langle \hat{x}^{i}_{\tau} - \hat{z}^{i}_{\tau}, g(\hat{x}^{i}_{\tau}) - g(\hat{z}^{i}_{\tau})\rangle + \xi_{\tau}^{i}\delta_{\tau}^{i}$ instead?
	
	\end{enumerate}

        \section*{ANSWER TO REBUTTAL}
        Please see comments in the thread.

	
	\section*{REFERENCES}
	
	\begin{enumerate}[ {[}1{]} ]
		\item Finn et al. ``Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.'' International Conference on Machine Learning. 2017.
		\item Nichol et al. ``On First-Order Meta-Learning Algorithms.'' arXiv preprint. 2018
		\item Wu et al. ``Understanding Short-Horizon Bias in Stochastic Meta-Optimization.'' International Conference on Learning Representations. 2018.
		\item Schwarz et al. ``Progress \& Compress: A scalable framework for continual learning.''  International Conference on Machine Learning. 2018.
		\item Serr{\`a} et al. ``Overcoming Catastrophic Forgetting with Hard Attention to the Task.''  International Conference on Machine Learning. 2018.
	\end{enumerate}	
\end{document}",8
"In this paper, the authors study an important transfer learning problem, i.e., knowledge transfer between distinct tasks, which is usually called 'far transfer' (instead of 'near transfer'). Specifically, the authors propose a lightweight framework called Leap, which aims to achieve knowledge transfer 'across learning processes'. In particular, a method for meta-learning (see Algorithm 1) is developed, which focuses on minimizing 'the expected length of the path' (see the corresponding term in Eqs.(4-6)). Empirical studies on three public datasets show the effectiveness of the proposed method. Overall, the paper is well presented.

Some comments/suggestions:
(i) The details of the experiments such as parameter configurations are missing, which makes the results not easy to be reproduced.

(ii) For the baseline methods used in the experiments, the authors are suggested to include more state-of-the-art transfer learning methods in order to make the results more convincing.

(iii) Finally, if the authors can use some commonly used datasets in existing transfer learning works, the comparative results will be more interesting. 
",6
"This paper presents a model to combine neural network and logic programming. It proposes to use 3 primitive logic rules to model first-order predicate calculus in the neural networks. Specifically, relations with different numbers of arguments over all permutations of the groups of objects are represented as tensors with corresponding dimensions. In each layer, a MLP (shared among different permutations) is applied to transform the tensor. Multiple layers captures multiple steps of deduction. On several synthetic tasks, the proposed method is shown to outperform the memory network baseline and shows strong generalization.  

The paper is well written, but some of the contents are still a bit dense, especially for readers who are not familiar with first-order predicate calculus. 

The small Python example in the Appendix helps to clarify the details. It would be good to include the details of the architectures, for example, the number of layers, and the number of hidden sizes in each layer, in the experiment details in the appendix. 

The idea of using the 3 primitive logic rules and applying the same MLP to all the permutations are interesting. However, due to the permutation step, my concern is whether it can scale to real-world problems with a large number of entities and different types of relations, for example, a real-world knowledge graph.

Specifically:

1. Each step of the reasoning (one layer) is applied to all the permutations for each predicate over each group of objects, which might be prohibitive in real-world scenario. For example, although there are usually only binary relations in real-world KG, the number of entities is usually >10M. 

2. Although the inputs or preconditions could be sparse, thus efficient to store and process, the intermediate representations are dense due to the probabilistic view, which makes the (soft) deduction computationally expensive. 

Some clarification questions: 

Is there some references for the Remark on page 3? 

Why is there a permutation before MLP? I thought the [m, m-1, …, m-n+1] dimensions represent the permutations. For example, if there are two objects, {x1, x2}. Then the [0, 1, 0] represents the first predicate applied on x1, and x2. [1, 0, 0] represents the first predicate applied on x2 and x1. Some clarifications would definitely help here. 

I think this paper presents an interesting approach to model FOPC in neural networks. So I support the acceptance of the paper. However, I am concerned with its scalability beyond the toy datasets. 
",6
"In this paper the authors propose a neural-symbolic architecture, called Neural Logic Machines (NLMs), that can learn logic rules.

The paper is pretty clear and well-written and the proposed system is compelling. I have only some small concerns.
One issue concerns the learning time. In the experimental phase the authors do not state how long training is for different datasets.
Moreover it seems that the “rules” learnt by NSMs cannot be expressed in a logical formalism, isn’t it? If I am right, I think this is a major difference between dILP (Evans et. al) and NLMs and the authors should discuss about that. If I am wrong, I think the authors should describe how to extract rules from NLMs.
In conclusion I think that, once these little issues are fixed, the paper could be considered for acceptance.

[minor comments]
p. 4
“tenary” -> “ternary”
 p. 5
“ov varying size” -> “of varying size”
“The number of parameters in the block described above is…”. It is not clear to me how the number of parameters is computed.
“In Eq. equation 4” -> “In Eq. 4”

p. 16
“Each lesson contains the example with same number of objects in our experiments.”. This sentence sounds odd.
",7
"The paper introduces Neural Logic Machines, a particular way to combine neural networks and first order but finite logic. 

The paper is very well written and structured. However, there are also some downsides.

First of all, Section 2.1 is rather simple from a logical perspective and hence it is not clear what this gets a special term. Moreover, why do mix Boolean logic (propostional logic) and first order logic? Any how to you deal with the free variables, i.e., the variables that are not bounded by a quantifier? The semantics you define later actually assumes that all free variables (in your notation) are bounded by all quantifiers since you apply the same rule to all ground instances. Given that you argue that you want a neural extension of symbolic logic (""NLM is a neural realization of (symbolic) logic machines"") this has to be clarified as it would not be an extension otherwise. 

Furthermore, Section 2.2 argues that we can use a MLP with a sigmoid output to encode any joint distribution. This should be proven. It particular, given that the input to the network are the marginals of the ground atoms. So this is more like a conditional distribution? Moreover, it is not clear how this is different to other approaches that encode the weight of weighted logical rule (e.g. in a MLN) using neural networks, see
e.g. 

Marco Lippi, Paolo Frasconi:
Prediction of protein beta-residue contacts by Markov logic networks with grounding-specific weights. 
Bioinformatics 25(18): 2326-2333 (2009)

Now of course, and this is the nice part of the present paper, by stacking several of the rules, we could directly specify that we may need a certain number of latent predicates. 
This is nice but it is not argued that this is highly novel. Consider again the work by Lippi and Frasconi. We unroll a given NN-parameterized MLN for s fixed number of forward chaining steps. This gives us essentially a computational graph that could also be made differentiable and hence we could also have end2end training. The major difference seems to be that now objects are directly attached with vector encodings, which are not present in Lippi and Frasconi's approach. This is nice but also follows from Rocktaeschel and Riedel's differentiable Prolog work (when combined with Lippi and Frasconi's approach).
Moreover, there have been other combinations of tensors and logic, see e.g. 

Ivan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:
Logic Tensor Networks for Semantic Image Interpretation. 
IJCAI 2017: 1596-1602
 
Here you can also have vector encodings of constants. This also holds for 

Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt:
DeepProbLog: Neural Probabilistic Logic Programming. CoRR abs/1805.10872 (2018)

The authors should really discuss this missing related work. This should also involve
a clarification of the ""ILP systems do not scale"" statement. At least if one views statistical relational learning methods as an extension of ILP, this is not true. Probabilistic ILP aka statistical relational learning has been used to learn models on electronic health records, see e.g., the papers collectively discussed in 

Sriraam Natarajan, Kristian Kersting, Tushar Khot, Jude W. Shavlik:
Boosted Statistical Relational Learners - From Benchmarks to Data-Driven Medicine. Springer Briefs in Computer Science, Springer 2014, ISBN 978-3-319-13643-1, pp. 1-68

So the authors should either discuss SRL and its successes, separating SRL from ILP, or they cannot argue that ILP does not scale. In the related work section, they decided to view both as ILP, and, in turn, the statement that ILP does not scale is not true. Moreover, many of the learning tasks considered have been solved with ILP, too, of course in the ILP setting. Any ILP systems have been shown to scale beyond those toy domains.   
This also includes the blocks world. Here relational MDP solvers can deal e.g. with BW worlds composed of 10 blocks, resulting in MDPs with several million states. And the can compute relational policies that solve e.g. the goal on(a,b) for arbitrary number of blocks. This should be incorporated in the discussion of the introduction in order to avoid the wrong impression that existing methods just work for toy examples. 

Coming back to scaling, the current examples are on rather small datasets, too, namely <12 training instances. Moreover, given that we learn a continuous approximation with a limit depth of reasoning, it is also very likely that the models to not generate well to larger test instances. So the scaling issue has to be qualified to avoid to give the wrong impression that the present paper solves this issue. 

Finally, the BW experiments should indicate some more information on the goal configuration. This would help to understand whether an average number of moves of 84 is good or bad. Moreover, some hints about the MDP formulation should be provided, given that there have been relational MDPs that solve many of the probabilistic planning competition tasks. And, given that the conclusions argue that NLMs can learn the ""underlying logical rules"", the learned rules should actually be shown. 

Nevertheless, the direction is really interesting but there several downsides that have to be addressed. ",5
"This paper develops a method to accelerate the finite difference method in solving PDEs. Basically, the paper proposes a revised framework for fixed point iteration after discretization. The framework introduces a free linear operator --- the choice of the linear operator will influence the convergence rate. The paper uses a deep linear neural network to learn a good operator. Experimental results on Poisson equations show that the learned operator achieves significant speed-ups. The paper also gives theoretical analysis about the range of the valid linear operator (convex open set) and guarantees of the generalization for the learned operator. 

This is, in general, a good paper. The work is solid and results promising.  Solving PDEs is no doubt an important problem, having broad applications. It will be very meaningful if we can achieve the same accuracy using much less computational power.  Here, I have a few questions. 

1). Why didn’t you try the nonlinear deep network? Is it merely for computational efficiency? I expect that nonlinear networks might result in even better estimates of H and further reduce the number of fixed-point iterations, despite each operation of H will be more expensive. There might be some trade-off here. But I would like to see some empirical results and discussions.

2). The evaluation is only on Poisson equations, which are known to be easy. Have you tried other PDEs, such as Burger’s equations? I think your method will be more meaningful for those challenging PDEs, because they will require much more fine-grained grids to achieve a satisfactory accuracy and hence much more expensive. It will be great if your method can dramatically improve the efficiency for solving these equations. 

3). I am a bit confused about the statement of Th 3 --- the last sentence “H is valid for all parameters f and b if the iterator \psi converges …” I think it should be “for one parameter”. 

Miscellaneous:
1)	Typo. In eq. (7) 
2)	Section 3.3, H(w) should be Hw (for consistency)
",7
"==Summary==
This paper is well-executed and interesting. It does a good job of bridging the gap between distinct bodies of literature, and is very in touch with modern ML ideas. 

I like this paper and advocate that it is accepted. However, I expect that it would have higher impact if it appeared in the numerical PDE community. I encourage you to consider this conference paper to be an early version of a more comprehensive piece of work to be released to that community.

My main critique is that the paper needs to do a better job of discussing prior work on data-driven methods for improving PDE solvers.
==Major comments==
* You need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach. 

* You do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?
* I’m surprised that you didn’t impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator. 

==Minor comments==

* Valid iterators converge to a valid solution. However, can’t there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?

* In (9), why do you randomize the value of k? Wouldn’t you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver? 

* In future work it may make sense to learn a different H_i for each step i of the iterative solver. 

* When introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G. 
",8
"Summary:
The authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and
achieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   

Strengths:
Solving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. 

Weaknesses:
The method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. 

Questions:
- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? 
- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? 
- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?
- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?
- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?",6
"Authors propose a method of embedding training data examples into low-dimensional spaces such that mixture probabilities from a mixture model on these points are close to probability predictions from the original model in terms of KL divergence. Authors suggest two use-cases of such an approach: 1) data visualization, and 2) zero-shot learning. For the visualization use-case, authors compare against other dimensionality reduction methods with qualitative analysis on a synthetic problem, as well as evaluation metrics such as Neighborhood-Preservation Ratio and Clustering Distance Preservation Ratio. For zero-shot use-case, they take pre-trained models on two zero-shot tasks, and improve the accuracy by using probability outputs from pre-trained models as target.

Regarding the benefit of using the proposed method for visualization, the DRPR is making a strong assumption that representations of data points that belong to the same class form a uni-modal, Gaussian distribution (since authors don't experiment with distance functions other than L2). This inductive bias comes with a strong benefit when the assumption is true - as demonstrated in the toy dataset experiment - but when it is not true, the visualization would strongly distort the underlying structure of the model. And I don't believe this is a realistic assumption, because there has to be a reason that most deep-learning based classification models in the literature don't always use a model like (3) or Prototypical Networks instead of typical fully-connected + softmax layer, unless the data size is small and we need stronger inductive bias to improve the performance of the model.  That is, we usually don't think unimodality is the right assumption, even with learned representations. I suspect that the while DRPR might be good at visualizing relationships between class labels - especially which class can be easily confused with another - but would be worse at faithfully representing each data point, especially the ambiguity of class labels on individual ones. I would argue, however, that faithful representation of each data point is more important for scatter plots than relationship between classes, because the latter can be more effectively analyzed with other methods such as confusion matrices. As it is typical in most dimensionality reduction papers, I would encourage authors to consider more types of synthetic datasets which nonlinearity and multimodality are critical to be learned. I don't believe quantitative evaluation in Table 1 and 2 are very meaningful, because DRPR's objective function is much better aligned with these metrics than others. 

Zero-shot experiments show a promising lift over the baseline pre-trained models. The kind of bias we should be careful about, however, is that when we distillate one model into another, the performance generally improves even when the same exact model is both the teacher and the student: (Furlanello et al, ICML 2018 https://arxiv.org/abs/1805.04770 ). Therefore, it would be interesting to compare against distillation with baseline models themselves.

Pros:
* Extensive theoretical and empirical analysis
* Simple idea that generalizes to multiple use-cases, which implies robustness of the approach as a methodology

Cons:
* Unimodal assumption is likely not realistic, which would result in misleading visualization of data
* Visualization analysis focuses on how class-relationships are preserved rather than faithful representation of each data point, which is a wrong target
* Synthetic experiment is conducted on a single, too simplistic one; more examples are needed to understand the capabilities of the model in more detail
* The bias of knowledge distillation is not controlled",7
"The paper deals with a problem formulation adjacent to that of the sufficient dimension reduction: given training set of pairs (x_i,y_i), how to reduce the dimension of the first element, i.e. map x_i --> f(x_i), so that f(x_i)'s still have all the information to recover y_i's.

In the paper, the output y_i is a probability distribution over k labels that softly describes inclusion of example i into k classes.

They consider a nonlinear case, i.e. the mapping f is taken from a prespecified set of mappings, parameterized by Theta (e.g. neural network). Then by ""recovering y_i"" they mean that EM algorithm for {f(x_i)} will result in a clustering of the data into k soft clusters similar to given {y_i}.

The algorithm that is presented is quite natural, though no guarantees that it will converge to something relevant were given. Theoretical analysis deals with a question --- how far the empirical discrepancy could be from the true expected one. Especially, easiness of substitution of \bar{Y}_{ic} with Y_{ic} in the algorithm is unclear (roughly speaking, the latter means that E-step is omitted in EM). If matrix Y in algorithm is fixed, why we need to compute \pi in the loop? Isn't it going to be the same? Does this algorithm really minimizes the discrepancy?",6
"Summary:

This paper introduces a new supervised dimensionality reduction model. Supervision is provided in the form of class probabilities and the learning algorithm learns low-dimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities. The representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lower-dimensional space. The authors provide an extensive theoretical analysis of the proposed method and evaluate it on dimensionality reduction, visualization, and zero-shot learning tasks.

Review:

Overall, I thought this was an excellent paper. The idea is well-motivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (I will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials). I was honestly impressed by the shear volume of content in this paper, particularly since I found none of it to be superfluous. Frankly, this paper might be better served as two papers or a longer journal paper, but that is hardly a reason not to accept it. I strongly recommend acceptance and have only a couple of comments on presentation.

Comments:

- When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing. 

- The authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup.",9
"This is an interesting paper on a statistical analysis of batch normalization. It takes a holistic approach, 
combining techniques and ideas from various fields, and considers multiple endpoints, such as tuning of learning rates and estimation of generalization error. Overall it is an interesting paper.

Some aspects of the paper that could be improved:

1) Theorem 1 is not particularly compelling, and may be misleading at a first reading. It considers the simple model of Equation (1) in a straightforward bias-variance decomposition, and may not be useful in general. Some aspects of the theorem are not technically correct or unclear. E.g., \gamma is a single parameter, what does it mean to have a Fisher information matrix?

2) The problem is not motivated well. It may be a good idea to bring some discussions from Section 6 early in the introduction of the paper. When does BN work well? And what is the current understanding (prior to the paper) and how does the paper compare/contribute? I think the paper does a good job on that front, but it follows a disordered narration flow which makes it hard to read. I understand there is a lot of material to cover, but it would help a lot to reorganize the paper in a more linear way.

3) What about alternatives, such as implicit back propagation that stabilizes learning?  [1]

4) I don't find Figure 1 (and 3) particularly useful on how it handles vanilla SGD. In practice, it would be straightforward to avoid the mentioned pathologies. Overall, the experiments are interesting but it may be hard to generalize the findings to non-linear settings.


[1] Implicit back propagation, Fagan & Iyengar, 2017

",5
"This is a thought provoking paper that aims to understand the regularization effects of batch-normalization (BN) under a probabilistic interpretation. The authors connect BN to population normalization (PN) and a gamma-decay term that penalizes the scale of the weights. They analyze the generalization error of BN for a single-layer perceptron using ideas in statistical physics.

Detailed comments:

1. Theorem 1 uses the loss function of a single-layer perceptron in the proof. This is not mentioned in the main writeup. This theorem is not valid in general.

2. The main contribution of this paper is Theorem 1 which connects BN to population normalization and weight normalization. It shows that the regularization of BN can be split into two components that depend on the mini-batch mean and variances: the former penalizes the magnitude of activations while the latter penalizes their correlation.

3. Although the theoretical analysis is conducted under simplistic models, this paper corroborates a number of widely-known observations about BN in practice. It validates these predictions on standard experiments.

4. The scaling of BN regularization with batch-size can be easily seen from Teye et al., 2018, so I think the experiments that validate this prediction are not strictly necessary.

5. It is difficult to use these techniques for deep non-linear networks.

6. The predictions in Section 3.3 are very interesting: it is often seen that fully-connected layers (where BN helps significantly) need small learning rates to train without BN; with BN one can use larger learning rates.

7. The experimental section is very rough. In particular the experiments on CIFAR-10 and downsampled-ImageNet with CNNs seem to have very high errors and it is difficult to understand whether some of the predictions about generalization error apply here. Why not use a more recent architecture for CIFAR-10?

8. There is a very large number of grammatical and linguistic errors in the narrative.

9. The presentation of the paper is very dense, I would advise the authors to move certain parts to the appendix and remove the inlining of important equations to improve readability.",6
"This paper investigates batch normalization from three points of view. i) Loss decomposition, ii) learning rate selection, iii) generalization. If carefully read, I believe authors have interesting results and insightful messages. However, as a whole, I found the paper difficult to follow. Too much content is packed into too little space and they are not necessarily coherent with each other. Many of the technical terms are not motivated and even not defined. Overall, cleaning up the exposition would help a lot for readability. 

I have a few other technical comments.
1) Theorem 1 is not acceptable for publication. It is not a rigorous statement. This should be fixed.
2) Effective and maximum learning rate is not clear from the main body of the paper. I can intuitively guess what they are but they lack motivation and definition (as far as I see).
3) In Section 3 I believe random data is being assumed (there is expectation over x in some notation). This should be stated upfront. Authors should broadly comment on the applicability of the learning rates calculated as N->\infty in the finite N,P regime?",6
"[Paper Summary]
This paper tackles the problem of learning dynamics of non-rigid objects in a physics simulator. This learned dynamics can then be used for planning later. The non-rigid objects are represented via a particle-based system. The dynamics model is learned using NVIDIA's particle-based simulator ""Flex"". The main idea is to adapt Interaction Networks [Battaglia, 2016] which was earlier proposed for rigid-body simulators to particle-based simulators. Instead of maintaining interactions at the level of objects as in [Battaglia, 2016], the proposed approach models interaction at the level of particles.

[Paper Strengths]
The paper is clearly written and tackles an important research problem. The existing literature is presented well.

[Paper Weaknesses]
=> The introduction and the text in the first two pages seem to be introducing a new way to model ""dynamic"" interactions between particles for handling non-rigid transformations. However, upon reading the method section, the approach seems to be a direct application of the Interaction Graph Networks (originally applied to the rigid-body simulator) to the particle-based simulator. The only difference is that instead of maintaining a fully-connected graph (memory and computational bottleneck), each particle is only connected to the near-by particles within distance d.

=> One of the major issue with the paper is the experimental section of the paper. Since the proposed method is quite incremental over the prior work, a strong empirical section is must to justify the approach. Here are the comments:
   - Since the proposed approach is an adaptation of [Battaglia, 2016], it should be compared to other existing methods. The experiment section in its current state does not compare to any baseline. The well-written related work (section-2) talks about (Mrowca et.al. 2018) and (Schenck and Fox, 2018) as the works which investigate learning dynamics of deformable objects using a particle-based simulator. However, no comparison is provided to either of the methods. Hence, it is not possible to judge the quality of the presented results.

   - All results in Figure-5 or Figure-3 are quite close to each other. It is not clear whether the improvement is significant or not since the error bars are not provided at all.

   - No ablation is performed to test the sensitivity of the proposed method with respect to the hyper-parameters introduced; for instance, the distance 'd'.

=> The name ""Dynamic Particle Interaction"" is overloaded with terms, especially, the use of word 'dynamic' here just refers to the interaction of particles to model deformable objects. This ""dynamic"" interaction is not ""learned"" but simply hard-coded by deleting the edges which are farther than d distance apart and adding near ones. Something like ""Particle-level Interaction Networks"" would be a more honest description of the approach.

[Final Recommendation]
I request the authors to address the comments raised above and will decide my final rating based on that. With the current set of experiments, the paper doesn't seem to be ready yet.",6
"The authors present an algorithm for learning the dynamics prediction of deformable and fluid bodies by modeling them as (potentially hierarchical) systems of many interacting particles.  This model applies a shared encoder to the particle states (positions and velocities), a shared relation network to nearby pairs, and a shared propagator network to the summed relation network outputs.  In some cases this process is applied in a multi-scale hierarchical fashion.  The authors demonstrate accurate rollouts of system dynamics and usefulness for manipulative control of deformable objects.

I find the motivation in the introduction persuasive and the algorithmic approach sound.  I also like the application to RL.  However, I do have some concerns, as follows:

1)  The novelty of the method is questionable.  Specifically, the hierarchical interaction network proposed here seems extremely similar to the prior (and cited) paper (Mrowca et al., 2018), which the authors do not directly compare against.  If there is a non-negligible difference between the two algorithms, then the authors should explicitly discuss the difference and empirically compare the two, in order to benefit others in the community who otherwise would not know which to use.

2)  The paper would benefit a lot from a diagram of the model.  Specifically, it would be good to have a diagram of the hierarchical interaction network demonstrating the multiscale propagation.  This could go in Figure 1, perhaps replacing elements (b) and (d) of the current Figure 1, which in my opinion are unnecessary and can be removed.

3)  The paper uses domain-specific hyperparameters, yet does not discuss or analyze the effects of these hyperparameters much.  Specifically, for this method to be useful to others, we would like to know how to choose (i) the propagation step L, (ii) the number of roots, and (iii) the neighborhood distance d.  In the paper, these numbers are chosen differently for the different environments without explanation.  Graphs showing performance on each task over a range of values of these parameters would be good (perhaps in the supplementary material).  Also, using the same hyperparameters for all environments (or at least a common generating function) would help support the generality of this model.

4)  The treatment of rigid bodies seems a bit hand-held.  Specifically, to determine the dynamics of rigid bodies, there is a ground-truth calculation which calculates computes the velocity and angular velocity of the body from the model predictions for its constituent particles.  Furthermore, if I understand correctly, there is a different motion predictor network for those particles in a rigid body than those in the surrounding fluid --- is this correct?  If so, this raises the questions:  (i) What happens if the same motion-predictor network is used for all particles, and (ii) What happens if the ground-truth rigid dynamics calculation is not done, so the model has to do all the work?  It would be interesting to have these as baselines.

5)  It would be nice to see more generalization results.  There is only one generalization experiment, testing for generalization over particle number in FluidShake.  However, the FluidShake model is not hierarchical.  The hierarchical models are a big emphasis in the paper, so showing generalization on BoxBath or RiceGrip would be much more meaningful.

6)  No confidence intervals for the quantitative results.  Confidence intervals would be good to see in the table in Figure 3-a.  Also, the bar graph in Figure 5 really would benefit from errorbars --- it is difficult to determine if the results are significant.

7)  While the text is generally clear and definitely understandable, I have a couple of comments about it:  (i) The last three paragraphs of the introduction are repetitive and I think they can be removed, or at least shortened a lot.  There are also quite a number of grammatical errors throughout the paper, though it is still comprehensible.

EDIT:  In their revision the authors addressed these concerns well and the paper is much more convincing (see longer comment below).  In light of this I have changed my rating from a 5 to an 8.
",8
"This work demonstrates that a particle dynamics model can be learned to approximate the interaction of various objects. The resulting differentiable simulator has a strong inductive bias, which makes it possible to efficiently solve complex manipulation tasks over deformable objects.

# Quality

This work is an impressive proof-of-concept of the capabilities of differentiable programming for learning complex (physical) processes, such as particle dynamics. In my opinion, the resulting particle interaction network would deserve publication for itself. However, this work goes already one step further and demonstrates that the resulting differentiable simulator can be used for the manipulation of deformable objects.

The method is evaluated on a well-rounded set of experiments which demonstrates its potential. More real-world experiments would be welcome to leave any doubt.

EDIT: However, the current manuscript lacks a proper comparison with (cited) previous work, such as 1806.08047. 

# Clarity

The paper is well written, although I do feel it was difficult to remain within the 8-page limit given the breadth of the work.

# Originality

As far as I know, this work is (very) original. (That being said, I am not too familiar with the related work.)

EDIT: This work is actually quite similar to 1806.08047. A proper discussion of the differences should be included. 

# Significance

This work will certainly be of interest for several research communities, including deep learning, physics, control and robotics.
",7
"The paper proposes a two-timescale framework for learning the value function and a state representation altogether with nonlinear approximators. The authors provide proof of convergence and a good empirical evaluation.

The topic is very interesting and relevant to ICLR. However, I think that the paper is not ready for a publication.
First, although the paper is well written, the writing can be improved. For instance, I found already the abstract a bit confusing. There, the authors state that they ""provide a two-timescale network (TTN) architecture that enables LINEAR methods to be used to learn values [...] The approach facilitates use of algorithms developed for the LINEAR setting [...] We prove convergence for TTNs, with particular care given to ensure convergence of the fast LINEAR component.""
Yet, the title says NONLINEAR and in the remainder of the paper they use neural networks. 

The major problem of the paper is, however, its organization. The novelty of the paper (the proof of convergence) is relegated to the appendix, and too much is spent in the introduction, when actually the idea of having the V-function depending on a slowly changing network is also not novel in RL. For instance, the authors say that V depends on \theta and w, and that \theta changes at slower pace compared to w. This recalls the use of target networks in the TD error for many actor-critic algorithms. (It is not the same thing, but there is a strong connection).
Furthermore, in the introduction, the authors say that eligibility traces have been used only with linear function approximators, but GAE by Schulman et al. uses the same principle (their advantage is actually the TD(\lambda) error) to learn an advantage function estimator, and it became SOTA for learning the value function.

I am also a bit skeptical about the use of MSBE in the experiment. First, in Eq 4 and 5 the authors state that using the MSTDE is easier than MSBE, then in the experiments they evaluate both. However, the MSBE error involves the square of an expectation, which should be biased. How do you compute it? 
(Furthermore, you should spend a couple of sentences to explain the problem of this square and the double-sampling problem of Bellman residual algorithms. For someone unfamiliar with the problem, this issue could be unclear.)

I appreciate the extensive evaluation, but its organization can also be improved, considering that some important information are, again, in the appendix.
Furthermore, results on control experiment are not significative and should be removed (at the current stage, at least). In the non-image version there is a lot of variance in your runs (one blue curve is really bad), while for the image version all runs are very unstable, going always up and down. 

In conclusion, there is a lot of interesting material in this paper. Even though the novelty is not great, the proofs, analysis and evaluation make it a solid paper. However, because there is so much do discuss, I would suggest to reorganize the paper and submit directly to a journal track (the paper is already 29 pages including the appendix).",6
"This paper proposes Two-Timescale Networks (TTNs), a reinforcement learning algorithm where feature representations are learned by a neural network trained on a surrogate loss function (i.e. value), and a value function is learned on top of the feature representation using a ""fast"" least-squares algorithm. The authors prove the convergence of this method using methods from two time-scale stochastic approximation. 

Convergent and stable nonlinear algorithms is an important problem in reinforcement learning, and this paper offers an interesting approach for addressing this issue. The idea of using a ""fast"" linear learner on top of a slowly changing representation is not new in RL (Levine et. al, 2017), but the authors somewhat motivate this approach by showing that it results in a stable and convergent algorithm. Thus, I view the convergence proof as the main contribution of the paper.

The paper is written clearly, but could benefit from more efficient use of space in the main paper. For example, I feel that the introduction and discussion in Section 3 on surrogate objectives could be considerably shortened, and a formal proof statement could be included from the appendix in Section 4, with the full proof in the appendix.

The experimental evaluation is detailed, and ablation tests show the value of different choices of surrogate loss for value function training, linear value function learning methods, and comparisons against other nonlinear algorithms such as DQN and Nonlinear GTD/TD/variants. A minor criticism is that it is difficult to position this work against the ""simpler but not sound"" deep RL methods, as the authors only compare to DQN on a non-standard benchmark task.

As additional related work, SBEED (Dai et. al, ICML 2018) also shows convergence for a nonlinear reinforcement learning algorithm (in the control setting), and quantifies the convergence rate while accounting for finite sample error. It would be good to include discussion of this work, although the proposed method and proofs are derived very differently.",7
"Summary:
This paper presents a Two-Timescale Network (TTN) that enables linear methods to be used to learn values. On the slow timescale non-linear features are learned using a surrogate loss. On the fast timescale, a value function is estimated as a linear function of those features. It appears to be a single network, where one head drives the representation and the second head learns the values.  They investigate multiple surrogate losses and end up using the MSTDE for its simplicity, even though it provides worse value estimates than MSPBE as detailed in their experiments.  They provide convergence results - regular two-timescale stochastic approximation results from Borkar, for the two-timescale procedure and provide empirical evidence for the benefits of this method compared to other non-linear value function approximation methods.

Clarity and Quality:
The paper is well written in general, the mathematics seems to be sound and the experimental results appear to be thorough. 

Originality:
Using two different heads, one to drive the representation and the second to learn the values appears to be an architectural detail. The surrogate loss to learn the features coupled with a linear policy evaluation algorithm appear to be novel, but does not warrant, in my opinion, the novelty necessary for publication at ICLR. 

The theoretical results appear to be a straightforward application of Borkar’s two-timescale stochastic approximation algorithm to this architecture to get convergence. This therefore, does not appear to be a novel contribution.

You state after equaltion (3) that non-linear function classes do not have a closed form solution. However, it seems that the paper Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation does indeed have a closed form solution for non-linear function approximators when minimizing the MSPBE (albeit making a linearity assumption, which is something your work seems to make as well). 

The work done in the control setting appears to be very similar to the experiments performed in the paper: Shallow Updates for Deep Reinforcement Learning.

Significance:
Overall, I think that the paper is well written and the experimental evaluation is thorough. However, the novelty is lacking as it appears to be training using a multi-headed approach (which exists) and the convergence results appear to be a straightforward application of Borkars two-timescale proof. The novelty therefore appears to be using a surrogate loss function for training the features which does not possess the sufficient novelty in my opinion for ICLR. 

I would suggest the authors' detail why their two-timescale approach is different from that of Borkars. Or additionally add some performance guarantee to the convergence results to extend the theory. This would make for a much stronger paper.",6
"The paper introduces an algorithm (TTN) for non-linear online and on-policy value function approximation. The main novelty of the paper is to view non-linear value estimation as two separate components. One of representation learning from a non-linear mapping and one of linear value function estimation. The soundness of the approach stems from the rate at which each component is updated. The authors argue that if the non-linear component is updated at a slower rate than the linear component, the former can be viewed as fixed in the limit and what remains is a linear value function estimation problem for which several sound algorithms exist. TTN is evaluated on 4 domains and compared to several other value estimation methods as well as DQN on a control problem with two variations on the task's state space.

I'll start off the review by stating that I find the idea and theoretical justification of separating the non-linear and linear parts of value function estimation to be quite interesting, potentially impacting RL at large. Indeed, this view promises to reconcile latest developments in deep RL with the long-lasting work on RL with linear function approximators. However, there are a few unclear aspects that do not allow one to be fully convinced that this paper lives up to the aforementioned promise.

- For the theoretical contribution. The authors claim that the main challenge was to deal with the potentially dependent features outputted by the neural network. It is dealt with by using a projection that projects the linear parameters of the value function to a compact subset of the parameter space. Bar the appendix, there is no mention of this projection in the paper, on how this compact subset (that must include the optimal parameter) is defined and if this projection is merely a theoretical tool or if it was necessary to implement it in practice. There is a projection for the neural net weights too but I can see how for these it might not be necessary to use in practice. However, for the linear weights, as their computation potentially involves inverting ill-conditioned matrices, they can indeed blow-up relatively fast.

- I found the experimental validation to be quite rich but not done in a systematic enough manner. For instance, the experiment ""utility of optimizing the MSPBE"" demonstrates quite nicely the importance of each component but is only performed on a single task. As the theoretical analysis does not say anything about the improvements the representation learning can have on the linear value estimation nor if the loss used for learning the representation effectively yields better features for the MSPBE minimization, this experiment is rather important and should have been performed on more than a single domain.

Secondly, I do not find the chosen baselines to be sufficiently competitive. The authors state in Sec. 2 that nonlinear-GTD has not seen widespread use, but having this algorithm as the main competitor does not provide strong evidence that TTN will know a better fate. In the abstract, it is implied that outside of nonlinear-GTD, value function approximation methods are not sound. In approximate policy iteration algorithms such as DDPG or TRPO, there is a need in performing value estimation. It is done by essentially a fitted-Q iteration procedure which is sound. Why wasn't TTN compared to these methods? If it is because they are not online, why being online in the experiments of the paper is important? Showing that TTN is competitive with currently widespread methods for value estimated would have been more convincing than the comparison with nonlinear-GTD.

Thirdly, for the sake of reproducibility, as LSTD seems to be the method of choice for learning the linear part, it would have been adequate to provide an algorithm box for this version as is done for GTD2/TDC. LSTD is essentially a batch algorithm and there could be many ways to turn it into an online algorithm. With which algorithm were the results in the experimental section obtained?

Finally, on the control task, the authors add several modifications to their algorithm which results in an algorithm that is very close to that of Levine et al., 2017. Why was not the latter a baseline for this experiment? Especially since it was included in other experiments.",6
"This paper proposes the temporal difference variational auto-encoder framework, a sequential general model following the intuition of temporal difference learning in reinforcement learning. The idea is nice and novel, and I vote for acceptance.
1. The introduction of belief state in the sequential model is smart. How incorporate such technique in such an autoregressive model is not easy.
2. Fig 1 clearly explained the VAE process.
3. Four experiments demonstrated the main advantages of the proposed framework, including the effectiveness of proposed belief state construction and ability to jumpy rolling-out, 


Other Comments and Questions:
1. Typo, p(s_{t_2}|s_{t_1}) in the caption of Fig 1.
2. Can this framework partially solve the exposure bias?
3. The author used uniform distribution for t_2 - t1, and from the ``NOISY HARMONIC OSCILLATOR`` we can indeed see larger interval will result in worse performance. However, the author also mentioned other distortion could be investigated, so I am wondering if the larger probability mass is put on larger dt, what the performance will become.
4. The code should be released. I think that it is a fundamental framework deserving further development  by other researchers.",8
"The authors propose TD-VAE to solve an important problem in agent learning, simulating the future by doing jumpy-rollouts in abstract states with uncertainty. The authors first formulate the sequential TD-VAE and then generalize it for jumpy rollouts. The proposed method is well evaluated for four tasks including high dimensional complex task.

Pros.
- Advancing a significant problem
- Principled and quite original modeling based on variational inference
- Rigorous experiments including complex high dimensional experiments
- Clear and intuitive explanation (but can be improved further)

Cons. 
- Some details on the experiments are missing (due to page limit). It would be great to include these in the Appendix. 
- It is a complex model. For reproducibility, detail specification on the hyperparameters and architecture will be helpful.

Minor comments
- Why q(z_{t-1}|z_t, b_{t-1}, b_t) depends both  b_{t-1}, b_t, not only b_t?
- The original model does not take the jump interval as input. Then, it is not clear how the jump interval is determined in p(z’|z)?
",9
"There are several ingredients in this paper that I really liked. For example, (1) the notion that an agent should build a deterministic function of the past which implicitly captures the belief (the uncertainty or probability distribution about the state), by opposition for example to sampling trajectories to capture uncertainty, (2) modelling the world's dynamic in a learned encoded state-space (by opposition to the sensor space), (3) instead of modeling next-step probabilities p(z(t+1)|z(t)), model 'jumpy transitions' p(z(t+delta)|z(t)) to avoid unrolling at the finest time scale.

Now for the weak points:
(a) the justification for the training loss was not completely clear to me, although I can see that it has a variational flavor
(b) there is no discussion of the issue that we can't get a straightforward decomposition of the joint probability over the data sequence according to next-step probabilities via the chain rule of probabilities, so we don't have a clear way to compare the TD-VAE models with jumpy predictions against other more traditional models
(c) none of the experiments make comparisons against previously published models and quantitative results (admittedly because of (b) this may not be easy).

So I believe that the authors are onto a great direction of investigation, but the execution of the paper could be improved.",7
"In this paper the authors propose optimizing for adversarial examples against black box models by considering minimizing the distance to the decision boundary.  They show that because this gives real valued feedback, the optimizer is able to find closer adversarial examples with fewer queries.  This would be heavily dependent on the model structure (with more complex decision boundaries likely being harder to optimize) but they show empirically in 4 models that this method works well.

I am not convinced that the black box model setting is the most relevant (and 20k queries is still a fair bit), but this is important research nonetheless.  I generally found the writing to be clear and the idea to be elegant; I think readers will value this paper. 
",7
"This paper proposed a reformulation of objective function to solve the hard-label black-box attack problem. The idea is interesting and the performance of the proposed method seem to be capable of finding adversarial examples with smaller distortions and less queries compared with other hard-label attack algorithms. 

This paper is well-written and clear.

==============================================================================================
Questions

A. Can it be proved the g(theta) is continuous? Also, the theoretical analysis assume the property of Lipschitz-smooth and thus obtain the complexity of number of queries. Does this assumption truly hold for g(theta), when f is a neural network classifiers? If so, how to obtain the Lipschitz constant of g that is used in the analysis sec 6.3? 

B. What is the random distortion in Table 1? What initialization technique is used for the query direction in the experiments? 

C. The GBDT result on MNIST dataset is interesting. The authors should provide tree models description in 4.1.3. However, on larger dataset, say imagenet, are the tree models performance truly comparable to ImageNet? If the test accuracy is low, then it seems less meaningful to compare the adversarial distortion with that of imagenet neural network classifiers. Please explain. 

D. For sec 3.2, it is not clear why the approximation is needed. Because the gradient of g with respect to theta is using equation (7) and theta is already given (from sampling); thus the Linf norm of theta is a constant. Why do we need the approximation? Given that, will there be any problems on the L2 norm case? 
",6
"This paper addresses black-box classifier attacks in the “hard-label” setting, meaning that the only information the attacker has access to is single top-1 label predictions. Relative to even the standard black-box setting where the attacker has access to the per-class logits or probabilities, this setting is difficult as it makes the optimization landscape non-smooth. The proposed approach reformulates the optimization problem such that the outer-loop optimizes the direction using approximate gradients, and the inner-loop estimates the distance to the nearest attack in a given direction. The results show that the proposed approach successfully finds both untargeted and targeted adversarial examples for classifiers of various image datasets (including ImageNet), usually with substantially better query-efficiency and better final results (lower distance and/or higher success rate) than competing methods.

=====================================

Pros:

Very well-written and readable paper with good background and context for those (like me) who don’t closely follow the literature on adversarial attacks. Figs. 1-3 are nice visual aids for understanding the problem and optimization landscape.

Novel formulation and approach that appears to be well-motivated from the literature on randomized gradient-free search methods. Novel theoretical analysis in Appendix that generalizes prior work to approximations (although, see notes below).

Good empirical results showing that the method is capable of query-efficiently finding attacks of classifiers on real-world datasets including ImageNet. Also shows that the model needn’t be differentiable to be subject to such attacks by demonstrating the approach on a decision-tree based classifier. Appears to compare to and usually outperform appropriate baselines from prior work (though I’m not very familiar with the literature here).

=====================================

Cons/questions/suggestions/nitpicks:

Eq 4-5: why \texttt argmin? Inconsistent with other min/maxes.

Eq 4-5: Though I understood the intention, I think the equations are incorrect as written: argmin_{\lambda} { F(\lambda) } of a binary-valued function F would produce the set of all \lambdas that make F=0, rather than the smallest \lambda that makes F=1. I think it should be something like:

min_{\lambda>0} {\lambda}
s.t. f(x_0+\lambda \theta/||\theta||) != y_0

Sec 3.1: why call the first search “fine-grained”? Isn’t the binary search more fine-grained? I’d suggest changing it to “coarse-grained” unless I’m misunderstanding something.

Algorithm 2: it would be nice if this included all the tricks described as “implementation details” in the paragraph right before Sec. 4 -- e.g. averaging over multiple sampled directions u_t and line search to choose the step size \eta. These seem important and not necessarily obvious to me.

Algorithm 2: it could be interesting to show how performance varies with number of sampled directions per step u_t.

Sec: 4.1.2: why might your algorithm perform worse than boundary-attack on targeted attacks for CIFAR classifiers? Would like to have seen at least a hypothesis on this.

Sec 6.3 Theorem 1: I think the theorem statement is a bit imprecise. There is an abuse of big-O notation here -- O(f(n)) is a set, not a quantity, so statements such as \epsilon ~ O(...) and \beta <= O(...) and “at most O(...)” are not well-defined (though common in informal settings) and the latter two are redundant given the meaning of O as an upper bound. The original theorem from [Nesterov & Spokoiny 2017] that this Theorem 1 would generalize doesn’t rely on big-O notation -- I think following the same conventions here might improve the theorem and proof.

=====================================

Overall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and well-motivated approach with strong empirical results.",7
"This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness. It is accomplished by utilizing a VAE structure on the observation and latent variable separately. The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE.  I have several concerns about the paper:

1.	It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N(u|0,I). Even if the latent variable closely resemble N(u|0,I), How does it make sure the generated images are realistic? I admit that the VAE model can reconstruct realistic data based on its inferred latent variable, however, when given a random sample from N(u|0,I), the generated images are not good, which is true when the dimension of the latent space is high. I still can’t understand why a second-stage VAE can relief this problem.
2.	The adversarial auto-encoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper?
3.	Why do you set the model as two separate stages? Will it enhance the performance if we train theses two-stages all together?
4.	The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r, while in reality it is difficult to obtain this r, do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation? How will it affect the performance of the proposed method?
5.	The value of r and k in each experiment should be specified.

",6
"Overview:
I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the ""pass"" category.

Pros: 
- Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric.

Cons:
- The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior).
- None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years.
- A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount.
- While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility.

Recommendations / Typos

I noted a few typos and omissions that need correction.

- Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission.
- The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity.
- Equation (4): the true posterior has an x as its argument instead of the latent z.
- Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r > d relevant here?

* EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. *",7
"The paper provides a number of novel interesting theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called ""2 stage VAEs"" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. 

Main theoretical contributions:

1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1).
In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered.

2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5).
In case when r < d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r ""informative"" dimensions to produce the outputs perfectly landing on the true data manifold. 

Main algorithmic contributions:
(0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. 

Review summary: 
I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. 

***************
*** Couple of comments and typos:
***************
(0) Is the code / checkpoints going to be available anytime soon?
(1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning.
(2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. 
(3) It would be nice to specify the dimensionality of the Sz matrix in definition 1.
(4) Line ater Eq. 3: I think it should be $\int p_gt(x) \log p_\theta(x) dx$ ?
(5) Eq 4: p_\theta(x|x)
(6) Page 4: ""... mass to most all measurable..."".
(7) Eq 34. Is it sqrt(\gamma_t) or just \gamma_t?
(8) Line after Eq 40. Why exactly D(u^*) is finite?

I only checked proofs of Theorems 1 and 2 in details and those looked correct. 

[1] Lucic et al., 2018.
[2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html
[3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642",9
"Deep Learning 3D Shapes using Alt-Az Anisotropic 2-Sphere Convolution

This paper presents a polar anisotropic convolution scheme on a unit sphere. The known non-shift-invariance problems of current manifold neural nets are avoided by replacing filter translation with filter rotation on a sphere. Spherical convolution are thus enabled and are rotation invariant compared to manifold convolutions. This shift also enables a proposed angular max-pooling scheme. Results are presented on mesh projections, shape classification and shape retrieval. 

The paper generally reads well. Tackling the learning problem on a unit sphere has high potential, however, the proposed paper seems to be highly constrained by heuristics on a 2-sphere, such as constraining filters on a reduced rotation group to 2 rotations. This could be fine for many 3D application, but results may lack an exhaustive comparison with other spherical and manifold-based methods on the proposed experiments. Currently, several variants of data augmentations are used, and discussion may lack an explicit comparison with the state-of-the-art of spherical and spectral methods. This may impair understanding in which context the proposed method would work best.


Other comments, possible clarification and improvements:

[Method]
- Can this be extended to unit 2-balls?
- Isn't the ""alt-az rotation group"" the same as SO(3)?  If orientation is removed, what quotient group would this be?
- What is the benefit of containing a filter on this quotient group rather than using convolution filters within the full rotation group?  Could a simple experiment convince the reader that the proposed approach is better than using convolutions in SO(3)?
- Is there a dependence created by the spherical parameterization strategy?
- How robust is the convolution scheme to topological defects, such as holes, noise?
- Spherical images may induce parameterization distorsion if using a lat-lon grid, which would require complex variable filters on the spherical image. Are these variable filters burdening the computational complexity?
- How to handle the distortion induced by the spherization process?
- How to handle discontinuities around the sphere poles?
- Computing geodesic may be costly - how does impact performance?
- Does this rely on data augmentation to cover rotation invariance of filters?
- Now icosahedrons are used - could the convolution work on an arbitrary mesh discretization, ranging from an ideal isoparametric sphere to a highly irregularly-triangulated mesh?
- The remeshing strategy to a sphere also looses information from the original mesh connectivity - For instance, links between mesh nodes on the original surface may convey important information (e.g., brain connectivity in neuroscience), remising to a sphere would loose such connectivity information.

[Results]
- The experiments shows the proposed method with several augmented approaches - How exactly are data augmented?
- Comparison with other spherical methods (Cohen et al 2018), or manifold-based methods (Monti et al 2018)?  Illustrating the pros and cons with these respective state-of-the-art?
- Improvements could be better emphasized in Fig 6, Table 3 - how is the method better than others?
",6
"# Weaknesses
Applications are a bit unclear.
It would be nice to see a better case made for spherical convolutions within the experimental section.  The experiments on SHREC17 show all three spherical methods under-performing other approaches.  It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.  Is there a task that this representation significantly outperforms other spherical methods and non-spherical methods?  Or a specific useful application where spherical methods in general outperform other approaches?  

# Strengths:
The method is well developed and explained.  
Ability to implement in a straight-forward manner on GPU.
",8
"# Summary
This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems. Previous work has either used general anisotropic convolution or azimuthally isotropic convolution. The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly. The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity. This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps. The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.


# Strengths
The paper has several strong points. It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity. Much of the relevant related work is discussed, and this is done in a balanced way. Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution. The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator). The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.


# Weaknesses
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published. To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense. This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form. For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega). As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations). So the closure axiom of a group is violated.

This matters, because the notion of equivariance really only makes sense for a group. If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations. As we saw before, this is the whole rotation group. This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution. Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant. This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary. The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.

I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.

Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole. The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi. But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis. This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking. The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).

The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable. I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks. I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.


# Other comments

The experiments show that the method is quite effective. For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost. That they do not substantially outperform these and other methods is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods. An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.

It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2). Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers. It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.

Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1. I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned. Some more explanation / discussion would be good. 

It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant? 


Typos & minor issues

- Abstract: ""to extract non-trivial features"". The word non-trivial really doesn't add anything here. Similarly ""offers multi-level feature extraction capabilities"" is almost meaningless since all DL methods can be said to do so.
- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi). The order is reversed when inverting.
- ""Different notations of convolutions"" -> notions
- ""For spherical functions there is no consistent and well defined convolution operators."" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.
- ""rationally symmetric"" -> rotationally
- ""exact hierarchical spherical patterns"" -> extract
- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields. References would be in order. Similarly, hexagonal convolution has a history in DL and outside.
- Bottom of page 7, capitalize ""for"".
- ""principle curvatures"" -> principal.
- ""deferent augmentation modes"" -> different
- ""inspite"" -> in spite
- ""reprort"" -> report
- ""utlize"" -> utilize
- ""computer the convolution"" -> compute


# Conclusion

Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper. Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues. For now I will give an intermediate rating to the paper.


[1] Kondor, Trivedi, ""On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups""",7
"The authors propose an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy.

Several key terms are used in ways that differ from the rest of the literature. The authors claim options are learned in an ""unsupervised"" manner, but it is unclear what this means. Previous work (none of which is cited) has dealt with unsupervised option discovery in the context of mutual information maximization (Variational intrinsic control, diversity is all you need, etc), but they do so in the absence of reward, unlike this paper. ""Optimal policy"" is similarly abused, with it appearing to mean optimal from the perspective of the current model parameters, rather than optimal in any global sense. Or at least I think that is what the authors intend. If they do mean the globally optimal policy, then its unclear how to interpret Equation 8, with its reference to a behavior policy and an advantage function, neither of which would be available if meant to represent the global optimum.

Equation 10 comes out of nowhere. One must assume they meant ""maximize mutual information"" and not ""minimize"", but who knows. Why is white-noise being added to the states and actions? Is this some sort of noise-contrastive estimation approach to mutual information estimation? It doesn't appear to be, but it is unclear what else could motivate it. Even the appendices fail to shine light on this equation.

The algorithm block isn't terribly helpful. The ""t"" variable is used outside of its for loop, which draws into question the exact nesting structure of the underlying algorithm (which isn't obvious for HRL methods). There aren't any equations referenced, with the option policy network's update not even referencing the loss nor data over which the loss would be evaluated.

Some of the experimental results show promise, but the PPO Ant result raises some questions. Clearly the OpenAI implementation of PPO used would have tuned for the OpenAI gym Ant implementation, and the appendix shows it getting decent results. But it never takes off in the harder RlLab version -- were the hyper-parameters adjusted for this new environment?

It is also odd that no other HRL approaches are evaluated against, given the number cited. Running these methods might be too costly, but surely a table comparing results reported in those papers should be included.

A minor point: another good baseline would be TD3 with the action repeat adjusted to be inline with the gating policy.

I apologise if this review came off as too harsh -- I believe a good paper can be made of this with extensive rewrites and additional experiments. But the complete lack of clarity makes it feel like it was rushed out prematurely.

EDIT: Now this is a paper that makes sense! With the terminology cleared up and the algorithm fully unpacked, this approach seems quite interesting. The experimental results could always be stronger, but no longer have any holes in them. Score 3-->6",6
"Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this.

Summary:
The paper proposes an HRL system in which the mutual information of the latent (option) variable and the state-action pairs is approximately maximized. To approximate the mutual information term, samples are reweighted based on their estimated advantage. TD3 is used to optimize the modules of the system. The system is evaluated on continuous control task from OpenAI gym and rllab.

For the most part, the paper is well-written and it provides a good overview of related work and relevant terminology. The experiments seem sound even though the results are not that impressive. The extra analysis of the option space and temporal distribution is interesting. 

Some parts of the theoretical justification for the method are not entirely clear to me and would benefit from some clarification. Most importantly, it is not clear to me why the policy in Equation 7 is considered to be optimal. Given some value or advantage function, the optimal policy would be the one that picks the action that maximizes it. The authors refer to earlier work in which similar equations are used, but in those papers this is typically in the context of some entropy maximizing penalty or KL constraint. A temperature parameter would also influence the exploration-exploitation trade-off in this ‘optimal’ policy. I understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it. However, this is not the policy that is used to generate behavior. In short, the paper is clear enough about how the method is constructed but it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from.

HRL is an interesting area of research with the potential to learn complicated behaviors. However, it is currently not clear how to evaluate the importance/usefulness of hierarchical RL systems directly and the tasks in the paper are still solvable by standard systems. That said, the occasional increase in sample efficiency over plain TD3 looks promising. It is somewhat disappointing that the number of beneficial option is generally so low. To get more insight in the methods it would have been nice to see a more systematic ablation of related methods with different mutual information pairings (action or state only) and without the advantage weighting. Could it be that the number of options has to remain limited because there is no parameter sharing between them? It would be interesting to see results on more challenging control problems where the hypothesized multi-modal advantage structure is more likely to be present.

All in all I think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification. In addition, experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.

Minor issues/typos:
- Contributions 2 and 3 have a lot of overlap.
- The ‘o’ in Equation 2 should not be bold font. 
- Appendix A. Shouldn’t there be summations over ‘o’ in the entropy definitions?


",7
"The paper considers the problem of hierarchical reinforcement learning, and proposes a criterion that aims to maximize the mutual information between options and state-action pairs.

The idea of having options partition the state-action space is appealing, because this allows options visit the same states, so long as they act differently, which is natural. The authors show empirically that the learned options do indeed decompose the state-action space, but not the state space.

There is a lot in the paper already, but the exposition could be much improved. Many of the design choices appear very ad hoc, and some are outright confusing. Some detailed comments:

* I got really confused in Section 3 re: advantage-weighted importance sampling. Why do this? If the option policies are trying to optimize reward, won’t they become optimal eventually (or so we usually hope in RL)? This section seems to assume that the advantage function is somehow given. It also doesn’t look like this gets used in the actual algorithm, and in fact on page 5 it is stated that “we decided to use the on-policy buffer in our implementation”. Then why introduce the off-policy bit at all, and list it as a contribution?
* Please motivate the choices. The paper mentions that one of its contributions are options with deterministic policies. This isn’t a contribution unless it addresses some problem that stochastic policies fail at. For example, DPG allows one to address continuous control problems.
Same with using information maximization. The paper literally states that “an interpretable representation can be learned by maximizing mutual information”. Representation of what? MI between what?
* Although the qualitative results are nice (separation of the state-action space), empirical results are modest at best. This may be ok, because based on the partition of the state-action space it seems that the option policies learn diverse behaviors in the same states. Maybe videos visualizing different options from the same states would be informative.
* Please add more discussion on why the options are switched at every step",5
"The paper considers 'replica exchange' Langevin dynamics. These methods are very popular among practitioners, and developing some theory backing the empirical successes is an important goal.
Unfortunately this paper offers only weak results. 
- The first 6 pages set up the general formalism. This is textbook material adapted to the current problem.
- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory. 
- Page 8 gives a Poincare inequality. Again, this follows from known results. More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.
- Similar comments hold for the following pages. They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.",4
"This paper gives a theoretical analysis of an interesting statistical physics technique known as replica exchange. The basic idea is that Langevin dynamics at low temperature is slow to converge, and that one could potentially boost the convergence by alternating between low and high temperature. At the extreme one could imagine running in parallel a random search and a gradient descent, and ``teleporting"" the gradient descent algorithm whenever the random search algorithm finds a point with better value. This makes a lot of sense and it is nice to see a theoretical analysis of this. The mathematics are sound, but I do not know whether it is an appropriate submission for ICLR.

One comment from the math side: it would be interesting (albeit probably difficult) to study kappa in (3.10) as a function of a. In particular at face value it looks like one only benefits from taking a larger, so why not study the limiting behavior of a->infty? What is the limiting value of kappa? Can you perform those calculations in the convex case at least?",7
"PROS:
- The text is very well written, with a good balance between mathematical details and intuitions.
- I really like the high-level description of the algorithms and proof techniques

CONS:
to be completely honest, I am not sure I have learnt anything new from the paper. 
1) the proof techniques are very standard
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:
a. large deviation principles
b. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.)

and
c. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.

I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al. 

REMARKS:
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.

",6
"This paper proposes a method for continual learning. The model has three components: a) a data generator to be used at training time to replay past examples, b) a parameter generator that takes the input observation to produce parameters for c) the actual classifier. The authors demonstrate the method on simple datasets with a stream of 2 or 3 tasks.

Strenghts:
- the combination of components is novel
- the method does not rely on task descriptors neither at training nor test time
Weaknesses:
- the paper needs a major rewrite to improve fluency and to better organize and describe the proposed approach
- the empirical validation is weak.

Relevance
Learning in a continual setting is certainly very relevant for this venue.

Novelty
While each component is by itself not very novel (replay methods for continual learning have already been used, networks predicting parameters have also become a fairly common approach in meta-learning literature), the proposed combination is novel in this sub-field.

Clarity
Clarity is very poor and definitely does not meet the acceptance bar for this conference. I believe that the authors would need to make a major revision to address this issue. While ICLR allows authors to revise papers, I think the revision needed to fix this draft goes beyond the acceptable limit, as reviewers would then need to make a whole new revision.
First, fluency is very poor. There are lots of grammatical errors (see first sentence of introduction ""neural networks suffers..""),  a plethora of un-necessary acronyms which force the reader to go back and forth to figure out what they refer to (MA, DG, DPG, DPG&S, ...), and several sentences are not well formed (e.g., read first sentence of introduction).
Second, some statements are contradictory; e.g., the authors define ""basic unit"" as ""simple MLP with one hidden layer"", but then say it ""is an activation function plus a matrix transformation""..
Third, graphics and formulas are too small and not legible.
Fourth, the organization of the paper is poor, it is very wordy yet vague. For instance, the authors should precisely describe how the data generator is trained in sec. 2.3. The authors should provide an algorithm summarizing how the different components interplay both at training and test time. At present, I am making educated guesses about how this system works.
For instance, how are real and generated examples interleaved? how is forgetting prevented in the data generator?   

References to prior work
While there are lots of references in sec. 4, they are not sufficiently well described - see third paragraph of sec. 4 where the authors cite almost 20 papers by simply saying they are ""some other approaches"". 
Also, I did not find mention to methods predicting parameters in the meta-learning community but also others like:
Denil et al. ""Predicting network parameters in deep learning"" NIPS 2013
 
Empirical validation
The empirical evaluation does show an advantage of the proposed approach on some simple streams composed by up to three tasks. However, a) the tasks are really simple because of the small number of tasks considered and b) the baselines are weak. For instance, EWC is now a bit out-dated as there are variants that work a bit better, like:
Chaundry et al. ""Riemannian Walk ..."" ECCV 2018
and there are other methods like ""Progress and Compress"" Schwartz ICML 2018 the authors could have compared against.
Besides, the authors do not mention anything about memory and time cost both at training and test time, possibly including the time to cross validate all the hyper-parameters of this method.
Overall, I am left with the sense that the proposed approach will be hard to scale to many more tasks and more realistic images (for which we do not quite know how to train efficiently and well generators).

Other comments
I did not find the formalization in eq. 9 very useful. The first and last term in that equation can be very big and there is no sense of how lose this bound is.
Also, it is not clear whether there is a general principle to partition the set of parameters (to determine which ones should be shared).",5
"Summary: 
- In this paper, an algorithm to improve the catastrophic forgetting of the model is proposed. The key idea consists of 1) introducing the dynamic parameter generator (DPG) for ""model's adaptation"" to data at test time and 2) data generator (DG) for remembering previously trained dataset. 

Pros: 
- Empirical results seem strong. The proposed result outperforms existing algorithms by quite large margin.

Cons:
- In general, I felt that the paper is unorganized and hard to read. Clarity should be definitely improved if this paper is to be published as a conference paper.

- Output of dynamic parameter generator is very high dimensional (it requires weight with dimension of input dim x NN weight dim). I think this approach is not scalable to higher dimension and typically requires even more memory than storing the whole dataset. 

- Although auto-encoder and generative replay was considered to reduce memory consumption, there is no description of how much memory is saved by them. In order to make the argument more convincing, the authors should explicitly describe the amount of memory consumed by each algorithms. 

- There seems to be a lot of ideas introduced, i.e, DPG for generation of weights, auto-encoders for generation of data and layer output constraint, i.e., Equation (7).  I think each of introduced method deserves some amount of empirical evaluation to validate its contribution to the performance.  

- Experiments only consider 2~3 tasks, which does not seem very representative for the lifelong learning tasks.
",6
"This paper proposes a Dynamic Parameter Generator (DPG) that given a test input modifies the parameters of a classification model. They also propose to regularize the training using a Data Generator (DG) to slow down catastrophic forgetting. DG is used to constrain the training that the internal representations of data generated by DG does not rapidly change. DG removes the need for storage of data or labels.

Positives:
- Both ideas of DPG and DG are novel in preventing catastrophic forgetting.
- DG is novel because it does not require storage of data and does not depend on labels.
- Experimental results are significantly better than the previous state-of-the-art.

Suggestions and clarification requests:
- Figures are very small and equations are cramped because of reduced spacing.
- There are some vague explanations in the intro that could be reduced. It would be nice to first introduce concrete math then give the intuitions. That saves some space.
- It would nice to compare to the recent Progress & compress [1]. Unfortunately, they have not provided results on benchmark MNIST tasks.
- This work is related to a recently proposed idea in architecture search [2] that learns to predict the weights of a network given its architecture.
- Can you clarify whether you have used DG at test time?
- Can you report results without using DG? It is not clear whether DPG is accountable for preventing the catastrophic forgetting or the sluggishness enforced by DG.
- Questions 1 and 2 need more formalization if the authors want to clearly prove a statement.
- As the answer to Question 1 suggests, have you explored enforcing a Lipschitz constraint?
- The answer to Question 2 is interesting. Could you rewrite it more formally? It seems like you can argue that DG’s objective encourages the employment of unused parameters which is important in tackling catastrophic forgetting.
- Can you elaborate on how much forgetting happens for DG?
- It seems that in figure 3.f and 3.c the MA method is unable to reach the best possible performance on the last task. Can you also report the table of accuracies on the last task?

[1] Schwarz, Jonathan, et al. ""Progress & Compress: A scalable framework for continual learning."" arXiv preprint arXiv:1805.06370 (2018).
[2] Brock, Andrew, et al. ""SMASH: one-shot model architecture search through hypernetworks."" arXiv preprint arXiv:1708.05344 (2017).",7
"This paper studies the problem of understanding the representation power of neural nets with Relu activations for representing structured data. In order to formalize this, the authors consider data generated from a sparse generative model as follows: A sparse m-dimensional vector Z is sampled from a distribution over sparse vectors. In input X is formed 
as AZ, where A is an incoherent matrix. The corresponding output is Y= w. X. The goal is to fit the data of the form (X_i, Y_i). The main result of the paper is that a 2-layer ReLU network can fit the data with near optimal error. On the other hand, low degree polynomials~(of degree up to log m) cannot fit the data with non-trivial error. Finally,
the authors also show that polynomials of degree polylog(m) can, in fact, fit the data as well as a 2-layer ReLU network. The paper is well written and provides new insights into the representation power of neural nets. It is also nice to know that ReLU networks can be approximated by low degree polynomials in the non-worst case scenario. This
is a good paper and I recommend acceptance.",7
"The paper studies the representational power of two-layer ReLU networks and polynomials for approximating a linear generative model for data with sparsity in the latent vector. They show that ReLU networks achieve optimal rate whereas low degree polynomials get a much worse rate.

Overall, the results are strong, the authors provide a lower bound on the degree of polynomial needed to approximate the model indicating the power of non-linearity. The observation of moving away from uniform approximators is well-motivated. The approximation theorem for ReLU is intriguing and uses new ideas which I have not seen before and are potentially useful in other applications. So far, only rational functions have been able to give such approximation guarantees. However, the motivation for studying sparse linear regression from a representation view-point is not very clear. Ideally, you would like to study representation for more complex models. 

Questions/Comments:
- Related work is missing prior work at the intersection of kernel methods and neural networks, please update.
- Define notation before using, for example, \rho_\tau^{⨂m}
- Expand proof sketches, they are not very clear, also full proofs are written with not much detail.
- Is the dependence on \mu tight? The current dependence sort of suggests that you need the observation matrix to be very close to identity.
- Proof of Lemma B.1 is unclear, could you explain how you deduce the lemma from the inequality?",7
"In this paper, authors analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. They give an almost-tight theoretical analysis of the performance and verify them with simulations.
 
Authors motivated the theoretical analysis from typical applications, for which the desired function can be only important to be approximated well on the relevant part of domains. Instead of formalizing the above problem, authors tackle a particular simple question. However, it is not easy to understand the relationships between the two problems.
 
A regression task is studied where the data has a sparser latent structure. Authors measure the performance of estimators via the expected reconstruction error from theoretical perspectives for both two-layer ReLU network and polynomial kernel. Empirical experiments will be even better to show the performance of some applications consistent with the theoretical results.",7
"This paper investigates internal working of RNN, by mapping its hidden states
to the nodes of minimal DFAs that generated the training inputs and its 
abstractions. Authors found that in fact such a mapping exists, and a linear
decoder suffices for the purpose. 
Inspecting some of the minimal DFAs that correspond to regular expressions, 
induced state abstractions are intuitive and interpretable from a viewpoint of
training RNNs by training sequences.

This paper is interesting, and the central idea of using formal languages to
generate feeding inputs is good (in fact, I am also doing a different research
that also leverages a formal grammar with RNN).

Most of the paper is clear, so I have only a few minor comments:

- In Figures 4 and 5, the most complex MDFA of 14 nodes does not have the
  lowest testing accuracies. In other words, testing accuracies is not
  generally proportional to the complexity of MDFA. Why does this happen?

- As noted in the footnote in page 5, state abstraction is driven by the idea
  of hierarchical grammars. Then, as briefly noted in the conclusion, why not
  using a simple CFG or PCFG to generate training sequences? 
  In this case, state abstractions are clear by definition, and it is curious
  to see if RNN actually learns abstract states (such as NP and VP in natural
  language) through mapping from hidden states to abstracted states.

- Because this paper is exploratory, I would like to see more examples
  beyond only the two in Figure 6. Is it possible to generate a regular 
  expression itself randomly to feed into RNN?
",7
"This paper aims to show that an RNN trained to recognize regular languages effectively focuses on a more abstract representation of the FSA of the corresponding language. 

Understanding the type of information encoded in the hidden states of RNNs is an important research question. Recent results have shown connections between existing RNN architectures and both weighted (e.g., Chen et al., NAACL 2018, Peng et al., EMNLP 2018) and unweighted (Weiss et al., ACL 2018) FSAs. This paper asks a simple question: when trained to recognize regular languages, do RNNs converge on the same states as the corresponding FSA? While exploring solutions to this question is potentially interesting, there are significant clarity issues in this paper which make it hard to understand it. Also, the main claim of the paper — that the RNN is focusing on a low level abstraction of thew FSA — is not backed-up by the results.

Comments:

— The authors claim that the RNN states map to FSA states with *low* coarseness, but Figure 3b (which is never referred to in text…) shows that in most cases the ratio of coarseness is at least 1/3, and in some cases > 1/2. 

— Clarity:
While the introduction is relatively clear starting from the middle of section 3 there are multiple clarity issues in this paper. In the current state of affairs it is hard for me to evaluate the full contribution of the paper.

- The definitions in section 3 were somewhat confusing. What is the conceptual difference between the two accuracy definitions? 

- When combining two states, does the new FSA accept most of the strings in the original FSAs? some of them? can you quantify that? Also, figure 6 (which kind of addresses this question) would be much more helpful if it used simple expressions, and demonstrated how the new FSA looks like after the merge.

- section 4 leaves many important questions unanswered:
1. Which RNN was used? which model? which parameters? which training regime? etc.
2. How were the expressions sampled? the authors mention that they were randomly sampled, so how come they talk about DATE and EMAIL expressions?
3. What is the basic accuracy of the RNN classifier (before decoding)? is it able to learn to recognize the language? to what accuracy? 

- Many of the tables and figures are never referred to in text (Figure 3b, Figure 5)

- In Figure 6, there is a mismatch between the regular expression (e.g., [0-9]{3}….) and the transitions on the FSA (a-d, @).

- How come Figure 3a goes up to 1.1? isn’t it bounded by 1? (100%?)

- The negative sampling procedure should be described in the main text, not the appendix. Also, it is not clear how come shuffling the characters is considered an independent distribution.

",5
"Paper Summary -
The authors trained RNNs to recognize formal languages defined by random regular expressions, then measured the accuracy of decoders that predict states of the minimal deterministic finite automata (MDFA) from the RNN hidden states. They then perform a greedy search over partitions of the set of MDFA states to find the groups of states which, when merged into a single decoder target, maximize prediction accuracy. For both the MDFA and the merged classes prediction problems, linear decoders perform as well as non-linear decoders.
Clarity - The paper is very clear, both in its prose and maths.
Originality - I don't know of any prior work that approaches the relationship between RNNs and automata in quite this way.
Quality/Significance - I have one major concern about the interpretation of the experiments in this paper.

The paper seems to express the following logic:
1 - linear (and non-linear) decoders aren't so good at predicting MDFA states from RNN hidden states
2 - if we make an ""abstract"" finite automata (FA) by merging states of the MDFA to optimize decoder performance, the linear (and non-linear) decoders are much better at predicting this new, smaller FA's states.
3 - thus, trained RNNs implement something like an abstract FA to recognize formal languages.

However, a more appropriate interpretation of these experiments seems to be:
1 - (same)
2 - if we find the output classes the decoder is most often confused between, then merge them into one class, the decoder's performance increases -- trivially. in other words, you just removed the hardest parts of the classification problem, so performance increased. note: performance also increases because there are fewer classes in the merged-state FA prediction problem (e.g., chance accuracy is higher).
3 - thus, from these experiments it's hard to say much about the relationship between trained RNNs and finite automata.

I see that the ""accuracy"" measurement for the merged-state FA prediction problem, \rho, is somewhat more complicated than I would have expected; e.g., it takes into account \delta and f(h_t) as well as f(h_{t+1}). Ultimately, this formulation still asks whether any state in the merged state-set that contains f(h) transitions under the MDFA to the any state in the merged state-set that contains f(h_{t+1}). As a result, as far as I can tell the basic logic of the interpretation I laid out still applies.

Perhaps I've missed something -- I'll look forward to the author response which may alleviate my concern.

Pros - very clearly written, understanding trained RNNs is an important topic
Cons - the basic logic of the conclusion may be flawed (will await author response)

Minor -
The regular expression in Figure 6 (Top) is for phone numbers instead of emails.
""Average linear decoding accuracy as a function of M in the MDFA"" -- I don't think ""M"" was ever defined. From contexts it looks like it's the number of nodes in the MDFA.
""Average ratio of coarseness"" -- It would be nice to be explicit about what the ""ratio of coarseness"" is. I'm guessing it's (number of nodes in MDFA)/(number of nodes in abstracted DFA).
What are the integers and percentages inside the circles in Figure 6?
Figures 4 and 5 are difficult to interpret because the same (or at least very similar) colors are used multiple times.
I don't see ""a"" (as in a_t in the equations on page 3) defined anywhere. I think it's meant to indicate a symbol in the alphabet \Sigma. Maybe I missed it.",5
"In this paper, the authors present a new technique to learn from positive and unlabeled data. Specifically they are addressing the issues that arise when the positive and unlabeled data do not come from the same distribution. The way to achieve this is to learn a scoring function which preserves -the order- of the label posteriors. In other words, the authors are not making assumptions and then learning the exact posterior of p(y|...) but rather just a function r(x) with the property that if p(y_i) < p(y_j) then r(x_i) < r(x_j).

I am not super familiar in the area but I didn't see any fundamental flaws. The approach makes sense and although I cannot judge the novelty of this paper, it is a useful tool in the PU learning toolbox addressing an arguably important problem (selection bias). Except for section 5.3, the experiments are not that interesting as they are made up artificially by the authors.

Thoughts:
- In example 1, be specific about what p(y|...) and p(o|...) are.
- In example 2, I wasn't sure what p(o|...) exactly would be.
- Assumption 1, the first sentence I understand. The ""if and only if"" part I don't see. Can you clarify?
",7
"The paper proposes an approach to learning from positive and unlabelled data with a sample selection bias. Specifically, it is assumed that the observed positive instances are not necessarily drawn iid from the true positive distribution: rather, there is some bias as to which positive examples are selected. Under an assumption on the selection probability being proportional to the true probability, it is established that one may equally rank instances based on their probability of being labelled. Two algorithms are proposed for this task.

Learning under sample selection bias is an important and interesting problem. It is also arguably more realistic than the classic PU learning setting. The paper proposes a reasonable solution, which builds on some recent advances in the literature on PU learning.

My only critique is that the results are somewhat unsurprising in light of existing work on this topic (the idea of constructing unbiased risk estimators), and also on the topic of learning from label loans. Further, I believe some clarifications would better position the contributions of the paper, both in terms of strengths and limitations. More specifically:

- it seems the problem could be cast as a (interesting) special case of learning from instance dependent label noise. The assumption of the selection (i.e., label flip) probability preserving the ordering of the true class probability has a fair amount of precedent in these works; see, e.g.,

Bylander '97, Learning probabilistically consistent linear threshold functions
Du and Cai '15, Modelling class noise with symmetric and asymmetric distributions
Bootkrajang '16, A generalised label noise model for classification in the presence of annotation errors

It is in light of these works that I do not find Theorem 1 surprising. I note that the sample-selection bias setting could be seen as an interesting special case, but some discussion on the connection seems prudent.

- like in the above works, the proposed approach does not construct an unbiased estimator to the underlying risk. Instead, what is shown in e.g. Theorem 3 is that the Bayes-optimal solution to the risk is sensible. This is of course a minimal desiderata for any learning method, but unlike approaches for the classic PU learning setting, the lack of unbiasedness implies that minimizing over a restricted function class F may result in quite different solutions than if we had access to the true labels. Again, this isn't a limitation unique to this particular work, but I did feel the point could be made a little more explicit.

- also like the above work, there isn't a clear way of estimating P(y = 1). As this is crucial for the final risk estimate, it somewhat restricts the universality of the approach.

- with regards to the two algorithms proposed, both go about estimating the underlying ""noisy"" class probability (i.e., the probability of an instance being selected for labeling), just with different losses. While the logistic or ""LSIF"" loss are certainly valid choices, one could use any number of other similar loss (e.g., the exponential loss from class-probability estimation, or the ""KLIEP"" loss from density ratio estimation). Of course the specific choice of LSIF e.g. can be motivated since it has a closed-form solution, but the basic point is that the two approaches really boil down to changing the underlying loss function. This point could also be clarified.


Other comments:

- I believe the Elkan & Noto paper operates in the censoring rather than case-controlled setting.

- there are a few grammatical issues: e.g.., ""Several recent researches"", ""is to find anomaly data""

- I don't follow how the case-controlled setting is ""more general"" than the censoring setting, as claimed in Sec 2; do you mean it is more practically realistic?

- it is correct to say in 2.1 that one cannot estimate p(y = 1 | x) from only PU data without assumptions. The next sentence states that a typical assumption that is thus made is SCR. However, this also does not guarantee that we can estimate the probability, since estimating p(y = 1) is also not possible without even further assumptions (see e.g. the mutually contaminated distribution work of Scott et al., 2013).

- in Defn 1, it would be clearer to explicate the dependence of all quantities on r.

- it is interesting that one achieves the BEP with the choice of threshold given by (2). But given that p(y = 1) is in general hard to estimate, it seems one could equally cast the problem of estimating p(y = 1) as the problem of choosing a good threshold? (This of course ignores the fact that we ostensibly need p(y = 1) when constructing the risk estimate.)

- restricting attention to scorers with output in [eps, 1 - eps] is a little strange. I assume this is in order to avoid solutions at +- infinity, which is a well-known problem with the logistic loss. It may be more natural to simply state that you operate with the extended real numbers.

- in the proof of Thm 3, I don't see the need to go through an infinite dimensional Lagrangian route. Since one is optimizing over all possible measurable functions, can one not (under suitable regularity conditions on the distribution & loss) simply compute the minimizer point wise for each x? This optimization would be a one-dimensional problem over predictions the domain [eps, 1 - eps]. The ""inner risk"" to be optimized (in the sense of Steinwart '06, ""How to compare different loss functions and their risks"") would I believe be a convex function, admitting exactly the minimizer claimed in the statement of the theorem.

- it is a bit confusing to move from F to \hat{F} as the function class.
",6
"The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.

The proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \theta such that the number of datapoints x with r(x) > \theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \theta, and to port the original problem to a vanilla binary classification problem.

Concerns:
- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.
- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?
- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for ""unbiased PU learning (PU)"", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.
- Related work discussion is completely missing apart from one paragraph in the introduction.

Minor:
- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.
- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.
- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.
- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?
- Theorem 3: add some intuition and explain tradeoff on \epsilon
- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, ""we trained a classifier minimizing (4) and (7) with the model (10)"" is difficult to digest and follow.
- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}
- Too many acronyms and abbreviations.

",5
"This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good.

I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature.

One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry.

Even with this reservation, I support accepting the paper.

Minor: In Section 3.4, ""existing a room"" -> ""exiting a room""",7
"This paper proposes a method for learning how to explore environments. The paper mentions that the “exploration task” that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.

<<Pros>>

-The paper is well-written (except for a few typos).
-The overall approach is simple and does not have much complications. 
-The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.  

<<Cons>>

**The technical novelty is not significant**

-This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. 

**The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. **

-Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters?  Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? 

-The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup.

-What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map.

-The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the “learning for Navigation” section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both “exploration strategy” and “imitation learning” : “Pathak, Deepak, et al. ""Zero-shot visual imitation."" International Conference on Learning Representations. 2018.
“ is missed in navigation comparison. The aforementioned paper is also missed in the references.  

-Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning.

-The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. 


** Technical details are missing or not explained clearly**

- Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? 

-The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? 

-Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn’t it only 2.5D (information obtained by depth sensor) used in the proposed method?

**Presentation can be improved**

-The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. 

- Interpretation of “green vs white vs black” in the reconstructed maps is left to the reader in Fig. 1. 

- Last line in page 5: there is no need for reiteration. It is already clear.

**Missing references**

-Since the paper is about learning to explore, discussion about “exploration techniques in RL” is recommended to be added in at least the related work section. 

-A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. 


",3
"This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces.

Using an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned!

One crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) ""Asynchronous methods for deep reinforcement learning"" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage.

A second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent?

Finally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) ""Neural SLAM"" or Wayne et al. (2018) ""Unsupervised Predictive Memory in a Goal-Directed Agent"", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well.

Minor remark: the word ""finally"" is repeated twice at the end of the introduction.",7
"This work proposes an adaptation to MAML-type models that accounts for posterior uncertainty in task specific latent variables. This is achieved via a hierarchical Bayesian view of MAML, employing variational inference for the task-specific parameters. The key intuition of this paper is that one can perform fast and efficient test-time variational inference for the task-specific latent variables by learning a good initialization during meta-training. This is achieved in a very similar fashion to MAML, and allows for an interesting form of amortization of test-time inference.

Pros:
- For the most part, the approach presented is principled and well justified.
- The motivation is clear: in the few-shot learning regime we expect to have little data to infer the task-specific latent variables, and so we should perform posterior inference to account for uncertainty.
- The paper is well written, clear, and easy to follow.

Cons (more details below):
- It is not clear what the significant contributions of this paper are, as a number of methods have been proposed to account for uncertainty in the task-specific latent variables, and results for many of these methods appear to be better than those presented here.
- Experimental section does compare to many of the existing related methods 
- There are some conceptual issues that need to be addressed by the authors. 

I enjoyed reading this paper, and I think the ideas and work presented are, for the most part, solid. However, I am not sure to what extent the novel contribution of this paper is significant. Several papers, including Grant et al. (2018), but going back to Heskes (2000), have proposed the hierarchical Bayesian view of meta-learning. Grant et al. (2018) used a Laplace approximation to learn in such a model with MAML-type settings, presenting a method that accounts for uncertainty in this family of models. More recently, Finn et al. (2018) and Kim et al. (2018) have done this in a variational manner, albeit with variations in the implementation details. Gordon et al. (2018) proposed a more general presentation, unifying the above works (and others) in a Bayesian framework that allows for different functional forms of posterior inference (both point estimates and distributional) of the task-specific parameters, including gradient based procedures. All of these papers have been publicly available for a few months at the time of submission, such that this view of meta-learning as (amortized) Bayesian inference is not novel.

Here are some points that I would ask the authors to address during the rebuttal period:

- The method presented in the paper does not account for the meta-training splits into query and test sets, other than to mention that these led to empirical performance gains (this is somewhat typical of probabilistic meta-learning papers). However, it not clear that this is justified from a probabilistic inference perspective, which would favour conditioning on all available data at inference time. Further, in the experimental section, the authors state that ""For the few-shot learning experiments, we found it necessary to downweight the inner KL term for better performance in our model"". Put together, it is not quite clear exactly what form of approximate inference is being conducted here. Can the authors comment on this?

- I am not sure I agree with the authors' interpretation of the term ""amortized Bayesian inference"", at least in that it deviates from the way the term is typically used in the related literature. The method negates the need to maintain variational parameters for each latent variable, and approximate posterior inference for unseen tasks may be performed relatively efficiently, which is highly desirable. However, a gradient optimization procedure must still be performed for inference of task-specific variables for new tasks at test time. Thus, new variational parameters must be introduced and optimized at test time. It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process, but this implies that the cost of inference at test time has been reduced, not fully amortized to a fixed cost (unless one fixes the number of gradient steps, which is a further deviation from variational inference and requires a prior of the form used in Grant et al. (2018)). Full amortization of inference for the task-specific variables is proposed by Garnelo et al. (2018) and Gordon et al. (2018), as well as Edwards and Storkey (2016), all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables. In these cases, posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network, rather than several forward-backward passes, and does not require introducing new variational parameters to be optimized. Further, these methods negate the need for differentiating through gradient-based procedures at meta-training time, which is not avoided in this paper, but rather dealt with in the standard Hessian-vector product form. It would be highly useful in the paper (perhaps in the related work section) for the authors to conduct a more thorough comparison of their proposed method and the existing literature employing amortized inference for meta-learning, to put their work in context.

I also have a number of concerns regarding the experimental section of the paper, which I find to be lacking both in details and the empirical comparison of the method to existing works.
- The authors' cite recent works on meta-learning that take into account uncertainty in the local latent variables (e.g., Grant et al. (2018), Finn et al. (2018), Kim et al. (2018)), but do not compare to these methods.
- Results from Garnelo et al. (2018) are not provided for the contextual bandits experiment. Their results seem to be comparable or better to those presented in this paper. Can the authors comment on this?
- The same is true for the few-shot learning case, where MAML is the only method compared to, despite there being, at the time of submission, many papers which have significantly improved upon these results.
- In terms of details, it is unclear how many gradient steps were taken at test time, and how this affects performance of the model.
- In terms of accuracy, the proposed method appears to be under-performing significantly (i.e., below confidence bounds in almost all cases).
- The statement ""...we believe improvements could be made with better variance reduction methods for stochastic gradients"" should, in my opinion, either be investigated or omitted from the paper.
- In terms of uncertainty quantification, I find this experimental evaluation highly interesting. However, there is not a comparison to much of the existing work. The comparison to MAML is only of moderate interest in this case, as MAML is a deterministic method and is not expected to perform well in this regard. A comparison to Probabilistic or Bayesian MAML (at the least) would be more convincing if uncertainty calibration proved to be better for this method.

Overall, the paper proposes a principled approach to performing approximate posterior inference for task specific latent variables in meta-learning settings. The paper is well-written, and the method is clearly derived. However, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) meta-learning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works. As the method is, for the most part, principled and well-derived, and the paper well written, I am willing to reconsider my overall score if the authors can demonstrate either (i) significant novelty or (ii) that this particular flavour of inference for the task-specific parameters provides significant benefits over existing approaches.

[1] - T. Heskes. Empirical Bayes for learning to learn. 2000.
[2] - E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as hierarchical Bayes. 2018.
[3] - C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. 2018.
[4] - T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning. 2018.
[5] - J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner. Decision-theoretic meta-learning: versatile and efficient amortization of few-shot learning. 2018.
[6] - M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. 2018.
[7] - H. Edwards, and A. Storkey. Towards a neural statistician. 2016.",6
"The authors consider meta-learning to learn a prior over neural network weights. This is done via amortized variational inference. This means that a good initialisation of the variational parameters are learned across tasks, such that a good set of hyperparameters per task can be found in a few gradient steps. The proposed approach is evaluated on a toy and several popular benchmarks (like miniImagenet).

The topic is timely. The contribution is modest, essentially applying the same idea as the one proposed in MAML to a variational objective, but well executed. The paper is relatively well-written and the contributions clearly stated/motivated. Section 2 and 3 could be written in a more compact way (in particular the math), but it does not harm the flow. The authors conducted a good set of experiments, but are missing comparisons Bayesian versions of MAML.
",6
"The authors proposed a meta-learning approach which amortizes hierarchical variational inference across tasks, learning an initial variational distribution such that, after a few steps of stochastic optimization with the reparametrization trick, they obtain a good task-specific approximate posterior. The optimization is performed by applying backpropagation through
gradient updates. Experiments on a contextual bandit setting and on miniImage net show how the proposed approach can outperform a baseline based on the method MAML. Although in miniImagenet the proposed method does not produce
gains in terms of accuracy, it does produce gains in terms of uncertainty estimation.

Quality:

The derivation of the proposed method is rigorous and well justified. The experiments performed show that the proposed method can result in gains. However, the comparison is only with respect to MAML and other techniques could have also be included to make it more meaningful. For example,

Gordon, Jonathan, et al. ""Decision-Theoretic Meta-Learning: Versatile and
Efficient Amortization of Few-Shot Learning."" arXiv preprint arXiv:1805.09921
(2018).

or the methods included in the related work section, or Garnelo et al. 2018.

The authors do not comment on the computational cost of the proposed method.

Clarity:

The paper is clearly written and easy to read.

Novelty:

The proposed method is new up to my knowledge. This is one of the first methods to do Bayesian meta-learning.

Significance:

The experimental results show that the proposed method can produce gains. However, because the authors only compare with a non-Bayesian meta-learning method (MAML), it is not clear how significant the results are. Furthermore, the computational cost of the proposed method is described well enough.",5
"summary
The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. SoTA meta-learning frameworks (MAML and ProtoNet) typically require rather large labeled datasets and hand-specified task distributions to define a sequence of tasks on which the algorithms are trained on. This paper proposes to unsupervised generate the sequence of tasks using multiple partitions as pseudo labels via k-means and other clustering variants on the embedding space. Empirical experiments show the benefit of the meta-learning on the M-way K-shot image classification tasks.  Also, “sampling a partition from U(P)” on page 4, the U(P) notation seems not defined.

Evaluation
- The writing and presentation of the paper are in general well carried, except some part seems a little unclear, taking me quite a while to understand. For example,  in the “task generation for meta-learning” paragraph on page 3, the definition of task-specific labels (l_n) is puzzling to me at first glance.     

- The proposed task construction in an unsupervised manner for the meta-learning framework is indeed simple and novel. 

- The empirical experiments are thorough and well-conducted with good justifications. The benefit of unsupervised meta-learning compared to simply supervised learning on the few-shot downstream tasks is shown in Table 1 and 2; Different embedding techniques have also been studied; the results of Oracle upper bound are also presented; task construction ablation is also shown. 

- Unsupervised meta-learning consists of multiple components such as learning embedding space, clustering methods, and various choices within the meta-learning frameworks. This together consumes a lot of hyper-parameters and the choice can somehow seem heuristic.

Conclusion
- In general, I like this paper especially the empirical analysis section. Therefore, I vote for accepting this paper.
",7
"The paper proposes to employ metalearning techniques for unsupervised tasks. The authors construct tasks in an automatic way from unlabeled data and run meta-learning over the constructed tasks.

Although the paper presents a novel approach and the experiments included in the work show promising results, in my opinion, the paper is still not mature. There are some importants problems:
* The motivation of the paper is weak. The authors include the problem statement as well as the definitions used in the paper without knowing what is the goal of the proposed algorithm. A clear example of a real problem where the proposed framework could be applied is necessary to motivate the work.
* The paper is difficult to read and follow. The paper is composed by a set of parts without many links. This makes difficult to read the paper to not very experienced readers. A running example could be useful to increase the readability of the work. In my opinion, the paper contains too much material for the length of the conference. In fact, some important information has been moved to the appendices. 
*Experimental section is specially hard to follow. The authors want to solve too many questions in a short space. Comparisons with other related papers should be included. 

",6
"This paper proposes to construct multiple classification tasks from unsupervised data.

Quality:
The detail of the proposed method is not mathematically presented and its performance is not theoretically analyzed.
Although the proposed method is empirically shown to be superior to other approaches, the motivation is not clearly presented.
Hence the overall quality of this paper is not high.

Clarity:
The readability of this paper is not high as it is redundant or unclear at several points.
For example, Sections 2.1, 2.3 and Sections 2.2, 2.4 can be integrated, respectively, and more mathematical details can be included instead.

Originality:
The proposal of constructing meta-learning based on unsupervised learning seems to be original.

Significance:
- The motivation is not clear. The proposed method artificially generates a number of classification tasks. But how to use such classifiers for artificially generated labels in real-world applications is not motivated.
  It is better to give a representative application, to which the proposed method fits.
- There is no theoretical analysis on the proposed method.
  For example, why is the first embedding step required? Clustering can be directly performed on the give dataset D = {x_i}.
- Although the paper discusses using unsupervised learning for meta-learning, only k-means is considered in the proposed method.
  There are a number of types of unsupervised learning, including other clustering algorithms and other tasks such as outlier detection, hence analyzing them is also interesting.
- The proposed method includes several hyper-parameters. But how to set them in practice it not clear.

Pros:
- An interesting approach to meta-learning is presented.

Cons:
- Motivation is not clear.
- There is no theoretical analysis.
",6
"In this paper, the task of performing meta-learning based on the unsupervised dataset is considered. The high-level idea is to generate 'pseudo-labels' via clustering of the given dataset using existing unsupervised learning techniques. Then the meta-learning algorithm is trained to easily discriminate between such labels. This paper seems to be tackling an important problem that has not been addressed yet to my knowledge. While the proposed method/contribution is quite simple, it possesses great potential for future applications and deeper exploration. The empirical results look strong and tried to address important aspects of the algorithm. The writing was clear and easy to follow. I especially liked how the authors tried to exploit possible pitfalls of their experimental design. 

Minor comments and questions:
- Although the problem of interest is non-trivial and important, the proposed algorithm can be seen as just a naive combination of clustering and meta-learning. It would have been great to see some clustering algorithm that was specifically designed for this type of problem. Especially, the proposed CACTUs algorithm relies on sampling without replacement from the clustered dataset in order to enforce ""balance"" of the labels among the generated task. This might be leading to suboptimal results since the popularity of each cluster (i.e., how much it represents the whole dataset) is not considered. 

- CACTUs seems to be relying on having random scaling of the k-means algorithm in order to induce diversity on the set of partitions being generated. I am a bit skeptical about the effectiveness of such a method for diversity. If this holds, it would be interesting to see the visualization of such a concept.

- Although only MAML was considered as the meta-learning algorithm, it would have been nice to consider one or more candidates to show that the proposed framework is generalizable. Still, I think the experiment is persuasive enough to expect that the algorithm would work well at practice.

- Would there be a trivial generalization of the algorithm to semi-supervised learning?  

-------

I am satisfied with the author's response and changes they made to the text. I still think the paper brings significant contributions to the area, by showing that even generating the pseudo-tasks via unsupervised clustering method allows the meta-learning to happen.  ",8
"The paper proposes a scheme for transitioning to favorable starting states for executing given options in continuous domains. Two learning processes are carried out simultaneously: one learns a proximity function to favorable states from previous trajectories and executions of the option,  and the other learns the transition policies based on dense reward provided by the proximity function.
	
Both parts of the learning algorithms are pretty straightforward, but their combination turns out to be quite elegant. The experiments suggest that the scheme works,  and in particular does not get stuck in local minima. 

The experiments involve fairly realistic robotic applications with complex options,  which renders credibility to the results.    

Overall this is a nice contribution to the options literature. The scheme itself is quite simple and straightforward, but still useful. 

One point that I would like to see elaborated is the choice of exponential (""discounted"") proximity function. Wouldn't a linear function of ""step"" be 
 more natural here? The exponent loses sensitivity as the number of steps away increases, which may lead to sparser rewards.
  
",7
"The paper presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks. In the heart of the method is state proximity estimator, which measures the distance between states in the originator and destination tasks. This estimator is used in the reward for the transition policy. The method is evaluated on number of MojoCo tasks, including locomotion and manipulation.

Strengths:
+ Well motivated and relevant topic. One of the big downsides in the current state of the art is lack of understanding how to learn complex tasks. This papers tackles that problem.
+ The paper is well written and the presentation is clear.
+ The method is simple, yet original. Overall, an elegant approach that appears to be working well.
+ Comprehensive evaluations over several tasks and several baselines.

Questions:
- In the metapolicy, what ensures consistency, i.e. it selects the same policy in the consecutive steps?
- Can the authors comment on the weaknesses and the limits of the method?",9
"** Summary **
The authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.

** Quality & Clarity **
The paper is well written and features an extensive set of experiments.

** Originality **
I am not aware of similar work and believe the idea is novel.

** Significance **
Several recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.",7
"The paper proposes a method for neural network training under a hard energy constraint (i.e. the method guarantees the energy consumption to be upper bounded). Based on a systolic array hardware architecture the authors model the energy consumption of transferring the weights and activations into different levels of memory (DRAM, Cache, register file) during inference. The energy consumption is therefore determined by the number of nonzero elements in the weight and activation tensors. To minimize the network loss under an energy constraint, the authors develop a training framework including a novel greedy algorithm to compute the projection of the weight tensors to the energy constraint.

Pros:

The proposed method allows to accurately impose an energy constraint (in terms of the proposed model), in contrast to previous methods, and also yields a higher accuracy than these on some data sets. The proposed solution seems sound (although I did not check the proofs in detail, and I am not very familiar with hardware energy consumption subtleties).

Questions:

The experiments in Sec. 6.2 suggest that the activation mask is mainly beneficial when the data is highly structured. How are the benefits (in terms of weight and activation sparsity) composed in the experiments on Imagenet? How does the weight sparsity of the the proposed method compare to the related methods in these experiments? Is weight sparsity in these cases a good proxy for energy consumption?

How does the activation sparsity (decay) parameter (\delta) q affect the accuracy-energy consumption tradeoff for the two data sets?

The authors show that the weight projection problem can be solved efficiently. How does the guarantee translate into wall-clock time?

Filter pruning methods [1,2] reduce both the size of the weight and activation tensors, while not requiring to solve a complicated projection problem or introducing activation masks. It would be good to compare to these methods, or at least comment on the gains to be expected under the proposed energy consumption model.

Knowledge distillation has previously been observed to be quite helpful when constraining neural network weights to be quantized and/or sparse, see [3,4,5]. It might be worth mentioning this.

Minor comments:
- Sec. 3.4. 1st paragraph: subscript -> superscript
- Sec. 6.2 first paragraph: pattens -> patterns, aliened -> aligned

[1] He, Y., Zhang, X., & Sun, J. (2017). Channel pruning for accelerating very deep neural networks. ICCV 2017.
[2] Li, H., Kadav, A., Durdanovic, I., Samet, H., & Graf, H. P. Pruning filters for efficient convnets. ICLR 2017.
[3] Mishra, A., & Marr, D. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. ICLR 2018.
[4] Tschannen, M., Khanna, A., & Anandkumar, A. StrassenNets: Deep learning with a multiplication budget. ICML 2018.
[5] Zhuang, B., Shen, C., Tan, M., Liu, L., & Reid, I. Towards effective low-bitwidth convolutional neural networks. CVPR 2018.",7
"This paper describes a procedure for training neural networks via an explicit constraint on the energy budget, as opposed to pruning the model size as commonly done with standard compression methods.  Comparative results are shown on a few data sets where the proposed method outperforms multiple different approaches.  Overall, the concept is interesting and certainly could prove valuable in resource-constrained environments.  Still I retain some reservations as detailed below.

My first concern is that this paper exceeds the recommended 8 page limit for reasons that are seemingly quite unnecessary.  There are no large, essential figures/tables, and nearly the first 6 pages is just introduction and background material.  Likewise the paper consumes a considerable amount of space presenting technical results related to knapsack problems and various epsilon-accurate solutions, but this theoretical content seems somewhat irrelevant and distracting since it is not directly related to the greedy approximation strategy actually used for practical deployment.  Much of this material could have been moved to the supplementary so as to adhere to the 8 page soft limit.  Per the ICLR reviewer instructions, papers deemed unnecessarily long relative to this length should be judged more critically.

Another issue relates to the use of a mask for controlling the sparsity of network inputs.  Although not acknowledged, similar techniques are already used to prune the activations of deep networks for compression.  In particular, various forms of variational dropout essentially use multiplicative weights to remove the influence of activations and/or other network components similar to the mask M used is this work.  Representative examples include Neklyudov et al., ""Structured Bayesian Pruning via Log-Normal Multiplicative Noise,"" NIPS 2017 and Louizos et al., ""Bayesian Compression for Deep Learning,"" NIPS 2017, but there are many other related alternatives using some form of trainable gate or mask, possibly stochastic, to affect pruning (the major ML and CV conferences over the past year have numerous related compression papers).  So I don't consider this aspect of the paper to be new in any significant way.

Moreover, for the empirical comparisons it would be better to compare against state-of-the-art compression methods as opposed to just the stated MP and SSL methods from 2015 and 2016 respectively.  Despite claims to the contrary on page 9, I would not consider these to be state-of-the-art methods at this point.

Another comment I have regarding the experiments is that hyperparameters and the use of knowledge distillation were potentially tuned for the proposed method and then simultaneously applied to the competing algorithms for the sake of head-to-head comparison.  But to me, if these enhancements are to be included at all, tuning must be done carefully and independently for each algorithm.  Was this actually done?  Moreover it would have been nice to see results without the confounding influence of distillation to isolate sources of improvement, but no ablation studies were presented.

Finally, regarding the content in Section 5, the paper carefully presents an explicit bound on energy that ultimately leads to a constraint that is NP-hard just to project on to, although approximate solutions exist that depend on some error tolerance.  However, even this requires an algorithm that is dismissed as ""complicated.""  Instead a greedy alternative is derived in the Appendix which presumably serves as the final endorsed approach.  But at this point it is no longer clear to me exactly what performance guarantees remain with respect to the energy bound.  Theorem 3 presents a fairly inscrutable bound, and it is not at all transparent how to interpret this in any practical sense.  Note that after Theorem 3, conditions are described whereby an optimal projection can be obtained, but these seem highly nuanced, and unlikely to apply in most cases.

Additionally, it would appear that crude bounds on the energy could also be introduced by simply penalizing/constraining the sparsity on each layer, which leads to a much simpler projection step.  For example, a simple affine function of the L0 norm would be much easier to optimize and could serve as a loose bound on the energy, given that the latter should be a non-decreasing function of the L0 norm.  Any idea how such a bound compares to those presented given all the approximations and greedy steps that must be included?


Other comments:
- As an implementation heuristic, the proposed Algorithm 1 gradually decays the parameter q, which controls the sparsity of the mask M.  But this will certainly alter the energy budget, and I wonder how important it is to employ a complex energy constraint if minimization requires this type of heuristic.

- I did not see where the quantity L(M,W) embedded in eq. (17) was formally defined, although I can guess what it is.

- In general it is somewhat troublesome that, on top of a complex, non-convex deep network energy function, just the small subproblem required for projecting onto the energy constraint is NP-hard.  Even if approximations are possible, I wonder if this extra complexity is always worth it relative so simple sparsity-based compression methods which can be efficiently implemented with exactly closed-form projections.

- In Table 1, the proposed method is highlighted as having the smallest accuracy drop on SqueezeNet.  But this is not true, EAP is lower.  Likewise on AlexNet, NetAdapt has an equally optimal energy.",7
"The paper is dedicated to energy-based compression of deep neural networks. While most works on compression are dedicated to decreasing the number of parameters or decreasing the number of operations to speed-up or reducing of memory footprint, these approaches do not provide any guarantees on energy consumption. In this work the authors derived a loss for training NN with energy constraints and provided an optimization algorithm for it. The authors showed that the proposed method achieves higher accuracy with lower energy consumption given the same energy budget. The experimental results are quite interesting and include even highly optimized network MobileNetV2.

Several questions and concerns.
‘Our energy modeling results are validated against the industry-strength DNN hardware simulator ScaleSim’. Could the authors please elaborate on this sentence?

One of the main assumptions is the following. If the value of the data is zero, the hardware can skip accessing the data. As far as I know, this is a quite strong assumption, that is not supported by many architectures. How do the authors take into account overhead of using sparse data formats in such hardware in their estimations? Is it possible to simulate such behavior in ScaleSim? Moreover, in many modern systems DRAM can only be read in chunks. Therefore it can decrease number of DRAM accesses in (4).

Small typos and other issues:
Page 8. ‘There exists an algorithm that can find an an \epsilon’
Page 8.’ But it is possible to fan approximate solution’
Page 4.  It is better to put the sentence ‘where s convolutional stride’  after (2).
In formulation of the Theorem 3, it is better to explicitly state that A contains rational numbers only since gcd is used.
Overall, the paper is written clearly and organized well, contains interesting experimental and theoretical results.
",7
"Summary of paper
This paper presents an approach  for quantising neural networks such that the resulting quantised model is robust to adversarial and random perturbations.
The core idea of the paper is to enforce the Lipschitz constant of each linear layer of the network approximately close to 1. Since the Lipschitz constant of the neural network is bounded by the product of the
Lipschitz constant of its linear layer (assuming Lipschitz 1 activation functions) the Lipschitz constant of the trained neural network is bounded by 1. This results in a model which is robust to adversarial and random noise ad all directions in the model space are non-expansive. Algorithmically, controlling the Lipschitz constant is achieved by using the orthogonal regulariser presented in the paper Cisse et.al which has the same motivation for this work but for standard neural network training but not quantising. The authors presents thorough experimental study showing why standard quantisation schemes are prone to adversarial noise and demonstrate clearly how this approach improves robustness of quantised network and sometimes even improve over the accuracy of original model. 

Review:
The paper is well written with clear motivation and very easy to follow. 
The core idea of using orthogonal regulariser for improving the robustness of neural network models have been presented in Cisse et.al and the authors re-use it for improving the robustness of quantised models. The main contribution of this work is in identifying that the standard quantised models are very vulnerable to adversarial noise which is illustrated through experiments and then empirically showing that the regulariser presented in Cisse et. al improves the robustness of quantised models with rigorous experiments. The paper add value to the research community through thorough experimental study as well as in industry since quantised models are widely used and the presented model is simple and easy to use. 

Some suggestions and ideas:

1. It will be great if the authors could add a simple analytical explanation why the quantised networks are not robust.                      

2. The manifold of Orthogonal matrices does not include all 1 - Lipschitz matrices and also the Orthogonal set is not convex. I think a better strategy for this problem is to regularise the spectral norm to be 1.  Regularising the spectral norm is computationally cheaper than Orthogonal regulariser when combined with SGD using power iterations.  Moreover the regulariser part of the model becomes nice and convex.

3. Another strategy to control the Lipschitz constant of the network is to directly penalise the norm of the Jacobian as explained in Improved Training of Wasserstein GANs (Gulrajani et. al).
",7
"Summary: 
The paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks. The authors observe that quantized models become less robust to adversarial attacks if the quantization includes the inner layers of the network. They propose a Lipschitz constant filtering of the inner layers' input-output to fix the issue.  

Strengths:
The key empirical observation that fully quantized models are more exposed to adversarial attacks is remarkable in itself and the explanation given by the authors is reasonable. The paper shows how a simple regularization scheme may become highly effective when it is supported by a good understanding of the underlying process.

Weaknesses:
Except for observing the empirical weakness of fully quantized models, the technical contribution of the paper seems to be limited to combining the Lipschitz-based regularization and quantization. Has the Lipschitz technique already been proposed and analysed elsewhere? If not, the quality of the paper would be improved by investigating a bit more the effects of the regularization from an empirical and theoretical perspective. If yes, are there substantial differences between applying the scheme to quantized models and using it on full-precision networks? It looks like the description of the Lipschitz method in Section 4 is restricted to linear layers and it is not clear if training is feasible/efficient in the general case.
 
Questions:
- has the Lipschitz technique been proposed and analysed elsewhere? Is the robustness of full-precision models under adversarial attacks also improved by Lipschitz regularization?
- how popular is the practice of quantizing inner layers? Has the performance of fully quantized models ever been compared to full-precision or partially quantized models in an extensive way (beyond adversarial attack robustness)? 
- are the adversarial attacks computed using the full-precision or the quantized models? would this make any difference?
- the description of the Lipschitz regularization given in Section 4 assumes the layers to be linear. Does the same approach apply to non-linear layers? Would the training be feasible in this case? ",6
"imho, this manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues. 
reading the other comments online, the authors seem to have addressed those concerns as well.",7
"This paper considers the problem of VarMisuse, a kind of software bug where a variable has been misused. Existing approaches to the problem create a complex model, followed by enumerating all possible variable replacements at all possible positions, in order to identify where the bug may exist. This can be problematic for training which is performed using synthetic replacements; enumeration on non-buggy positions does not reflect the test case. Also, at test time, enumerating is expensive, and does not accurately capture the various dependencies of the task. This paper instead proposes a LSTM based model with pointers to break the problem down into multiple steps: (1) is the program buggy, (2) where is the bug, and (3) what is the repair. They evaluate on two datasets, and achieve substantial gains over previous approaches, showing that the idea of localizing and repairing and effective.

I am quite conflicted about this paper. Overall, the paper has been strengths:
- It is quite well-written, and clear. They do a good job of describing the problems with earlier approaches, and how their approach can address it.
- The proposed model is straightforward, and addresses the problem quite directly. There is elegance in its simplicity.
- The evaluation is quite thorough, and the resulting gains are quite impressive.

However, I have some significant reservations about the novelty and the technical content. The proposed model doesn't quite bring anything new to the table. It is a straightforward combination of LSTMs with pointers, and it's likely the benefits are coming from the reformulation of the problem, not from the actual proposed model. This, along with the fact that VarMisuse is a small subset of the kinds of bugs that can appear in software, makes me feel the ideas in this paper may not lead to significant impact on the research community.

As a minor aside, this paper addresses some specific aspects of VarMisuse task and the Allamanis et al 2018 model, and introduces a model just for it. I consider the Allamanis model a much more general representation of programs, and much more applicable to other kinds of debugging tasks (but yes, since they didn't demonstrate this either, I'm not penalizing this paper for it).

--- Update ----
Given the author's response and the discussion, I'm going to raise the score a little. Although there are some valid concerns, it provides a clear improvement over Allamanis et al paper, and provides an interesting approach to the task.
",7
"This paper presents an LSTM-based model for bug detection and repair of a particular type of bug called VarMisuse, which occurs at a point in a program where the wrong identifier is used. This problem is introduced in the Allamanis et al. paper. The authors of the paper under review demonstrate significant improvements compared to the Allamanis et al. approach on several datasets.

I have concerns with respect to the evaluation, the relation of the paper compared to the state-of-the-art in automatic program repair (APR), and the problem definition with respect to live-variable analysis.

My largest concern about both this paper and the Allamanis et al. paper is how it compares to the state-of-the-art in APR in general. There is a large and growing amount of work in APR as shown in the following papers:
[1] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, pp. 1–1, 2017.
[2] M. Monperrus, “Automatic Software Repair: A Bibliography,” ACM Comput. Surv., vol. 51, no. 1, pp. 17:1–17:24, Jan. 2018.
[3] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, “Do automated program repair techniques repair hard and important bugs?,” Empir Software Eng, pp. 1–47, Nov. 2017.

Although the proposed LSTM-based approach for VarMisuse is interesting, it seems to be quite a small delta compared to the larger APR research space. Furthermore, the above papers on APR are not referenced.

The paper under review mostly uses synthetic bugs. However, they do have a dataset from an anonymous industrial setting that they claim is realistic. In such a setting, I would simply have to trust the blinded reviewers. However, the one industrial software project tells me little about the proposed approach’s effectiveness when applied to a significant number of widely-used software programs like the ones residing in state-of-the-art benchmarks for APR, of which there are at least the following two datasets:
[4] C. L. Goues et al., “The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,” IEEE Transactions on Software Engineering, vol. 41, no. 12, pp. 1236–1256, Dec. 2015.
[5] R. Just, D. Jalali, and M. D. Ernst, “Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs,” in Proceedings of the 2014 International Symposium on Software Testing and Analysis, New York, NY, USA, 2014, pp. 437–440.

The above datasets are not used or referenced by the paper under review.

My final concern about the paper is the formulation of live variables. A variable is live at certain program points (e.g., program statements, lines, or tokens as called in this paper). For example, from Figure 1 in the paper under review, at line 5 in (a) and (b), object_name and subject_name are live, not just sources.  In the problem definition, the authors say that ""V_def^f \subseteq V denotes the set of all live variables"", which does not account for the fact that different variables are alive (or dead) at different points of a program. The authors then say that, for the example in Figure 1, ""V_def^f contains all locations in the program where the tokens in V appear (i.e., tokens in the Blue boxes), as well as token sources from line 1”. The explanation of the problem definition when applied to the example does not account for the fact that different variables are alive at different program points. I’m not sure to what extent this error negatively affects the implementation of the proposed model. However, the error could be potentially quite problematic.",6
"Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present.

This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug.

Unfortunatley this leads to a number of weaknesses:
 - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.
 - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range)
 - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on.
 - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines?

Strengths of the paper are:
 - Well-written and easy to follow and understand.
 - Evaluation on several datasets.
 - Interesting architecture for bug-localization if the idea really works.

[1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection",5
"This paper presents a study of tradeoffs between adversarial and standard accuracy of classifiers. Though it might be expected that training for adversarial robustness always leads to improvement in standard accuracy, however the authors claim that the actual situation is quite subtle. Though adversarial training might help towards increasing standard accuracy in certain data regimes such as data scarcity, but when sufficient data is available there exists a trade-off between the two goals. The tradeoff is demonstrated in a fairly simple setting in which case data consists of two kinds of features - those which are weakly correlated with the output, and those which are strongly correlated. It is shown that adversarial accuracy depends on the feature which exhibit strong correlation, while standard accuracy depends on weakly correlated features.

Though the paper presents some interesting insights. Some of the concerns  are :
 - The paper falls short in answering the tradeoff question under a more general setup. The toy example is very specific with a clear separation between weak and strongly correlated features. It would be interesting to see how similar results can be derived when under more complicated setup with many features with varying extent of correlation.
 - The tradeoff between standard and robustness under linear classification has also been demonstrated in a recent work [1]. In [1], it is also argued that for datasets consisting of large number of labels, when some of the labels are under data-scarce regimes, an adversarial robustness view-point (via l1-regularization) helps in accuracy improvement for those labels. However, for other set of labels for which there is sufficient data available,  l2-regularization is more suited, and adversarial robustness perspective decreases standard accuracy. From this view-point, one could argue that some of the main contributions in the current paper, could be seen as empirical extensions for deep learning setup. It would be instructive to contrast and explore connections between this paper, and the observations in [1].
[1] Adversarial Extreme Multi-label Classification, https://arxiv.org/abs/1803.01570
==============post-rebuttal======
thanks for the feedback, I update my rating of the paper",8
"This paper discusses the hypothesis of the existence of intrinsic tradeoffs between clean accuracy and robust accuracy and corresponding implications. Specifically, it is motivated by the tradeoffs between clean accuracy and robust accuracy of adversarially trained network. The authors constructed a toy example and proved that any classifier cannot be both accurate and robust at the same time. They also showed that regular training cannot make soft-margin SVM robust but adversarial training can. At the end of the paper, they show that input gradients of adversarially trained models are more semantically meaningful than regularly trained models.

The paper is well written and easy to follow. The toy example is novel and provides a concrete example demonstrating robustness-accuracy tradeoff, which was previously speculated. Demonstrating adversarially trained models has more semantically meaningful gradient is interesting and provides insights to the field. It connects robustness and interpretability nicely.

My main concern is on the overclaiming of applicability of the ""inherent tradeoff"". The paper demonstrated that the ""inherent tradeoff"" could be a reasonable hypothesis for explaining the difficulty of achieving robust models. I think the authors should emphasize this in the paper so that it does not mislead the reader to think that it is the reason.

On a related note, Theorem 2.2 shows adversarial training can give robust classifier while standard training cannot. Then the paper says ""adversarial training is necessary to achieve non-trivial adversarial accuracy in this setting"". The word ""necessary"" is misleading, here Thm 2.2 showed that adversarial training works, but it doesn't exclude the possibility that robust classifiers can be achieved by other training methods. 

minor comments
- techinques --> techniques
- more discussion on the visual difference between the gradients from L2 and L_\infty adversarially trained networks
- Figure 5 (c): what does ""w Robust Features"" mean? are these values accuracy after perburtation?
",7
"The paper demonstrates the trade-off between accuracy and robustness of a model. The phenomenon is shown in previous works, but this work interestingly proposes a theoretical model that supports the idea. The proving technique can be particularly beneficial to developing theoretical understanding for the phenomenon. Besides, the authors also visualize the gradients and adversarial examples generated from standard and adversarially trained models, which show that these adversarially trained models are more aligned to human perception.

Quality: good, clarity: good, originality: good, significance: good

Pros: 
- The paper is fairly well written and the idea is clearly presented
- To the best of my knowledge (maye I am wrong), this work is the first one that 
provides theoretical explanation for the tradeoff between accuracy and robustness
- The visualization results supports their hypothesis that adversarially trained models 
percepts more like human.

Suggestions:
It would be interesting to see what kind of real images can fool the models and see whether the robust model made mistakes more like human.
",8
"Summary:
This submission proposes a reinforcement learning framework based on human emotional reaction in the context of autonomous driving. This relies on defining a reward function as the convex combination of an extrinsic (goal oriented) reward, and an intrinsic reward. This later reward is learnt from experiments with humans performing the task in a virtual environment, for which emotional response is quantified as blood volume pulse wave (BVP). The authors show that including this intrinsic reward lead to a better performance of a deep Q networks, with respect to using the extrinsic reward only. 
Evaluation:
Overall the proposed idea is interesting, and the use of human experiments to improve a reinforcement learning algorithm offers interesting perspectives. The weakness of the paper in my opinion is the statistical analysis of the results, the lack of in depth evaluation of the extrinsic reward prediction and the rather poor baseline comparison.
Detailed comments:
1.	Statistical analysis
The significance of the results should be assessed with statistical methods in the following results:
Section 4.1: Please provide and assessment of the significance of the testing loss of the prediction. For example, one could repetitively shuffle blocks of the target time series and quantify the RMSE obtained by the trained algorithm to build an H0 statistic of random prediction.
Section 4.2: the sentence “improves significantly when lambda is either non-zero or not equal to 1” does not seem valid to me and such claim should in any case be properly evaluated statistically (including correction for multiple comparison etc…).
Error bars: please provide a clear description in the figure caption of what the error bars represent. Ideally in case of small samples, box plots would be more appropriate.
2.	Time lags in BVP
It would be interesting to know (from the literature) the typical latency of BVP responses to averse stimuli (and possible the latency of the various mechanisms, e.g. brain response, in the chain from stimuli to BVP). Moreover, as latency is likely a critical factor in anticipating danger before it is too late, it would important to know how the prediction accuracy evolves when learning to predict at different time lags forward in time, and how such level of anticipation influence the performance of the Q-network.
3.	Poor baseline comparison
The comparison to reward shaping in section 4.4 is not very convincing. One can imagine that what counts is not the absolute distance to a wall, but the distance to a wall in the driving direction, within a given solid angle. As a consequence, a better heuristic baseline could be used. 
Moreover, it is unclear whether the approaches should be compared with the same lambda: the authors need to provide evidence that the statistics (mean and possibly variance) of the chosen heuristic is match to the original intrinsic reward, otherwise it is obvious that the lambda should be adapted.
4.	Better analysis of figure 5-6(Minor)
I find figure 5-6 very interesting and I would suggest that the authors fully comment on these results. E.g. : (1) why the middle plot of Fig. 6 mostly flat, and why such differences between each curve from the beginning of the training. (2) Why the goal oriented task leads to different optimal lambda, is this just a normalization issue?
",6
"Starting from the hypothesis that humans have evolved basic autonomic visceral responses that influence decision making in a meaningful way and that these are at work in driving a car, the authors propose to use such signals within the RL framework. This is accomplished by augmenting the RL reward function with a model learned directly from human nervous system responses. This leads to a 
convex combination of extrinsic rewards and visceral responses, with the goal to maximize extrinsic rewards and minimizing the physiological arousal response. The authors first show that they can train a CNN to predict systolic peaks from the pulse waveform based on the input images. The output of this network is then used with parametrically altered weightings in combination with the task related reward to evaluate performance on different driving tasks. The authors show that for different weightings performance on a number of driving tasks performance as measured by the collected extrinsic rewards is better.

Overall, this is an interesting application of RL. It is OK to be inspired by biology, neuroscience, or psychology, but further reaching claims or interpretations of results in these fields need to be chosen carefully. The discussion of neuroscience and psychology are only partially convincing, e.g. there is extensive evidence that autonomic responses are highly dependent on cognition and not just decisions dependent on visceral, autonomic responses of the SNS. Currently, the manuscript is rather loosely switching between inspirations, imprecise claims, and metaphorical implementations with relation to neuroscience. The authors are encouraged to relate their work to some of the multi-criteria and structural credit assignment literature in RL, given the convex combination of rewards.  It may also be important to relate this work to imitation learning, given that the physiological measurements certainly also reflects states and actions by the human agents. While one indication for the reasons of higher extrinsic rewards with the augmented system is mentioned by the authors, namely that the autonomic signal is continuous and while the extrinsic rewards are sparse is convincing, it is not at all clear, why the augmented system performs better as shown in figure 5. 
",6
"The method proposes to use physiological signals to improve performance of reinforcement learning algorithms. By measuring heart pulse amplitude the authors build an intrinsic reward function that is less sparse that the extrinsic one. It helps to be risk averse and allows getting better performances than the vanilla RL algorithm on a car-driving task. 

I found the paper well written and the idea is quite nice. I like the idea that risk aversion is processed as a data-driven problem and not as an optimisation problem or using heuristics. I think this general idea could be pushed further in other cases (like encourage fun, surprise, happiness etc. ). 

There are some issues with this paper yet. First, modifying the reward function also modifies the optimal policy. In the specific case of car driving, it may not be bad to modify the policy so that it makes passenger less stressed but in general, it is not good. This is why most of works based on intrinsic motivation also schedule the lambda parameter to decrease with time. This is not something explored in this paper. Also, this work is well suited to the car-driving scenario because stress is closely related to risk and accident. But it may not work with other applications. I would thus suggest that the title of the paper reflects the specific case of risk aversion. ",7
"In this paper the authors propose a neural model that, given a logical formula as input, predicts whether the formula is a tautology or not. Showing that a formula is a tautology is important because if we can classify a formula A -> B as a tautology then we can say that B is a logical consequence of A. The structure of the formula is a feedforward neural network built in a top-down manner. The leaves of this network are vectors (each of them represents a particular occurrence of an atom) which, after the construction of the formula, are processed by some recurrent neural networks.

The proposed approach seems interesting. However, my main doubt concerns the model. It seems to outperform the state-of-the-art, but the authors do not give any explanations why. There is no theoretical or intuitive explanation of why the model works. Why we need RNNs and not feedforward NNs? I think this is an big issue.
In conclusion, I think that the paper is a bit borderline. The model should be better explained. However, I think that the approach is compelling and, after a minor revision, the paper could be considered for acceptance.

[Minor comments]
Page 4. 
“The dataset contains train (99876 pairs)”, pairs of what?

Page 5. 
What is the measure of the values reported in Table 1? Precision? 
",6
"In this paper, the authors provide a new neural-net model of logical formulae. The key feature of the model is that it gathers information about a given formula by traversing its parse tree top-down. One neural net of the model traverses the parse tree of the formula from the root all the down toward the leaves, and generates vectors for the leaves of the tree. Then, another RNN-based neural net collects these generated vectors, and answers a query asked for the formula, such as logical entailment. When experimented with Evans et al.'s data set for logical entailment queries, the authors' model outperforms existing models that encode formulae by traversing their parse trees bottom-up.

I found the idea of traversing a parse tree of a formula top-down and converting it to a vector very interesting. It is also good to know that the idea leads to a competitive model for at least one dataset. 

However, I am hesitant to be a strong supporter for this paper. I feel that the cons and pros of the model and its design decisions are not fully analyzed or explained in the paper; when reading this paper, I wanted to learn a rule of thumb for deciding when (and why if so) a top-down model of logical formulae works better than a bottom-up model. I understand that what I ask for is very difficult to answer, but experiments with more datasets and different types of queries (such as satisfiability) might have made me happier.

Here are some minor comments.

* Abstract: I couldn't quite understand your point about atoms. According to Figure 1, there is a neural net for each propositional symbol, and this means that your model tracks information about which occurrences of propositional symbols are about the same one. Is your point about the insensitivity of your model to a specific name given to each symbol? 

* p1: this future ===> this feature

* p2: these constrains ===> these constraints

* p2: recursively build model ===> recursively built model

* p2: Change the font of R in the codomain of ci.

* p3: p1 at the position of ===> p1 is at the position of
",6
"Cons

1.	There is no study of the representations developed by the model, which is unfortunate because this is a conference on learning representations and because there is little light shed on how the network achieves its rather high level of performance.
2.	It seems less generally useful to have such a special-purpose network for computing global properties like tautologicality than to have a network that produces actual vector encodings of propositions, as typical of the bottom-up tree-structured models.

Pros

3.	The paper is quite clear.
4.	The problem is important.
5.	The paper pursues the familiar path of a tree-structured network isomorphic to the parse tree of a propositional-calculus formula, but with the original twist of passing information top-down rather than bottom-up.
6.	The results are impressively strong. In particular, it improves by 10% absolute over the special-purpose and highly performant PossibleWorldNet on the most difficult category of problems, the ‘massive’ category, achieving 83.6% accuracy.

Pro/Con mix

7.	Although the paper did not provide much insight into what was going on in the network to allow it to perform well (point 1 in ‘Cons’), I was able to convince myself I could understand a way the architecture *could* succeed (whether this possible approach matches the actual processing in the model I have no way of assessing). In brief, the vector that is passed down the network can be thought of as a list of truth values across multiple possible worlds of the tree node at which the vector resides. To search for a counterexample to tautologicalhood, the original input vector to the root node could be the zero (false) vector. If the kth value in the vector at a parent node labeled ‘or’ is 0 (the disjunction is false in world k) then in the two children the kth value must also be 0. If the kth value of the vector at an XOR node is 0, the kth value of the two children must both be 0 or both be 1; actually these values need not reside in position k so the children could both have value 0 at some position i and both have value 1 at another position j. Then in the RNN-Var component of the network, which checks for consistency across multiple tokens of the same proposition variable, each position k in all vectors for the same variable can be checked for equality, producing a value 1 in the output vector if all have value 1, producing 0 if all have value 0, and producing value -1 if the values do not all agree. Then RNN-All checks across all vectors for proposition variable types to see if there’s a position k in which no value -1 occurs; if so, the values of the variable vectors at position k give the truth values for all variables such that the overall proposition has the desired value 0: a counterexample exists. If no such position k exists, the proposition is a tautology. This seems roughly right, at least.",6
"Review:

This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning.
	
Quality: 

	The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods

Clarity: 

	The paper is well written in general with a few typos, e.g., 

	""The weights of the Bi-LSTM θ, is learned during the search process. The weights θ determines""

Originality: 

	The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks.

	Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening.
	
Significance:

	Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results.

	It also seems that the authors have not repeated the experiments several times since there are no error bars in the results.
	This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels.

	Besides this, the authors may want to cite this paper

	Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).	

	which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time).

Pros:

	- Well written paper.
		
	- Simply idea.

	- Extensive experiments.

Cons:
	
	- The proposed  approach is a combination of well known methods.

	- The significance of the results is in question since the authors do not include error bars in the experiments.",6
"================
Post-Rebuttal
================

I thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods,  clarifications of the proposed approach, adding references to related work, I will  increase my score and suggest to accept the paper.




The paper describes a new neural architecture search strategy based on Bayesian optimization to find a compressed version of a teacher network. The main contribution of the paper is to learn an embedding that maps from a discrete encoding of an architecture to a continuous latent vector such that standard Bayesian optimization can be applied. 
The new proposed method improves in terms of compressing the teacher network with just a small drop in accuracy upon an existing neural architecture search method based on reinforcement learning and random sampling.


Overall, the paper presents an interesting idea to use Bayesian optimization on high dimensional discrete problems such as neural architecture search. I think a particular strength of this methods is that the embedding is fairly general and can be combined with various recent advances in Bayesian optimization, such as, for instance, multi-fidelity modelling.
It also shows on some compression experiments superior performance to other state-of-the-art methods.

However, in its current state I do not think that the paper is read for acceptance:

- Since the problem is basically just a high dimensional, discrete optimization problem, the paper misses comparison to other existing Bayesian optimization methods such as TPE [1] / SMAC [2] that can also handle these kind of input spaces. Both of these methods have been applied to neural architecture search [3][4] before. Furthermore, since the method is highly related to NASBOT [5], it would be great to also see a comparison to it.

- I assume that in order to learn a good embedding, similar architectures need to be mapped to latent vector that are close in euclidean space, such that the Gaussian process kernel can model any correlation[7]. How do you make sure that the LSTM learns a meaningful embedding space? It is also a bit unclear why the performance f is not used directly instead of p(f|D). Using f instead of p(f|D) would probably also make continual training of the LSTM easier, since function values do not change.

- The experiment section misses some details:
  - Do the tables report mean performances or the performance of single runs? It would also be more convincing if the table contains error bars on the reported numbers.
  - How are the hyperparameters of the Gaussian process treated?
  
- The related work section misses some references to Lu et al.[6] and Gomez-Bombarelli et al.[7] which are highly related.

- What do you mean with the sentence  ""works on BO for NAS can only tune feed-forward structures"" in the related work section? There is no reason why other Bayesian optimization should not be able to also optimize recurrent architectures (see for instance Snoek et al.[8]). 

- Section 3.3 is a bit confusing and to be honest I do not get the motivation for the usage of multiple kernels. Why do the first architectures biasing the LSTM? Since Bayesian optimization with expected improvement samples around the global optimum, should not later evaluated, well-performing architectures more present in the training dataset for the LSTM?


[1] Algorithms for Hyper-Parameter Optimization
    J. Bergstra and R. Bardenet and Y. Bengio and B. Kegl
    Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'11)

[2] Sequential Model-Based Optimization for General Algorithm Configuration
    F. Hutter and H. Hoos and K. Leyton-Brown
    Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11)

[3] Towards Automatically-Tuned Neural Networks
    H. Mendoza and A. Klein and M. Feurer and J. Springenberg and F. Hutter
    ICML 2016 AutoML Workshop

[4] Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures
    J. Bergstra and D. Yamins and D. Cox
    Proceedings of the 30th International Conference on Machine Learning (ICML'13)

[5] Neural Architecture Search with Bayesian Optimisation and Optimal Transport
    K. Kandasamy and W. Neiswanger and J. Schneider and B. P{\'{o}}czos and E. Xing
    abs/1802.07191

[6] Structured Variationally Auto-encoded Optimization
    X. Lu and J. Gonzalez and Z. Dai and N. Lawrence
    Proceedings of the 35th International Conference on Machine Learning

[7] Automatic chemical design using a data-driven continuous representation of molecules
    R. Gómez-Bombarelli and J. Wei and D. Duvenaud and J. Hernández-Lobato and B. Sánchez-Lengeling and D. Sheberla and J. Aguilera-Iparraguirre and T. Hirzel. and R. Adams and A. Aspuru-Guzik
    American Chemical Society Central Science

[8] Scalable {B}ayesian Optimization Using Deep Neural Networks
    J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams
    Proceedings of the 32nd International Conference on Machine Learning (ICML'15)",7
"In this work, the authors propose a new strategy to compress a teacher neural network. Briefly, the authors propose using Bayesian optimization (BO) where the accuracy of the networks is modelled using a Gaussian Process function with a squared exponential kernel on continuous neural network (NN) embeddings. Such embeddings are the output of a bidirectional LSTM taking as input the “raw” (discrete) NN representations (when regarded as a covariance function of the “raw” (discrete) NN representations, the kernel is a deep kernel).

The authors apply this framework for model compression. In this application, the search space is the space of networks obtained by sampling reducing operations on a teacher network. In applications to CIFAR-10 and CIFAR-100 the authors show that the accuracies of the compressed network obtained through their method exceeds accuracies obtained through other methods for compression, manually compressed networks and random sampling.

I have the following concerns/questions:

1)	The authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to “generate a priority ordering of architectures for evaluation”. Within the proposed BO framework, this would require the optimization of the expected improvement in a high-dimensional and discrete space (the space of NN architectures), which “is non-trivial”. In this work, the authors do not try to solve this general problem, but specialize their work to model compression, which has a much lower dimensional search space (space of networks obtained by sampling reducing operations on a teacher network). For this reason, I believe the presentation and motivation of this work is not presented clearly. Specifically, while I agree that the methods and results in this paper can be relevant to the problem of getting NN embeddings for a larger search space, this should be discussed in the conclusion/discussion as future direction, rather than as motivating example. Generally, I think the method should be described in the context of model compression rather than as a general method for neural architecture search (NAS) method (in my understanding, its use for NAS would be unfeasible). 

2)	I have been wondering why the authors optimize the kernel parameters by maximizing the predictive GP posterior rather than maximizing the GP log marginal likelihood as in standard GP regression?

3)	The sampling procedure should be explained in greater detail. How many reducing operations are sampled? This would be important to fully understand the random search method the authors consider for comparison in their experiments. I expect that the results from that method will strongly depend on the sampling procedure and different choices should probably be explored for a fair comparison. Do the authors have any comment on this?
",5
"The paper suggests to use TD3 to compute an off-policy update instead of the TRPO/PPO updates in GAIL/AIRL in order to increase sample efficiency.
The paper further discusses the problem of implicit step penalties and survival bias caused by absorbing states, when using the upper-bounded/lower-bounded reward functions log(D) and -(1-log(D)) respectively. To tackle these problem, the paper proposes to explicit add a unique absorbing state at the end of each trajectory, such that its rewards can be learned as well.

Pro:
The paper is well written and clearly presented. 

Using a more sample efficient RL method for the policy update is sensible and turned out effective in the experiments.

Properly handling simulator resets in MDPs is a well known problem in reinforcement learning that I think is insufficiently discussed in the context of IRL.


Cons:
The contributions seem rather small.
a) Replacing the policy update is trivial, since the rl methods are used as black-box modules for the discussed AIL methods. 

b) Using importance weighting to reuse old trajectories for the discriminator update hardly counts as a contribution either--especially when the importance weights are simply omitted in practice. I also think that the reported problems due to the high variance have not been sufficiently investigated. There should be a better solution than just pretending that the replay buffer corresponds to roll-outs of the current policy. Would it maybe help to use self-normalized importance weights? The paper does also not analyze how such assumption/approximation affects the theoretical guarantees.

c) The problem with absorbing states is in my opinion the most interesting contribution of the paper. However, the discussion is rather shallow and I do not think that the illustrative example is very convincing. Section 4.1.1. argues that for the given policy roll-out, the discriminator reward puts more reward on the policy trajectory than the expert trajectory. However, it is neither surprising nor problematic that the discriminator reward does not produce the desired behavior during learning. By assigning more cumulative reward for s2_a1->s1 than for s2_a2->g, the policy would (after a few more updates) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q(s2,a2) > Q(s2,a1)--when the policy would match the expert exactly. The illustrative example also uses more policy-labeled transitions than agent-labeled ones for learning the classifier, which may also be problematic. The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states, which I think is not true in general. A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. Hence, the immediate reward for choosing such action can be made larger than the discounted future reward when not ending the episode (for any gamma < 1). Even for state-only reward functions the problem does not persist when reseting the environment after reaching the absorbing state such that the training trajectories contain states that are only reached if the simulator gets reset. Hence, I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. This may be different for resets due to time limits that can not be predicted by the last state-action tuple. However, issues relating to time limits are not addressed in the paper. I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation.

---------------
Update 21.11.2018

I think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely:
- The technical contributions are minor.
- The theoretical discussion (in particular regarding absorbing states) is quite shallow.

The merits of the paper are:
- Good results due to off-policy learning
- Raising awareness and providing a fix for a common pitfall 

I think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. 
Some suggestions: 

Section 3.1
""As we discuss in detail in Section 4.2 [...]""
I think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states.

""We will demonstrate empirically in Section 4.1 [...]""
The demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)?

Section 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics.

Section 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how  the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. 


-------------
Update 22.11.2018
By highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. 
I think that the submission can get accepted and I adapted my rating accordingly.

Minor:
Conclusion should also squeeze in somehow that the reward biases are caused by the implementations.
Typo in 4.2: ""Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic]
were previous hidden, [...]""
",6
"The authors find 2 issues with Adversarial Imitation Learning-style algorithms: I) implicit bias in the reward functions and II) despite abilities of coping with little data, high interaction with the environment is required. The authors suggest ""Discriminator-Actor-Critic"" - an off-policy Reinforcement Learning reducing complexity up to 10 and being unbiased, hence very flexible. 

Several standard tasks, a robotic, and a VR task are used to show-case the effectiveness by a working implementation in TensorFlow Eager.

The paper is well written, and there is practically no criticism.

",8
"This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed ""Discriminator-Actor-Critic"". They key point here being that they propose a replay buffer to sample transitions from. 

It is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. 

Pros:
	* Well written
	* Motivation is clear
	* Example on biased reward functions 
	* Experiments are carefully designed and thorough
Cons:
	* The analysis of the results in section 5.1 is a bit short

Questions:
	* You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning?

	* What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend

	* In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments?

	* Is it possible to show results of the effect of absorbing states on the Mujoco environments?

Minor suggestions:
In Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.",7
"The authors demonstrate the generalization bound for deep neural networks using the PAC-Bayesian approach. They adopt the idea of noise resilience in the analysis and obtain a result that has improved dependence in terms of the network dimensions, but involves parameters (e.g., pre-activation) that may be large potentially. 

My major concern is also regarding the dependence on the pre-activation that can be very large in practice. This is also shown in the numerical experiments. Therefore, the overall generalization bound can be larger than existing results, though the later have stronger dependence on the network sizes. By examining the analysis for the main result, it seems to me that the reason the authors can induce weaker dependence on network sizes is essentially they involved the pre-activation parameters. This can be viewed as a trade-off how strong the generalization bound depend on the network sizes and other related parameters (like the pre-activation here) rather than strictly tighten the error bound from a more refined/structured way. I also suggest that the authors provide the comparison of their bound and existing ones to see the quantitative difference of the results. 

Regarding the noise resilience, it is not clear to where the noise resilience shows up from the analysis or the result. From the proof of the main result, the analysis seems to be standard as in the PAC-Bayesian analysis, which is based on bounding the difference of the network before and after injecting randomness into the parameters. The difference with respect to the previous result due to the different way of bounding such a gap, where the Jacobian, the pre-activation and function output pop up. But this does not explain how well a network can tolerate the noise, either in the parameter space of the data space. This is different with the previous analysis based on the noise resilience, such as [1]. So, the title and the way the authors explain as noise resilience is somewhat misleading. More detailed explanation will help.

[1] Arora et al. Stronger generalization bounds for deep nets via a compression approach. 
",5
"The fact that a number of current generalization bounds for (deep) neural networks are not expressed on the deterministic predictor at stake is arguably an issue. This is notably the case of many recent PAC-Bayesian studies of neural networks stochastic surrogates (typically, a Gaussian noise is applied to the network weight parameters). The paper proposes to make these PAC-Bayesian bounds deterministic by studying their ""noise-resilience"" properties. The proposed generalization result bounds the margin of a (ReLU) neural network classifier from the empirical margin and a complexity term relying on conditions on the values of each layer (e.g., via layer Jacobian norm, the layer output norm, and the smallest pre-activation value). 

I have difficulty to attest if the proposed conditions are sound. Namely, the authors genuinely admit that the empirically observed pre-activation values are not large enough to make the bound informative (I must say that I truly appreciate the authors' candor when it comes to analyzing their result). That being said, the fact that the bounds does not scale with the spectral norm of the weight matrices, like previous PAC-Bayesian result for neural networks, is an asset of the current analysis.

I must say that I had only a quick look to it the proofs, all of them being in the supplementary material along most of the technical details. Nevertheless, it appears to me as an honest, original and rigorous theoretical study, and I think it deserves to be presented to the community. It can bring interesting discussion and suggest new paths to explore to explain the generalization properties of neural networks.

Minor comment: For the reader benefit, Theorem F.1 in page 7 should quickly recall the meaning of some notation, even if it's the ""short version"" of the theorem statement.

====
update: The bound comparison added value to the paper. It strengthens my opinion that this work deserves to be published. I therefore increase my score to 7. ",7
"This paper presents a PAC-Bayesian framework that bounds the generalization error of the learned model. While PAC-Bayesian bounds have been studied before, the focus of this paper is to study how different conditions in the network (e.g. behavior of activations) generalize from training set to the distribution. This is important since prior work have not been able to handle this issue properly and as a consequence, previous bounds are either on the networks with perturbed weights or with unrealistic assumptions on the behavior of the network for any input in the domain.

I think the paper could have been written more clearly. I had a hard time following the arguments in the paper. For example, I had to start reading from the Appendix to understand what is going on and found the appendix more helpful than the main text. Moreover, the constraints should be discussed more clearly and verified through experiments.

I see Constraint 2 as a major shortcoming of the paper. The promise of the paper was to avoid making assumptions on the input domain (one of the drawbacks in Neyshabur et al 2018) but the constraint 2 is on any input in the domain. In my view, this makes the result less interesting.

Finally, as authors mention themselves, I think conditions in Theorem F.1 (the label should be 4.1 since it is in Section 4) could be improved with more work. More specifically, it seems that the condition on the pre-activation value can be improved by rebalancing using the positive homogeneity of ReLU activations.

Overall, while I find the motivation and the approach interesting, I think this is not a complete piece of work and it can be improved significantly.

===========
Update: Authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.  I recommend accepting this paper. ",8
"This paper provides new generalization bounds for deep neural networks using the PAC-Bayesian framework. Recent efforts along these lines have proved bounds that 
either apply to a classifier drawn from a distribution or to a compressed form of the trained classifier. In contrast, the paper uses PAC Bayesian bounds to 
provide generalization bounds for the original trained network. At this same time, the goal is to provide bounds that do not scale exponentially in the depth of the
network and depend on more nuanced parameters such as the noise-stability of the network. In order to do that the paper formalizes properties that a classifier must 
satisfy on the training data. While these are a little difficult to understand in general, in the context of ReLU networks these boil down to bounding the l2-norms
of the Jacobian and the hidden layer outputs on each data point. Additionally, the paper also requires the pre-activations to be sufficiently large, which as the authors 
acknowledge, is an unrealistic assumption that is not true in practice. Despite that, the paper makes an important contribution towards our current understanding of 
generalization of deep nets. It would have been helpful if the authors had a more detailed discussion on how their assumptions relate to the specific assumptions in the papers
of Arora et al. and Neyshabur et al. This would help when comparing the results of the paper with existing ones. ",7
"The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert’s and agent’s stationary state-action distributions. 

The proposed method outperforms existing imitation learning approach (GAIL & BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. 
The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose).

While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.).

Nonetheless, the paper overall presents a strong submission based on novelty & relevance of the proposed method and is recommended for publication. 

Minor issues:
- Related work: improve transitions between the section about trajectory tracking and BC.
- Ablation studies with less flexible probabilistic models would strengthen the experiment section further. 
- Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access.
- A release of the code base would further strengthen the contributions of the submission.

General recommendation:
- The authors are encouraged to further investigate off-policy corrections for improved convergence.
",7
"The paper proposes to use predecessor models for imitation learning to overcome the issue of only observing expert samples during imitation from expert trajectories.

The paper is very well written. But the proposed method is really not novel. The idea of using predecessor models have already been explored in multiple places [1], [2] (but not in imitation learning scenario!). Hence, the novelty comes from using the predecessor models for imitation learning. The introduction of the paper should mention this  to reflect the contribution. 

[1] Recall Traces: Efficient Backtracking models for efficient RL https://arxiv.org/abs/1804.00379
[2] Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains
https://arxiv.org/abs/1806.04624

Both of these papers should be cited and discussed.

Results: The proposed method outperforms GAIL and behaviour cloning in terms of sample efficiency   on simulation-based manipulation tasks.

Regarding experiments, I would like to see certain baselines.

- What happens when you predict sequentially using predecessor models ? I understand that the sequential generation is prone to accumulating errors, but as [1] points out, using predecessor models you can sample from many states on the expert trajectory. And Hence possible to get good learning signal even while sampling shorter trajectories using predecessor models.

- Comparison with Dyna based methods. For this baseline, authors would learn a forward model. And then sample from the forward model, and use the samples from the forward model for imitation learning.

",5
"This paper studies the problem of matching the state-action distributions of agent and expert demonstrations. In order to address this problem, the authors consider a likelihood treatment comprising a conditional probability (which is estimated from demonstrations) and a state distribution (which is estimated from sampling approximations). 

The authors provide a descent result (i.e., equ. (7)) to estimate the gradient of the logarithmic state distribution. One problem is that it is unclear how the discount factor $\gamma$ influence this result?

In addition, in (12), two scaling factors are used, so how to balance these weights?

Specifically, in (11), it seems the authors are considering the stationary joint state-action distribution, which is different from the state-action distribution generated by the agent on-line, it is suggested to clarify this issue.",6
"This paper explains that range coding as a mechanism for transmitting latent-variable codes from source to target for decoding is severely sensitive to floating point errors.

The authors propose what amounts to an integer version of Balle 2018, and demonstrate that it allows for transmission between platforms without catastrophic errors due to numerical round-off differences.

The paper (and problem) is of low significance, but the authors present a neat solution.

Pros:
- Well defined problem and solution.
- Practical question relating to use of ANNs for data en/de-coding.

Cons:
- Presentation needs brushing up: e.g. why give two examples for H, b, v bit widths?
- Some approximations are not well motivated or justified.  E.g. why is it valid to replace gradient of a function that has 0 gradients with the identity?
- Paper needs some rewriting for clarity.  E.g. where is the kernel K defined?
- Lack of experimentation to justify the fact that the construction of (16) leads to instabilities, and is therefore less suitable than the method outlined here.  

",7
"The paper presents a very important problem of utilizing a model on different platforms with own numerical round-offs. As a result, a model run on a different hardware or software than the one on which it was trained could completely fail due to numerical rounding-off issues. This problem has been considered in various papers, however, the classification task was mainly discussed. In this paper, on the other hand, the authors present how the numerical rounding-off issue could be solved in Latent-Variable Models (LVM).

In order to cope with the numerical rouding-off issue, the authors propose to use integer networks. They consider either quantized ReLU (QReLU) or quantized Tanh (Qtanh). Further, in order to properly train the integer NN, they utilize a bunch of techniques proposed in the past, mainly (Balle, 2018) and (Balle et al., 2018). However, as pointed out in the paper, some methods prevent training instabilities (e.g., Eqs. 18 and 19). All together, the paper tackles very important problem and proposes very interesting solution by bringing different techniques proposed for quantized NNs together .

Pros:
+ The paper is well-written.
+ The considered problem is of great importance and it is rather neglected in the literature.
+ The experiments are properly carried out.
+ The obtained results are impressive.

Cons:
- A natural question is whether the problem could be prevented by post-factum quantization of a neural network. As pointed out in the Discussion section, such procedure failed. However, it would be beneficiary to see an empirical evidence for that.
- It would be also interesting to see how a training process of an integer NN looks like. Since the NN is quantized, instabilities during training might occur. Additionally, its training process may take longer (more epochs) than a training of a standard (float) NN. An exemplary plot presenting a comparison between an integer NN training process and a standard NN training process would be highly appreciated.
- (Minor remark). The paper is well-written, however, it would be helpful to set the final learning algorithm. This would drastically help in reproducibility of the paper.

--REVISION--
After reading the authors response and looking at the new version of the paper I decided to increase my score. The paper tackles very important problem and I strongly believe it should be presented during the conference.",8
"This well-written paper addresses the restrictions imposed by binary communication channels on the deployment of latent variable models in practice. In order to range code the (floating point) latent representations into bit-strings for practical data compression, both the sender and receiver of the binary channel must have identical instances of the prior despite non-deterministic floating point arithmetic across different platforms. The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue.

Pros:
- The problem statement is clear, as well as the approach taken to addressing the issue.
- Section 5 did a nice job tying together the relevant literature on using latent variable models for compression with the proposed integer network framework.
- The experimental results are good; particularly, Table 1 provides a convincing case for how using integer networks remedies the issue of decompression failure across heterogeneous platforms.

Cons:
- In Section 3, it wasn’t clear to me as to why the authors were using their chosen gradient approximations with respect to H’, b’ and c’. Did they try other approximations but empirically find that these worked best? Where did the special rescaling function s come from? Some justifications for their design choices would be appreciated. 
- The authors state in Section 2 that the input scaling is best determined empirically -- is this just a scan over possible values during training? This feels like an added layer of complexity when trying to train these networks. It would be nice if the authors could provide some insight into exactly how much easier/difficult it is to train integer networks as opposed to the standard floating point architectures.
- In Section 6, the authors state that the compromised representational capacity of integer networks can be remedied by increasing the number of filters. This goes back to my previous point, but how does this “larger” integer network compare to standard floating point networks in terms of training time?
",6
"The majority of approaches for preventing posterior collapse in VAEs equipped with powerful decoders to better model local structure involve either: alteration of the ELBO training objective, or a restriction on the decoder structure.

This paper presents an approach which broadly falls into the latter category; by limiting the family of the variational approximation to the posterior, the minimum KL divergence between the prior and posterior is lower bounded to a 'delta' value, preventing collapse.

The paper is well written, and the methodology clearly explained.

The experiments show that the proposed approach (delta VAE combined with the 'anti-causal' architecture) captures both local and global structure, and appears to do so while preserving SOTA discriminative performance on some tasks.  Tests are performed on both generative image and language tasks.

I believe that the paper is of low-medium significance: whilst it does outline a different method of restricting the family of posteriors, it does not give a detailed reasoning (empirical or theoretical) as to why this should be a generally better solution as compared to other approaches.

Pros:
- Very clear and well written.
- Good execution and ablation/experimentation section.

Cons:
- Lack of theory (and minimal experimentation) as to why this approach should be better than competing methods.
",6
"General:
The paper attacks a problem of the posterior collapse that is one of the main issues encountered in deep generative models like VAEs. The idea of the paper relies on introducing a constraint on the family of variational posteriors in such a way that the KL term could be controlled.

The authors propose to use a linear autoregressive process (AR(1)) as the prior. Alternatively, they trained a single-layer LSTM network with conditional-Gaussian outputs as the prior (the auxiliary prior). Additionally, the authors claim that the encoder should contain anti-causal dependencies in order to introduce additional bias that may diminish the posterior collapse.

The experiments present various results on image and text datasets. Interestingly, the proposed techniques allowed to perform on a par with purely autoregressive models, however, the latent variables were utilized (i.e., no posterior collapse). For instance, in Figure 3(a) we can notice that a decoder is capable of generating similar images for given latent variable. A similar situation is obtained for text data (e.g., Figure 12).

In general, I find the paper interesting and I believe it should be discussed during ICLR.

Pros:
+ The paper is well-written and all ideas are clearly presented.
+ The idea of “hard-coded” constraints is interesting and constitutes an alternative approach to utilizing either quantized values in the VAE (VQ-VAE) or a constrained family of variational posteriors (e.g., Hyperspherical VAE).
+ The obtained results are convincing. Additionally, I would like to highlight that at the first glance it might seem that there is no improvement over the autoregressive models. However, the proposed approach allows to encode an image or a document and then decode it. This is not a case for purely autoregressive models.
+ The introduction of the Slow Features into the VAE framework constitutes an interesting direction for future research.

Cons:
- The quality of Figure 4 is too low.
- I am not fully convinced that the auxiliary prior is significantly better than the AR(1) prior. Indeed, the samples seem to be a bit better for the aux. prior but it is rather hard to notice by inspecting quantitative metrics.
- In general, the proposed approach is a specific solution rather than a general framework. Nevertheless, I find it very interesting with a potential for future work.",7
"The paper proposes a method to prevent posterior collapse, which refers to the phenomenon that VAEs with powerful autoregressive decoders tend to ignore the latent code, i.e., the decoder models the data distribution independently of the code. Specifically, the encoder, decoder, and prior distribution families are chosen such that the KL-term in the ELBO is bounded away from 0, meaning that the encoder output cannot perfectly match the prior. Assuming temporal data, the authors employ a 1-step autoregressive (across) prior with an encoder whose codes are independent conditionally on the input. Furthermore, they propose to use a causal decoder together with an anti-causal or non-causal encoder, which translates into a PixelSNAIL/PixelCNN style decoder and an anti-causal version thereof as encoder in the case of image data. The proposed approach is evaluated on CIFAR10, Imagenet 32x32, and the LM1B data set (text).

Pros:

The method obtains state-of-the-art performance in image generation. The paper features extensive ablation experiments and is well-written. Furthermore, it is demonstrated that the code learns an abstract representation by repeatedly sampling form the decoder conditionally on the code.

Cons:

One question that remains is the relative contribution of 1) lower-bounding the KL-term 2) using causal decoder/anti-causal encoder to the overall result. Is the encoder-decoder structure alone enough to prevent posterior collapse? In this context it would also be interesting to see how the encoder-decoder structure performs without \delta-constraint, but with regularization as in \beta-VAE.

What data set are the ablation experiments performed on? As far as I could see this is not specified.

Also, I suggest toning down the claims that the proposed method works ""without altering the ELBO training objective"" in the introduction and conclusion. After all, the encoding and decoding distributions are chosen such that the KL term in the ELBO is lower-bounded by \delta. In other words the authors impose a constraint to the ELBO.

Minor comments:
- Space missing in the first paragraph of p 5: \kappaas
- ""Auxiliary prior""-paragraph on p 5: marginal posterior -> aggregate posterior?",6
"The paper presents and discusses a new phenomenon that infrequent words tend to learn degenerate embeddings. A cosine regularization term is proposed to address this issue.

Pros
1. The degenerate embedding problem is novel and interesting.
2. Some positive empirical results.

Cons and questions
1. The theory in Section 4 suggests that the degeneration problem originates from underfitting; i.e., there's not enough data to fit the embeddings of the infrequent words, when epsilon is small. However, the solution in Section 5 is based on a regularization term. This seems contradictory to me because adding regularization to an underfit model would not make it better. In other words, if there's not enough data to fit the word embeddings, one should feed more data. It seems that a cosine regularization term could only make the embeddings different from each other, but not better.
2. Since this is an underfitting problem (as described in Section 4), I'm wondering what would happen on larger datasets. The claims in the paper could be better substantiated if there are results on larger datasets like WT103 for LM and en-fr for MT. Intuitively, by increasing the amount of total data, the same word gets more data to fit, and thus epsilon gets large enough so that degeneration might not happen.
3. ""Discussion on whether the condition happens in real practice"" below Theorem 2 seems not correct to me. Even when layer normalization is employed and bias is not zero, the convex hull can still contain the origin as long as the length of the bias vector is less than 1. In fact, this condition seems fairly strong, and surely it will not hold ""almost for sure in practice"".
4. The cosine regularization term seems expensive, especially when the vocab size is large. Any results in terms of computational costs? Did you employ tricks to speed it up?
5. What would happen if we only apply the cosine term on infrequent words? An ablation study might make it clear why it improves performance.

UPDATE:
I think the rebuttal addresses some of my concerns. I am especially glad to see improvement on en-fr, too. Thus I raised my score from 5 to 7.",7
"This work proposes a simple regularization term which penalize cosine similarity of word embedding parameters in the loss function. The motivation comes from empirical studies of word embedding parameters in three tasks, translation, word2vec and classification, and showed that the parameters for the translation task are not distributed when compared with other tasks. The problem is hypothesized by the rare word problem especially when parameters are tied for softmax and input embedding, and proposes a cosine similarity regularization. Experiments on English/German show consistent gains over non-regularized loss.

Pros:

-  The proposed method is well motivated from empirical studies by visualizing parameters of three tasks, and the analysis on rare words are convincing.

- Good performance in language modeling and translation tasks by incorporating the proposed regularization.

Cons:

- The visualization might be slightly miss leading in that the size of classification, e.g., the vocabulary size, is different, e.g., BPE for translation, word for word2vec and categories of MNIST. I'd also like to see visualization for comparable experiments, e.g., language modeling with or without tied parameters.

- Given that BPE is used in translation, the analysis might not hold since rare words would not occur very frequently, and thus, the gain might come from other factors, e.g., tied source/target embedding parameters in Transformer.

- I'd like to see experiments under un-tided parameters with the proposed regularization.
",7
"The authors propose a new understanding of word embedding in natural language generation tasks like language model and neural machine translation. 
The paper is clear and original. The experiment results support their argument. 

The problem they raised is quite interesting, however, it is not clear why the representation degeneration problem is important in language generation performance. In Figure 1, the classification is from MNIST, which is much different from words. The authors might want to explain more clearly why the uniformly distributed singular values are helpful in language generation tasks. 
",7
"The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.

- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?
- (minor) I do not think that the size of the search space a very meaningful metric

Pros:
- Good exposition
- Interesting and fairly elegant idea
- Good experimental results

Cons
- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers
- No source code available

Some typos:

- Fo example, when proxy strategy -> Fo*r* example
- normal training in following ways. -> in *the* following ways
- we can then derive optimized compact architecture.",7
"
This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on ""proxy"" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training ""cumbersome"" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation.

Strengths

 + The paper is in general well-written and provides a clear description of the methods.

 + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.)

 + The results achieve state of art while being able to trade off other objectives such as latency

 + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. 

Weaknesses
 
 - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements.

 - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose ""to save time"". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach.

 - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4.

 - There were several typos throughout the paper (""great impact BY automatically designing"", ""Fo example"", ""is build upon"", etc.)

 In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary. ",6
"It seems the authors propose an efficient method to search platform-aware network architecture aiming at high recognition accuracy and low latency. Their results on CIFAR-10 and ImageNet are surprisingly good.  But it is still hard to believe that the author can  achieve 2.08% error rate with only 5.7M parameter on CIFAR10 and 74.5% top-1 accuracy on ImageNet with less GPU hours/memories than prior arts.

Given my concerns above, the author must release their code and detail pipelines since NAS papers are difficult to be reproduced. 

There is a small typo in reference part:
Jing-Dong Dong's work should be DPP-Net instead of PPP-Net (https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper.pdf)
and I think this paper ""Neural Architecture Optimization"" shoud be cited.",6
"This paper proposes that randomly encoding a sentence using a set of pretrained word embeddings is almost as good as using a trained encoder with the same embeddings. This is shown through a variety of tasks where certain tasks perform well with a random encoder and certain ones don't.

The paper is well written and easy to understand and the experiments show interesting findings. There is a good analysis on how the size of the random encoder affects performance which is well motivated by Cover's theorem.

However, the random encoders that are tested in the paper are relatively limited to random projections of the embeddings, a randomly initialized LSTM and an echo state network. Other comparisons would make the results significantly more interesting and would move away from the big assumption stated in the first sentence, i.e. that sentence embeddings are: ""learned non-linear recurrent combinations"". Some major models that are missed by this include paragraph vectors (which do not require any initial training if initialized with pretrained word embeddings), CNNs and Transformers. Given this, the takeaways from this paper seem quite limited to recurrent representations and it's unclear how it would generalize to other representations.

An additional problem is that the paper states that ST-LN used different and older word embeddings which may make the comparison flawed when compared with the random encoders. In this case, the only fairly trained sentence encoder that is compared with is InferSent. The RandLSTM also has an issue in that the biases are intialized around zero whereas it's well known that using an initially higher forget gate bias significantly improves the performance of the LSTM.

Finally, the analysis of the results seems weak. The tasks are very different from each other and no reason or potential explanation is given why certain tasks are better than others with random encoders, except for SOMO and CoordInv. E.g. Could some tasks be solved by looking at keywords or bigrams? Do some tasks intrinsically require longer term dependencies? Do some tasks have more data?

Other comments:
- The results and especially random encoder results should be shown with confidence intervals.
- Section 3.1.3 the text refers to W^r but that does not appear in any equations.

=== After rebuttal ===
Thanks for adding the additional experiments (particularly with fully random embeddings) and result analyses to the paper. I feel that this makes the paper stronger and have raised my score accordingly.",7
"This paper is about exploring better baselines for sentence-vector representations through randomly initialized/untrained networks. I applaud the overall message of this paper that we need to evaluate our models more thoroughly and have better baselines. The experimentation is quite thorough and I like that you
1) explored several different architectures
2) varied the dimensionality of representations
3) examine representations with probing tasks in the Analysis section. 

Main Critique
- In your takeaways you say that, “For some of the benchmark datasets, differences between random and trained encoders are so small that it would probably be best not to use those tasks anymore.” I don’t think this follows from your results. Just because current trained encoders do not perform better than random encoders on these tasks doesn’t in itself mean these tasks aren’t good evaluation tasks. These tasks could be faulty for other reasons, but just because we have no better technique than random encoders currently, doesn’t make these evaluation tasks not worthwhile. Perhaps you could further examine what features (n-gram, etc.) it takes to do well on these tasks in order to argue that they shouldn’t be used.
- In your related work section you say that “We show that a lot of information may be crammed into vectors using randomly parameterized combinations of pre-trained word embeddings: that is, most of the power in modern NLP systems is derived from having high-quality word embeddings, rather than from having better encoders.” Did you run experiments with randomly initialized embeddings? This paper (https://openreview.net/forum?id=ryeNPi0qKX) finds that representations from LSTMs with randomly initialized embeddings can perform quite well on some transfer tasks. I think in order to make such a claim about the power of high-quality word embeddings you should include numbers comparing them to randomly initialized embeddings.

Questions
- Did you find that your results were sensitive to the initialization technique used for your random LSTMs / projections?
- Do you have a sense of why random non-linear features are able to perform well on these tasks? What kind of features are the skip-thought and InferSent representations learning if they do not perform much better? It’s interesting that many of the random encoder methods outperform the trained models on word content. I think you could discuss these Analysis section findings more.

Other Critiques
- In the introduction, instead of simply describing what is commonly done to obtain and evaluate sentence embeddings, it would be better to include a sentence or two about the motivation for sentence embeddings at all.
- The first sentence, “Sentence embeddings are learned non-linear recurrent combinations of pre-trained word embeddings”, doesn’t seem to be true as BOE representations are also sentence embeddings and CNNs/transformers could also work. “Non-linear” and “recurrent” are not inherent requirements for sentence embeddings, but just techniques that researchers commonly use.
- In the second paragraph of introduction instead saying “Natural language processing does not yet have a clear grasp on the relationship between word and sentence embeddings…” it might be better to say “NLP researchers” or the “NLP community” instead of “NLP” as a field doesn’t have a clear grasp.
- In the introduction: “It is unclear how much sentence-encoding architectures improve over the raw word embeddings, and what aspect of such architectures is responsible for any improvement.” It would be also good to mention that it’s unclear how much the training task / procedure also is affects improvements.
- You could describe more about applications of reservoir computing in your related work section as it’s been used in NLP before.
- I don’t think you actually ever describe the type of data that InferSent is trained on, only that it is “expensive” annotated data. It might be useful to add a sentence about natural language inference for clarity.
- In the conclusion, change “performance improvements are less than 1 and less than 2 points on average over the 10 SentEval tasks, respectively” to  “performance improvements are less than 2 percentage points on average over the 10 SentEval tasks, respectively”
- It would be nice if you bolded/underlined the best performing numbers in your results tables.
",7
"This paper tests a number of untrained sentence representation models - based on random embedding projections, randomly-initialized LSTMs, and echo state networks - and compares the outputs of these models against influential trained sentence encoders (SkipThought, InferSent) on transfer and probing tasks. The paper finds that using the trained encoders yields only marginal improvement over the fully untrained models.

I think this is a strong paper, with a valuable contribution. The paper sheds important light on weaknesses of current methods of sentence encoding, as well as weaknesses of the standard evaluations used for sentence representation models - specifically, on currently-available metrics, most of the performance achievements observed in sentence encoders can apparently be accomplished without any encoder training at all, casting doubt on the capacity of these encoders - or existing downstream tasks - to tap into meaningful information about language. The paper establishes stronger and more appropriate baselines for sentence encoders, which I believe will be valuable for assessment of sentence representation models moving forward. 

The paper is clearly written and well-organized, and to my knowledge the contribution is novel. I appreciate the care that has been taken to implement fair and well-controlled comparisons between models. Overall, I am happy with this paper, and I would like to see it accepted. 

Additional comments:

-A useful addition to the reported results would be confidence intervals of some kind, to get a sense of the extent to which the small improvements for the trained encoders are statistically significant.

-I wonder about how the embedding projection method would compare to simply training higher-dimensional word embeddings from the start. Do we expect substantial differences between these two options?",8
"This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network. The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training. Both techniques have been widely used in the literature for similar settings. This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results. 

After reading the authors’ feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. 

In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? 

This paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss? ",7
"Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine. To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.

The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.  The following points out a couple of items that could probably help further improve the paper.

*FW vs BCFW*

The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.

*Batch Size*

Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).

*Convex-Conjugate Loss*

The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss). All convex loss function can derive a dual formulation based on its convex-conjugate. See [1,2] for examples. It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.

[1] Shalev-Shwartz, Shai, and Tong Zhang. ""Stochastic dual coordinate ascent methods for regularized loss minimization."" JMLR (2013)
[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. ""Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation."" JMLR (2011).

*BCFW vs BCD*

Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables. For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost. See the details in, for example, [3, appendix for the multiclass hinge loss case].

[3] Fan, Rong-En, et al. ""LIBLINEAR: A library for large linear classification."" JMLR (2008).

*Hyper-Parameter*

The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD. ",7
"This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function. They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost. Their experimental results showed competitive performance to SGD/Adam on the same network architectures. 

1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function. While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case. An appendix with more numerical comparisons on other loss functions might also be insightful. 
2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2). In Table 2 I saw some optimizers end up with much lower test accuracy. Can the authors show the convergence plots of these methods (similar to Figure 2)?",8
"Summary:
========
The paper presents rates of convergence for estimating nonparametric functions in Besov
spaces using deep NNs with ReLu activations. The authors show that deep Relu networks,
unlike linear smoothers, can achieve minimax optimality. Moreover, they show that in a
restricted class of functions called mixed Besov spaces, there is significantly milder
dependence on dimensionality. Even more interestingly, the Relu network is able to
adapt to the smoothness of the problem.

While I am not too well versed on the background material, my educated guess is that the
results are interesting and relevant, and that the analysis is technically sound.



Detailed Comments:
==================


My main criticism is that the total rate of convergence (estimation error + approximation
error) has not been presented in a transparent way. The estimation error takes the form
of many similar results in nonparametric statistics, but the approximation error is
given in terms of the parameters of the network, which depends opaquely on the dimension
and other smoothness parameters. It is not clear which of these terms dominate, and
consequently, how the parameters W, L etc. should be chosen so as to balance them.


While the mixed Besov spaces enables better bounds, the condition appears quite strong.
In fact, the lower bound is better than for traditional Holder/Sobolev classes. Can you
please comment on how th m-Besov space compares to Holder/Sobolev classes? Also, can
you similiarly define mixed Holder/Sobolev spaces where traditional linear smoothers
might achieve minimax optimal results?


Minor:
- Defn of Holder class: you can make this hold for integral beta if you define m to be
the smallest integer less than beta (e.g. beta=7, m=6). Imo, this is standard in most
texts I have seen.
- The authors claim that the approximation error does not depend on the dimensionality
  needs clarification, since N clearly depends on the dimension. If I understand
  correctly, the approximation error is in fact becoming smaller with d for m-Besov
  spaces (since N is increasing with d), and what the authors meant was that the
  exponential dependnence on d has now been eliminated. Is this correct?

Other
- On page 4, what does the curly arrow notation mean?
- Given the technical nature of the paper, the authors have done a good job with the
  presentation. However, in some places the discussion is very equation driven. For e.g.
  in the 2nd half of page 4, it might help to explain many of the quantities presented in
  plain words.



Confidence: I am reasonably familiar with the nonparametric regression literature, but
not very versed on the deep learning theory literature. I did not read the proofs in
detail.
",8
"This paper makes two contributions:
* First, the authors show that function approximation over Besov spaces for the family of deep ReLU networks of a given architecture provide better approximation rates than linear models with the same number of parameters.
* Second, for this family and this function class they show minimax optimal sample complexity rates for generalization error incurred by optimizing the empirical squared error loss.

Clarity: Very dense; could benefit from considerably more exposition.

Originality: afaik original. Techniques seem to be inspired by a recent paper by Montanelli and Du (2017).

Significance: unclear.

Pros and cons: 
This is a theory paper that focuses solely on approximation properties of deep networks. Since there is no discussion of any learning procedure involved, I would suggest that the use of the phrase ""deep learning"" throughout the paper be revised.

The paper is dense and somewhat inaccessible. Presentation could be improved by adding more exposition and comparisons with existing results.

The generalization bounds in Section 4 are given for an ideal estimator which is probably impossible to compute.",6
"This paper describes approximation and estimation error bounds for functions in Besov spaces using estimators corresponding to deep ReLU networks. The general idea of connecting network parameters such as depth, width, and sparsity to classical function spaces is interesting and could lead to novel insights into how and why these networks work and under what settings. The authors carefully define Besov spaces and related literature, and overall the paper is clearly written. 

Despite these strengths, I'm left with several questions about the results. The most critical is this: piecewise polynomials are members of the Besov spaces of interest, and ReLU networks produce piecewise linear functions. How can piecewise linear approximations of piecewise polynomial functions lead to minimax optimal rates? The authors' analysis is based on cardinal B-spline approximations, which generally makes sense, but it seems like you would need more terms in a superposition of B-splines of order 2 (piecewise linear) than higher orders to approximate a piecewise polynomial to within a given accuracy. The larger number of terms should lead to worse estimation errors, which is contrary to the main result of the paper. I don't see how to reconcile these ideas. 

A second question is about the context of some broad claims, such as that the rates achieved by neural networks cannot be attained by any linear or nonadaptive method. Regarding linear methods, I agree with the author, but I feel like this aspect is given undue emphasis. The key paper cited for rates for linear methods is the Donoho and Johnstone Wavelet Shrinkage paper, in which they clearly show that nonlinear, nonadaptive wavelet shrinkage estimators do indeed achieve minimax rates (within a log factor) for Besov spaces. Given this, how should I interpret claims like ""any linear/non-linear approximator
with fixed N -bases does not achieve the approximation error ... in some parameter settings such as 0 < p < 2 < r ""?
Wavelets provide a fixed N-basis and achieve optimal rates for Besov spaces. Is the constraint on p and r a setting in which wavelet optimality breaks down? If not, then I don't think the claim is correct. If so, then it would be helpful to understand how relevant this regime for p and r is to practical settings (as opposed to being an edge case). 

The work on mixed Besov spaces (e.g. tensor product space of 1-d Besov spaces) is a fine result but not surprising.

A minor note: some of the references are strange, like citing a 2015 paper for minimax rates for Besov spaces that have been known for far longer or a 2003 paper that describes interpolation spaces that were beautifully described in DeVore '98. It would be appropriate to cite these earlier sources. ",6
"The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator’s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo.

The paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results.

Section specific comments:
- The loss of modality of reconstruction loss (section 3.2) seems like something which doesn’t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss.

- The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help.

The paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea.

=== Update after rebuttal:
Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.

The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision.


References:
[1] Kendall, Alex, and Yarin Gal. ""What uncertainties do we need in bayesian deep learning for computer vision?."" Advances in neural information processing systems. 2017.
[2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018.
[3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",4
"The paper describes an alternative to L1/L2 errors (wrt output and one ground-truth example) that are used to augment adversarial losses when training conditional GANs. While these augmented losses are often needed to stabilize and guide GAN training, the authors argue that they also bias the optimization of the generator towards mode collapse. To address this, the method proposes two kinds of alternate losses--both of which essentially generate multiple sample outputs from the same input, fit these with a Gaussian distribution by computing the generating sample mean and variance, and try to maximize the likelihood of the true training output under this distribution. The paper provides theoretical and empirical analysis to show that the proposed approach leads to generators that produce samples that are both diverse and high-quality.

I think this is a good paper and solves an important problem---where one usually had to sacrifice diversity to obtain stable training by adding a reconstruction loss. I recommend acceptance.

An interesting ablation experiment might be to see what happens when one no longer includes the GAN loss and trains only with the MLMM or MCMLE losses, and compare this to training with only the L1/L2 losses. The other thing I'd like the authors to comment on are the potential shortcomings of using a simple un-correlated Gaussian to model the sample distributions. It seems that such a distribution may not capture the fact that multiple dimensions of the output (i.e., multiple pixel intensities) are not independent conditioned on the input. Perhaps, it may be worth exploring whether Gaussians with general co-variance matrices, or independent in some de-correlated space (learned from say simply the set of outputs) may increase the efficacy of these losses.

====Post-rebuttal

I've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly.",8
"This paper analyzes the model collapse problems on training conditional GANs and attribute it to the mismatch between GAN loss and reconstruction loss. This paper also proposes new types of reconstruction loss by measuring higher statistics for better multimodal conditional generation.

Pros:
1.	The analysis in Sec 4.4 is insightful, which partially explains the success of MLMM and MCMLE over previous method in generating diverse conditional outputs.
2.	The paper is well written and easy to follow.

Cons:
Analysis on the experiments is a little insufficient, as shown below.

I have some questions (and suggestions) about experiments. 
1.	How does the training process affected by changing the reconstruction loss (e.g., how the training curve changes?)? Do MLMM and MCMLE converge slower or faster than the original ones? What about training stability? 
2.	Why only MLMM_1 is not compared with other methods on SRGAN-celebA and GLCIC-A? From pix2pix cases it seems that Gaussian MLMM_1 performs much better than MLMM_{1/2}.
",7
"This paper designed a GapNet-PL architecture and applied GapNet-PL, DenseNet, Multi-scale CNN etc. to the protein image (multi-labels) classification dataset.

Pros:

1. The proposed method has a good performance on the given task. Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance.

Cons:

1. The novelty of the proposed architecture is limited. The main contribution of this work is the application of CNN-based methods to the specific biological images.

2. The existing technical challenge of this task is not significant and the motivation of the proposed method could be hardly found in this paper. 

3. The baselines are not convincing enough. Since the performance of Liimatainen et al. is calculated on a different test dataset, the results here are not comparable. The prediction from a human expert, which may vary from individuals, fails to provide a confident performance comparison.

4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited.",4
"The paper proposes a CNN variant tailored for high-resolution
immunofluorescence confocal microscopy data.  The authors show
that the method outperforms a human expert.

The proposed method is evaluated on benchmark instances
distributed by Cyto Challenge '17, which is presumably the best
data source for the target application.  Indeed, the method
performs better than several competitors plus a single human
expert.

The paper is well written and easy to follow.  I could not spot any
major technical issues.

This is an applicative paper targeting a problem that is very
relevant in bioinformatics, but it sports little methodological
innovation.  On the biological side, the contribution looks
significant.  Why not targeting a bioinformatics venue?


Detailed comments:

Papers that stretch multiple fields are always hard to review.  On
one hand, having contributions that cross different fields is a
high-risk (but potentially highly rewarding) route, and I applaud
the authors for taking the risk.  On the other hand, there's the risk
of having unbalanced contributions.

I think that the contribution here is mostly on the bioinformatics
side, not on the deep learning side.  Indeed, the method boils
down to a variant of CNNs.  I am skeptical that this is enough to
spark useful discussion with practitioners of deep learning
(although I could be wrong?).

Finally, I am always skeptical of ""human-level"" performance claims.
These are strong claims that are also hard to substantiate.  I don't
think that comparing to a *single* expert is quite enough.  The fact
that ""the human expert stated that he would be capable to localize
proteins with the provided data"" doesn't sound quite enough.  I
agree that the user study could be biased (and that ""It would be
a tremendous effort to find a completely fair experimental
setting""), but, if this is the case, the argument that the method
reaches human-level performance is brittle.


Other remarks and questions:

- Why wasn't the dataset of Liimatainen et al. used for the
comparison?

- The authors say that ""due to memory restrictions, the smallest
variant of DenseNet was used"".  How much of an impact could have
this had on performance?

- ""One random crop per training sample is extracted in every epoch"".
Doesn't this potentially introduce labeling errors?  Did you observe
this to occur in practice?

- The authors claim that the method is close to perfect in terms
of AUC.  In decision-making applications, the AUC is a very
indirect measure of performance, because it is independent of
any decision threshold.  In other words, the AUC does not measure
the yes/no decisions suggested by the method.  Why is the AUC
important in the biological application at hand?  Why is it important
to the users (biologists, I suppose) of the system?

In particular, ""our method performs nearly perfect, achieving an
average AUC of 98% and an F1 score of 78%"" seems inconsistent
to me---the F1 is indeed ""only"" 78%.

- I would appreciate if there was a thorough discussion of the
failure mode of the expert.  What kind of errors did he/she
make?  How are these cases handled by the model?",5
"This manuscript describes a deep convolutional neural network for
assigning proteins to subcellular compartments on the basis of
microscopy images.

Positive points:

- This is an important, well-studied problem.

- The results appear to improve significantly on the state of the art.

- The experimental comparison is quite extensive, including
  reimplementations of four, competing state-of-the-art methods, and
  lots of details about how the comparisons were carried out.

- The manuscript also includes a human-computer competition, which the
  computer soundly wins.

- The manuscript is written very clearly.

Concerns:

There is not much here in the way of new machine learning methods.
The authors describe a particular neural network architecture
(""GapNet-PL"") and show empirical evidence that it performs well on a
particular dataset.  No claims are made about the generalizability of
the particular model architecture used here to other datasets or other
tasks.

A significant concern is one that is common to much of the deep
learning literature these days, namely, that the manuscript fails to
separate model development from model validation. We are told only
about the final model that the authors propose here, with no
discussion of how the model was arrived at.  The concern here is that,
in all likelihood, the authors had to try various model topologies,
training strategies, etc., before settling on this particular setup.
If all of this was done on the same train/validation/test split, then
there is a risk of overfitting.

The dataset used here is not new; it was the basis for a competition
carried out previously.  It is therefore somewhat strange that the
authors chose to report only the results from their reimplementations
of competing methods.  There is a risk that the authors'
reimplementations involve some suboptimal choices, relative to the
methods used by the originators of those methods.

Another concern is the potential circularity of the labels.  At one
point, we are told that ""Most importantly, these labels have not been
derived from the given microscopy images, but from other
biotechnologies such as microarrays or from literature.""  However,
earlier we are told that the labels come from ""a large battery of
biotechnologies and approaches, such as microarrays, confocal
microscopy, knowledge from literature, bioinformatics predictions and
additional experimental evidence, such as western blots, or small
interfering RNA knockdowns.""  The concern is that, to the extent that
the labels are due to bioinformatics predictions, then we may simply
be learning to re-create some other image processing tool.

The manuscript contains a fair amount of biology jargon (western
blots, small interfering RNA knockdowns, antibodies, Hoechst staining,
etc.) that will not be understandable to a typical ICLR reader.

At the end, I think it would be instructive to show some examples
where the human expert and the network disagreed.

Minor:

p. 2: ""automatic detection of malaria"" -- from images of what?

p. 2: Put a semicolon before ""however"" and a comma after.

p. 2: Change ""Linear Discriminant"" to ""linear discriminant."" Also, remove
the abbreviations (SVM and LDA), since they are never used again in
this manuscript.

p. 5: Delete comma in ""assumption, that.""

p. 8: ""nearly perfect"" -> ""nearly perfectly""

The confusion matrices in Figure 5 should not be row normalized --
just report raw counts.  Also, it would be better to order the classes
so that confusable ones are nearby in the list.
",8
"The authors state nicely and clearly the main contributions they see in their work (Intro, last paragraph). Specifically the state the paper: 1) present a new and important machine learning task, 2) present a family of models that capture the structure of edits and compute efficient representations, 3) create a new source code edit dataset, 4) perform a set of experiments on the learned edit representations and present promising empirical evidence that the models succeed in capturing the semantics of edits. 

We decided to organize this review by commenting on the above-stated contributions one at a time:

“A new and important machine learning task”

Regarding “new task”:

PRO: We are unfamiliar with past work which presents this precise task; the task is new. Section 5 makes a good case for the novelty of this work.

CON: None.


Regarding “important task”:

PRO: The authors motivate the task with tantalizing prospective applications-- automatically editing text and code, e.g. for grammar, clarity, and style. Conceptualizing edits as NLP objects of interest that can be concretely represented, clustered, and used for prediction is an advance.

CON: Many text editors, office suites, and coding IDEs already include features which automatically suggest or apply edits for grammar, clarity, and style. The authors do not describe shortcomings in existing tools that might be better addressed using distributed representations of edits. Consequently, the significance of the proposed contribution is unclear.


“A family of models that capture the structure of edits and compute efficient representations”

Regarding “a family of models”:

PRO: The family of models presented by the authors clearly generalizes: such models may be utilized for computational experiments on datasets and edit types beyond those specifically utilized in this evaluation. The authors apply well-utilized neural network architectures that may be trained and applied to large datasets. The architecture of the neural editor permits evaluation of the degree to which the editor successfully predicts the correct edit given a pre-edit input and a known representation of a similar edit.

CON: The authors do not propose any scheme under which edit representations might be utilized for automatically editing text or code when an edit very similar to the desired edit is not already known and its representation available as input. Hence, we find the authors do not sufficiently motivate the input scheme of their neural editor. The input scheme of the neural editor makes trivial the case in which no edit is needed, as the editor would learn during training that the output x+ should be the same as the input x- when the representation of the “zero edit” is given as input. While the authors discuss the importance of “bottlenecking” the edit encoder so that it does not simply learn to encode the desired output x+, they do not concretely demonstrate that the edit encoder has done otherwise in the final experiments. Related to that: If the authors aimed to actually solve automated edits in text/code then it seems crucial their data contained ""negative examples"" i.e. segments which require no edits. In such an evaluation one would test also when the algorithm introduces unnecessary/erroneous edits. 


Regarding “capture structure of edits”:

PRO: The authors present evidence that edit encoders tightly cluster relatively simple edits which involve adding or removing common tokens. The authors present evidence that relatively simple edits completed automatically by a “fixer” often cluster together, i.e. a known signal is retained in clustering. The authors present evidence that the nearest neighbors of edits in an edit-representation space often are semantically or structurally similar, as judged by human annotators. Section 4.3 includes interesting observations comparing edit patterns better captured by the graph or seq edit encoders. 

CON: The details of the human annotation tasks which generated the numerical results in Tables 1 and 2 are unclear: were unbiased third parties utilized? Were the edits stripped of their source-encoder label when evaluated? Objectively, what separates an “unrelated” from a “similar” edit, and what separates a “similar” from a “same” edit? Did multiple human annotators undertake this task in parallel, and what was their overall concordance (e.g. “intercoder reliability”)? Without concrete answers to these questions, the validity and significance of the DCG/NDCG results reported in Tables 1 and 2 are unclear. It is not clear from the two examples given in Table 1 that the three nearest neighbors embedded by the Seq encoder are “better”, i.e. overall more semantically and/or syntactically similar to the example edit, than those embedded by the Bag of Words model. It is unclear which specific aspects of “edit structure” are better captured by the Seq encoder than the Bag of Words model. The overall structure of Tables 1 and 2 is awkward, with concrete numerical results dominated by a spatially large section containing a small number of examples.


“create a new source code edit dataset”

PRO: The authors create a new source code edit dataset, an important contribution to the study of this new task.

CON: Minor: is the provided dataset large enough to do more than simple experiments? See note below on sample size.


“present promising empirical evidence that the models succeed in capturing the semantics of edits”

PRO: The experiment results show how frequently the end-to-end system successfully predicted the correct edit given a pre-edit input and a known representation of a similar edit. Gold standard accuracies of more than 70%, and averaged transfer learning accuracies of more than 30%, suggest that this system shows promise for capturing the semantics of edits.

CON: Due to concerns expressed above about the model design and evaluation of the edit representations, it remains unclear to what degree the models succeed in capturing the semantics of edits. Table 11 shows dramatic variation in success levels across fixer ID in the transfer learning task, yet the authors do not propose ways their end-to-end system might be adjusted to address areas of weak performance. The authors do not discuss the impact of training set size on their evaluation metrics. The authors do not discuss the degree to which their model training task would scale to larger language datasets such as those needed for the motivating applications.

##############
Based on the authors' response, revisions, and disucssions we have updated the review and the score. ",6
"This paper looks at learning to represent edits for text revisions and code changes. The main contributions are as follows:
* They define a new task of representing and predicting textual and code changes 
* They make available a new dataset of code changes (text edit dataset was already available) with labels of the type of change
* They try simple neural network models that show good performance in representing and predicting the changes

The NLP community has recently defined the problem of predicting atomic edits for text data (Faraqui, et al. EMNLP 2018, cited in the paper), and that is the source of their Wikipedia revision dataset. Although it is an interesting problem, it is not immediately clear from the Introduction of this paper what would be enabled by accurate prediction of atomic edits (i.e. simple insertions and deletions), and I hope the next version would elaborate on the motivation and significance for this new task. 

The ""Fixer"" dataset that they created is interesting. Those edits supposedly make the code better, so modeling those edits could lead to ""better"" code. Having that as labeled data enables a clean and convincing evaluation task of predicting similar edits.

The paper focuses on the novelty of the task and the dataset, so the models are simple variations of the existing bidirectional LSTM and the gated graph neural network. Because much of the input text (or code) does not change, the decoder gets to directly copy parts of the input. For code data, the AST is used instead of flat text of the code. These small changes seem reasonable and work well for this problem.

Evaluation is not easy for this task. For the task of representing the edits, they show visualizations of the clusters of similar edits and conduct a human evaluation to see how similar these edits actually are. This human evaluation is not described in detail, as they do not say how many people rated the similarity, who they were (how they were recruited), how they were instructed, and what the inter-rater agreement was. The edit prediction evaluation is done well, but it is not clear what it means when they say better prediction performance does not necessarily mean it generalizes better. That may be true, but then without another metric for better generalization, one cannot say that better performance means worse generalization. 

Despite these minor issues, the paper contributes significantly novel task, dataset, and results. I believe it will lead to interesting future research in representing text and code changes.",7
"The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 http://aclweb.org/anthology/Q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims.

I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar ""neural editor"" model.

Some more specific points:

- I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are.

- If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this?

- The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set.

- On the human evaluation: Who were the annotators? The categories ""similar edit"", and ""semantically or syntactically same edit"" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits.

- On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another ""soft"" sequence metric?

- It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.",6
"General:
The paper tackles a problem of learning long-range dependencies in images in order to obtain high fidelity images. The authors propose to use a specific architecture that utilizes three main components: (i) a decoder for sliced small images, (ii) a size-upscaling decoder for large image generation, (iii) a depth-upscaling decoder for generating high-res image. The main idea of the approach is slicing a high-res original image and a new factorization of the joint distribution over pixels. In this model various well-known blocks are used like 1D Transformer and Gated PixelCNN. The obtained results are impressive, the generated images are large and contain realistic details.

In my opinion the paper would be interesting for the ICLR audience.

Pros:
+ The paper is very technical but well-written.
+ The obtained results constitute new state-of-the-art on HQ image datasets.
+ Modeling long-range dependencies among pixels is definitely one of the most important topics in image modeling. The proposed approach is a very interesting step towards this direction.

Cons:
- The authors claim that the proposed approach is more memory efficient than other methods. However, I wonder how many parameters the proposed approach requires comparing to others. It would be highly beneficial to have an additional column in Table 1 that would contain number of parameters for each model.
- All samples are take either at an extremely high temperature (i.e., 0.99) or at the temperature equal 1. How do the samples look for smaller temperatures? Sampling at very high temperature is a nice trick for generating nicely looking images, however, it could hide typical problems of generative models (e.g., see Rezende & Viola, “Taming VAEs”, 2018).

--REVISION--
I would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2).",9
"Summary: 
This paper addresses an important problem in density estimation which is to scale the generation to high fidelity images. Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA). This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples). The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos. Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits. The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations). Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation. 

Details:
Major:
-1. Can you point out the total number of parameters in the models? Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)? If not, I am curious to know how many GPUs were used.
0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this.
1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don't think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger.
2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.
3. I didn't quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn't it just normal padding -> space to depth in a structured block-wise fashion? 
4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. 
5. I also don't understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice? 
6. It is really cool that you don't lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation.
7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, ""parameter attention"", conv channels? 

Minor: 
Typo: unpredented --> unprecedented ",1
"Authors propose a decoder arquitecture model named Subscale Pixel Network. It is meant to generate overall images as image slice sequences with memory and computation economy by using a Multidimensional Upscaling method.
The paper is fairly well written and structured, and it seems technically sound.
Experiments are convincing.
Some minor issues:
Figure 2 is not referenced anywhere in the main text.
Figure 5 is referenced in the main text after figure 6.
Even if intuitively understandable, all parameters in equations should be explicitly described (e.g., h,w,H,W in eq.1)",7
"The authors of this paper studied the popular belief that deep neural networks do information compression for supervised tasks. They studied this compression behavior with tanh and ReLU (and it's variants) activation functions which are saturating and non saturating in nature respectively. 

The compression score is computed using Mutual Information Estimation which when computed are usually infinite. For finite mutual information values, noise can be added to hidden activations. For this purpose, two approaches namely Entropy Based Binning(EBAB) and adaptive Kernel Density Estimation(aKDE) were explored. EBAB adds noise to the hidden activations by binning and aKDE by Gaussian noise. Their results show that both EBAB and aKDE exhibit compression in case of ReLU, although this behavior is the strongest in tanh. 

Finally, When compression score was plotted against accuracy, higher rates of compression did not show significant correlation with generalization. Hence showing evidence that generalization(or good performance) can be achieved even without information bottleneck(information compression).

Qualms:
1. Figure 7's description that ELU, Swish and centered softplus functions doing compression is not very apparent. 
2. Figure 9b: Regression line between compression score and accuracy shows a positive correlation between them. This seems contradictory to the inference.
3. The experiments were done on a 5-layer network with 10-7-5-4-3 nodes respectively on a toy data of 12-bit binary vectors. The study could have included bigger networks with popular datasets which would give substantial support to the trend observed on toy data.",7
"This paper proposes a method for the estimation of mutual information for networks with unbounded activation functions and the use of L2 regularization to induce more compression.  The use of information planes to study the training behavior of networks is not new.  This paper addresses the issue of unbounded  hidden state activities.  As the differential mutual information in DNN is ill-defined, the authors proposed to add noise to the hidden activity by using the binning process.   It is not clear in the paper that if the binning is applied just for visualizing the information plane or for computing the activities of hidden units in upper layers.   If it is the latter one, it creates unnecessary distortions to the DNN.  As the authors pointed out, different initializations can lead to different behaviour on the information plane.  It would be difficult to draw conclusions based on the experimental results, even they come from the average of 50 individual networks.  Also, the experiences are performed using a particular task, it is not sure if similar behavior is observed in other tasks.   It is, however, more important to understand what makes the compression.   For the L2 regularization, the compression is expected as the regularization tends to limit the values of the  weights. ",6
"This paper has 3 principal contributions: it proposes a different way of measuring mutual information in a neural network, proposes a compression score to compare different models, then empirically analyses different activation functions and L2 weights.

This work seems like a welcome addition to the IB thread. To me the most interesting result is simply that activation functions aren't simply about gradient flow, and that they may each have properties that are more or less desirable depending on the domain they might be used on. The authors are careful in the wording of their conclusions, I think with reason; while these results are useful in that there seem to be consistently different behaviors coming from different hyperparameters, information planes show a relatively qualitative part of the picture.

Quantitatively, the proposed compression score is interesting, but as the authors say, simplistic. It seems to me that we care more about the converged models than the whole training trajectory; how does this score evolve with time?

I think an important part of discussion that lacks in this paper is a more in-depth take as to how these findings relate to the Zhang et al [1] memorization vs generalization paper and its follow ups. There seem to be many links to be drawn.

This work is overall a good contribution, but I'll have to agree with the authors' conclusion that more principled analysis methods are required to have a solid grasp of the training dynamics of DNNs. The writing of the paper is good, but the writing of the captions could be improved. (the hard page limit of ICLR is 10 pages and your paper has a lot of captions, so I think investing into a bit more text would be good)


Comments:
- It might be worth to re-explain what the information plane plots are in a figure caption, not just in the text (the text also doesn't really explain that each point is a moment in training, and each thread a different layer, this paper should be readable by someone who has never seen these plots before). 
- It's not clear what is going on in figure 5, I can guess but, again, this paper should be readable by anyone in the field. You mention different initializations, but which exactly? What makes you say that 5c has no compression but that 5a does compression first? It should be explained explicitly.
- I believe what you say about Figure 8, but the plots are so similar that it is hard to compare them visually. Maybe a different kind of superposition into a single plot would better illustrate the compression effect of L2?
- Typo in the x axis caption of figures 9.
- Figure 9a is not readable in greyscale (or by a colorblind person), consider using a different symbol for the softmax scatter (and adding this symbol to the legend).
- The first Schmidhuber citation of the paper seems a bit out of place. I think he himself would say that deep learning has been going on for much longer than since 2015. (in fact I think you could just remove the entire first paragraph, it is just unnecessary boilerplate)
- Why should there be a direct correlation between compression and generalization? For example, it is known that training DNNs with soft targets improves test accuracy in classification, or even forcing softness in both targets and representations [2] also improves test accuracy.
- I'm still personally not sold on binning as a strategy to evaluate MI. Did you perform experiments that show that the observed difference is consistent if more computation is done to approximate MI, and not just an artefact of max-entropy binning?

[1] Zhang et al (2016) https://arxiv.org/abs/1611.03530
[2] Verma et al (2018) https://arxiv.org/abs/1806.05236
",7
"The paper presents a new model FlowQA for conversation reading comprehension. Compared with the previous work on single-turn reading comprehension, the idea in this paper differs primarily in that it alternates between the context integration and the question flow in parallel. The parallelism enables the model to be trained 5 to 10 times faster. Then this process is formulated as layers of a neural network that are further stacked multiple times. Besides, the unanswerable question is predicted with additional trainable parameters. Empirical studies confirm FlowQA works well on a bunch of datasets. For example, it achieves new state-of-the-art results on two QA datasets, i.e., CoQA and QuAC, and outperforms the best models on all domains in SCONE. Ablation studies also indicates the importance of the concept Flow.

Although the idea in the paper is straightforward (it is not difficult to derive the model based on the previous works), this work is by far the first that achieves nontrivial improvement over CoQA and QuAC. Hence I think it should be accepted.

Can you conduct ablation studies on the number of Reasoning layers (Figure 3) in FlowQA? I am quite curious if a deeper/shallower model would help.",7
"The paper proposes a method to model the flow of context in multi-turn machine comprehension (MC) tasks. The proposed model achieves amazing improvements in the two recent conversational MC tasks as well as an instruction understanding task. I am very impressed by the improvements and the ablation test that actually shows the effectiveness of the FLOW mechanism they proposed.

However, this paper has a lack of clarity (especially, Section 3) which makes it difficult to follow and easy to lose the major contribution points of the work. I summarized the weaknesses as follows:

# lack of motivation and its validation
The paper should have more motivational questions at the beginning of why such flow information is necessary for the task. Authors already mentioned about some of it in Figure 1 and here: “such as phrases and facts in the context, for answering the previous questions, and hence provide additional clues on what the current conversation is revolving around”. However, the improvement of absolute scores in the Experiment section didn’t provide anything related to the motivation they mentioned. Have you actually found the real examples in the testing set that are correctly predicted by the FLOW model but not by the baseline? Are they actually referring to the “phrases and facts in the context”, “additional clues on what the current conversation is revolving around”? Another simple test authors can try is to show the attention between the context in a flow and question and see whether appropriate clues are actually activated given the question. 

# unclear definition of “flow”
The term “flow” is actually little over-toned in my opinion. Initially, I thought that flow is a sequence of latent information in a dialog (i.e., question-answer) but it turns to be a sequence of the context of the passage. The term “flow” is more likely a sequence of latent and hierarchical movement of the information in my opinion. What is your exact definition of “flow” here? Do you believe that the proposed architecture (i.e., RNN sequence of context) appropriately take into account that? RNN sequence of the passage context actually means your attention over the passage given the question in turn, right? If yes, it shouldn’t be called a flow. 

# Lack of clarity in Section 3
Different points of contributions are mixed together in Section 3 by themselves or with other techniques proposed by others. For example, the authors mention the computational efficiency of their alternating structure in Figure 2 compared to sequential implementation. However, none of the experiment validates its efficiency. If the computational efficiency is not your major point, Figure 2 and 3 are actually unnecessary but rather they should be briefly mentioned in the implementation details in the later section. Also, are Figure 2 and 3 really necessary? 

Section 3.1 and 3.3.1 are indeed very difficult to parse: This is mainly because authors like to introduce a new concept of “flow” but actually, it’s nothing more than a thread of a context in dialog turns. This makes the whole points very hyped up and over-toned like proposing a new “concept”. Also, the section introduces so many new terms (“context integration”. “Flow”, “integration layers”, “conversational flow”, “integration-flow”) without clear definition and example. The name itself looks not intuitive to me, too. I highly recommend authors provide a high-level description of the “flow” mechanism at first and then describe why/how it works without any technical terms. If you can provide a single example where “flow” can help with, it would be nicer to follow it.

# Some questions on the experiment
The FLOW method seems to have much more computation than single-turn baselines (i.e., BiDAF). Any comparison on computational cost?

In Table 3, most of the improvements for QuAC come from the encoding N answer spans to the context embeddings (N-ans). Did you also compare with (Yatskar, 2018) with the same setting as N-ans? 

I would be curious to see for each context representation (c), which of the feature(e.g., c, em, g) affect the improvement the most? Any ablation on this?

The major and the most contribution of the model is probably the RNN of the context representations and concatenation of the context and question at turn in Equation (4). For example, have you tested whether simple entity matching or coreference links over the question thread can help the task in some sense? 

Lastly for the model design, which part of the proposed method could be general enough to other tasks? Is the proposed method task-specific so only applicable to conversational MC tasks or restricted sequential semantic parsing tasks? 
",6
"In this paper, authors proposed a so-called FLOWQA for conversational question answering (CoQA). Comparing with machine reading comprehension (MRC),  CoQA includes a conversation history. Thus, FLOWQA makes use of this property of CoQA and adds an additional encoder to handle this. It also includes one classifier to handle with no-answerable questions.

Pros:
The idea is pretty straightforward which makes use of the unique property of CoQA.

Results are strong, e.g., +7.2 improvement over current state-of-the-art on the CoQA dataset. 

The paper is well written.

Cons:
It is lack of detailed analysis how the conversation history affects results and what types of questions the proposed model are handled well.

Limited novelty. The model is very similar to FusionNet (Huang et al, 2018) with an extra history encoder and a no-answerable classifier. 

Questions:
One of simple baseline is to treat this as a MRC task by combining the conversation history with documents. Do you have this result?

The model uses the full history. Have you tried partial history? What's the performance? 
",7
"Summary:
This paper discusses an advance in the framework of normalizing flows for generative modeling, named FFJORD. The authors consider normalizing flows in the form of ordinary differential equations, as also discussed in [1]. Their contributions are two-fold: (1) they use an unbiased estimator of the likelihood of the model by approximating the trace of the jacobian with Hutchinson’s trace estimator, (2) they have implemented the required ODE solvers on GPUs. 

The models are evaluated on a density estimation task on tabular data and two image datasets (MNIST and CIFAR10), as well as on variational inference for auto-encoders, where the datasets MNIST, Omniglot, Freyfaces and Caltech Silhouettes are considered. 

The authors argue that the trace estimator, in combination with reverse-mode automatic differentiation to compute vector-Jacobian products, leads to a computational cost of O(D), instead of O(D^2) for the exact trace of the jacobian. 
They compare this to the cost of computing a Jacobian determinant for finite flows, which is O(D^3) in general. They argue that in general all works on finite flows have adjusted their architectures for the flows to avoid the O(D^3) complexity, and that FFJORD has no such restriction.
However, I would like the authors to comment on the following train of thought: autoregressive models, such as MAF, as well as IAF (inverse of an autoregressive model) do not require O(D^3) to compute jacobian determinants as the jacobian is of triangular form. Note however, they are still universal approximators if sufficient flows are applied, as any distribution can be factorized in an autoregressive manner. With this in mind, I find the red cross for MAF under free-form Jacobian slightly misleading. Perhaps I misunderstood something, so please clarify. 

Another topic that I would like the authors to comment on is efficiency and practical use. One of the main points that the authors seem to emphasise, is that contrary to autoregressive models, which require D passes through the model to sample a datapoint of size D, FFJORD is a ‘single-pass’ model, requiring only one pass through the model. They therefore indicate that they can do efficient sampling. However, for FFJORD every forward pass requires a pass through an ODE solver, which as the authors also state, can be very slow. I could imagine that this is still faster than an autoregressive model, but I doubt this is actually of comparable speed to a forward pass of a finite flow such as glow or realNVP. 
On the other hand, autoregressive models do not require D passes during training, whereas, if I understand correctly, FFJORD relies on two passes through ODE solvers, one for computing the loss, and a second to compute the gradient of the loss with respect to model parameters. So autoregressive models should train considerably faster. The authors do comment on the fact that FFJORD is slower than other models, but they do not give a hint as to how much slower it is. This would be of importance for practical use, and for other people to consider using FFJORD in future work. 

For the density estimation task, FFJORD does not have the best performance compared other baselines, except for MNIST, for which the overall best model was not evaluated (MAF-DDSF). For variational inference FFJORD is stated to outperform all other flows, but the models are only evaluated on the negative evidence lower bound, and not on the negative log-likehood (NLL). I suspect the NLL to be absent from the paper as it requires more computation, and this takes a long time for FFJORD. Without an evaluation on NLL the improvement over other methods is questionable. Even if the improvement still holds for the NLL, the relative improvement might not weigh heavily enough against increased runtime. FFJORD does require less memory than its competitors.

The improved runtime by implementing the ODE solvers on GPU versus the runtime on a CPU would be useful, given that this is listed as one of the main contributions.

Besides these questions/comments, I do think the idea of using Hutchinsons trace estimator is a valid contribution, and the experimental validation of continuous normalizing flows is of interest to the research community. Therefore, in my opinion, the community will benefit from the information in this paper, and it should be accepted. However I do wish for the authors to address the above questions as it would give a clearer view of the practical use of the proposed model. 
 
See below for comments and questions:

Quality
The paper has a good setup, and is well structured. The scope and limitations section is very much appreciated. 

Clarity
The paper is clearly written overall. The only section I can comment on is the related work section, which is not the best part of the paper. The division in normalizing flows and partitioned transformations is a bit odd. Partitioned transformations surely are also normalizing flows. Furthermore IAF by Kingma et al. is put in the box of autoregressive models, whereas it is the inverse of an autoregressive model, such that it does not have the D-pass sample problem. For a reader who is not too familiar with normalizing flows literature, I think this section is a little confusing. Furthermore, there is no related work discussed on continuous time flows, such as (but not limited to) [2].

Originality
The originality of the paper is not stellar, but sufficient for acceptance. 

Significance
The community can benefit from the experimental analysis of continuous time flows, and the GPU implementation of the ODE solver. Therefore I think this work is significant. 

Detailed questions/comments:

1. In section 4.2, as an additional downside to MAF-DDSF, the authors argue that sampling cannot be performed analytically. Since FFJORD needs to numerically propagate the ODE, I do not think FFJORD can sample analytically either. Is this correct?
2. The authors argue that they have no restriction on the architecture of the function f, even if they have O(D) estimation of the trace of the jacobian. However, they also say they make use of the bottle-neck trick to reduce the variance that arises due to Hutchinson’s estimate of the trace. This seems like a limitation on the architecture to me. Can the authors comment?
3. In B.1 in the appendix, the street view house numbers dataset is mentioned, but no results appear in the main text, why not?
4. In the results section, it is not clear to me which numbers of the baselines for different datasets are taken from other papers, and which numbers are obtained by the authors of this paper. Please clarify.
5. In the conclusions, when discussing future work, the authors state that they are interested in reducing the number of function evaluations in the ODE solvers. In various disciplines many people have worked on this problem for a long time. Do the authors think major improvements are soon to be made?
6. In section 5.2 the dependence of the number of function evaluations (NFE) on the data dimension D is discussed. As a thought experiment they use the fact that going from an isotropic gaussian distribution (in any D), to an isotropic gaussian distribution has a corresponding differential equation of zero. This should convince the reader that NFE is independent of D. However, this seems to me to be such a singular example, that I gain no insight from it, and it is not very convincing. Do the authors agree that this particular example does not add much? If not, please explain. 

[1] Chen et al. Neural ordinary differential equations. NIPS 2018
[2] Chen et al. Continuous-time flows for deep generative models.

**** EDIT *****

I have read the response of the authors and appreciate their clarifications and the additional information on the runtimes. See my response below for the concern that remains about the absence of the estimate of the log likelihood for the VAE experiments. Besides this issue, the other comments/answers were satisfactory, and I think this paper is of interest to the research community, so I will stick with my score.

",7
"This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.
The contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. 
The density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. 
Although the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.
Concerning discussions and analysis:
- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?
In algorithm 1, is \epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator.",7
"This paper discusses a technique for continuous normalization flow in which the transformations are not required to be volume preserving (the transformation with unity Jacobian), and architecture of neural network does not need to be designed to hold such property. Instead authors proposed no restriction on architecture of neural network to design their reversible mapping.
The Paper has good background and literature review, and as authors mentioned  this paper is base on the idea of  Chen, Tian Qi, et al. ""Neural Ordinary Differential Equations."" arXiv preprint arXiv:1806.07366 (2018). Chapter two of this paper is summary of  ""Neural Ordinary Differential Equations."" and chapter Three is main contribution of this paper that can be summarized under two points:

1- Authors borrowed the ""continuous normalizing flow "" in Chen et al. and they have designed unbiased log density estimator using Hutchinson trace estimator and evaluated the trace with complexity of O(D) (dimension of data) instead of O(D^2) that  is used in chen et al. Paper

2- They proposed by reducing the hidden layer dimension of neural network, it is possible that variance of estimator to be reduced 

Novelty and Quality:
the main contribution of this paper is summarized above.
The paper do not contain any significant theorem or mathematical claims, it is more focused on design of linear algorithm that estimate continuous normalizing flow that they have borrowed from the Chen et al. paper.  This is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. Also no guarantees or bound has been given about the variance reduction of  estimator and it is more based on the authors intuition.

Clarity:
The paper is well written and previous relevant methods have been reviewed well. There are a few issues that are listed below:
1-in section 3 the reason that dimensionality of estimator can reduce to D from D^2 can be explained more clearly 

2- Figure 1 is located on first page of the paper but it has never been referred in main paper, just it is mentioned once in appendix , it can be moved to appendix.

3- in section 3.1.1 the “view view” can be changed to “view”

significance and experiments:
The experiments are very detailed and extensive and authors have compared their algorithm with many other competing algorithms and showed improvement in many of the cases. 
As mentioned in Quality and Novelty part of the review, just one comparison is missing and that is the comparison to method that the paper is inspired by. It would be interesting to see how much trace estimator approach that has been used in this paper, would sacrifice the negative log-likelihood or ELBO specially in real data like MNIST and CIFAR 10.  it seems original paper has not reported the performance on those data-sets as well, is this difficult as chen et. al. paper algorithm for trace calculation has complexity of O(D^2)? 
",7
"This paper proposes a Self-Modulation framework for the generator network in GANs, where middle layers are directly modulated as a function of the generator input z.
Specifically, the method is derived via batch normalization (BN), i.e. the learnable scale and shift parameters in BN are assumed to depend on z, through a small one-hidden layer MLP. This idea is something new, although quite straight-forward.
Extensive experiments with varying losses, architectures, hyperparameter settings are conducted to show self-modulation improves baseline GAN performance.

The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method. It is still not clear why self-modulation stabilizes the generator towards small conditioning values.

The paper presents two loss functions at the beginning of section 3.1 - the non-saturating loss and the hinge loss. It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1]. It seems that the authors are not aware of this difference.

In addition to report the median scores, standard deviations should be reported.

===========  comments after reading response ===========

I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected. 

Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.",5
"Summary:
The manuscript proposes a modification of generators in GANs which improves performance under two popular metrics for multiple architectures, loss, benchmarks, regularizers, and hyperparameter settings. Using the conditional batch normalization mechanism, the input noise vector is allowed to modulate layers of the generator. As this modulation only depends on the noise vector, this technique does not require additional annotations. In addition to the extensive experimentation on different settings showing performance improvements, the authors also present an ablation study, that shows the impact of the method when applied to different layers.

Strengths:
- The idea is simple. The experimentation is extensive and results are convincing in that they show a clear improvement in performance using the method in a large majority of settings.
- I also like the ablation study showing the impact of the method applied at different layers.

Requests for clarification/additional information:
- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?
- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated). It seems modulation on layer 4 comes in as a close second. I am curious about why that might be.
- I would like to see some more interpretation on why this method works.
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?

Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).",7
"The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance. The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z. Performance is measured in terms of Fréchet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet). The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings. An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.

I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation. Here are the questions I would like the authors to discuss further:

- The proposed approach is a fairly specific form of self-modulation. In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks. In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves. This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator. Can you clarify how you view the relationship between the approaches mentioned above?
- It’s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the “information” contained in the noise vector to better propagate to and influence different parts of the generator. ResNets also have this ability to “propagate” the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial. I’m curious to hear the authors’ thoughts in this.
- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?
",7
"This paper proposes a novel method to speedup softmax computation at test time. Their approach is to partition the large vocabulary set into several discrete clusters, select the cluster first, and then do a small scale exact softmax in the selected cluster. Training is done by utilizing the Gumbel softmax trick.

Pros:
1. The method provides another way that allows the model to learn an adaptive clustering of vocabulary. And the whole model is made differentiable by the Gumbel softmax trick. 
2. The experimental results, in terms of precision, is quite strong. The proposed method is significantly better than baseline methods, which is a really exciting thing to see. 
3. The paper is written clearly and the method is simple and easily understandable. 
Cons:
1. I’d be really expecting to see how the model will perform if it is trained from scratch in NMT tasks. And I have reasons for this. Since the model is proposed for large vocabularies, the vocabulary of PTB (10K) is by no terms large. However, the vocabulary size in NMT could easily reach 30K, which would be a more suitable testbed for showing the advantage of the proposed method.  
2. Apart from the nice precision results, the performance margin in terms of perplexity seems not as big as that of precision. And according to earlier discussions in the thread, the author confirmed that they are comparing the precision w.r.t. original softmax, not the true next words. This could raise a possible assumption that the model doesn’t really get the probabilities correct, but somehow only fits on the rank of the words that was predicted by the original softmax. Maybe that is related to the loss? However, I believe sorting this problem out is kind of beyond the scope of this paper.  
3. In another scenario, I think adding some qualitative analysis could better present the work. For example, visualize the words that got clustered into the same cluster, etc. 

In general, I am satisfied with the content and enjoys reading the paper. 
",7
"This paper presents an approximation to the softmax function to reduce the computational cost at inference time and the proposed approach is evaluated on language modeling and machine translation tasks. The main idea of the proposed approach is to pick a subset of the most probable outputs on which exact softmax is performed to sample top-k targets. The proposed method, namely Learning to Screen (L2S), learns jointly context vector clustering and candidate subsets in an end-to-end fashion, so that it enables to achieve competitive performance.

The authors carried out NMT experiments over the vocabulary size of 25K. It would be interesting if the authors provide a result on speed-up of L2S over full softmax with respect to the vocabulary size. Also, the performance of L2S on larger vocabularies such as 80K or 100K needs to be discussed.

Any quantitative examples regarding the clustering parameters and label sets would be helpful.
L2S is designed to learn to screen a few words, but no example of the screening part is provided in the paper.",6
"The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. 

* pros: 
- the paper is well written. 
- the idea is simple but BRILLIANT. 
- the used techniques are good (especially to learn word clusters). 
- the experimental results  (speed up softmax at test time) are impressive. 

* cons: 
- the model is not end-to-end because word clusters are not continuous. But it not an important factor. 
- it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time.
- it would be better if the authors show some clusters for both input examples and corresponding word clusters.


",8
"GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications. Unfortunately, GANs often show unstable behaviour during the training phase. The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.

While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:
1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.
4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.

While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.

---

After paper revisions: 

Thank you for the updates. The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community. ",6
"Summary:
This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. The paper motivates the proposed method as follows:
-       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution
-       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity*
-       Then, the paper proves that this condition can be satisfied by ensuring that generator’s objective (L) is concave
-       Since it’s difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective
Experiments confirm the validity the proposed approach. Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets
 
Strengths:
-   	The proposed method is very interesting and is based on sound theory
-   	Connection to optimal transport theory is also interesting
-   	In practice, the method is very simple to implement and seems to produce good results
 
Weaknesses:
-       Readability of the paper can be generally improved. I had to go over the paper many times to get the idea.
-       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines).
 
Questions/Comments:
-       Equation (7) has typos (uses theta_old instead of theta in some places)
-       Section 4.1 (effect of monoticity) is a bit confusing. My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there. Can you clarify what you do there and in general this experiment a bit more?
-       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I’m wondering if you can compare it on low dimensional settings (e.g. as in Fig 2). It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1.

 
Overall recommendation:
The paper is based on sound theory and provides very interesting perspective. The method seems to work in practice on a variety of experimental setting. Therefore, I recommend accepting it.
",7
"The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself. Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets. I found that this is an interesting paper, both original ideal and numerical results.",8
"In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.

The paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  

",7
"In a distributed learning system where a parameter server maintains a full resolution copy of the parameters, communication costs can be reduced by (a) discretizing the weights that the server broadcasts to the workers, and (b) discretizing the gradients that the workers return to the parameter server. Following existing literature, the authors propose to discretize the parameters in a manner that limits its impact on the loss function by means of a diagonal approximation of the Hessian. This also means that one can bound the difference between the gradient for the full precision parameter and the gradient for the discretized parameter.  In contrast, they discretize the gradients stochastically so that the discretized version is an unbiased estimator of the full precision stochastic gradient. Since the stochastic gradient is itself an unbiased estimator of the gradient, this means we are dealing with an estimator whose variance has increased in a manner we can bound as well. The theoretical analysis consists in pushing these two bounds through classical analyses of the stochastic gradient algorithm, in this case, a regret-based version in the style of Zinkevich or Duchi.  Although i did not check the minute details of the proof, the argument feels correct and familiar.  They also give an interesting result in favor of clipping gradients, worth developing.

Although the title promises an analysis that holds for deep networks, this analysis strictly applies only to convex models. The author argue that the predictions made by this analysis also apply to deep networks, and support this argument with extensive experiments (which certainly represent a fair amount of work).  This result is believable but should not be construed as an analysis. Nevertheless, both results (the theoretical result for convex model and the empirical result for deep networks) are interesting and worth sharing.

The main caveat comes from the style the parallel learning algorithm they are considering.  In the data-parallel case (which they consider), parameter servers approaches have been displaced by setups where all workers update their copy of the weights using the allReduced gradients.  One could also use discretized gradients to speedup the allReduce operation (this is less of a win because latencies dominate) but this would only result in an increased variance and a much simpler analysis.

Finally I am not completely up-to-date with this line of work and cannot evaluate the novelty with confidence. This was not known to me, which is only a piece of evidence.

-- bumping down my score because the misleading title was not addressed by the author response.
-- bumping it up again because the authors have reacted.
",7
"
Summary:

This paper studies the convergence properties of loss-aware weight quantization with different gradient precisions in the distributed environment, in which servers keeps the full-precision weights and workers keeps quantized weights. The authors provided convergence analysis for weight quantization with full-precision, quantized and quantized clipped gradients. Specifically, they find that: 1) the regret of loss-aware weight quantization with full-precision gradient converge to an error related to the weight quantization resolution and dimension d. 2) gradient quantization slows the convergence by a factor related to gradient quantization resolution and dimension d. 3) gradient clipping renders the speed degradation dimension-free. 

Comments:

Pros:

- The paper is generally well written and organized. The notation is clean and consistent. Detailed proofs can be found in the appendix, the reader can appreciate the main results without getting lost in details.

- The paper provides theoretical analysis for the convergence properties of loss-aware weight quantization with full-precision gradients, quantized gradient and clipped quantized gradient, which extends existing analysis beyond full-precision gradients, which could be useful for distributed training with limited bandwidth. 

Cons:

- It is unclear what problems the authors try to solve. The problem is about gradient compression, or how the gradient precision will affect the convergence for training quantized nets in the distributed environment, in which workers have limited computation power and the network bandwidth is limited. It is an interesting setting, however, the author does not make it clear the questions they are asking and how the theoretical results can guide the practical algorithm design. 

- The authors mentioned that quantized gradient slows convergence (relative to using full-precision gradient) in contribution 2 while also claims that quantizing gradients can significantly speed up training of quantized weights in contribution 4, which is contradictory to each other.

- It is not clear what relaxation was made on the assumptions of f_t in section 3.1. The analysis are still based on three common assumptions: 1) f_t is convex 2) f_t is twice differentiable 3) f_t has bounded gradients. The assumptions and theoretical results may not hold for non-convex deep nets. E.g., the author does not valides the theorems results on d with neural networks but only with linear models in section 4.1.

- The author demonstrate training quantized nets in the distributed environment with quantized gradients, however, no comparison is made with other related works (e.g., Wen et al, 2017). 

Questions: 

- Theorem 1 is an analysis for training with quantized weights and full-precision gradients, which is essentially the same setting as BinaryConnect. Similar analysis has been done in Li et al, 2017. What is the difference or connection with their bound?

- It is not clear how gradienta are calculated w.r.t. quantized weights on worker, is straight through estimator (STE) used for backpropagation through Q_w?

- In section 3.3, why is \tilde{g}_t stochastically quantized gradient? How about statiscally quantized gradients?

- Why do the authors use linear model in section 4.1? Why are the solid lines in Figure 3 finished earlier than dashed lines? For neural networks, a common observation is that the larger the dimension d, the better the generalization performance. However, Figure 3 and Theorem 1 seem to be contradictory to this common belief. Would it possible to verify the theorem on deep nets of different dimension?

- Why does the number of worker affect the performance? I failed to see why the number of workers affect the performance of training if it is a synchronized distributed training with the same total batch size. After checking appendix C, I think it is better to discuss the influence of batch sizes rather than the number of workers.

- Why is zero weight decay used for CIFAR-10 experiment but non-zero weight decay for imagenet experiment? How was weight decay applied in Adam for quantized weights? 

Minor issues: 
- The notation of full-precision gradient w.r.t quantized weights in Figure 1 should be \hat{g}_t, however, g_t is used.
",6
"Summary
------

The authors proposes an analysis of the effect of simultaneously quantizing the weights and gradients in training a parametrized model in a fully-synchronized distributed environment, using RMSProp training updates.

The authors provide a theoretical analysis in term of regret bound, when the objective functions are smooth, convex and gradient-bounded wrt the parameter. They also assume that the parameters remains in a compact space. Their conclusions are as follow (thm 1, 2 and 3):

- weight quantization, which is deterministic and therefore introduces a bias in the objective functions, introduces a non-vanishing term in the average reget, that depens on the quantization error, where the vanishing term decreases in O(d /sqrt(T)).

- gradient quantization, which is performed in a stochastic, unbiased way (wrt to the full-precision gradient) do not introduce a further non-vanishing term, but augments the constant factor in the vanishing term.

- gradient clipping onto gradient quantization reduced this constant factor, at the cost of ntroducing a further non-vanishing term in the average regret.

An experimental setting is performed to assess how much the theoretical conclusions derived ina simpe setting apply to predictive functions parametrized with neural-network. The experiments are three folded:
- a first toy experiment with convex objective validates the theoretical findings
- a second experiment performed on CIFAR assess the performance on a grid of weight/gradient quantization with or without gradient clipping
- a third experiement, that is profiled (synthetically) assesses the performance of wieght/gradient quantization when training a model on imagenet.

In conclusion, the authors observe that quantizing weight/gradients systematically lead to a slight decrease in performance but provides promising improvement in term of training speed

Review
------

The paper is well written, documented and well-sectioned, with well written theoretical guarantees and thorough experiments, including one on a large dataset. The theoretical guarantees are relatively non-surprising and their proofs are indeed little involved. The authors are yet the first to analyse the effect of biased weight quantization on one hand, and of gradient clipping on the other hand.

The reviewer would have appreciated further comparison with existing analysis, in particular a comparison between stochastic weight quantization and loss-aware deterministic weight quantization. The bias introduced by the latter seems the culprit in the reduction of predictive performance. What if we applied non-biased weight quantization, with stochastic quantized gradient ?

The experiments as presented are a little underwhelming: first of all, there is no report of training time on ImageNet, and I believe that the profiling as been made in a communication model and not in a real setting. It would be great to see the best training time that you achieve by weight/gradient quantization (say on 4 bits).

Moreover, it appears that even with 4 bit quantization, the test accuracy of the trained model is significantly reduced. Why not increase the size to say 6 or 8 bits ? 

On a related aspect, can the communication quantization be used jointly with a forward/backward quantized evalution ?

Overall, although this paper is relatively incremental and has underwhelming experiments, it is a thorough work that is worthy of being presented at ICLR 2019, in the reviewer's opinion.

Minor
-----

p 2: the notation w_i is overloaded

Eq 1: S_w^d should read (S_w)^d (cartesian product)

Thm 3: the notation R() is overloaded

Figure 1 is very hard to read: increase the font size

Figure 3 4 6: increase the legend size, ensure that the color used vary in lightness for printing

Table 1: use bold font to indicate the best performing FP/FP model, and your best performing model

Fig 7 c: training curve
",7
"This paper formulates the black-box adversarial attack as a gradient estimation
problem, and provide some theoretical analysis to show the optimality of an
existing gradient estimation method (Neural Evolution Strategies) for black-box
attacks.

This paper also proposes two additional methods to reduce the number of queries
in black-box attack, by exploiting the spacial and temporal correlations in
gradients. They consider these techniques as priors to gradients, and a bandit
optimization based method is proposed to update these priors. 

The ideas used in this paper are not entirely new. For example, the main
gradient estimation method is the same as NES (Ilyas et al. 2017);
data-dependent priors using spatially local similarities was used in Chen et
al. 2017.  But I have no concern with this and the nice thing of this paper is 
to put these tricks under an unified theoretical framework, which I really 
appreciate.

Experiments on black-box attacks to Inception-v3 model show that the proposed
bandit based attack can significantly reduces the number of queries (2-4 times
fewer) when compared with NES. 

Overall, the paper is well written and ideas are well presented.
I have a few concerns:

1) In Figure 2, the authors show that there are strong correlations between the
gradients of current and previous steps. Such correlation heavily depends on
the selection of step size.  Imagine that the step size is sufficiently large,
such that when we arrive at a new point for the next iteration, the
optimization landscape is sufficiently changed and the new gradient is vastly
different than the previous one. On the other hand, when using a very small
step-size close to 0, gradients between consecutive steps will be almost the
same. By changing step-size I can show any degree of correlation.  I am not
sure if the improvement of Bandit_T comes from a specific selection of
step-size. More empirical evidence on this need to be shown - for example, run
Bandit_T and NES with different step sizes and observe the number of queries
required.

2) This paper did not compare with many other recent works which claim to
reduce query numbers significantly in black-box attack. For example, [1]
proposes ""random feature grouping"" and use PCA for reducing queries, and [2]
uses a good gradient estimator with autoencoder. I believe the proposed method
can beat them, but the authors should include at least one more baseline to 
convince the readers that the proposed method is indeed a state-of-the-art.

3) Additionally, the results in this paper are only shown on a single model
(Inception-v3), and it is hard to compare the results directly with many other
recent works. I suggest adding at least two more models for comparison (most
black-box attack papers also include MNIST and CIFAR, which should be easy to
add quickly). These numbers can be put in appendix.

Overall, this is a great paper, offering good insights on black-box adversarial
attack and provide some interesting theoretical analysis. However currently it
is still missing some important experimental results as mentioned above, and
not ready to be published as a high quality conference paper. I conditionally
accept this paper as long as sufficient experiments can be added during the
discussion period.


[1] Exploring the Space of Black-box Attacks on Deep Neural Networks, by Arjun
Nitin Bhagoji, Warren He, Bo Li and Dawn Song, https://arxiv.org/abs/1712.09491
(conference version accepted by ECCV 2018)

[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking
Black-box Neural Networks, by Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia
Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng,
https://arxiv.org/abs/1805.11770

==========================================

After discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments. The paper looks much better than before. Thus I increased my rating.",7
"UPDATE:

I've read the revised version of this paper, I think the concernings have been clarified.

-------

This paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.

Although I think this paper is a decent paper that deserves an acceptance, there are several concernings:

1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become ""vanishingly small"", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).

2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.

3. In the experimental results, what is the difference between one ""query"" and one ""iteration""? It looks like in one iteration, the Algorithm 2 queries twice?",7
"Paper formalizes the gradient estimation problem in a black-box setting, and provs the equivalence of least Squares with NES. It then improves on state of the art by using priors coupled with a bandit optimization technique.

The paper is well written. The idea of using priors to improve adversarial gradient attacks is an enticing idea. The results seem convincing.

Comments:
- I missed how data dependent prior is factored into the algorithms 1-3. Is it by the choice of d? I suggest a clearer explanation.
- In fig 4, I was confused that the loss of the methods is increasing. it took me a minute to realize this is the maximized adversarial loss, and thus higher is better. you may want to spell this out for clarity. I typically associate lower loss with better algorithms.
- I am confused by Fig 4c. If I am comparing g to g*, I do expect a high cosine similarity. cos = 1 is the best. Why is correlation so small? and why is it 0 for NES? You may also want to offer additional insight in the text explaining 4c. 

Minor comments:
- Is table one misplaced?
- The symbol for ""boundary of set U"" may be confused with a partial derivative symbol
- first paragraph of 2.4: ""our estimator a sufficiently"". something missing?
- ""It is the actions g_t (equal to v_t) which..."" refering to g_t as actions is confusing. Although may be technically correct in bandit setting
- Further explain the need for the projection of algorithm 3, line 7.
- Fig 4: refer to true gradient as g*

Caveat: Although I am well versed in bandits, I am not familiar with adversarial training and neural network literature. There is a chance I may have misevaluated central concepts of the paper.",8
"This is an application paper on dense volumetric synthesis of liquids and smoke. Given densely registered 4D implicit surfaces (volumes over time) for a structured scene, a neural-network based model is used to interpolate simulations for novel scene conditions (e.g. position and size of dropped water ball). The interpolation model composes two components -- given these conditions, it first regresses weights combining a set of precomputed deformation fields, and then a second model regresses dense volumetric deformation corrections -- these are helpful as some events are not easily modeled with a set of basis deformations. 

I found the paper hard to read at first, since the paper is heavy on terminology, only really understood what is going on when I went through the examples in the appendix, which are helpful and then on a second read the content was clear and appears technically correct. I would advise considering defining in more detail early the problem setup (e.g. Fig 13 was helpful), explain some of the variables in context. 

This is primarily an application paper on simulating liquids in controlled scenes using nets and appears novel in that narrow domain. The specific way deformations are composed -- using v_inv to backwards correct basis deformations, following up the mixing of those with a correction model -- is intuitive and is also something I see for the first time. 

The experimental results are sufficient for simulating liquids/smoke, except I would like to also see a comparison to using deformation field network only, without its predecessor. This was done for Fig 6, but would be nice to also see it numerically in ablation in Fig. 4. Another useful experiment would be to vary the number of bases and/or the resolution of the deformation correction network and see the effects. 

More importantly, it would be very helpful is to try this approach for modeling deforming object and body shapes for which there are many datasets (e.g. Shapenet). Right now the implicit surface deformation model is only tested on liquids examples, which limits the impact to that specialist domain -- it's a bit more of a SIGGRAPH type of paper than ICLR. 

---- Post author feedback comment ---- 
I raised my rating to 7 as the paper itself is solid, main concern as another reviewer points out is it may be a bit too specialist for ICLR. If the AC decides to reject based on this fact I am ok with that as well. 

I think it would be helpful to add more ablation (deformation-only results for all cases) and experiments with different numbers of bases in the final version. If that's added it will strengthen the paper. 
",7
"The paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. 

1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. This is an interesting idea to generate the required training data and build a generalizable model. 

2. But based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances. So, this is closer to a graphics approach and deep learning has been used before extensively in a similar manner for shape generation, shape transformation etc.    

3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model.   

xxxxxxxxxxxxxxxxxxx

I appreciate the rebuttals from the authors, updated my score, but I still believe (just like another reviewer) that this is better suited for a workshop or a conference like SIGGRAPH. ",5
"This paper introduces a deep learning approach for physical simulation. The approach combines two networks for synthesizing 4D data that represents 3D physical simulations. Here the first network outputs an initial guess, and the second network adds details. The first network utilizes a set of precomputed deformations, while the weights can be set to generate different output shapes. The precomputed deformations are applied in a recurrent manner. The second network is a variant of STN. 

The results are impressive from the perspective of the current abilities of deep neural networks. The synthesized simulations are not physically accurate, but with certain visual realism. Experimental results are sufficient. 

However, it is also necessarily to add more intuitions to the current approach. First, it would be good to discuss why the current network design is desired. For example, when designing the first network, can we also design another neural network that applies the deformation backwards and enforce some consistency to improve the results? Also, many simulations use adaptive sampling (high-resolution near the surface and low-residual in the interior). Can we use an adaptive grid-structure (say Octree) to increase the resolution? 

Also, is there a simple setting so that the current network design generates accurate results. If not, would increase the number of pre-computed deformations improve the approximation. If so, what would be the optimal basis for $u_i$? What is the tradeoff between using more basis for the first network and increasing the complexity of the second network?

For visualization, it would also good to show the 3D grid.

Overall, it is good paper to see at ICLR.
",7
"The authors present HPG, which applies the hindsight formulation already applied to off-policy RL algorithms (hindsight experience replay, HER, Andrychowicz et al., 2017) to policy gradients.
Because the idea is not new, and formulating HPG from PG is so straightforward (simply tie the dynamical model over goals), the work seems incremental. Also, going off policy in PG is known to be quite unstable, and so I'm not sure that simply using the well known approach of normalized importance weights is in practice enough to make this a widely useful algorithm for hindsight RL.


Evaluation      3/5 How does HPG compare to HER? The only common experiment appears to be bit-flipping, which it appears (looking back at the HER paper, no reference to HER performance in this paper) to signifcantly underperform HER. In general I think that the justification for proposing HPG and possible advantages over HER need to be discussed: why should we generalize what is considered an on-policy algorithm like PG to handle hindsight, when HER seems ideally suited for such scenarios? Why not design an experiment that showcases the advantages of HPG over HER? 
Clarity              4/5 Generally well explained.
Significance    3/5 The importance of HPG relative to off-policy variants of hindsight is not clear. Are normalized importance weights, a well established variance reduction technique, enough to make HPG highly effective? Do we really want to be running separate policies for all goals? With the practical need to do goal sub-sampling, is HPG really a strong algorithm (e.g. compared to HER)? Why does HPG degrade later in training sometimes when a baseline is added? This is strange, and warrants further investigation.
Originality     2/5 More straightforward extension of previous work based on current presentation. 

Overall I feel that HPG is a more straightforward extention of previous work, and is not (yet at least) adequately justified in the paper (i.e. over HER). Furthermore, the experiments seem very preliminary, and the paper needs further maturation (i.e. more discussion about and experimental comparision with previous work, stronger experiments and justification).
Rating          5/10 Weak Reject
Confidence      4/5

Updated Review: 

The authors have updated the appendix with new results, comparing against HER, and provided detailed responses to all of my concerns: thank you authors.

While not all of my concerns have been addressed (see below), the new results and discussion that have been added to the paper make me much more comfortable with recommending acceptance. The formuation, while straightforward and not without limitations, has been shown in preliminary experiments to be effective. While many important details (e.g. robust baselines and ultimate performance) still need to be worked out, HPG is almost certainly going to end up being a widely used addition to the RL toolbox. Good paper, recommend acceptance.

Evaluation/Clarity/Originality/Significance: 3.5/4/3/4

Remaining concerns: 
- The poor performance of the baselines may indeed be due to lack of hindsight, but this should really be debugged and addressed by the final version of the paper.
- Results throughout the paper are shown for only the first 100 evaluation steps. In many of the figures the baselines are still improving and are highly competitive... some extended results should be included in the final version of the paper (at least in the appendix).
- As pointed out, it is difficult to compare the HER results directly, and it is fair to initially avoid confounding factors, but Polyak-averaging and temporal difference target clipping are important optimization tricks. I think it would strengthen the paper to optimize both the PG and DQN based methods and provide additional results to get a better idea of where things stand on these and/or possibly a more complicated set of tasks.


",7
"Following recent work on Hindsight Experience Replay (Andrychowicz et al. 2017), the authors extend the idea to policy gradient methods. They formally describe the goal-conditioned policy gradient setup and derive the extensions of the classical policy gradient estimators. Their key insight to deriving a computationally efficient estimator is that for many situations, only a small number of goals will be ""active"" in a single trajectory. Then, they conduct extensive experiments on a range of problems and show that their approach leads to improvements in sample efficiency for goal-conditioned tasks.

Although the technical novelty of the paper is not high (many of the estimators follow straightforwardly from previous results, however, the goal subsampling idea is a nice contribution), the paper is well written, the topic is of great interest, and the experiments are extensive and insightful. I expect that this will serve as a nice reference paper in the future, and launching point for future work. 

The only major issue I have is that there is no comparison to HER. I think it would greatly strengthen the paper to have a comparison with HER. I don't think it diminishes their contributions if HER outperforms HPG, so I hope the authors can add that.

Comments:

In Sec 6.1, it seems surprising that GCPG+B underperforms GCPG. I understand that HPG+B may underperform HPG, but usually for PG methods a baseline helps. Do you understand what's going on here?

In Sec 6.2, it would be helpful to plot the average return of the optimal policy for comparison (otherwise, it's hard to know if the performance is good or bad). Also, do you have any explanations for why HPG does poorly on the four rooms?

====

Raising my score after the authors responded to my questions and added the HER results.",7
"This paper extends the work of Hindsight Experience Replay to (goal-conditioned) policy gradient methods. Hindsight, which allows one to learn policies conditioned on some goal g, from off-policy experience generated by following goal g’, is cast in the framework of importance sampling. The authors show how one can simply rewrite the goal-conditioned policy gradient by first sampling a trajectory, conditioned on some goal $g’$ and then computing the closed form gradient in expectation over all goals. This gradient is unbiased if the rewards are off-policy corrected along the generated trajectories. While this naive formulation is found to be unstable , the authors propose a simple normalized importance sampling formulation which appears to work well in practice. To further reduce variance and computational costs, the authors also propose goal subsampling mechanisms, which sample goals which are likely along the generated trajectories. The method is evaluated on the same bit-flipping environment as [1], and a variety of discrete environments (grid worlds, Ms. Pac-Man, simulated robot arm) where the method appears highly effective. Unfortunately for reasons which remain unclear, hindsight policy gradients with value baselines appear unstable.

Quality:
This paper scores high wrt. quality. The theoretical contributions of the method are solid, the experiments are well designed and highlight the efficacy of the method, as well as areas for improvement. In particular, I commend the authors for the rigorous analysis (bootstrapped error estimates, separate seeds for hyper-parameters and reporting test error, etc.), including the additional results found in the appendix (sensitivity and ablative analyses). That being said, the paper could benefit from experiments in the continuous control domain and a direct head-to-head comparison with HER. While I do not anticipate the proposed method to outperform HER in terms of data-efficiency (due to the use of replay) the comparison would still be informative to the reader.

Clarity:
The paper is well written and easy to follow. If anything, the authors could have abridged sections 2 and 3 in favor of other material found in the Appendix, as goal-conditioned policy gradients (and variants) are straightforward generalizations of standard policy gradient methods.

Originality:
Novelty is somewhat low for the paper as Hindsight Experience Replay already presented a very similar off-goal-correction mechanism for actor-critic methods (DDPG). The method is also very similar to [2], the connection to which should also be discussed.

Significance.
Despite the low novelty, I do believe there is value in framing “hindsight” as importance sampling in goal-conditioned policy gradients. This combined with the clear presentation and thorough analysis in my opinion warrants publication and will certainly prove useful to the community. Significance could be improved further should the paper feature a more prominent discussion / comparison to HER, along with a fix for the instabilities which occur when using their method in conjunction with a value baseline.

[1] Hindsight Experience Replay. Marcin Andrychowicz et al.
[2] Data-Efficient Hierarchical Reinforcement Learning. Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine.

Detailed Comments:
* Section 2: “this formulation allows the probability of a state transition given an action to change across time-steps within an episode”. I do not understand this statement, as $p(s_{t+1} \mid s_t, a_t)$ is the same transition distribution found in standard MDPs, and appears stationary wrt. time.
* Theorems 3.1 - 3.1 (and equations). A bit lengthy and superfluous. Consider condensing the material.
* Section 5: I found the change in notation (from lower to upper-case) somewhat jarring. Also, the notation used for empirical samples from the mini-batch is confusing. If $A^{(i)}_t}$ is meant to be the action at time-step $t$ for the $i$-th trajectory in the minibatch, then what does $G^{(i)} = g$ mean ? I realize this means evaluating the probability by setting the goal state to $g$, but this is confusing especially when other probabilities are evaluated conditioned on $G^{(i)}$ directly.
* Section 6. “Which would often require the agent to act after the end of an episode”. Do you mean that most episodes have length T’ < T, and as such we would “waste time” generating longer trajectories ?
* RE: Baseline instabilities. Plotting the loss function for the value function could shed light on the instability.
",7
"This paper proposes N-ball embedding for taxonomic data. An N-ball is a pair of a centroid vector and the radius from the center, which represents a word.

Major comments:

- The weakness of this paper is lack of experimental comparisons with other prominent studies. The Poincare embedding and the Lorentz model are recently proposed and show a good predictive performance in hypernymy embedding.
- WordNet concepts are actually structed in DAG. Recent studies on structure embedding can hadle DAG data. It is not clear how to extend N-ball embedding for handling DAT structures. 

- Related work is not sufficiently described.

- It is not clear why N-ball embedding is suitable for hierarchical structures.
",3
"Attention!!! This submission contains Github and Google Drive links to author-related accounts (see e.g. the abstract). I do not think this is permitted or standard. I leave the decision regarding ""automatic rejection"" of the submission to meta-reviewers of the paper.
------------------------------------------------
The paper presents a method for tweaking existing vector embeddings of categorical objects (such as words), to convert them to ball embeddings that follow hierarchies. Each category is represented as a Eucldiean norm ball in high dimensional space, with center and radii adaptable to data. Next, inclusion and exclusion constraints on each pair of  balls are imposed based on the hierarchical structure. These constraints are imposed via an algorithmic approach.  The empirical study includes investigating the consistency of the representation with the hierarchy and demonstrating nearest neighbors for a set of words.

On the positive side, the paper addresses an important problem. It is readable and well organized. The related work could be improved by adding a number of representative related works such as [3,4].  

The major concern about the paper is the originality of the method. Encoding hierarchies with high dimensional balls and encoding inclusion and exclusion as constraints on those balls is a neat and powerful idea from modeling perspective. However, it is not novel, since the approach is already established for example in [1, 2 Chapter 5]. 
The next major concern is regarding the evaluation of the quality of embeddings.
The empirical evaluation does not sufficiently evaluate the quality of tweaked embeddings. In contrast, the quantitative evaluation is more concerned with if the embeddings being consistent with the given hierarchy. In particular, not enough quantitative evidence that the proposed embeddings are actually effective in capturing semantics or in prediction tasks is provided. It should be noted that, the first aspect, ie consistency of the feasible solutions with hierarchy, can be theoretically established (see e.g. [1]).  The first paragraph of 3.2 seems unclear or wrong. See for example [2] for a gradient based solution for the problem.
Finally, using an algorithmic approach as opposed to learning method for constructing embeddings, makes the method not directly related to the topic of ICLR conference.

Overall, due to the above reasons, I vote the paper to be rejected. (The poor anonymization makes it a strong case for a reject.)

[1] Mirzazadeh, F., Ravanbakhsh S., Ding N., Schuurmans D.,  ""Embedding inference for structured multilabel prediction"", NIPS 2015.
[2] Mirzazadeh, F.""Solving Association Problems with Convex Co-embedding"", PhD thesis, 2017. (Chapter 5)
[3] Vilnis, Luke, and Andrew McCallum. ""Word representations via gaussian embedding."", ICLR 2015.
[4] Vendrov, I., Kiros, R., Fidler, S., Urtasun, R. ""Order-embeddings of images and language."" ICLR 2016.",4
"This paper focuses on adjusting the pretrained word embeddings so that they respect the hypernymy/hyponymy relationship by appropriate n-ball encapsulation. They propose to do so by augmenting the word embeddings with information from a resource like Wordnet and applying 3 kinds of geometric transformations to enforce the encapsulation.

The motivation of doing this is not very clear and experimental results are mainly qualitative (and subjective) showing that hypernymy relation can be predicted and preserved by their adjustment. Since, this work relies on Wordnet, the coverage of vocabulary is severely limited and as the authors discus in the results with the section titled ""Experiment 3: Method 2"", they had to remove many words in the standard semantic similarity datasets which casts shadow on the usefulness of the proposed approach. It is unclear what the main contribution of such an approach.

Apart from this, the paper is diffcult to read and some parts (especially those pertaining to Figure 3) encode a simple concept that has been expressed in a very complicated manner.

Overall, I give a score of 4 because of the limited coverage of the approach because of reliance on Wordnet and inadequate empirical evidence of usefulness of this approach.",4
"The paper presents a learning-based method for learning the latent context codes from demonstrations along with a GAIL model. 
This amounts to learning the option segments and the policies simultaneously. 
The main contribution is the model the problem as a time-dependent context and then use a directed information flow loss instead of the mutual information loss.

1. What is the effect of models of the underlying distribution of latent codes. 
Can it be categorical only, or can it be continuous? 
Could we also model it as multidimensional?
The current results only provide single dimensional categorial distribution as latent codes. 

2. The paper missed an important line of work which solves nearly the same problem -- option discovery and policy learning. 
Krishnan -- Discovery of Deep Option(1703.08294). This work was used by authors in continuous options and then again for program generation (https://openreview.net/pdf?id=rJl63fZRb). 

They explicitly infer the option parameters, along with termination conditions with the Expectation Propagation method. 
The results are in very similar domains hence comments, if not a comparison, would be useful. 


3. The authors state that the main problem with an InfoGail style method is dependence on the full trajectory as in eq 1. Hence the directed info flow is required to solve the problem. However in the actual model, the authors make a sequence of variational approximations -- (a) reduction of eq2 to eq1 with a variation lower bound on posterior p(c|c,\tau) and then replace the prior p(c) with q(c|c,\tau) in eq 5. But looking at the model diagram in fig 2. the VAE actually makes the Markovian assumption -- i.e. c only depends on c_{t-1} and s_{t}. If that is true then how would this be very different from InfoGAIL mutual information loss. 
It appears that to capture the authors' mathematical intuition the VAE should have a recurrent generator which should have a hidden state factor passing in to capture dependence on history until the current time. 

3a. In fact the first term in eq 6 looks closer to the actually used model. If that is not true then the authors should clarify. 

4. Experiments do capture the notion discovery of options. But the simplicity of data leaves much to be desired. 
One of the main difference of this work in comparison to unsupervised segmentation models GMM or BP-AR-HMM is the fact that the options learned are composable. But the authors only show this composability on the circle domain -- which is arguably a toy-domain. 
A reasonable confirmation that the model indeed learns composition is to generate a trajectory for a sequence of latent code not seen in data. -- like walking -- normal -- left-right-left can be converted to limping gait -- left-left-right-right. This is only a suggestive example. 

5. In appendix eq 8 how is the reduction from line 3 to line 4 of the equation made -- what is the implicit assumption. 
joint distribution p(c, \tau) is written out as p (\tau|c) p(c) without an integral.
",6
"Summary:

This paper proposes an extension over the popular GAIL method for imitation learning  for the multi-modal data or tasks that have hierarchical structure in them. To achieve that the paper introduces an unsupervised variational objective by maximizing the directed mutual information between the latents c’s and the trajectories. The advantage of using directed information instead of regular MI based criterion is two-folds: 1) Being able to express the causal and temporal dependencies among the c’s changing across time. 2) Being able to learn a macro-policy without needing to condition on the future trajectories. Authors present results both on continuous and discrete environments.


Questions: 
1) Can you give more detailed information about the hyperparameters of your model? For example how many seeds have you used?
2) Have you tried pre-training c_t’s as continuous latent variables?
3) Have you tried pre-training your model as Variational RNN instead of VAE?
4) Have you tried training your model on the pixels on the continuous control tasks?

Pros:
* Although the approach bears some similarity to Info-GAIL approach. The idea of using directed information for GAIL is novel and very interesting. This approach can be in particular useful for the tasks that have 
* The paper is very well-written the goal and motivation of the paper is quite clear.

Cons:
* Experiments are quite weak. Both the discrete and the continuous environment experiments are conducted on very simplistic and toyish tasks. There are much more complicated and modern continuous control environments such as control suite [1] or manipulation suite [2].  In particular tasks where there is a more clear hierarchy would be interesting to investigate.
* Experimental results are underwhelming. For example Table 1, the results of the proposed approach is only barely better than the baseline.

[1] https://github.com/deepmind/dm_control
[2] Learning by Playing-Solving Sparse Reward Tasks from Scratch, M Riedmiller, R Hafner, T Lampe, M Neunert et al - arXiv preprint arXiv:1802.10567, 2018

",6
"The paper describes a new learning framework, based on generative
adversarial imitation learning (GAIL), that is able to learn sub-tasks
policies from unsegmented demonstrations. In particular, it follows
the ideas presented in InfoGAIL, that depends on a latent variable,
and extend them to include a sequence of latent variables representing
the sequence of different subtasks. The proposed approach uses a
pre-training step, based on a variational auto-encoder (VAE), to
estimate latent variable sequences. The paper is well written and
relates the approach with the Options framework. It also shows,
experimentally, its performance against current state-of-the-art
algorithms.  

Although the authors claim in the appendix that the approach is
relatively independent on the dimensionality of the context variable,
this statement needs further evidence. The approach is similar to HMMs
where the number f hidden states or latent variables can make a
difference in the performance of the system.

Also, it seems that the learned contexts do not necessarily correspond
to meaningful sub-tasks, as shown in the circle-world. In this sense,
it is not only relevant to determine the ""right"" size of the context
variable, but also how to ensure a meaningful sub-task segmentation. 
",8
"Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. 

In this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.

If the authors address my major concerns, I would increasing my rating 1-2 points.

Major Comments:

The results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.

1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:

  --> ""These experiments provide conclusive behavioural evidence in favour of the texture hypothesis""
  --> ""we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.""

I would prefer to see language such as ""We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs."" or ""We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements"". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.

2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?

Although the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?

3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. ""the convex outline of object segmentation"", etc.).

Minor Comments:

- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.

- Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.

- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.

- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?

- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?

- Please use names of Shape-ResNet, etc. in Table 2.

- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.

- A.2. ""not not used in the experiment"" --> ""not used in the experiment""
",8
"The paper is well written and easy to follow. It was a nice read for me.

The paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style.

I believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains.

The paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes ""novel Stylized-ImageNet dataset"" and shows that models can learn shape/texture features both but there is not much detail/explanation on why ""Stylized"" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it ""novel"". Additionally, I would like answers to the following questions:

1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results?
2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold?
3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results?
4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn't show if these biases are removed by training on SIN.
5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how's the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset?
6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?",7
"This paper talks about the behavior bias between human and advanced CNN classifier when classifying objects. A clear conclusion is that DNN classifiers lean on texture cues more than human, which is in contrast to empirical evidence. The experimental results are delighting and convincing to some extent. This paper is also inspiring and potentially useful to interpret how CNN works in object classification task. 

Nevertheless, I have several small issues: 
-	I like the writing of this paper, fluent description and clear topic. Besides, it provides sufficient information about experiment details, thus I think the experiments are fully reproducible. But I want to remind the authors to downplay their claims. Some sentences are not with academic rigor. i.e. “Textures, not object shapes, are the most important cues for CNN object recognition.”I don’t think it a good idea to claim textures as the “most important”cue.
-	Although adequate experiments are conducted on ResNet-50 on ImageNet, I miss experiments on a different object classification dataset i.e. PASCAL VOC, and a different network backbone such as very deep ResNet-152 or wider DenseNet. This lies in the concern that a different (deeper or wider) framework may behave quite differently and also the slightly shifted data distribution may induce controversial results. The adopted network ResNet-50, AlexNet, VGG-16 and GoogLeNet are not deep enough or either wide as DenseNet. Although transfer learning experiment is carried out upon PASCAL VOC, it’s not straightforward and not so truly telling. We’re curious about universal conclusions rather than that based on one dataset or network architecture of the same category. As a matter of fact, I’m nearly convinced by the provided results. But I think the demanding experiments will make the conclusions more solid. 

Besides, I think the constructed dataset is beneficial to further research or fair comparison of future works, and I wonder the authors’ intention to publish such a dataset in the future. 

I would raise my scores if the aforementioned problems are convincingly checked and solved.",8
"- Summary: This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of the network under adversarial examples. Experimental results are shown for semantic specifications for CIFAR, errors in predicting sum of two digits and conservation of energy in a simple pendulum. 

- Clarity and correctness: It is a well-written and well-organized paper. Notations and expressions are clear. The math seems to be correct. 

- Significance: The paper claims to have introduced a class of convex-relaxable specifications which constitute specifications that can be verified using a convex relaxation. However, as described later in the paper, it is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts (it would be better to tone down the claims in the abstract and introduction parts.)

- Novelty: The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical. This paper introduces some nice ideas to generalize linear verification functions to a larger class of convex-relaxable functions, however, it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results.

** More detailed comments:

** The idea of generalizing verifications to a convex-relaxable set is interesting, however, applying it in general is not very clear -- as the authors worked on a case by case basis in section 3.1. 

** One of my main concerns is regarding the relaxation step. There is no discussion on the effects of the tightness of the relaxation on the actual results of the models; when in reality, there is an infinite pool of candidates for 'convexifying' the verification functions. It would be nice to see that analysis as well as a discussion on how much are we willing to lose w.r.t. to the tightness of the bounds -- especially when there is a trade-off between better approximation to the verification function and tightness of the bound. 

** I barely found the experimental results satisfying. To find ""reasonable"" inputs to the model, authors considered perturbing points in the test set. However, I am not sure if this is a reasonable assumption when there would be no access to test data points when training a neural network with robustness to adversarial examples. And if bounding them is a very hard task, I am wondering if that is a reasonable assumption to begin with.

** It is hard to have a sense of how good the results are in Figure 1 due to lack of benchmark results (I could not find them in the Appendix either.)

** The experimental results in section 4.4 are very limited. I suggest that the authors consider running more experiments on more data sets and re-running them with more settings (N=2 for digit sums looks very limited, and if increasing N has some effects, it would be nice to see them or discuss those effects.)

** Page 2, ""if they do a find a proof"" should be --> ""if they do find a proof"" 
** Page 5, ""(as described in Section (Bunel et al., 2017; Dvijotham et al., 2018)"", ""Section"" should be omitted.

******************************************************
After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring.
One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.",7
"This paper uses convex relaxation to verify a larger class of specifications
for neural network's properties. Many previous papers use convex relaxations on
the ReLU activation function and solve a relaxed convex problem to give
verification bounds.  However, most papers consider the verification
specification simply as an affine transformation of neural network's output.
This paper extends the verification specifications to a larger family of
functions that can be efficiently relaxed.

The author demonstrates three use cases for non-linear specifications,
including verifying specifications involving label semantics, physic laws and
down-stream tasks, and show some experiments that the proposed verification
method can find non-vacuous bound for these problems. Additionally, this paper
shows some interesting experiments on the value of verification - a more
verifiable model seems to provide more interpretable results.

Overall, the proposed method seems to be a straightforward extension to
existing works like [2]. However the demonstrated applications of non-linear
specifications are indeed interesting, and the proposed method works well on 
these tasks.

I have some minor questions regarding this paper:

1) For some non-linear specifications, we can convert these non-linear elements
into activation functions, and build an equivalent network for verification
such that the final verification specification becomes linear. For example, for
verifying the quadratic specification in physics we can add a ""quadratic
activation function"" to the network and deal with it using techniques in [1] or
[2].  The authors should distinguish the proposed technique with these existing
techniques. My understanding is that the proposed method is more general, but
the authors should better discussing more on the differences in this paper.

2) The authors should report the details on how they solve the relaxed convex
problem, and report verification time. Are there any tricks used to improve
solving time? What is the largest scale of network that the algorithm can
handle within a reasonable time?

3) The detailed network architecture (Model A, Model B) is not shown. How many
layers and neurons are there in these networks? This is important to show the
scalability of the proposed method.

4) For the Mujoco experiment, I am not sure how to interpret the delta values
in Figure 1. For CIFAR I know it is the delta of pixel values but it is not
clear about the delta in Mujoco model. What is the normal range of predicted
numbers in this model?  How does the delta compare to it? Is the delta very
small or trivial?

5) Is it possible to show how loose the convex relaxation is for a small toy
example? For example, the specification involving quadratic function is a
good candidate.

There are some small glitches in equations:

* In (4), k is undefined
* In (20), I am not sure if it is equivalent to the four inequalities after (22).
There are 4 inequalities after (22) but only 3 in (20).


Many papers uses convex relaxations for neural network verification. However
very few of them can deal with general non-linear units in neural networks.
ReLU activation is usually the only non-linear element than we can handle in
most neural network verification works. Currently the only works that can
handle other general non-linear elements are [1][2]. This paper uses more
general convex relaxations than these previous approaches, and it can handle
non-separable non-linear specifications. This is a unique contribution to this
field. I recommend accepting this paper as long as the minor issues mentioned
above can be fixed.

[1] ""Efficient Neural Network Robustness Certification with General Activation
Functions"" by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel.
NIPS 2018

[2] ""A dual approach to scalable verification of deep networks."" by
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and
Pushmeet Kohli. UAI 2018.

",7
"This paper considers more general non-linear verifications, which can be convexified, for neural networks, and demonstrate that the proposed methodology is capable of modeling several important properties, including the conversation law, semantic consistency, and bounding errors.

A few other comments

*) Is it critical that the non-linear verifications need to be convex relaxable. Recently, people have observed that a lot of nonconvex optimization problems also have good local solutions. Is it true that the convex relaxable condition is only required for provable algorithm? As the neural network itself is nonconvex, constraining the specification to be convex is a little awkward to me.

*) The paper contains the example specification functions derived for three specific purpose, I'm wondering how broad the proposed technique could be. Say if I need my neural network to satisfy other additional properties, is there a general recipe or guideline. If not, what's the difficulty intuitively speaking?

The paper needs to be carefully proofread, and a lot of commas are missing.",5
"PAPER SUMMARY:

This paper proposes a new POVI method for posterior inference in BNN. Unlike existing POVI techniques that optimize particles in the weight space which often yields sub-optimal results on BNN due to its over-parameterized nature, the new POVI method aims to maintain and update particles  directly on the space of regression functions to overcome this sub-optimal issue.

NOVELTY & SIGNIFICANCE:

In general, I am inclined to think that this paper has made an important contribution with very promising results but I still have doubts in the proposed solution technique (as detailed below) and am not able to converge to a final rating at this point.

TECHNICAL SOUNDNESS:

The authors claim that the new POVI technique operates directly on the function-space posterior to sidestep the over-parameterized issue of BNN but ultimately each function particle is still identified by a weight particle (as detailed in Eq. (2)). In terms of high-level ideas, I am not sure I understand the implied fundamental differences between this work and SVGD and how significant is it.

On the technical level, the key difference between the proposed work and SVGD seems to be the particle update equation in (2): The gradient flow is multiplied with the derivative of the BNN evaluated at the corresponding weight particle (in SVGD, the gradient flow was used alone). The authors then mentioned that this update rule results from minimizing the difference between f(X, theta) and f(X, theta) + \epsilon * v(f(., theta))(X). I do not follow this step -- please elaborate.

The theoretical justification that follows Eq. (3) is somewhat incoherent: What is \Epsilon(q(f(x)))? This has not been defined before or anywhere in the main text. Furthermore, the paragraph that follows the theoretical justification implies the computation of the gradient flow in (3) involves the likelihood term -- why is that?

In Algorithm 1, why do we sample from both the training set and some measure \mu? I am sure there must be a reason for this but I could not find it anywhere except for a short statement that ""for convenience, we choose \mu in such a way that samples from \mu always consists a mini-batch from X"". Please elaborate.

Will the proposed POVI converge?

CLARITY:

I think this paper has clarity issue with the technical exposition. The explanation tends to be very limited and even appear coherent at important points. For example, see
my 3rd point above. 
",7
"Based on the revision, I am willing to raise the score from 5 to 7.

========================================== 

The authors address the problems of variational inference in over-parameterized models and the problem of the collapse of particle-optimization-based variational inference methods (POVI). The authors propose to solve these problems by performing POVI in the space of functions instead of the weight space and propose a heuristic approximation to POVI in function spaces.

Pros:
1) I believe that this work is of great importance to the Bayesian deep learning community, and may cause a paradigm shift in this area.
2) The method performs well in practice, and alleviates the over-parameterization problem, as shown in Appendix A.
3) It seems scalable and easy to implement (and is similar to SVGD in this regard), however, some necessary details are omitted.

Cons:
1) The paper is structured nicely, but the central part of the paper, Section 3, is written poorly; many necessary details are omitted.
2) The use of proposed approximations is not justified

In order to be able to perform POVI in function-space, the authors use 4 different approximations in succession. The authors do not check the impact of those approximations empirically, and only assess the performance of the final procedure. I believe it would be beneficial to see the impact of those approximations on simple toy tasks where function-space POVI can be performed directly. Only two approximations are well-motivated (mini-batching and approximation of the prior distribution), whereas the translation of the function-space update and the choice of mu (the distribution, from which we sample mini-batches) are stated without any details.

Major concerns:
1) As far as I understand, one can see the translation of the function-space update to the weight-space update (2) as one step of SGD for the minimization of the MSE \sum_x (f(x; \theta^i) - f^i_l(x) - \eps v(f^i_l)(x))^2, where the sum is taken over the whole space X if it is finite, or over the current mini-batch otherwise. The learning rate of such update is fixed at 1. This should be clearly stated in the paper, as for now the update (2) is given without any explanation.

2) I am concerned with the theoretical justification paragraph for the update rule (3) (mini-batching). It is clear that if each marginal is matched exactly, the full posterior is also exactly matched. However, it would usually not be possible to match all marginals using parametric approximations for f(x). Moreover, it is not clear why would updates (3) even converge at all or converge to the desired point, as it is essentially the update for an optimization problem (minimization of the MSE done by SGD with a fixed learning rate), nested into a simulation problem (function-space POVI). This paragraph provides a nice intuition to why the procedure works, but theoretical justification would require more rigor.

3) Another approximation that is left unnoted is the choice of mu (the distribution over mini-batches). It seems to me from the definition of function-space POVI that we need to use the uniform distribution over the whole object space X (or, if we do not do mini-batching, we need to use the full space X). However, the choice of X seems arbitrary. For example, for MNIST data we may consider all real-values 28x28 matrices, where all elements lie on the segment [0,1]. Or, we could use the full space R^28x28. Or, we could use only the support of the empirical distribution. I have several concerns here:
3.1) If the particles are parametric, the solution may greatly depend on the choice of X. As the empirical distribution has a finite support, it would be dominated by other points unless the data points are reweighted. And as the likelihood does not depend on the out-of-dataset samples, all particles f^i would collapse into prior, completely ignoring the training data.
3.2) If the prior is non-parametric, f(x) for all out-of-dataset objects x would collapse to the prior, whereas the f(x) for all the training objects would perfectly match the training data. Therefore we would not be able to make non-trivial predictions for the objects that are not contained in the training set unless the function-space kernel of the function-space prior somehow prevents it. This poses a question: how can we ensure the ability of our particles to interpolate and extrapolate without making them parametric? Even in the parametric case, if we have no additional regularization and flexible enough models, they could overfit and have a similar problem.
These two concerns may be wrong, as I did not fully understand how the function-space prior distribution works, and how the function-space kernel is defined (see concern 4).

4) Finally, it is not stated how the kernels for function-space POVI are defined. Therefore, it is not clear how to implement the proposed technique, and how to reproduce the results. Also, without the full expression for the weight-space update, it is difficult to relate the proposed procedure to the plain weight-space POVI with the function value kernel, discussed in Appendix B.

Minor comments:
1) It is hard to see the initial accuracy of different models from Figure 3 (accuracy without adversarial examples). Also, what is the test log-likelihood of these models?
2) It seems that sign in line 5 on page 4 should be '-'

I believe that this could be a very strong paper. Unfortunately, the paper lacks a lot of important details, and I do not think that it is ready for publication in its current form.",7
"This paper considers particle optimization variational inference methods for Bayesian neural networks.  To avoid degeneracies which arise when these algorithms are applied to the weight space posterior, the authors consider applying the approach in the function space.  A heuristic motivation is given for their algorithm and it seems to have good empirical performance.

I find the paper well-motivated and the suggested algorithm original and interesting.  As the authors mention at one point the derivation is rather heuristic, so much depends on the empirical assessment of their approach.  I was wondering if it was worthwhile to include an architecture search of some kind in the empirical comparisons in the examples?  This is because if wider than needed hidden layers are used this will worsen some of the degeneracies of the weight space posterior which could make the weight space algorithms perform worse.  Also the authors use a Gaussian process approximation in part of their algorithm and wide hidden layers make that approximation more reasonable and may advantage their approach for that reason too.  The authors discuss in Appendix B other approaches to improving weight space POVI.  I wonder also if parameter constraints would be helpful for improving the performance of the weight space methods, such as order constraints on the hidden layer biases for example to remove at least some of the sources of unidentifiability.  The authors talk in the introduction about the difficulties of exploring a complex high-dimensional posterior, the curse of dimensionality, and the limitations of current variational families but only 20 points are used to represent the posterior in the examples.   Are many more particles required to obtain good performance in more complex models and does the approach scale well in terms of its computational requirements in that sense?",7
"The main contribution of this paper in practice seems to be a way to initialize the Continuous Matrix Space Model so that training actually converges, followed by a slightly different contrastive loss function used to train these models. The paper explores the pure matrix model and a mixed matrix / vector model, showing that both together improve on simpler methods on many benchmark tasks.

My main concern is that the chained matrix multiplication involved in this method is not substantially simpler than an RNN or LSTM sentence encoding model, and there are no comparisons of training and inference cost between the models proposed in this paper and conceptually simpler RNNs and LSTMs. The FastSent paper, used here as a baseline, does compare against some deep models, but they choose far more complex baselines such as the NMT encoding, which is trained on a very different loss function. Indeed the models proposed here do not seem to outperform fasttext and fastsent despite having fairly similar computational costs.

I think this paper could use a little more justification for when it's appropriate to use the method proposed here versus more straightforward baselines.",6
"
The authors propose CMOW, an extension of the CBOW model that allows the model to capture word order. Instead of each word being represented as a vector, words are represented by matrices. They extend the CBOW objective to take into account word order by replacing the averaging of vectors to create the context with matrix multiplication (a non-commutative operation). This is the first time this model has been applied in a large scale unsupervised setting. They are able to do this using their objective and an initialization strategy where the matrix embeddings are set to the identity matrix with some Gaussian noise added.

The results of this paper are its main weakness. I did enjoy reading the paper, and it is nice to see some results using matrices as embeddings and matrix multiplication as a compositional function. They include a nice analysis of how word order is captured by these CMOW embeddings while CBOW embeddings capture the word content, but it doesn't seem to make much of a difference on the downstream tasks where CBOW is better than CMOW and close to the performance of the hybrid combination of CBOW and CMOW.

I think it's clear that their model is able to capture word information to some extent, but other models  (RNNs etc.) can do this as well, that admittedly are more expensive, but also have better performance on downstream tasks. I think a stronger motivation for their method besides an analysis of some phenomena it captures and a slight improvement on some downstream tasks when combined with CBOW is needed though for acceptance. Could it be used in other settings besides these downstream transfer tasks?

PROS:
- introduced an efficient and stable approach for training CMSM models
- Show that their model CMOW is able to capture word order information
- Show that CMOW compliments CBOW and a hybrid model leads to improved results on downstream tasks. 

CONS
- The results on the hybrid model are only slightly better than CBOW. CMOW alone is mostly worse than CBOW.",5
"The paper presents new training schemes and experiments for a matrix-multiplicative variant of CBOW. This variant is called a CMSM (Yessenalina and Cardie, 2011; Asaadi and Rudolph, 2017) which swaps the bag of vectors to a product of square matrices for encoding context to incorporate word ordering. It seems this model has not been trained successfully before (at least with a simple approach) due to the vanishing gradient problem.

The paper's main contributions are an initialization scheme for context matrices (to I + [N(0,0.1)]) to counter the vanishing gradient problem and a modification of the CBOW objective so that the target word is drawn uniformly at random from the context window (rather than the center word). Both are shown to improve the quality of learned representations when evaluated as sentence embeddings. Concatenating CBOW and CMSM architectures is additionally helpful. 

I was not aware of the matrix-multiplicative variant of CBOW previously so it's possible that I don't have the expertise to judge the novelty of the approach. But the idea is certainly sensible and the proposed strategies seem to work. The main downside is that for all this work the improvements seem a little weak. The averaged fastText embeddings are clearly superior across the board, though as the authors say it's probably unfair to compare based on different training settings. But this doesn't hurt the simplicity and effectiveness of the proposed method when compared against CBOW baselines. ",6
"This paper presents a novel method for budgeted cost sensitive learning from Data Streams.
This paper seems very similar to the work of Contrado’s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively “purchasing” the most valuable features for the current datapoint under evaluation according to a budget. 

In this process, a sample (S_i) with up to “d” features arrives for evaluation.  A partially revealed feature vector x_i arrives at time “t” for consideration.  There seems to exist a set of “known features” that that are revealed “for free” before the budget is considered (Algorithm 1).  Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility.  When the stopping condition is reached, a prediction is made.  After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated.

The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado’s RADIN paper.   The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks.
The value of this paper is in the idea that we can learn online and in a cost sensitive way.  The most compelling example of this is the idea that a patient shows up at time “t” and we would like to make a prediction of disease in a cost sensitive way.  To this end I would have liked to have seen a chart on how well this algorithm performs across time/history.  How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).

Am I correct in assuming there are some base features that are revealed “for free” for all samples?  If so how are these chosen?  If so how does the number of these impact the results?  

In Contrado’s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset “cardio.”  Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset?  Did you actually re-implement RADIN or just take the numbers from their paper?  In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).

With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm.  For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost.  The second example uses computational cost vs relevance gain.  This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost.  With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). 

 In reality, these costs would be bundled.  You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience.  Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers.  Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront.  It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey.  The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage.  To show the value of your work, a better discussion of the cost savings would be appreciated.             
",7
"The paper presents a RL approach for sequential feature acquisition in a budgeted learning setting, where each feature comes at some cost and the goal is to find a good trade-off between accuracy and cost. Starting with zero feature, the model sequentially acquires new features to update its prediction and stops when the budget is exhausted. The feature selection policy is learned by deep Q-learning. The authors have shown improvements over several prior approaches in terms of accuracy-cost trade-off on three datasets, including a real-world health dataset with real feature costs.

While the results are nice, the novelty of this paper is limited. As mentioned in the paper, the RL framework for sequential feature acquisition has been explored multiple times. Compared to prior work, the main novelty in this paper is a reward function based on better calibrated classifier confidence. However, ablations study on the reward function is needed to understand to what extent is this helpful.

I find the model description confusing. 
1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text.
2. What is the reward function? Only immediate reward is given.
3. What is the state representation? How do you represent features not acquired yet?

It is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better. There are at least 3 improvements: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Right now not clear which one gives the most improvement.

Overall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.",6
"I like the approach, however: I consider the paper to be poorly written.  The presentation needs to be improved for me to find it acceptable.

It presents a stream-oriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm.  This is particularly critical in this area because feature acquisition costs during the ""warm-up"" phase are actual costs, and given the inherent sample complexity challenges of reinforcement learning, I would expect them to be significant in practice.  This would be fine if the setup is ""we have a fixed offline set of examples where all features have been acquired (full cost paid) from which we will learn a selector+predictor for test time"".

The algorithm 1 float greatly helped intelligibility, but I'm left confused.  
  * Is this underlying predictor trained simultaneously to the selector?  
        * Exposition suggests yes (""At the same time, learning should take place by updating the model while maintaining the budgets.""), but algorithm block doesn't make it obvious.
        * Maybe line 21 reference to ""train data"" refers to the underlying predictor.
  * Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but:
        * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and 
        * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but
        * this is not discussed at all.

Also, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the
algorithm is not really an online algorithm).  The experiments are all silent on the ""exploration"" feature acquisition cost.  Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0?

I also found the following disturbing: ""It is also worth noting that, as the proposed method is
incremental, we continued feature acquisition until all features were acquired and reported the average
accuracy corresponding to each feature acquisition budget.""  Does this mean the underlying predictor was trained on data 
that it would not have if the budget constraint were strictly enforced?
",6
"Summary:
The authors present a novel adversarial attack scheme where a neural net is repurposed or ""reprogrammed"" to accomplish a different task than it the one it was originally trained on. This reprogramming from task1 to task2  is done through a given image from task2 additively enhanced with an adversarial program which is trained given the knowledge of the models parameters. A mapping from the repurposed output from task1 to relevant output for taks2 is also necessary (h_g function).

Review:
This approach seems quite novel as it enables the repurposing of ImageNet classifiers to be used for counting dots in images, MNIST and CIFAR10 classifications. This new type of ""adversarial attack"" by repurposing a model shows surprising efficacy at allowing an attacked models to change its task at hand. Some tasks being more difficult (CIFAR10) than MNIST or counting dots.

The paper is well-written and explains clearly the proposed technique. The proposed technique is simple in its formulation.
The assumption it is based on (access to model parameters) is acceptable for the sake of proof of concept.
Overall it is an interesting paper to read and seems of significance for the community working on adversarial attacks.

Few comments/questions come to mind though:
- The adversarial images are quite different from a common image as they embed the program around the new task images. This makes the technique itself quite susceptible to detection (just look at the statistics of the input images).
- How do you handle front end processing? Usually for ImageNet classification, a system will (for instance) resize its input to 256x256, center crop to 224x224 and renormalize the RGB features to match the statistics from the training data. It looks like the images generated are passed as inputs to the system. Do you assume that the front-end steps are not applied or do you assume it is (by including them in the network while training your program W).  My assumption is that you include those steps in the training network for W.
- The size of the program is disproportionately big compare to the task2 embedded image. This begs the question: what happens when you limit the size of the program to a smaller percentage of the whole image? When do you see a break in the reprogramming? Do you need that much extra programming W in your adversarial images?
- As the adversarial images seem to be quite easy to detect, would it be easy to integrate it into some task1 images? The equation (2) gives X_{adv} = \tilda{X} + P, could you use X_{adv} + w * X_{task1}, basically finding a way to hide the program and task2 image within a task1 image. This seems difficult, but have you thought of such approach?

Overall this is a paper that is a pleasant read and should be considered for publication.

Post Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8.  ",8
"This paper proposed ""adversarial reprogramming"" of well-trained and fixed neural networks, which can be viewed as learning a trainable input perturbation on a fixed network for multi-tasking by using a different dataset (e.g., MNIST) from the original dataset (ImageNet) as input. Domain mapping functions (h_g and h_f) are required if the data have different dimensions. The key factor to enable adversarial reprogramming of a fixed network to perform a different task is by training the additive adversarial program as defined in (1). Experimental results show that 7 different ImageNet models (adversarially trained or not) can be reprogrammed for performing counting tasks, and MNIST and CIFAR-10 classifications. The authors also show that adversarial reprogramming is less effective on untrained networks. 

Although the idea of this paper is interesting,  the contribution is unclear and the ""adversarial"" setting is not well motivated. The detailed comments are as follows.

1. Unclear contribution - As mentioned in this paper, the main difference between ""adversarial reprogramming"" and transfer learning or multi-task learning is the fact that the network to be reprogrammed is fixed during reprogramming and was trained on a single task that is independent of the targeted task. However, the reprogramming results are not surprising given the fact that multi-task learning can be achieved on the same network. Given the fact that the perturbed input data (e.g., MNIST) is different from the original input data (ImageNet), what adversarial reprogramming demonstrates is actually a simple way of learning a new task via input perturbation to an unseen dataset at training time. However, transfer learning can be done in a similar way by simply fine-tuning the last (few) layers of a well-trained network. So the number of parameters required to be modified in order to ""reprogram"" a network is already known to be quite small via fine-tuning, which may even be less than the dimension of the adversarial program. In addition, given that the input of ImageNet model is high-dimensional and ImageNet images are likely to lie on a low dimensional manifold (but they are very different from hand-written digits or CIFAR images), the capability of reprogramming using deep models under this setting is expected and thus the contribution is unclear.

2. The ""adversarial"" setting is vague - I am very confused about why the experimental settings should be considered ""adversarial"", given the fact that ImageNet images and the three sets of adversarially perturbed images are quite different. What the experiments show is that a well-trained classifier has a large enough capacity to perform other tasks by simply training a perturbation on a different (out-of-distribution) dataset as inputs. It would make more sense to call this method ""adversarial"" if it can be used on ImageNet images to secretly implement some programmed tasks, while on the surface they are seemingly simply performing a typical classification task.

3. Limited novelty - How is adversarial program different from additional perturbation? Let alone the mapping function M in eqn (3), the adversarial program is nothing but a constrained perturbation (ranging from [-1,1] in each dimension). The optimization formulation in (3) can be seen as a  Carlini-Wager L2 attack with a simplified attack loss + L2 distortion regularization. Therefore, the proposed method has limited technical contribution and novelty.

In summary, this paper has some interesting ideas, but the current presentation lacks clear motivation, and its technical contribution and implications need to be better highlighted.  The authors are suggested to better motivate this paper from the angle of studying the learning capacity of input perturbation induced multi-tasking learning of a well-trained and fixed neural network model, and compare the pros and cons with transfer learning based on fine-tuning and joint multi-task learning / meta-learning on the same network architecture. Based on my own reading, I truly feel that advocating  ""adversarial"" reprogramming does not add any value to this work, as its use for an adversary is not properly motivated (e.g., visual imperceptibility) and its training has no adversarial nature (e.g., GAN training). Titles like ""(Out-of-domain) Input perturbation induced reprogramming of neural networks"" should better justify the contents and experiments presented in this work. Lastly, the authors need to specify how equation (3) is different from the formulation of finding adversarial perturbations in existing literature. Otherwise,  the novelty of ""adversarial program"" is quite limited.

----
Post-rebuttal review:

I appreciate the authors' efforts in including the new experiments in Sections 4.4 and 4.5. In my opinion, these new results and the discussion in Section 5.2 add great values to this work and make the contributions of this paper substantially clear. I've increased my rating to 6.",6
"This paper extends the idea of 'adversarial attacks' in supervised learning of NNs, to a full repurposing of the solution of a trained net. 

The note of the authors regarding 'Transfer learning' is making sense even to the extend that I fail to see how the proposed study differs from the setting of Transfer learning. The comment of 'parameters' does not make much sense in a semi-parametric approach as studied. The difference might be significant, but I leave it up to the authors to formulate a convincing argument.

",4
"This paper has a simple message. When predicting families (weight vectors) of labels, it makes sense to use an ensemble of predictors and average them using a Wasserstein barycenter, where the ground metric is defined using some a priori knowledge on the labels, here usually distances between word embeddings or more elaborate metrics (or kernels K, as described in p.8). Such barycenters can be easily computed using an algorithm proposed by Benamou et al. 18. When these histograms are not normalized (e.g. their count vectors do not sum to the same quantity) then, as shown by Frogner, Zhang et al, an alternative penalized formulation of OT can be studied, solved numerically with a modified Sinkhorn algorithm, which also leads to a simple W barycenter algorithm as shown by Chizat et al.

The paper starts with a lot of reminders, shows some simple theoretical/stability results on barycenters, underlines the role of the regularization parameter, and then spends a few pages showing that this idea does, indeed, work well to carry out ensemble of multi-tag classifiers.

The paper is very simple from a methodological point of view. Experimental results are convincing, although sometimes poorly presented. Figure are presented in a sloppy way, and a more clear discussion on what K should be used would be welcome, beyond what's proposed in p.8. For these reasons I am positive this result should be published, but I'd expect an additional clarification effort from the authors to reach a publishable draft.

minor comment:
- in remark 1 you mention that as epsilon->0 the solution of Benamou et al. converges to a geometric mean. I would have thought that, on the contrary, the algorithm would have converged to the solution of the true (marginal-regularized) W barycenter. Hence the result your propose is a bit counter-intuitive, could you please develop on that in a future version? Is this valid only because \lambda here is finite? on the contrary, what would happen when eps -> infty then, and K = ones?

- GW for generalized Wasserstein is poor naming. GW usually stands for Gromov-Wasserstein (see Memoli's work).

- \lambda and \lambda_l somewhat clash...",6
"The paper proposes a framework based on Wasserstein barycenter to ensemble learning models for a multiclass or a multilabel learning problem. The paper has theoretically shown that the model ensembling using Wasserstein barycenters preserves accuracy, and has a higher entropy than the individual models. Experimental results in the context of attribute-based classification, multilabel learning, and image captioning generation have shown the effectiveness of Wasserstein-based ensembling in comparison to geometric or arithmetic mean ensembling.

The paper is well-written and the experiments demonstrate comparable results. However, the idea of Wasserstein barycenter based ensembling comes at the cost of time complexity since computation of Wasserstein barycenter is more costly than geometric or arithmetic mean. An ensemble is designed to provide lower test error, but also estimate the uncertainty given by the predictions from different models. However, it is not clear how Wasserstein barycenter based ensembling can provide such uncertainty estimate. 

Can the authors comment on the time-complexity of the proposed framework in comparison with its baseline methods? Moreover, is it possible to evaluate the uncertainty of predictions with the proposed framework?

In the context of multilabel learning, Frogner et. al. (2015, https://arxiv.org/abs/1506.05439) suggested using Wasserstein distance as a loss function. In the model, they also leverage the side information from word embedding of tag labels. Is the proposed ensembling framework comparable with theirs?

In short, this paper can provide a useful addition to the literature on model ensembling.  Though the proposed framework does improve the performance of predictions in several applications, I am still not fully convinced on time-complexity introduced when computing Wasserstein barycenters.",6
"Paper overview: Model ensembling techniques aim at improving machine learning model prediction results by i) executing several different algorithms on the same task and ii) solving the discrepancies in the responses of all the algorithms, for each task. Some common methods are voting and averaging (arithmetic or geometric average) on the results provided by the different algorithms. 
Since averaging amounts to computing barycenters with different distance functions, this paper proposes to use the Wassertein barycenter instead of the L2 barycenter (arithmetic average) or the extended KL barycenter (geometric mean). 

Remarks, typos and experiences that would be interesting to add: 
     1) Please define the acronyms before using them, for instance DNN (in first page, 4th line), KL (also first page), NLP, etc. 
    2) In practice, when ensembling different methods, the geometric and arithmetic mean are not computed with equal weights ($\lambda_l$ in Definition 1). Instead, these weights are computed as the optimal values for a given small dev-set. It would be interesting to see how well does the method compare to these optimal weighted averages, and also if it improves is we also compute the optimal $\lambda_l$ for the Wasserstein barycenter. 
    3) How computationally expensive are these methods? 
    4) So the output of the ensembling method is a point in the word embedding space, but we know that not all points in this space have an associated word, thus, how are the words chosen?
    5) The image captioning example of Fig.4 is very interesting (although the original image should be added to understand better the different results), can you show also some negative examples? That is to say, when is the Wassertein method is failing but not the other methods.


Points in favor: 
     1)Better results: The proposed model is not only theoretically interesting, but it also improves the arithmetic and geometric mean baselines.
    2) Interesting theoretical and practical properties: semantic accuracy, diversity and robustness (see Proposition 1). 

Points against: The paper is not easy to read. Ensembling methods are normally applied to the output of a classifier or a regression method, so it is not evident to understand why the 'underlying geometry' is in the word embedding space (page 2 after the Definition 1). I think this is explained in the second paragraph of the paper, but that paragraph is really not clear. I assume that is makes sense to use the word-embedding space for the image caption generation or other ML tasks where the output is a word, but I am not sure how this is used in other cases. 

Conclusion: The paper proposes a new method for model assembling by rethinking other popular methods such as the arithmetic and geometric average. It also shows that it improves the current methods. Therefore, I think it presents enough novelties to be accepted in the conference.",6
"The paper proposes a version of GANs specifically designed for generating point clouds. The core contribution of the work is the upsampling operation: in short, it takes as an input N points, and produces N more points (one per input) by applying a graph convolution-like operation.

Pros:
+ The problem of making scalable generative models for point clouds is clearly important, and using local operations in that context makes a lot of sense.

Cons:
- The paper is not particularly well-written, is often hard to follow, and contains a couple of confusing statements (see a non-exhaustive list of remarks below).
- The experimental evaluation seems insufficient: clearly it is possible to come up with more baselines. Even a comparison to other types of generative models would be useful (e.g. variants of VAEs, other types of GANs). There also alternative local graph-convolution-like operations (e.g. tangent convolutions) that are designed for point clouds. In addition, it is quite strange that results are reported not for all the classes in the dataset.

Various remarks:
p.1, “whereby it learns to exploit a self-similarity prior to sample the data distribution”: this is a confusing statement.
p.2, “(GANs) have been shown on images to provide better approximations of the data distribution than other generative models”: This statement is earthier too strong (all other models) or does not say much (some other models)
p.2, “However, this means that they are unable to learn localized features or exploit weight sharing.”: I see the point about no weight sharing in the generator, but feature learning 
p.3, “the key difference with the work in this paper is that PointNet and PointNet++ are not
generative models, but are used in supervised problems such as classification or segmentation.”: Yet, the kind of operation that is used in the pointnet++ is quite similar to what you propose?
p.4: “because the high dimensionality of the feature vectors makes the gridding approach unfeasible.”: but you are actually dealing with the point clouds where each point is 3D?
",6
"This paper proposes graph-convolutional GANs for irregular 3D point clouds that learn domain (the graph structure) and features at the same time. In addition, a method for upsampling at the GAN generator is introduced. The paper is very well written, addresses a relevant problem (classification of 3D point clouds with arbitrary, a priori unknown graph structure) in an original way, and supports the presented ideas with convincing experiments. It aggregates the latest developments in the field, the Wasserstein GAN, edge-conditional convolutions into a concise framework and designs a novel GAN generator. I have only some minor concerns:

1)	My only serious concern is the degree of novelty with respect to (Achlioptas et al., 2017). The discriminator is the same and although the generator is a fully connected network in that paper, it would be good to highlight conceptual improvements as well as quantitative advantages of the paper at hand more thoroughly. Similarly, expanding a bit more on the differences and improvements over (Grover et al., 2018) would improve the paper. 

2)	P3, second to last line of 2.1: reference needs to be fixed ""…Grover et al. (Grover et al., 2018)""

3)	It would be helpful to highlight the usefulness of artificially generating irregular 3D point clouds from an application perspective, too. While GANs have various applications if applied to images it is not obvious how artificially created irregular 3D point clouds can be useful. Although the theoretical insights presented in the paper are exciting, a more high-level motivation would further improve its quality.

4)	A discussion of shortcomings of the presented method seems missing. While it is understandable that emphasis is put on novelty and its advantages, it would be interesting to see where the authors see room for improvement. 
",9
"The authors present a method for generating points clouds with the help of graph convolution and a novel upsampling scheme. The proposed method exploits the pairwise distances between node features to build a NN-graph. The upsampling scheme generates new points via a slimmed down graph convolution, which are then concatenated to the initial node features. The proposed method is evaluated on four categories of the ShapeNet dataset. Resulting point clouds are evaluated via a qualitative and quantitative comparison to r-GAN.

As far as I know, the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.


The following questions could be addressed by the authors in a revised manuscript:

* The upsampling operation is not well motivated, e.g., neighboring node features are weighted independently, but root node features are not. What is the intuition besides reducing the number of parameters? Are there significant differences when not using diagonal weight matrices?
* As computation of pairwise node feature distances and graph generation based on nearest neighbors are expensive tasks, more details on the practical running time and theoretical complexity should be provided. Can the complexity be reduced by rebuilding graphs only after upsampling layers? How would this impact the performance of the proposed model?
* Although the evaluation on four categories is reported, Table 2 only gives results for two categories.
* How is the method related to GANs which generates graphs, such as GraphGAN or NetGAN?",7
"This paper pushes forward our understanding of learning neural networks. The authors show that they can learn a two-layer (one hidden layer) NN, under the assumption that the input distribution is symmetric. The authors convincingly argue that this is not an excessive limitation, particularly in view of the fact that this is intended to be a theoretical contribution. Specifically, the main result of the paper relies on the concept of smoothed analysis. It states that give data generated from a network, the input distribution can be perturbed so that their algorithm then returns an epsilon solution. 

The main machinery of this paper is using a tensor approach (method of moments) that allows them to obtain a system of equations that give them their “neuron detector.” The resulting quadratic equations are linearized through the standard lifting approach (making a single variable in the place of products of variables). 

This is an interesting paper. As with other papers in this area, it is somewhat difficult to imagine that the results would extend to tell us about guarantees on learning a general depth neural network. Nevertheless, the tools and ideas used are of interest, and while already quite difficult and sophisticated, perhaps do not yet seem stretched to their limits. ",7
"This paper studies the problem of learning the parameters of a two-layer (or one-hidden layer) ReLU network $y=A\sigma(Wx)$, under the assumption that the distribution of $x$ is symmetric. The main technique here is the ""pure neuron detector"", which is a high-order moment function of a vector. It can be proved that the pure neuron detector is zero if and only if the vector is equal to the row vector of A^{-1}. Hence, we can ""purify"" the two layer neural network into independent one layer neural networks, and solve the problem easily.

This paper proposes interesting ideas, supported by mathematical proofs. This paper contains analysis of the algorithm itself, analysis of finding z_i's from span(z_i z_i^T), and analysis of the noisy case. 
This paper is reasonably well-written in the sense that the main technical ideas are easy to follow, but there are several grammatical errors, some of which I list below. I list my major comments below:

1) [strong assumptions] The result critically depends on the fact that $x$ is symmetric around the origin and the requirement that activation function is a ReLU. Lemma 1, 2, 3 and Lemma 6 in the appendix are based on these two assumptions. For example, the algorithm fails if $x$ is symmetric around a number other than zero or there is a bias term (i.e. $y=A \sigma(Wx+b) + b'$ ). This strong assumptions significantly weaken the general message of this paper. Add a discussion on how to generalize the idea to more general cases, at least when the bias term is present. 

2) [sample efficiency] Tensor decomposition methods tend to suffer in sample efficiency, requiring a large number of samples. In the proposed algorithm (Algorithm 2), estimation of $E[y \otimes x^{\otimes 3}]$ and $E[y \otimes y \otimes (x \otimes x)]$ are needed. How is the sample complexity with respect to the dimension? The theory in this paper suggests a poly(d, 1/\epsilon) sample efficiency, but the exponent of the poly is not known. In Section 4.1, the authors talk about the sample efficiency and claim that the sample efficiency is 5x the number of parameters, but this does not match the result in Figure 2. In the left of Figure 2, when d=10, we need no more than 500 samples to get error of W and A very small, but in the right, when d=32, 10000 samples can not give very small error of W and A. I suspect that the required number of samples to achieve small error scales quadratically in the number of parameters in the neural network. Some theoretical or experimental investigation to identify the exponent of the polynomial on d is in order. Also, perhaps plotting in log-y is better for Figure 2.

3) The idea of ""purifying"" the neurons has a potential to provide new techniques to analyze deeper neural networks. Explain how one might use the ""purification"" idea for deeper neural networks and what the main challenges are. 

Minor comments: 

""Why can we efficiently learn a neural network even if we assume one exists?"" -> ""The question of whether we can efficiently learn a neural network still remains generally open, even when the data is drawn from a neural network.""

""with simple input distribution"" -> ""with a simple input distribution""

",6
"This is a strong theory paper and I recommend to accept.

Paper Summary:
This paper studies the problem of learning a two-layer fully connected neural network where both the output layer and the first layer are unknown. In contrast to previous papers in this line which require the input distribution being standard Gaussian, this paper only requires the input distribution is symmetric. This paper proposes an algorithm which only uses polynomial samples and runs in polynomial time. 
The algorithm proposed in this paper is based on the method-of-moments framework and several new techniques that are specially designed to exploit this two-layer architecture and the symmetric input assumption.
This paper also presents experiments to illustrate the effectiveness of the proposed approach (though in experiments, the algorithm is slightly modified).

Novelty:
1. This paper extends the key observation by Goel et al. 2018 to higher orders (Lemma 6). I believe this is an important generalization as it is very useful in studying multi-neuron neural networks.
2. This paper proposes the notation, distinguishing matrix, which is a natural concept to study multi-neuron neural networks in the population level.
3. The “Pure Neuron Detector” procedure is very interesting, as it reduces the problem of learning a group of weights to a much easier problem, learning a single weight vector. 

Clarity:
This paper is well written.

Major comments:
My major concern is on the requirement of the output dimension. In the main text, this paper assumes the output dimension is the same as the number of neurons and in the appendix, the authors show this condition can be relaxed to the output dimension being larger than the number of neurons. This is a strong assumption, as in practice, the output dimension is usually 1 for many regression problems or the number of classes for classification problems. 
Furthermore, this assumption is actually crucial for the algorithm proposed in this paper. If the output dimension is small, then the “Pure Neuron Detection” step does work. Please clarify if I understand incorrectly. If this is indeed the case, I suggest discussing this strong assumption in the main text and listing the problem of relaxing it as an open problem. 


Minor comments:
1. I suggest adding the following papers to the related work section in the final version:
https://arxiv.org/abs/1805.06523
https://arxiv.org/abs/1810.02054
https://arxiv.org/abs/1810.04133
https://arxiv.org/abs/1712.00779
These paper are relatively new but very relevant. 

2. There are many typos in the references. For example, “relu” should be ReLU.




",7
"This paper presents a model for text rewriting for multiple attributes, for example gender and sentiment, or age and sentiment. The contributions and strengths of the paper are as follows. 

* Problem Definition
An important contribution is the new problem definition of multiple attributes for style transfer. While previous research has looked at single attributes for rewriting, ""sentiment"" for example, one could imagine controlling more than one attribute at a time. 

* Dataset Augmentation
To do the multiple attribute style transfer, they needed a dataset with multiple attributes. They augmented the Yelp review dataset from previous related paper to add gender and restaurant category. They also worked with microblog dataset labeled with gender, age group, and annoyed/relaxed. In addition to these attributes, they modified to dataset to include longer reviews and allow a larger vocabulary size. In all, this fuller dataset is more realistic than the previously release dataset.

* Model
The model is basically a denoising autoencoder, a well-known, relatively simple model. However, instead of using an adversarial loss term as done in previous style transfer research, they use a back-translation term in the loss. A justification for this modeling choice is explained in detail, arguing that disentanglement (which is a target of adversarial loss) does not really happen and is not really needed. The results show that the new loss term results in improvements.

* Human Evaluation
In addition to automatic evaluation for fluency (perplexity), content preservation (BLEU score), and attribute control (classification), they ask humans to judge the output for the three criteria. This seems standard for this type of task, but it is still a good contribution.

Overall, this paper presents a simple approach to multi-attribute text rewriting. The positive contributions include a new task definition of controlling multiple attributes, an augmented dataset that is more appropriate for the new task, and a simple but effective model which produces improved results.",7
"This work proposes a new model that controls several factors of variation in textual data where the condition on disentanglement is replaced with a simpler mechanism based on back-translation. It allows control over multiple attributes, and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space.

One of the major arguments is it is unnecessary to have attribute-disentangled latent representations in order to have good style-transferring rewriting. In Table 2, the authors showed that ""a classifier that is separately trained on the resulting encoder representations has an easy time recovering the sentiment"" when the discriminator during training has been fooled. Is there any difference between the two discriminators/classifiers? If the post-fit classifier on top of the encoder representation can easily predict the correct sentiment, there should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation. On the other hand, this does not answer the question if a ""true"" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the ""entangled"" representations.

As the author pointed out, the technical contributions are the pooling operator and the support for multiple attributes since the loss function is the same as that in (Lample et. al 2018). These deserve more elaborated explanation and quantitative comparisons. After all, the title of this work is ""multiple-attribute text rewriting"". For example, the performance comparison between the proposed how averaged attribute  embeddings and simple concatenation, and the effect of the introduced trade-off using temporal max-pooling.

How important is the denoising autoencoder loss in the loss function (1)? From the training details in the supplementary material, it seems like the autoencoder loss is used as ""initialization"" to some degree. As pointed out by the authors, the main task is to get fluent, attribute-targeted, and content-preserving rewriting. As long as the ""back-translation"" gives expected result, it seems not necessary to have ""meaningful"" or hard ""content-preserving"" latent representations when the generator is powerful enough.

I think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of ""content-preserving"" do we look for? In Table 4, it shows that the BLEU between the input and the referenced human rewriting is only 30.6 which suggest many contents have been modified besides the positive/negative attribute. This can also be seen from the transferred examples. In Table 8, one of the Male example: ""good food. my wife and i always enjoy coming here for dinner. i recommend india garden."" and the Female transferred rewriting goes as ""good food. my husband and i always stop by here for lunch. i recommend the veggie burrito"". It's understandable that men and women prefer different types of food even though it is imagination without providing context. But the transfer from ""dinner"" to ""lunch"" is kind of questionable. Is it necessary to change the content which is irrelevant to the attributes?


Other issues:
- Towards the end of Section 3, it says that ""without back-propagating through the back-translation generation process"". Can you elaborate on this and the reason behind this choice?
- What does it mean by ""unknown words"" in ""... with 60k BPE codes, eliminating the presence of unknown words"" from Section 4?
- There is no comparison with (Zhang et. al. 2018), which is the ""most relevant work"".
- In Table 4, what is the difference among the three ""Ours"" model?
- In Table 4, the perplexity of ""Input Copy"" is very high compared with generated sentences.
- In Table 7, what does the ""attention"" refer to?
- In the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).
- Please unify the citation style.",6
"The paper proposes ""style transfer"" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.

This is an interesting application, and somewhat different from ""style transfer"" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.

The architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.

Other than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.
",6
"SUMMARY:
This paper is about potential problems of the information bottleneck principle in cases where the output variable Y is a deterministic function of the inputs X. Such a deterministic relationship between outputs and inputs induces the problem that the the IB ""information curve"" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate (""interesting"") solutions. The authors argue that most real classification problems indeed show such a deterministic relation between the class labels and the inputs X, and they explore several issues that result from such pathologies.

EVALUATION:
In my opinion, the whole story could be summarized as follows: if  Y is
a deterministic function of p-dimensional inputs X, then the joint distribution P(X,Y) is 
degenerate in that its support lies in a space of dimension p (an not p+1 as it would be in the non-degenerate situation), and this is the source of all pathologies observed. As a consequence, only the cumulative distribution is defined, but there is no density with respect to the Lebesgue measure of R^{p+1}. Thus, one has to be careful when defining the mutual information I(X,Y), which explains the problems with the IB information curve (which should asymptotically converge to I(X;Y) as I(X;T) gets large. Another consequence of this degeneracy concerns the latent variable interpretation of the IB: if T is treated as a latent variable (as, for instance, in the ""deep"" IB models) then we have the conditional independence relation ""Y independent of X given T"", which simply makes no sense if Y is deterministic in X (there is, of course, a deeper underlying problem here: the IB problem is difficult in that it is difficult to define a geneative model with a faithful DAG...).
Analyzing situations in which Y = f(X) (with f being a deterministic function) is certainly interesting from a theoretic point of view, but I am not convinced that this analysis is truly relevant for practical problems. 
In particular, I strongly disagree with the statement that ""in most classification problems, the labels Y are a deterministic function of X"". I would rather argue that the opposite is the case, because I don't think that there are too many such problems with zero Bayes error rate.  In particular, I would argue that digit recognition problems like MNIST so not have deterministic labels, since there will always be images of handwritten characters that will give room for interpretation...",2
"This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels).
Namely:
(1) the ""Information Bottleneck curve"" cannot be computed with the Information Bottleneck Lagrangian approach (because of optimization landscape issues: optimization of such a piecewise-linear function with a linear penalty will always yield the same optimum whatever the slope of the penalty is [same story as L1 vs. L0]); 
(2) there are many solutions to the optimization of the IB Lagrangian for any given compression/performance ratio (i.e. for any given beta in the IB Lagrangian method: I(Y,T)/I(X,T)) and some of them are provably trivial; thus optimizing just the IB Lagrangian does not imply that the solution will be interesting, and better (or complementary) criteria are needed.

Another point discussed also is about the successive layers of perfect classifiers (neural networks), in which I(Y,T) remains constant while I(X,T) decreases.


Pros:
- the paper is well written, mostly self-contained, and easy to read (for someone familiar with information theory);
- all mathematical points are detailed and well explained, with sufficient introduction;
- the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;)
- information bottleneck is a topic of prime interest in the community these days;
- the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning;
- the solution brought to the IB Lagrangian issues is simplistic though efficient (squaring I(X,T) so that it's not linear in I(X,T) anymore).


Cons:
- not much.

Remarks:
- there exist recent papers tackling the information bottleneck concept for neural networks from a variational perspective, which enables them to compute exactly the mutual informations (such as ""Compressing Neural Networks using the Variational Information Bottleneck"" by Dai & al., ICML 2018); I have not seen these papers cited in the article, nor discussed (nor used); I feel it would be appropriate, either in the general literature section, either for discussing how to compute in practice the mutual informations (exact values vs. estimates or lower bounds as here).
- at first reading, I had found the tone of the beginning of the paper (first section) a bit aggressive, though this feeling disappeared later. Maybe rephrase some expressions that might be wrongly perceived?
- About multilabel classification (end of section 2): multilabel classification can still be seen as with deterministic expected outputs, if considered as a task from X to P(Y) (power set of Y, i.e. set of all possible subsets of labels).
- As in practice T is constrained to belong to a particular space of functions (neural network layer with predefined architecture): how does this impact the study? For instance the T_alpha in equation (5) are not reachable anymore; the optimization space for the IB Lagrangian is different; etc. Which properties/conclusions can be kept, and which ones cannot?
- What about sampling on the other part of the IB curve, the horizontal one (same I(Y,T) for various I(X,T))? Would it bring any insight, and how to do it?
- A side remark about applying IB to neural networks: What about neural networks that are not a ""linear"" chain of layers (i.e. most networks now)? i.e. Inception, ResNet, U-nets, etc., where computational flows are parallel, sometimes keeping full information till the end. For instance in a U-net, meant for image processing, features computed at the beginning at a full pixelic resolution are communicated to the last layer. This is not an image classification task though, as predictions are made for each pixel; still, given an input image X, there is only one correct output Y, so, still in the deterministic supervised classification problem.
",8
"This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X).

The idea as I understood it is as follows:
1) In a first section the authors discuss the relationship between supervised learning through minimization of the empirical cross entropy and the maximization of the empirical mutual information with an intermediate latent variable T. 
2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. 
3) They show that the optimization of the IB Lagrangian for different \beta does not lead to a point by point exploration of the IB curve.
4) They propose a cure to the previous issue by introducing the squared IB Lagrangian. 
5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve.
6) They show that multiple successive representations (like in DNNs), have identical predicting power (mutual information with output Y) when they allow for perfect prediction. 
7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. 
	- they show that the optimization of the squared IB reaches more different points on the IB curve,
	- but that these representations are possibly uninteresting (hard clustering of uneven numbers of grouped classes) 
	- they show that for large enough value of beta, zero error is reached. 

The necessity of noise in the IB theory has been already pointed out by (Gilad-Bachrach et al., 2003; Shwartz-Ziv et al.  2017), although the more thorough analysis proposed here is novel. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. The motivation and impact of this work studying deterministic rules is therefore not completely convincing. 

Further pros and cons:

Pros:
- The discussion is generally well written. 
- This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. 
- These are demonstrated with experiments conducted on the MNIST dataset for concreteness.

Cons:
- The fact that multiple successive representations have identical predicting power when the prediction error is zero, was already observed for example in Shwartz-Ziv et al.  2017. It is not clear why this should be considered as an issue. It also seems to be a straightforward observation when restricting to the empirical measure on the training set. 
- The fact that the entire IB curve is not explored point by point by the IB Lagrangian is not necessarily an issue for learning. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. 

Questions:
- Do the authors know of an application where the full probing of the IB curve would be necessary?
- In Section 2, when injecting the decomposition of the prediction density q(y|x) over the intermediate variable t in eq (3) was a Jensen inequality replaced by an equality?
",6
"The paper demonstrates a method for constructing adversarial examples by modifications or perturbations to physical parameters in the scene itself---specifically scene lighting and object geometry---such that images taken of that scene are able to fool a classifier. It achieves this through a novel differentiable rendering engine, which allows the proposed method to back-propagate gradients to the desired physical parameters. Also interesting in the paper is the use of spherical harmonics, which restrict the algorithm to plausible lighting. The method is computationally efficient and appears to work well, generating plausible scenes that fool a classifier when imaged from different viewpoints.

Overall, I have a positive view of the paper. However, there are certain issues below that the authors should address in the rebuttal for me to remain with my score of accept (especially the first one):


- The paper has no discussion of or comparisons to the work of Athalye and Sutskever, 2017 and Zeng et al., 2017, except for a brief mention in Sec 2 that these methods also use differentiable renderers for adversarial attacks. These works address the same problem as this paper---computing physically plausible adversarial attacks---and by very similar means---back-propagation through a rendering engine. Therefore it is critical that the paper clarifies its novelty over these methods, and if appropriate, include comparisons.

- While the goal of finding physically plausible adversarial examples is indeed important, I disagree with the claim that image-level attacks are ""primarily tools of basic research, and not models of real-world security scenarios"". In many applications, an attacker may have access to and be able to modify images after they've been captured and prior to sending them through a classifier (e.g., those attempting to detect transmission of spam or sensitive images). I believe the paper can make its case about the importance of physical adversarial perturbations without dismissing image-level perturbations as entirely impractical.

- The Athalye 18 reference noted in Fig 1 is missing (the references section includes the reference to Athalye and Sutskever '17).

===Post-rebuttal

Thanks for addressing my questions. With the new comparisons and discussions wrt the most relevant methods, I believe the contributions of the paper are clearer. I'm revising my score from 6 to 7.
",7
"Quality of the paper:  The paper is quite clear on the background literature on adversarial examples, physics based rendering, and the core idea of generating adversarial perturbations as a function of illumination and geometric changes.   
Originality and Significance: The idea of using differential renderers to produce physically consistent adversarial perturbations is novel. 
References: The references in the paper given its scope is fine.  It is recommended to  explore references to other recent papers that use simulation for performance enhancement in the context of transfer learning, performance characterization (e.g. veerasavarappu et al in arxiv, WACV, CVPR (2015 - 17)) 

Pros:  Good paper , illustrates the utility of differentiable rendering and simulations to generate adversarial examples and to use them for improving robustness.
Cons: The experimental section needs to be extended and the results are limited to simulations on CIFAR-100 and evaluation on lab experimental data.  Inclusion of images showing CIFAR-100 images augmented with random lighting, adversarial lighting would have been good. The details of the image generation process for that experiment is vague and not reproducible. ",7
"Summary:
This work presents a method to generate adversary examples capable of fooling a neural network classifier. Szegedy et al. (2013) were the first to expose the weakness of neural networks against adversarial attacks, by adding a human-imperceptible noise to images to induce misclassification. Since then, several works tackled this problem by modifying the image directly in the pixel space: the norm-balls convention. The authors argue that this leads to non-realistic attacks and that a network would not benefit from training with these adversarial images when performing in the real world. Their solution and contributions are parametric norm-balls: unlike state-of-the-art methods, they perform perturbations in the image formation space, namely the geometry and the lighting, which are indeed perturbations that could happen in real life. For that, they defined a differentiable renderer by making some assumptions to simplify its expression compared to solving a light transport equation. The main simplifications are the direct illumination to gain computation efficiency and the distant illumination and diffuse material assumptions to represent lighting in terms of spherical harmonics as in Ramamoorthi et al. (2001), which require only 9 parameters to approximate lighting. This allows them to analytically derivate their loss function according to the geometry and lighting and therefore generate their adversary examples via gradient descent. They show that their adversary images generalize to other classifiers than the one used (ResNet). They then show that injecting these images into the training set increase the robustness of WideResNet against real attacks. These real attack images were taken by the authors in a laboratory with varying illumination.

Strength:
- The proposed perturbations in the image formation space simulate the real life scenario attacks.
- The presented results show that the generated adversary images do fool the classifier (used to compute the loss) but also new classifiers (different than the one used to compute the loss). As a consequence the generated adversary images increase the robustness of the considered classifier. 
- Flexibility in their cost function allows for diverse types of attacks: the same modified geometry can fool a classifier in several views, either into detecting the same object or detecting different false objects under different views. 

Major comments:
- Method can only compute synthetic adversary examples, unlike state-of-the-art.
- The main contribution claimed by the author is that their perturbations are realistic and that it would help better increase the robustness of classifiers against real attacks. However, they do not give any comparison to the state-of-the-art methods as is expected. 

Minor comments:
- Even if the paper is well written, they are still some typos. 
",6
"This paper presents a new way to represent a dense matrix in a compact format. First, the method prunes a dense matrix based on the Viterbi-based pruning. Then, the pruned matrix is quantized with alternating multi-bit quantization. Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm. It spots the problem of each existing approach and solve the problems by combining each method. The combination is new and the result is encouraging.

I find this paper is interesting and I like the strong results. It is an interesting combination of methods. However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.

1. The method should be compared with other combinations of components. At least, it should be compared with ""Multi-bit quantization only (Xu et al., 2018)"" and ""Multi-bit-quantization + Viterbi-based binary code encoding"".

2. The experiments with ""Don't Care"" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.

3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods. In Section 3.3., it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed. I feel that this is a kind of things which support the proposed method if it is properly assessed.

4. When you say ""slow"" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow ""sequential sparse matrix decoding process"".

Minor comments:

* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance. It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.",6
"Summary:

This paper addresses the computational aspects of Viterbi-based encoding for neural networks. 

In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns.  (The value of each nonzero is stored separately.)  However, it is clear that not all messages are possible, only those in the ""range space"" of my codes. (This part is previous work Lee 2018.) 

The ""Double Viterbi"" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. 

Pros:
 - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. 
 - The idea is theoretically sound and interesting.

Cons: 
 - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.
 - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. 
 - Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)

",7
"The paper proposes two additional steps to improve the compression of weights in deep neural networks. The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.

There are several weaknesses in this paper. The first one is clarity. The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.

The paper can be made more mathematically precise. The input and output types of each block in Figure 1. should be clearly stated. For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits. Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.

The figures are almost useless, because the captions contain very little information. For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned. Many more can be said in all the figures.

The second weakness is experimental design. There are two conflicting qualities that need to be optimized--performance and compression rate. When optimizing the compression rate, it is important not to look at the test set error. If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set. The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.

Optimizing compression rates should be done on the training set with a separate development set. The test set should not used before the best compression scheme is selected. Both the results on the development set and on the test set should be reported for the validity of the experiments. I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning. Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.",6
"This work looks into the phenomenon of posterior collapse, and shows that training the inference network more can reduce this problem, and lead to better optima. The exposition is clear. The proposed training procedure is simple and effective. Experiments were carried out in multiple settings, though I would've liked to see more analysis. Overall, I think this is a nice contribution. I have some concerns which I hope the authors can address.

Comments:
 - I think [1] should be cited as they first mentioned Eq 5 and also performed similar analysis.
 - Were you able to form an unbiased estimate for the log of the aggregate posterior which is used extensively in this paper (e.g. MI)? Some recents works also estimate this but they use biased estimators. If your estimator is biased, please add a sentence clarifying this so readers aren't mislead.
 - Apart from KL and (biased?) MI, a metric I really would've liked to see is the number of active/inactive units as measured in [2]. I think this is a more reliable and very explainable metric for posterior collapse, whereas real-valued information-theoretic quantites can be hard to interpret.

Questions:
 - Has this approach truly completely solved posterior collapse? (e.g. can you show that the mutual information between z and x is maximal or the number of inactive units is zero?) 
 - How robust is this approach to the effects of randomness during training such as initialization and use of minibatches? (e.g. can you show some standard deviations of the metrics you report in Table 1?)
 - (minor) I wasn't able to understand why the top right is optimal, as opposed to anywhere on the dashed line, in Figures 1(b) and 3?

[1] Hoffman, Matthew D., and Matthew J. Johnson. ""Elbo surgery: yet another way to carve up the variational evidence lower bound."" 
[2] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. ""Importance weighted autoencoders.""

--REVISION--

The paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of ""preventing"" or ""avoiding"" posterior collapse is too strong, as I agree with the authors that ""it is unknown whether there is a better local optimum that [activates] more or all latent units"". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like ""reducing"" or ""mitigate"" instead.",8
"Response to Authors
-------------
I've read all other reviews and the author responses. Most responses to my issues seem to be ""we will run more experiments"", so my review scores haven't changed. I'm glad the authors are planning many revised experiments, and I understand that these take time. It's too bad revised results won't be available before the review revision deadline (tomorrow 11/26). I guess I'm willing to take the author's promises to update in good faith. Thus, I think this is an ""accept"", but only if the authors really do follow through on promises to add uncertainty quantification and include some complete comparisons to KL annealing strategies. 

Review Summary
--------------

Overall, I think the paper offers a reasonable story for why its proposed innovation -- an alternative scheduling of parameter-specific updates where encoder parameters are always trained to convergence during early iterations -- might offer a reliable way to avoid posterior collapse that is far faster and easier-to-implement than other options that require some per-example iterations (e.g. semi-amortized VAE). My biggest concerns are that relative performance gains (in bound quality) over alternatives are not too large and hard to judge as significant because no uncertainty in these estimates is quantified. Additionally, I'd like to see more careful evaluation of the KL annealing baseline and more attention to within-model comparisons (do you really need to update until convergence?).

Given the method's simplicity and speed, I think with a satisfactory rebuttal and plan for revision I would lean towards acceptance.

Paper Summary
-------------
The paper investigates a common problem known as ""posterior collapse"" observed when training generative models such as VAEs (Kingma & Welling 2014) with high-capacity neural networks. Posterior collapse occurs when the encoder distribution q(z|x) (parameterized by a NN) becomes indistinguishable from the generative prior on codes p(z), which is often a local optima of the VI ELBO objective. While other better fixed points exist, once this one is reached during optimization it is hard to escape using the typical local gradient steps for VAEs that jointly update the parameters of an encoder and a decoder with each gradient step. 

The proposed solution (presented in Alg. 1) is to avoid joint gradient updates early in training, and instead use an alternating update scheme where after each single-gradient-step decoder parameter update, the encoder is updated with as many gradient steps as are needed to reach convergence. This proposed scheme, which the paper terms ""aggressive updates"", forces the encoder to better approximate the true posterior p(z|x) at each step.

Experiments study a synthetic task where visualizing the evolution of true posterior mean of p(z|x) side-by-side with approximate q(z|x) is possible in 2D, as well as benchmark comparisons to several other methods that address posterior collapse on text modeling (Yahoo, Yelp15) and image modeling (Omniglot). Studied baselines include annealing the KL term in the VI objective, the \beta VAE (which keeps the KL term fixed with a weight \beta), and semi-amortized VAEs (SA-VAEs, Kim et al. 2018). The presented approach is said to reach better values of the log likelihood while also being ~10x faster to train than the Kim et al. approach on large datasets.

Significance and Originality
----------------------------
There exists strong interest in deploying amortized VI to fit sophisticated models efficiently while avoiding posterior collapse, so the topic is definitely relevant to ICLR. Certainly solutions to this issue are welcome, though I worry with the crowded field that performance is starting to saturate and it is becoming hard to identify significant vs. marginal contributions. Thus it's important to interpret results across multiple axes (e.g. speed and heldout likelihood).

The paper does a nice job of highlighting related work on this problem, and I'd rate its methodological contributions as clearly distinct from prior work, even though the eventual procedure is simple.

The closest related works in my view are:

* Krishnan et al. AISTATS 2018, where VAE joint-training algorithms for nonlinear factor analysis problems are shown to be improved by an algorithm that uses the encoder NN as an *initialization* and then doing several standard SVI updates to refine per-example parameters. Encoder parameters are updated via gradient updates, *after* the decoder parameters are updated (not jointly).

* SA-VAEs (Kim et al. ICML 2018) which studies VAEs for deep text models and develops an algorithm that at each a new batch uses the encoder to initialize per-example parameters, updates these via several iterations of SVI, then *backpropagates* through those updates to compute a gradient update of the encoder NN.

Compared to these, the detailed algorithm presented in this work is both distinct and simpler. It does not require any per-example parameter updates, instead it only requires a different scheduling of when encoder and decoder NN updates occur. 


Concerns about Technical Quality (prioritized)
----------------------------------------------

## C1: Without error bars in Table 1 and 3, hard to know which gaps are significant

Are 500 Monte Carlo samples enough to be sure that the numbers reported in Table 1 are precise estimates and not too noisy? How much error is there in the estimation of various quantities like the NLL or the KL if we repeated 500-MC samples 5x or 10x or 25x? My experience is that even with 100 or more samples, evaluations of the ELBO bound for classic VAEs can differ non-trivally. I'd like to see evidence that these quantities are estimated with certainty, or (even better) some direct reporting of the uncertainties across several estimates.


## C2: Baseline comparison to KL annealing needs to be more thorough

The current paper dismisses the strategy that annealing the KL term as ineffective in addressing posterior collapse (e.g. VAE + anneal has a 0.0 KL term in Table 1). However, it's not clear that a reasonable annealing schedule was used, or even that any reasonable effort was made to try more than one schedule. For example, if we set the KL term to exactly 0.0 weight, the optimization has no incentive to push q towards the prior, and thus posterior collapse *cannot* occur. It may be that this leads to other problems, but it's unclear to me why a schedule that keeps the KL term weight exactly at 0 for a few updates and then gradually increases the weight should lead to collapse. To me, the KL annealing story is much simpler than the presented approach and I think as a community we should invest in giving it a fair shot. If the answer is that annealing takes too long or the schedule is tough to tune, that's sensible, but I think the claim that annealing still leads to collapse just means the schedule probably wasn't set right.

Notice that ""Ours"" is improved by ""Ours+Annealing"" for 2 datasets in Table 1. So annealing *can* be effective. Krishnan et al. 2018's Supplementary Fig. 10 suggests that if annealing is slow enough (unfolding over 100000 updates instead of 10000 updates), then KL annealing will get close to pure SVI in effective, non-collapsed posterior approximation. The present paper's Sec. B.3 indicates that the attempted annealing schedule was 0.1 to 1.0 linearly over 10 epochs with batch size 32 and train set size 100k, which sounds like only 30k updates of annealing were performed. I'd suggest comparing against KL annealing that both starts with a smaller weight (perhaps exactly at 0.0) and grows much slower.


## C3: Results do not analyze variability due to random initialization or random minibatch traversal

Many factors can impact the final performance values of a model trained via VI, including the random initialization of its parameters and the random order of minibatches used during gradient updates. Due to local optima, often best practice is to take the best of many separate initializations (see several figures in Bishop's PRML textbook). The present paper doesn't make clear whether it's reporting single runs or the best of many runs. I suggest a revision is needed to clarify. Quantifying robustness to initialization is important.


## C4: Results do not analyze relative sensitivity of encoder and decoder to using the same learning rate

One possible explanation for ""lagging"" might be that the gradient vectors of the encoder and the decoder have different magnitudes, and thus using the same fixed learning rate for both (as seems to be done from a skim of Sec. B) might not be optimal. Perhaps a quick experiment that separately tunes learning rates of encoder and decoder is necessary? If the learning rate for encoder is too small, this could easily explain the lagging when using joint updates.


## C5: Is it necessary to update until convergence? Or would a fixed budget of 25 or 100 updates to the encoder suffice?

In Alg. 1, during the ""aggressive"" phase the encoder is updated until convergence. I'd like to see some coverage of how long this typically takes (10 updates? 100 updates?). I'd also like to know if there are significant time-savings to be had by not going *all* the way to convergence. It's concerning that in Fig. 1 convergence on a toy dataset takes more than 2000 iterations.


## C6: Sensitivity to the initialization of the encoder is not discussed and could matter

In the synthetic example figure, it seems the encoder is initialized so that across many examples, the typical encoding will be near the origin and thus favored under the prior. Thus, the *initialization* is in some ways setting optimization up for posterior collapse. I wonder if some more diverse initialization might avoid the problem.



Presentation comments
---------------------

Overall the paper reads reasonably. I'd suggest mentioning the KL annealing comparison a bit earlier, but otherwise I have few complaints.

I'm not sure I like the chosen terminology of ""aggressive"" update. The procedure is more accurately a ""repeat-until-convergence"" update. There's nothing aggressive about it, it's just repeated.


Line-by-line Detailed comments
------------------------------

Citations for ""traditional"" VI with per-example parameters should go much further back than 2013. For example, Matthew Beal's thesis, work by Blei in 2003 on LDA, or work by MacKay or M.I. Jordan or others even further back.

Alg 1 Line 12: This update should be to \theta (model parameters), not \phi (approx posterior parameters).

Alg 1: Might consider using notation like g_\theta to denote the grad. of specific parameters, rather than have the same symbol ""g"" overloaded as the gradient of \theta, \phi, and both in the same Algo.


Fig. 3: This is interesting, but I think it's missing something as a visualization of the algorithm. There's nothing obvious visually that indicates the encoder update involves *many* steps, but the decoder update is only one step. I'd suggest at least turning each vertical arrow into *many* short arrows stacked end-to-end, indicating many steps. Also use a different color (not green for both).

Fig. 4: Shows various quantities like KL(q, prior) traced over optimization. This figure would be more illuminating if it also showed the complete ELBO objective and the expected log likelihood term. Then it would be clear why annealing is failing to avoid posterior collapse.

Table 1: How exactly is the negative log likelihood (NLL) computed? Is it the expected value of the data likelihood: -1 * E_q[log p(x|z)]? Or is it the variational lower bound on marginal likelihood? 


",7
"General:
The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE.

I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019.

Pros:
+ The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating.
+ It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors.
+ I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)).
+ The presented results are fully convincing.

Cons:
- It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented.
- The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique?
- The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered?

Neutral remark:
* Another problem, next to the posterior collapse, is the “hole problem” (see Rezende & Viola, “Taming VAEs”, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors’ opinion on this matter.
* Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit.

--REVISION--
I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.",8
"
# Summary:
The paper proposes a new approach for fully decentralized training in multi-agent reinforcement learning, termed probabilistic recursive reasoning (PR2). The key idea is to build agent policies that take into account opponent best responses to each of the agent's potential actions, in a probabilistic sense. The authors show that such policies can be seen as recursive reasoning, prove convergence of the proposed method in self-play, a demonstrate it in a couple of iterated normal form games with non-trivial Nash equilibria where baselines fail to converge.

I believe the community will find intuitions, methods, and theory developed by the authors interesting. However, I find some parts of the argument somewhat questionable as well as experimental verification insufficient (see comments below).


# Comments and questions:

## Weaknesses in the experimental evaluation:
I find it hard to justify a fairly complex algorithm (even though inspired by cognitive science), when most of the simpler alternatives from the literature haven't been really tested on the same iterated games (the baselines in the paper are all simple gradient-based policy search methods).

In the introduction (paragraph 2), the authors point out potential limitations of previous opponent modeling algorithms, but never compare with them in experiments. If the claim is that other methods ""tend to work only under limited scenarios"" while PR2 is more general, then it would be fair to ask for a comprehensive comparison of PR2 vs alternatives in at least 1 such scenario. I would be interested to see how the classical family of ""Win or Learn Fast"" (WoLF) algorithms (Bowling and Veloso, 2002) and the recent LOLA (Foerster et al, 2018) compare with PR2 on the iterated matrix game (section 5.1).

Also, out of curiosity, it would be interesting to see how PR2 works on simple iterated matrix games, eg iterated Prisoner's dilemma.

## Regarding the probabilistic formulation (section 4.3)
Eq. 8 borrows a probabilistic formulation of optimality in RL from Levine (2018). The expression given in Eq. 8 is proportional to the probability of a trajectory conditional on that each step is optimal wrt the agent's reward r^i, i.e., not for p(\tau) but for p(\tau | O=1).

If I understand it correctly, by optimizing the proposed KL objective, we fit both \pi^i and \rho^{-i} to the distribution of *optimal trajectories* with respect to r^i reward. That makes sense in a cooperative setting, but the problem arises when opponent's reward r^{-i} is different from r^i, in which case I don't understand how \rho^{-i} happens to approximate the actual policy of the opponent(s). Am I missing something here?

A minor point: shouldn't \pi^{-i} in eq. 9 be actually \rho^{-i}? (The derivations in appendix C suggest that.)

## Regarding alternative approaches (section 4.5)
The authors point out intractability of trying to directly approximate \pi^{-i}. The argument here is a little unclear. Wouldn't simple behavioral cloning work? Also, could we minimize KL(\pi^{-i} || \rho^{-i}) instead of KL(\rho^{-i} || \pi^{-i})?

# Minor
- I might be misreading it, but the last sentence of the abstract seems to suggest that this paper introduces opponent modeling to MARL, which contradicts the first sentence of paragraph 2 in the introduction.
- It is very hard to read plots in Figure 3. Would be nice to have them in a larger format.

Overall, I find the paper interesting, but it would definitely benefit from more thorough experimental evaluation.",7
"The high-level problem this paper tackles is that of endowing RL agents with recursive reasoning capabilities in a multi-agent setting, based on the hypothesis that recursive reasoning is beneficial for the agents to converge to non-trivial equilibria.

The authors propose the probabilistic recursive reasoning (PR2) framework for an n-agent stochastic game. The conceptual difference between PR2 and non-correlated factorizations of the joint policy is that, from the perspective agent i, PR2 augments the joint policy of all agents by conditioning the policies of agent i's opponents on the action that agent i took. The authors derive the policy gradient for PR2 and show that it is possible to learn these action-conditional opponent policies via variational inference in addition to learning the policy and critic for agent i.

The proposed method is evaluated on two experiments: one is an iterated matrix game and the other is a differential game (""Max of Two Quadratics""). The authors show in the iterated matrix game that baselines with non-correlated factorization rotate around the equilibrium point, whereas PR2 converges to it. They also show in the differential game that PR2 discovers the global optimum whereas baselines with non-correlated factorizations do not.

This paper is clear, well-motivated, and well-written. I enjoyed reading it. I appreciated the connection to probabilistic reinforcement learning as a means for formulating the problem of optimizing the variational distribution for the action-conditional opponent policy and for making such an optimization practical. I also appreciated the illustrative choice of experiments that show the benefit of recursive reasoning. 

Currently, PR2 provides a proof-of-concept of recursive reasoning in a multi-agent system where the true equilibrium is already known in closed form; it remains to be seen to what extent PR2 is applicable to multi-agent scenarios where the equilibrium the system is optimizing is less clear (e.g. GANs for image generation). Overall, although the experiments are still small scale, I believe this paper should be accepted as a first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities.

Below are several comments.

1. Discussion of limitations: As the authors noted in the Introduction and Related Work, multi-agent reinforcement problems that attempt to model opponents' beliefs often become both expensive and impractical as the number of opponents (N) and the recursion depth (k) grows because such complexity requires high precision in the approximate the optimal policy. The paper can be made stronger with experiments that illustrate to what extent PR2 practically scales to problems with N > 2 or K > 1 in terms of how practical it is to train.
2. Experiment request: To what extent do the approximation errors affect PR2's performance? It would be elucidating for the authors to include an experiment that illustrates where PR2 breaks down (for example, perhaps in higher-dimensional problems).
3. Minor clarification suggestion: In Figure 1: it would be clearer to replace ""Angle"" with ""Perspective.""
4. Minor clarification suggestion: It would be clearer to connect line 18 of Algorithm 1 to equation 29 on Appendix C.
5. Minor clarification suggestion: In section 4.5: ""Despite the added complexity"" --> ""In addition to the added complexity.""
6. Minor clarification: How are the importance weights in equation 7 reflected in Algorithm 1?
7. Minor clarification: In equation 8, what is the significance of integrating over time rather than summing?
8. Minor clarification: There seems to be a contradiction in section 5.2 on page 9. ""the learning outcomes of PR2-AC and MASQL are extremely sensitive to the way of annealing...However, our method does not need to tune the the annealing parameter at all..."" Does ""our method"" refer to PR2-AC here?",8
"The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.

(
Update: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. 

However, I wonder if the reasoning for PR2 is limited to ""self-play"", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.

Also, maybe explain self-play mathematically to make the paper self contained?
)

1. From the abstract, "" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario"". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the ""convergence"" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?

2. Theorem 2 also assumes that the opponent model $\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\rho$ or $Q_\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.

3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.

Minor points:
- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?
- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?
- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.
- Figure 4(a): what are the blue dots?
- Does (11) depend on the amount of data collected from the opponents? If so, how?
- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.
- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?
- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.",7
"Thanks for the updates and rebuttals from the authors. 

I now think including the results for HAT may not be essential for the current version of the paper. I now understand better about the main point of the paper - providing a different setting for evaluating algorithms for combatting CF, and it seems the widespread framework may not accurately reflect all aspects of the CF problems. 

I think showing the results for only 2 tasks are fine for other settings except for DP10-10 setting, since most of them already show CF in the given framework for 2 tasks. Maybe only for DP10-10, the authors can run multiple tasks setting, to confirm their claims about the permuted datasets. (but, I believe the vanilla FC model should show CF for multiple permuted tasks.)

I have increased my rating to ""6: Marginally above acceptance threshold"" - it could have been much better to at least give some hints to overcome the CF for the proposed setting, but I guess giving extensive experimental comparisons could be valuable for a publication. 

=====================
Summary:

The paper evaluates several recent methods regarding catastrophic forgetting with some stricter application scenarios taken into account. They argue that most methods, including EWC and IMM, are prone to CF, which is against the argument of the original paper. 

Pro:
- Extensive study on several datasets, scenarios give some intuition and feeling about the CF phenomenon. 

Con:
- There are some more recent baselines., e.g., Joan Serrà, Dídac Surís, Marius Miron, Alexandros Karatzoglou, ""Overcoming catastrophic forgetting with hard attention to the task"" ICML2018, and it would be interesting to see the performance of those as well. 
- The authors say that the permutation based data set may not be useful. But, their experiments are only on two tasks, while many work in literature involves much larger number of tasks, sometimes up to 50. So, I am not sure whether the paper's conclusion that the permutation-based SLT should not be used since it's only based on small number of tasks. 
- While the empirical findings seem useful, it would have been nicer to propose some new method that can get around the issues presented in the paper. ",6
"# [Updated after author response]
Thank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. 

In its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.

------------------------------------------

# Summary
The authors present an empirical study of catastrophic forgetting (CF) in deep neural networks. Eight models are tested against nine datasets with 10 classes each but a varying number of samples. The authors construct a number of sequential learning tasks to test the model performances in different scenarios. The main conclusion is that CF is still a problem in all models, despite claims in other papers.

# Quality
The paper shows healthy criticism of the methods used to evaluate CF in previous works. I very much like this.

While I like the different experimental set-ups and the attention to realistic scenarios outlined in section 1.2, I find the analysis of the experiments somewhat superficial. The accuracies of each model for each task and dataset are reported, but there is little insight into what causes CF. For instance, do some choices of hyperparameters consistently cause a higher/lower degree of CF across models? I also think the metrics proposed by Kemker et al. (2018) are more informative than just reporting the last and best accuracy, and that including these metrics would improve the quality of the paper.

# Clarity
The paper is generally clearly written and distinct paragraphs are often highlighted, which makes reading and getting an overview much easier. In particular, I like the summary given in sections 1.3 and 1.4.

Section 2.4 describing the experimental setup could be clearer. It takes a bit of time to decipher Table 2, and it would have been good with a few short comments on what the different types of tasks (D5-5, D9-1, DP10-10) will tell us about the model performances. E.g. what do you expect to see from the experiments of D5-5 that is not covered by D9-1 and vice versa? And why are the number of tasks in each category so different (8 vs 3 vs 1)?

I am not a huge fan of 3D plots, and I don't think they do anything good in section 4. The perspective can make it tricky to compare models, and the different graphs overshadow each other. I would prefer 2D plots in the supplementary, with a few representative ones shown in the main paper. I would also experiment with turning Table 3 into a heat map.

# Originality
To my knowledge, the paper presents the largest evaluation of CF in terms of evaluated datasets. Kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the CF even clearer. I think it would be good to cite this paper and briefly discuss it in connection with the current work.

# Significance
The paper is mostly a report of the outcome of a substantial experiment on CF, showing that all tested models suffer from CF to some extent. While this is interesting and useful to know, there is not much to learn in terms of what can cause or prevent CF in DNNs. The paper's significance lies in showing that CF is still a problem, but there is room for improvement in the analysis of the outcome of the experiments.

# Other notes
The first sentence of the second paragraph in section 5 seems to be missing something.

# References
Kemker, R., McClure, M., Abitino, A., Hayes, T., & Kanan, C. (2018). In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410",7
"The paper presents a study of the application of some well known methods on 9 datasets focusing on the issue of catastrophic forgetting when considering a sequential learning task in them.  In general the presentation of concepts and results is a bit problematic and unclear. Comments, such that the paper presents ' a novel training and model selection paradigm for incremental learning in DNNs ' is not justified.  A better description of the results, e.g., in Table 3 should be presented, as well a better linking with the findings; a better structure of the latter would also be required to improve consistency of them. Improving these could make the paper candidate for a poster presentation.  ",5
"+ An interesting problem to study on the stability of RNNs
+ Investigation of spectral normalization to sequential predictions is worthwhile, especially Figure 2
+ Some theoretical justification of SGD for learning dynamic systems following Hardt et al. (2016b).

- The take-home message of the paper is not clear. First, it defines a  notion of stability based on Lipchitz-continuity and proves SGD can learn it. Then the experiments show such a definition is actually not correct, but rather a data-dependent one. 
- The theory only looks at the instantaneous dynamics from time t to t+1, without unrolling the RNNs over time. Then it is not much different from analyzing feed-forward networks. The theorem on SGD is remotely related to the contribution of the paper. 
- The spectral normalization technique that is actually used in experiments is not new",6
"This is an interesting paper that I expect will generate some interest within the ICLR community and from deep learning researchers in general. The definition of stability is both intuitive and sound and the connection to exploding gradients is perhaps the most interesting and useful part of the paper. The sufficient conditions yield practical techniques for increasing the stability of, e.g., an LSTM, by constraining the weight matrices. They also show that stable recurrent models can be approximated by models with finite historical windows, e.g., truncated RNNs. Experiments in Sec 4 suggest that stable models produced by constraining standard RNN architectures can compete with their unconstrained unstable counterparts, and often without necessitating significant changes to architecture or hyperparameters. The perhaps most interesting observations are in Sec 4.3, in which the authors claim that even fundamentally unstable models, e.g., unconstrained RNNs, often operate in a stable regime, at least when being applied to in-sample data. I lean toward acceptance at the moment, but I am eager to discuss with the authors and other reviewers as I am not 100% confident that I fully understood the theory.

SUMMARY

This paper proposes a simple, generic definition of “stability” for recurrent, non-linear dynamical systems such as RNNs: that given two hidden states h, h’, the difference between their updated states given input x is bounded by the product between the difference between the states themselves and a small multiplier. The paper then immediately draws a connection between stability, asserting that unstable models are prone to gradient explosions during gradient descent-based training. In Sec 2.2, the paper presents sufficient conditions for basic RNNs and LSTMs to be stable. Secs 3.2 and 3.3 argue that stable recurrent models can be approximated by feedforward models during both inference and training with a finite history horizon, such as a RNN with a truncated history. Experiments in language and music modeling substantiate this claim: constrained, stable models are competitive with standard unconstrained models. Sec 4.3 sheds some light on this phenomenon, arguing that there is a weaker form of data-dependent stability and that even unstable models may operate in a stable regime for some problems, thus explaining the parity between stable and unstable models.

STRENGTHS

* This paper is surprisingly engaging and easy to read.
* The theorems are clearly stated and the proofs appear sound to me, though I will admit that I am not confident that I would catch a significant bug.
* This paper provides a new (to me, anyway) and thought-provoking analysis of RNNs. In particular, I was especially interested in the observation that stable models can be approximated by truncated models and that there is a connection between stability and long-term dependencies. This seems consistent with the fact that for many problems, non-recurrent models (ConvNets, Transformers, etc.) are often competitive with more complex architectures.

WEAKNESSES

* In practice it seems as though stability may depend on not only choice of  model architecture but also the data themselves. There is probably no good way to know a priori what the stability characteristics of a given data set are, making it tough to apply the ideas of this paper in practice
* The literature review seems a bit limited and appears to ignore the growing body of work on constraining RNN weight matrices to address both exploding and vanishing gradients. For example, I am pretty confident that the singular thresholding trick for renormalizing neural net weights has been  described in the literature previously.
* Although stable and unstable models appear to be competitive in experiments, the theoretical analysis provides no insights into stability and how it relates to accuracy.",7
"In this paper, the authors study the stability property of recurrent neural networks. Adopting the definition of stability from the dynamical system literature, the authors present a generic definition of stable recurrent models and provide sufficient conditions of stable linear RNNs and LSTMs. The authors also study the ""feed-forward"" approximation of recurrent networks and theoretically show that the approximation works for both inference and training. Experimental studies compare the performance of stable and unstable models on various tasks.

The paper is well-written and very pleasant to read. The notations are clear and the claims are relatively easy to follow. The theoretical analysis in Section 3 is novel, interesting and solid. However, the reviewer has concerns about the motivation of the presented analysis and insufficient empirical results.

The stability property only eliminates the exploding gradient problem, but not the vanishing gradient problem. The reviewer suspects that a stable recurrent model always suffers from vanishing gradient. Therefore, stability might not necessarily be a desirable property. There has been a line of work that constrain the weight matrix in RNNs to be orthogonal or unitary so that the gradient won't explode, e.g. [1], [2], [3]. It seems that the orthogonal or unitary conditions are stronger than the stability condition, and are probably less prone to the vanishing gradient problem. 

The vanishing gradient problem is also related to the analysis in Section 3. If a recurrent network is very stable and has vanishing gradient, then a small perturbation of the initial hidden state has little effect on later time steps. This intuitively explains why it can be well approximated by using only the last k time steps. However, the recurrent model itself might not be a desirable model.  In other words, although Theorem 1 shows that $y_T$ and $y_T^k$ can be arbitrarily close, $y_T$ might not be a good prediction.

The experimental study seems weak. Again, in the RNN case, constraining the singular values of the weight matrix is not a new idea. Furthermore, the results in Table 1 seem to suggest that the stable models perform worse than unstable ones. What is the benefit in using stable models? Proposition 2 is only a sufficient condition of a stable LSTM and it seems very restrictive, as the authors point out. This might explain the worse performance of the stable LSTMs in Table 1. The reviewer was expecting more experimental results to support the claims in Section 3. For example, an empirically study of the difference between a recurrent model and a ""feed-forward"" or truncation approximation.

Minor comments:
* Lemma 1: $\lambda$-contractive => $\lambda$-contractive in $h$?
* Theorem 1: $k=O(...)$ => $k=\Omega(...)$? Intuitively, a bigger k leads to a better feed-forward approximation.

[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. ICML, 2016.
[2] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. NIPS, 2016.
[3] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. ICML, 2017.",6
"The results are intriguing. However, similar methods like BN-LSTM [3] and Variational RNNs [4] achieve arguably the same with very similar mechanisms. We do not think they can be considered as orthogonal. This should be addressed by the authors. Also, hard long-term experiments like sequentially predicting pixels (like through MDLSTM-based PixelRNN) or language modelling should be favoured over short sentence image captions. 

It is possible that we will improve our ratings once our concerns are addressed.

Paper Summary:

The authors claim that the gradient along the computational path that goes through the cell state (the linear temporal path or A gradient) of an LSTM carries information about long-term dependencies. Those gradients can be corrupted by the gradient of all other computational paths (i.e. the B gradient). They claim that this makes it hard to learn long-term dependencies and has, therefore, significant negative effects on the convergence speed, training stability, and generalisation performance. They propose a method called h-detach and run experiments on the delayed copy task, sequential MNIST, permuted sequential MNIST (pMNIST), and caption generation on the MS COCO dataset. All show either somewhat improved performance or much more stable learning curves. At every step, h-detach randomly drops all gradients that flow through the h of the standard LSTM, the B gradients, and only keeps the ones from the linear temporal path, the A gradients. Experiments also suggest that the A gradients carry more long-term information than B gradients and that LSTMs with h-detach do not need gradient clipping for successful training.

Positive:

The paper is written clearly. It is well structured and well motivated. H-detach is simple, effective, and somewhat novel (see below). Experiments indicate that its main benefit is training stability as well as minor performance improvements.

Negative:

We are not sure how significant these results are for the following reasons:

- MS COCO image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on long-term dependencies. Why not apply h-detach to a language model such as [1] with official online implementations, e.g., [2]. A setting with PixelRNN [6] based on MD-LSTM [7] would also be a great testbed for h-detach.

- The purpose of h-detach is to scale down the B gradients. However, methods which apply e.g. BatchNorm to the hidden state learn a scale parameter which could be learned by the network explicitly. For the backward pass, this has the effect of scaling down the B gradient. Consider e.g. [3] which also achieves similar training stability on sequential MNIST and pMNIST with little overhead. 

- Another very related method is [4] which properly applies a random dropout mask over the recurrent inputs that is shared across timesteps of an RNN. We think that h-detach is essentially achieving the same in a similar way.

Problems with Introduction and Related Work Section:

- The vanishing gradient problem was first described by Hochreiter in 1991 [5] (not by Bengio in 1994). 

- Intro mentions GRU as if it was separate from LSTM. Clarify that GRU is essentially a variant of vanilla LSTM with forget gates [8]. Since one gate is missing, GRU is less powerful than the original LSTM [9]. 

[1] Zaremba et al. ""Recurrent neural network regularization."" arXiv:1409.2329 (2014).
[2] https://www.tensorflow.org/tutorials/sequences/recurrent
[3] Cooijmans et al. ""Recurrent batch normalization."" arXiv:1603.09025 (2016).
[4] Gal et al. ""A theoretically grounded application of dropout in recurrent neural networks."" NIPS 2016.
[5] Hochreiter, Sepp. ""Untersuchungen zu dynamischen neuronalen Netzen."" Diploma thesis, TUM (1991)
[6] Oord et al. ""Pixel recurrent neural networks."" arXiv preprint arXiv:1601.06759 (2016).
[7] Graves et al. ""Multi-Dimensional Recurrent Neural Networks"" arXiv preprint arXiv:0705.2011 (2011).
[8] Gers et al. “Learning to Forget: Continual Prediction with LSTM.“ Neural Computation, 12(10):2451-2471, 2000. 
[9] Weiss et al. On the Practical Computational Power of Finite Precision RNNs for Language Recognition. arXiv:1805.04908.


Comments after rebuttal:

The  paper has clearly improved. 

It leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. 

When dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:

Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 

Nevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points!

",7
"The author introduces a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, the authors show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent the LSTM from capturing them. Our algorithm prevents the gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. The experimental results show that the proposed algorithm appears to be effective. Some detailed comments are listed as follows, 

1 The h-detach algorithm seems to be the dropout technology. However, the authors did not discuss the relation or difference between the proposed h-detach algorithm and the dropout technology. 

2 The proposed method can transfer the positive knowledge. However, for the transfer learning, one concerned and important issue is that some negative knowledge information can be also transferred. So how to avoid the negative transferring? Some necessary discussions about this should be given in the manuscript.

2 There are many grammar errors in the current manuscript. The authors are suggested to improve the English writing.
",5
"In this paper, the authors propose a simple modification to the training process of the LSTM. The goal is to facilitate gradient propagation along cell states, or the ""linear temporal path"". It blocks the gradient propagation of hidden states with a probability of $1-p$ independently. The proposed method is evaluated on the copying task, sequential MNIST task, and image captioning tasks. The performance is sightly boosted on those tasks.

The paper is well-written. The h-detach method is very simple and easy to implement. It seems novel in dealing with the trainability issue with recurrent networks. Since LSTM is very commonly used, if the method is proved to be effective on other tasks, it will potentially benefit a large portion of the community. However, the reviewer thinks the paper is not sufficiently motivated and the quality of the paper could be further improved by conducting a more thorough analysis of the proposed method, and discussing the connection with other existing methods.

As the motivation of the work, the authors seem to claim that if the magnitude of $B_t$ is much bigger that $A_t$, then the backpropagation will be problematic. Is there any theoretical or empirical support of this claim?

In order to damp the gradient component of $B_t$, it does not have to be stochastic. Can we simply multiply the matrix $B_t$ by a constant factor $p$ during backpropagation? Or regularize the weights $W_{*h}$ to be small so that $\phi_t$ and $\tilde\phi_t$ are small?

It would be interesting to study the effect of the probability $p$ and to suggest an ""optimal"" choice of $p$, either theoretically or empirically. Is it still possible to train the model with a very small $p$?

The h-detach method seems to have a flavor of dropout, but the ""dropout"" only happens during backpropagation. The design goal also resembles the peephole LSTM, that is to disentangle the cell state and the hidden state. Is there any possible connections between the proposed method and the dropout and peephole LSTM?

The reviewer understands that a one percent difference in the accuracy on MNIST is probably not very meaningful, but it seems that the SOTA performance on pMNIST is at least 94.1% [1].

[1] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. NIPS, 2016.",6
"The paper introduces an iterative method to generate deformed images for adversarial attack. The core idea is to perturb the correctly classified image by iteratively applying small deformations, which are estimated based on a first-order approximation step, until the image is misclassified. Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. 

The idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about. The method is clearly presented and the results are mostly easy to access.  However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Particularly, the first-order approximation strategy (as shown in Eq.4 and Eq.5) is quite confusing. On one side (see Eq.4), the deformation \tau should be small enough in scale to make an accurate approximation. On the other side (see Eq. 5), \tau is required to be sufficiently large in order to generate misclassification. Such seemingly conflicting rules for estimating the deformation makes the proposed method less rigorous in math. 
As another downside, the related adversarial training procedure is not fully addressed. The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. In the meanwhile, the mentioned adversarial training framework follows straightforwardly from PGD (Madry et al. 2018), and thus the novelty of this contribution is also weak. More importantly, it is not clear at all, both in theory and algorithm, whether the advocated gradual deformation attack and defense can be unified inside a joint min-max/max-min learning formulation, as what PGD is rooted from.

Pros: 

- The way of constructing deformation adversarial is interesting and novel
- The paper is mostly clearly organized and presented.

Cons:

- The motivation of approach is questionable. 
- The related adversarial training problem remains largely unaddressed.
- Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. ",6
"The paper proposes a new way to construct adversarial examples: do not change the intensity of the input image directly, but deform the image plane (i.e. compose the image with Id + tau where tau is a small amplitude vector field).

The paper originates from a document provably written in late 2017, which is before the deposit on arXiv of another article (by different authors, early 2018) which was later accepted to ICLR 2018 [Xiao and al.]. This remark is important in that it changes my rating of the paper (being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to [Xiao and al.]).

Pros:
- the paper is well written, very easy to read, well explained (and better formalized than [Xiao and al.]);
- the idea of deforming images is new (if we forget about [Xiao and al.]) and simple;
- experiments show what such a technique can achieve on MNIST and ImageNet. Interestingly, one can see on MNIST the parts of the numbers that the adversarial attack is trying to delete/create.

Cons:
- the paper is a bit weak, in that it is not very dense, and in that there is not much more content than the initial idea;
- for instance, more discussions about the results obtained could have been appreciated (such as my remark above about MNIST);
- for instance, a study of the impact of the regularization would have been interesting (how does the sigma of the Gaussian smoothing affect the type of adversarial attacks obtained and their performance -- is it possible to fool the network with [very] smooth deformations?);
- for instance, what about generating adversarial examples for which the network would be fully (wrongly) confident? (instead of just borderline unsure); etc.
- The interpolation scheme (how is defined the intensity I(x,y) for a non-integer location (x,y) within the image I) is rather important (linear interpolation, etc.) and should be at least mentioned in the main paper, and at best studied (it might impact the gradient descent path and the results);
- question: does the algorithm converge? could there be a proof of this? This is not obvious, as the objective potentially changes with time (selection of the current m best indices k of |F_k - F_l|). Also, the final overshoot factor (1+eta) is not very elegant, and not guaranteed to perform well if tau* starts being not small compared to the second derivative (i.e. g''.tau^2 not small) while I guess that for image intensities, spatial derivatives can be very high if no intensity smoothing scheme is used.
- note: the approximation tau* = sum_i tau_i (section 2.3) does not stand in the case of non-small deformations.
- still in section 2.3, I do not understand the statement ""given that \nabla f is moderate"": where does this property come from? or is ""given"" meant to be understood as ""provided..."" (i.e. under the assumption that...)?
- computational times could have been given (though I guess they are reasonable).

Other remarks:
- suggestion: I find the ""slight abuse of notation"" (of confusing the derivative with the gradient) a bit annoying and suggest to use a different symbol, such as \nabla g. This could be useful in particular in the following perspective:
- Mathematical side note: the ""gradient"" of a functional is not a uniquely-defined object in that it depends on the metric chosen in the tangent space. More clearly: the space of small deformations tau comes with an inner product (here L2, but one could choose another one), and the gradient \nabla g obtained depends on this inner product choice M, even though the derivative Dg is the same (they are related by Dg(tau) = < \nabla_M g | tau >_M for any tau). The choice of the metric can then be seen as a prior over desired gradient descent paths. In the paper, the deformation fields get smoothed by a Gaussian filter at some point (eq. 7), in order to be smoother: this can be interpreted as a prior (gradient descent paths should be made of smooth deformations) and as an associated inner product change (there do exist a metric M such that the gradient for that metric is \nabla_M g = S \nabla_L2 g). It is possible to favor other kind of deformations (not just smooth ones, but for instance rigid ones, etc. [and by the way this could make the link with ""Manitest: Are classifiers really invariant?"" by Fawzi and Frossard, BMVC 2015, who observe that a rigid motion can affect the classifier output]). If interested, you can check ""Generalized Gradients: Priors on Minimization Flows"" by Charpiat et al. for general inner products on deformations (in particular favoring rigid motion), and ""Sobolev active contours"" by Sundaramoorthi et al. for inner products more dedicated to smoothing (such as with the H1 norm).
- Note: about the remark in section 3.2: deformation-induced transformations are a subset of all possible transformations of the image (which are all representable with intensity changes), so it is expected that a training against attacks on the intensity performs better than a training against attacks on spatial deformations.

",7
"In this paper, the authors proposed a new attack using deformation. The results are quite realistic to the naked eyes (at least for the example shown).  The idea is quite simple, generate small displacement and resample (interpolate) image until the label flips.

- I think this is a good contribution, It is a kind of attack we should consider.
- One thing which is good to consider is the type of interpolation. I believe the success rate would be different for linear versus say B-spline interpolation. Also, the width of the smoothing applied to the deformation field has an impact. The algorithm is straightforward, there is no reason to experiment with those.

- It is useful to report pixel displacement in Table 1. The reported values are not intuitive, the **average** displacement for Inception-v3 is 0.59.  Here is my back of envelope conversion of 0.59 which is probably off:

299 (# pixels of the smaller axis 299 for the Inception) x 1/2 (image are centered) x 0.59 =  88 pixels

This is huge! I think I am calculating something incorrectly because in Fig3,4 those displacements are not big. 

- The results of Table 2 is interesting. Why a networked trained with PGD is more robust to ADef attack that a network trained adversarially with Adef?




Minor:
- The paper is a bit nationally convoluted for no good reason, the general idea is straightforward. 
",7
"The paper shows Turing completeness of two modern neural architectures, the Transformer and the Neural GPU. The paper is technically very heavy and gives very little insight and intuition behind the results. Right after surveying the previous work the paper starts stacking definitions and theorems without much explanations.

While technical results are potentially quite strong I believe a major revision to the paper might be necessary in order to clarify the ideas. I would even suggest to split the paper into two, one about each architecture as in the current form it is quite long and difficult to follow. 

Results are claimed to hold without access to external memory, relying just on the network itself to represent the intermediate results of the computation. I am a bit confused by this statement -- what if the problem at hand is, say EXPSPACE-complete? Then the network would have to be of exponential size (or more generally of arbitrary size which is independent of the input). In this case the claim about not using external memory seems to be kind of vacuous as the network itself has unbounded size. The whole point of Turing-completeness is that the program size is independent of the input size so there seems to be some confusion here.
",6
"This paper presents interesting theoretical results on Turing completeness of the Transformer and Neural GPU architectures, as modern architectures based on attention and convolutions, under particular assumptions. The basis of proofs in the paper relies on Turing completeness of the seq2seq architecture, which is Turing complete since it contains Turing complete RNNs. Turing completeness of the Transformer and the Neural GPU is proven by showing they can simulate seq2seq architecture.

The Transformer, using additive hard attention and residual connections, is Turing complete in the case when positional encoding is used. Otherwise, if no positional encoding is used, the model is order-invariant which makes it not Turing complete.

A version of the Neural GPU, dubbed Uniform Neural GPU is proven to be Turing complete. Moreover, the presented theoretical results are backed by a recent publication by Karlis and Liepins. Interestingly, Neural GPUs using circular convolutions are not Turing complete, while the ones using zero padding are.

The repercussion of the paper for similar architectures is the not just in the theoretical section but also in a set of discoveries of practical importance, like the importance of the use of residual connections, positional coding in Transformers, and zero padding in Neural GPUs.

Albeit the paper presents an original and significant theoretical progress and is well written, it is not fit for ICLR, primarily as the paper is impossible to review and verify without a thorough perusal and analysis of the appendix. Although the results and the proof sketches fit the body of the paper, the necessity of verifying proofs makes this paper 23 pages long and makes it a better fit for a journal and not a conference.",7
"This paper seeks to answer the question of whether models which process sequences, but are not strictly classical RNNs, are Turing complete.

The authors present proofs that both the Transformer and Neural GPU are turing complete, under certain conditions. I do not consider myself qualified to properly verify the proof but it seems to be presented clearly. The authors note that the conditions involved are not how these models are used in the real world. Given the complex construction required for this more theoretically based proof, it seems reasonable that this should be published now, rather than waiting until the further work discussed in the final section is completed.

I have a number of questions where if a brief answer is possible, this would enhance the manuscript. The main question is, of the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice? For example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?

The rational numbers assumption is interesting - again I wonder how this would affect the model in reality, obviously all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.

Does the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?

Overall this paper is novel and interesting, I have to give a slightly low confidence score because I'm unfamiliar with a lot of the background here (eg the Siegelamnn & Sontag work). The paper does seem concise and well written.

typos and minor points:

Circular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?

paragraph above equation 5, 'vectores' -> 'vectors'",7
"Quality: sufficient though there are issues. Work done in automatic speech recognition on numerous variants of recurrent models, such as interleaved TDNN and LSTM (Peddinti 2017), is completely ignored [addressed in the revision]. The description of derivatives needs to mention the linear relationship between input features and derivatives (see trajectory HMMs by Zen and Tokuda) [addressed in the revision]. TIMIT is a very simple task [addressed by adding WSJ experiments]. Derivations in the appendices could be connected better [addressed in the revision]. 
 
Clarity: sufficient. It would be good to see some discussion of 1) split activations and other possible options [short comment added in the revision] if any 2) expressions of derivatives and their connection to standard RNN derivatives [short comment added in the revision], 3) computational complexity [addressed in the revision]. 

Originality: sufficient. This paper describes the extension of quaternion feed-forward neural networks to recurrent neural networks and a parameter initialisation method in the quaternial domain.

Significance: sufficient. 

Pros: Audience interested in quaternial neural networks would benefit from this publication. Experimental results even if limited suggest that quaternial representation may offer a significant reduction in the number of model parameters at no loss in performance.  

Cons: The choice of derivatives to yield quaternions as there are other more interesting views to contemplate both in speech and other fields. A simple task makes it hard to judge how the quaternion extension would scale.  

Other:

The format of references, the use of a number in parentheses, is unusual and distractive. [fixed in the revision] 
Please at least name all the terms used in the main paper body even if they are defined later in the appendix (e.g. h_{t}^{*} in equation 10). [fixed in the revision]
Do both W_{hh} and b_{h} contain the same \delta_{hh}^{t} term in their update equation 11? [fixed in the revision]
Page 7 by mistake mentions 18.2% which cannot be found in the Table 1. [fixed in the revision]
Page 12 ""is equals to"" [remains in the revision]
",7
"After the discussion with authors, I am happy to recommend acceptance.
————————————————————

1.	In “Consequently, for each input vector of size N, output vector of size M, dimensions are split into four parts: the first one equals to r, the second is xi, the third one equals to yj, and the last one to zk to compose a quaternion Q = r1 + xi + yj + zk”, are you splitting dimension M or M\times N? And if you split M \times N (I believe that’s what you are doing), in which order you are splitting (row major right?) Please explain.
2.	I did not understand why authors didn’t go in the negative direction of the gradient in Eq. (10-11)?
3.	In section 3.4, authors mentioned “Moreover, an hyper-complex parameter cannot be simply initialized randomly and component-wise, due to the interactions between components.” which I strongly agree. But in Eq. (7) and (9) why the update rules and activation function are applied component wise?
4.	I really like the elegance in the parameter initialization. Couple of minor things here: (1) It’s better to mention in Eq. (16) why E(|W|) is 0 because of symmetry. (2) Reference should be 6.1 instead of 5.1.
5.	Another reasonable baseline will be using a complex network like (https://openreview.net/forum?id=H1T2hmZAb) and use the first two terms in Eq. (19) for representation. This will also possibly justify the usefulness of using higher order partials. 
6.	The authors mentioned multiple times about the achieved state-of-the-art results without giving any citation. As a reader not well versed in the acoustic domain, it will be nice to see some references to cross-validate the claim made.



General Comments:
1.	I understand the necessity of defining RNN/ LSTM model in the space of quaternions. But unit quaternions can be identified with other spaces where convolution is defined recently, e.g., with S^3 (https://arxiv.org/abs/1809.06211). I can see that this paper is contemporary, but at least can authors comment on the applicability of this general method in their case? Given that in NIPS’18 the following paper talked about RNN model on non-Euclidean spaces (https://arxiv.org/pdf/1805.11204.pdf), one can extend these ideas to develop an RNN model in the space of quaternions. Authors should look into it rigorously as future directions? But at least please comments on the applicability.
2.	The experimental results section is somewhat weak, the overall claim of using fewer parameters and achieving comparable results is only validated on TIMIT data. More experimentation is necessary. 
3.	In terms of technical novelty, though quaternion algebra is well-known, I like the parameter initialization algorithm. I can see the merit of this in ML/ vision community.  

Pros: 
1. Nice well grounded methodological development on well-known algebra. (simple but elegant, so that's good).
2. Nicely written and all the maths check out (that's good).
3. Experimental result on TIMIT dataset shows usefulness in terms of using fewer parameters (but still can achieve SOA results).

Cons:
1. See my comments above. I expect the authors to rebut/ address the aforementioned comments. Overall though simple but nice (and necessary) development of RNN/ LSTM framework in the space of quaternions. 
2. Lacks extensive experimental validation.

My reason for my rating is mainly because of (1) lack of experimental validation. (2) being aware of the recent development of general RNN model on non-Euclidean spaces, I want some comments in this direction (see detailed comment and reference above).
",7
"The paper takes a good step toward developing more structured representations by exploring the use of quaternions in recurrent neural networks.  The idea is motivated by the observation that in many cases there are local relationships among elements of a vector that should be explicitly represented.  This is also the idea behind capsules - to have each ""unit"" output a vector of parameters to be operated upon rather than a single number.   Here the authors show that by incorporating quaternions into the representations used by RNNs or LSTMs, one achieves better performance at speech recognition tasks using fewer parameters.

The quaternionic representation of the spectrogram chosen here seems a bit arbitrary.  Why are these the attributes to be packaged together?  its not obvious.  Shouldn't this be learned?

",8
"Summary:
The paper presents a platform for predicting images of objects interacting with each other under the effect of gravitational forces. Given an image describing the initial arrangement of the objects in a scene, the proposed architecture first detects the objects and encode them using a perception module. A physics module then predicts the final arrangement of the object after moving under the effects of gravity. A rendering module takes as input the predicted final positions of objects and returns an image. The proposed architecture is trained by using pixel labels only, by reducing the gaps between the predicted rendered images and the images returned by the MuJuCo physics engine. This error's gradient is back-propagated to the physics and perception modules. The proposed platform is also used for planning object placements by sampling a large number of object shapes, orientations and colors, predicting the final configurations, and selecting initial placements that lead to final configurations that are as close as possible to given goal configurations using the L2 norm in the VGG features. Experiments performed in a simple blocks world show that the proposed approach is not only useful for prediction, but can also be used for planning object placements.
Clarity:
The paper is not very well written. The description of the architecture should be much more precise. Some details are given right before the conclusion, but they are still just numbers and leave a lot of questions unanswered. For instance, the perception module is explained in only a few line in subsection 2.1. Some concrete examples could help here. How are the object proposals defined? How are the objects encoded? What exactly is being encoded here? Is it the position and orientation? 
Originality:
The proposed architecture seems novel, but there are many closely related works that are based on the same idea of decomposing the system into a perception,  a physics simulation, and a rendering module. Just from the top of my head, I can think of the SE3-Nets. There is also a large body of work from the group of Josh Tanenbaum on similar problems of learning physics and rendering. I think this concept is not novel anymore and the expectations should be raised to real applications. 
Significance:
The simplicity of the training process that is fully based on pixel labeling makes this work interesting. There are however some issues related to the experimental evaluation that remains unsatisfactory. First, all the experiments are performed on a single benchmark, we cannot easily draw conclusions about a given algorithm based on a single benchmark. Second, this is a toy benchmark that with physical interactions that are way less complex than interactions that happen between real objects. The objects are also not diverse enough in their appearances and textures. I wonder why the authors avoided collecting a dataset of real images of objects and using it to evaluate their algorithm instead of the toy artificial data. I also suspect that with 60k training images, you can easily overfit this task. How can this work generalize to real physical interactions? How can you capture mass and friction, for example?
Planning is based on sampling objects of different shapes and colors, do you assume the existence of such library in advance? 
The baselines that are compared to are also not very appropriate. For instance, comparing to no physics does not add much information. We know that the objects will fall after they are dropped, so the ""no physics"" baseline will certainly perform badly. Comparisons to SAVP are also unfair because it requires previous frames, which are not provided here, and SAVP is typically used for predicting the very next frames and not the final arrangements of objects, as done here.
In summary: I think the authors are on something here and the idea is great. However, the paper needs to be made much clearer and more precise, and the experimental evaluation should be improved by performing experiments in a real-world environment. Otherwise, this paper will not have much impact. 

Post-rebuttal update:
The paper was substantially improved. New experiments using real objects have been included, this clearly demonstrates the merits of the proposed method in robotic object manipulation. ",7
"edit:  the authors nicely revised the submission, I think it is a very good paper. I increased my rating.

-----

This paper presents a method that learns to reproduce 'block towers' from a given image. A perception model, a physics engine model, and a rendering engine are first trained together on pairs of images.
The perception model predicts a representation of the scene decomposed into objects;  the physics engine predicts the object representation of a scene from an initial object representation; the rendering engine predicts an image given an object representation.

Each training pair of images is made of the first image of a sequence when introducing an object into a scene, and of the last image of the sequence, after simulating the object's motion with a physics engine. The 3 parts of the pipeline (perception, physics, rendering) are trained together on this data.

To validate the learned pipeline, it is used to recreate scenes from reference images, by trying to introduce objects in an empty scene until the given scene can be reproduced. It outperforms a related pipeline that lacks a scene representation based on objects.

This is a very interesting paper, with new ideas:
- The object-based scene representation makes a lot of sense, compared to the abstract representation used in recent work. 
- The training procedure, based on observing the result of an action, is interesting as the examples are easy to collect (except for the fact that the ground truth segmentation of the images is used as input, see below).

However, there are several things that are swept 'under the carpet' in my opinion, and this should be fixed if the paper is accepted.

* the input images are given in the form of a set of images, one image corresponding to the object segmentation. This is mentioned only once (briefly) in the middle of the paragraph for Section 2.1, while this should be mentioned in the introduction, as this makes the perception part easier. There is actually a comment in the discussion section and the authors promised to clarify this aspect, which should indeed be more detailed. For example, do the segments correspond to the full objects, or only the visible parts?

* The training procedure is explained only in Section 4.1. Before reaching this part, the method remained very mysterious to me. The text in Section 4.1 should be moved much earlier in the paper, probably between current sections 2.3 and 2.4, and briefly explained in the introduction as well.
This training procedure is in fact fully supervised - which is fine with me: Supervision makes learning 'safer'. What is nice here is that the training examples can be collected easily - even if the system was not running in a simulation.

* if I understand correctly the planning procedure, it proceeds as follows:
- sampling 'actions' that introduce 1 object at a time (?)
- for each sampled action, predicting the scene representation after the action is performed, by simulating it with the learned pipeline, 
- keeping the action that generates a scene representation close to the scene representation computed for the goal image of the scene.
- performing the selected action in a simulator, and iterate until the number of performed actions is the same as the number of objects (which is assumed to be known).

-> how do you compare the scene representation of the goal image and the predicted one before the scene is complete? Don't you need some robust distance instead of the MSE?
-> are the actions really sampled randomly?  How many actions do you need to sample for the examples given in the paper?

I also have one question about the rendering engine:  Why using the weighted average of the object images? Why not using the intensity of the object with the smallest predicted depth?  It should generate sharper images. Does using the weighted average make the convergence easier?
",9
"A method is proposed, which learns to reason on physical interactions of different objects (solids like cuboids, tetrahedrons etc.). Traditionally in related work the goal is to predict/forecast future observations, correctly predicting (and thus learning) physics. This is also the case in this paper, but the authors explicitly state that the target is to evaluate the learned model on downstream tasks requiring a physical understanding of the modelled environment.

The main contribution here lies in the fact that no supervision is used for object properties. Instead, a mask predictor is trained without supervision, directly connected to the rest of the model, ie. to the physics predictor and the output renderer. The method involves a planning phase, were different objects are dropped on the scene in the right order, targeting bottom objects first and top objects later. The premise here is that predicting the right order of the planning actions requires understanding the physics of the underlying scene.

I particularly appreciated the fact, that object instance renderers are combined with a global renderer, which puts individual images together using predicted heatmaps for each object. With a particular parametrization, these heatmaps could be related to depth maps allowing correct depth ordering, but depth information has not been explicitly provided during training.

Important issues:

One of the biggest concerns is the presentation of the planning algorithm, and more importantly, a proper formalization of what is calculated, and thus a proper justification of this part. The whole algorithm is very vaguely described in a series of 4 items on page 4. It is intuitively almost clear how these steps are performed, but the exact details are vague. At several steps, calculated entities are “compared” to other entities, but it is never said what this comparison really results in. The procedure is reminiscent of particle filtering, in that states (here: actions) are sampled from a distribution and then evaluated through a likelihood function, resulting in resampling. However, whereas in particle filtering there is clear probabilistic formalization of all key quantities, in this paper we only have a couple of phrases which describe sampling and “comparisons” in a vague manner.

Since the procedure performs planning by predicting a sequence of actions whose output at the end can be evaluated, thus translated into a reward, I would have also liked a discussion (or at least a remark) why reinforcement learning has not been considered here.

I am also concerned by an overclaim of the paper. As opposed to what the paper states in various places, the authors really only evaluate the model on video prediction and not on other downstream tasks. A single downstream task is very briefly mentioned in the experimental section, but it is only very vaguely described, it is unclear what experiments have been performed and there is no evaluation whatsoever.

Open questions:

Why is the proposed method better than one of the oracles?

Minor remarks:

It is unclear what we see in image 4, as there is only a single image for each case (=row) and method (=column). 

The paper is not fully self-contained. Several important aspects are only referred to by citing work, e.g. CEM sampling and perceptual loss. These are concepts which are easy to explain and which do not take much space. They should be added to the paper.

A threshold is mentioned in the evaluation section. A plot should be given showing the criterion as a function of this threshold, as is standard in, for instance, pose estimation literature.

I encourage the authors to use the technical terms “unary terms” and “binary terms” in the equation in section 2.2. This is the way how the community referred to interactions in graphical models for relational reasoning long before deep learning showed up on the horizon, let’s be consistent with the past.

I do not think that the physics module can be reasonable be called a “physics simulator” as has been done throughout the paper. It does not simulate physics, it predicts physics after learning, which is not a simulation.

A cube has not been confused with a rectangle, as mentioned in the paper, but with a rectangular cuboid. A rectangle is a 2D shape, a rectangular cuboid is a 3D polyhedron.
",5
"In this paper the authors revisit the problem of multi-class classification and propose to use pairwise similarities (more accurately, what they use is the co-occurrence pattern of labels) instead of node labels. Thus, having less stringent requirements for supervision, their framework has broader applicability: in supervised and semi-supervised classification and in unsupervised cross-task transfer learning, among others.

Pros: The idea of using pairwise similarities to enable a binary classifier encapsulate a multi-class classifier is neat.

Cons: My main gripe is with the conditional independence assumption on pairwise similarities, which the author use to simplify the likelihood down to a cross-entropy. Such an assumption seems too simple to be useful in problems with complicated dependence structure. Yes, the authors conduct some experiments to show that their algorithms achieve good performance in some benchmark datasets, but a careful discussion (if possible, theoretical) of when such an assumption is viable and when it is an oversimplification is necessary (analogous assumptions are used in naive Bayes or variational Bayes for simplifying the likelihood, but those are much more flexible, and we know when they are useful and when not). 

Secondly, by using co-occurrence patterns, one throws away identifiability---the (latent) labels are only learnable up to a permutation unless external information is available. This point is not made clear in the paper, and the authors should describe how they overcome this in their supervised classification experiments.
",6
"This paper proposed how to learn multi-class classifiers without multi-class labels. The main idea is shown in Figure 2, to regard the multi-class labels as hidden variables and optimize the likelihood of the input variables and the binary similarity labels. The difference from existing approaches is also illustrated in Figure 1, namely existing methods have binary classifiers inside multi-class classifiers while the proposed method has multi-class classifiers inside binary classifiers. The application of this technique to three general problem settings is discussed, see Figure 3.

Clarity: Overall, it is very well written. I just have two concerns.

First, the authors didn't discuss the underlying assumption of the proposed method except the additional independence assumption. I think there should be more underlying assumptions. For example, by the definition P(S_{i,j}=0 or 1|Y_i,Y_j) and the optimization of L(theta;X,S), does the ""cluster assumption"" play a role in it? The cluster assumption is popular in unsupervised/semi-supervised learning and metric learning where the X part of training data is in a form of pairs or triples. However, there is no such an assumption in the original supervised multi-class learning. Without figuring out the underlying assumptions, it is difficult to get why the proposed method works and when it may fail.

Second, there are too many abbreviations without full names, and some of them seem rather important such as KLD and KCL. I think full names of them should be given for the first time they appear. This good habit can make your audience more broad in the long run.

Novelty: As far as I know, the proposed approach is novel. It is clear that Section 3 is original. However, due to the writing style, it is hard to analyze which part in Section 4 is novel and which part is already known. This should be carefully revised in the final version. Moreover, there was a paper in ICML 2018 entitled ""classification from pairwise similarity and unlabeled data"", in which binary classifiers can be trained strictly following ERM without introducing the cluster assumption. The same technique can be used for learning from pairwise dissimilarity and unlabeled data as well as from pairwise similarity and dissimilarity data. I think this paper should be included in Section 2, the related work. 

Significance: I didn't carefully check all experimental details but the experimental results look quite nice and promising. Given the fact that the technique used in this paper can be applied to many different tasks in machine learning ranging from supervised learning to unsupervised learning, I think this paper should be considered significant.

Nevertheless, I have a major concern as follows. In order to derive Eq. (2), the authors imposed an additional independence assumption: given X_i and X_j, S_{i,j} is independent of all other S_{i',j'}. Hence, Eqs. (2) and (3) approximately hold instead of exactly hold. Some comments should be given on how realistic this assumption is, or equivalently, how close (1) and (3) are. One more minor concern: why P(X) appears in (1) and then disappears in (2) and (3) when Y is marginalized?  ",7
"The work is a special case of density estimation problems in Statistics, with a use of conditional independence assumptions to learn the joint distribution of nodes. While the work appears to be impressive, such ideas have typically been used in Statistics and machine learning very widely over the years(Belief Propagation,  Topic modeling with anchor words assumptions etc...). This work could be easily extended to multi-class classifications where each node belongs to multiple classes. It would be interesting to know the authors' thoughts on that. The hard classification rule in the paper seems to be too restrictive to be of use in practical scenarios, and soft classification would be a useful pragmatic alternative. 
",5
"Summary
This paper proposes a NeuroFovea (NF) model for generation of point-of-fixation metamers. As opposed to previous algorithms which use gradient descent to match the local texture and image statistics, NF proposed to use a style transfer approach via an Encoder-Decoder style architecture, which allows it to produce metamers in a single forward pass, allowing it to achieve a significant speed-up as compared to early approaches.

Pros
-The paper tackles a very intriguing topic.
-The paper is very well written using concise and clear language allowing it to present a large -amount of information in the 10 pages + appendix.
-The paper provides a thorough discussion of both the problem, related work and the model itself.
-A single forward pass nature of the model allows it to achieve a 1000x speed-up in generating metamers as opposed to previous GD based approaches.
-The authors provide enough details to allow for reproducibility.

Cons
-(Not necessarily a negative) Requires a very careful reading as the paper provides a lot of information (though as mentioned it is very well written)
-The quantitative evaluation is somewhat lacking in that there are no quantitative psychophysical experiments to compare this model to competing ones across different observers. For example, it would have been interesting to compare the ability of observers to distinguish between original images and metamers generated by different models. 

Additional comments
On page 10., you show Fig. 13 however you mention at the end of the first paragraph you further elaborate on Fig 13. in the Supplementary Materials. I think it would be better to either provide more discussion in the text and refer to the figure, or just move it fully to Supplementary materials.

Also, in the qualitative comparison of various models you mention that SideEye runs in milliseconds whereas NF runs in seconds. It would be interesting to discuss the potential trade-off between speed and the quality of generated metamers between the models.",7
"This paper presents an interesting analysis of metamerism and a model capable of rapidly producing metamers of value for experimental psychophysics and other domains.

Overall I found this work to be well written and executed and the experiments thorough. Specific points on positives and negatives of the work follow:

Positives:
- The paper shows a solid understanding of the literature in this domain and presents a strong motivation
- The problem itself is addressed at a deep level with many nuanced (but important) considerations discussed
- Ultimately the results of the model seem convincing in particular with the accompanying psychophysical experiments

Negatives:
- (Maybe not a negative, but a question) At the extreme tradeoff between intrinsic structure and texture, the notion of a metamer seems somewhat obscured. At what point is a metamer no longer a metamer?
- (Also not necessarily a negative) Exercising SSIM is a valid decision given it's widespread use. I am curious if MS-SSIM, IW-SSIM or other metrics make any significant difference. ",8
"Summary:
The paper proposes a fast method for generating visual metamers – physically different images that cannot be told apart from an original – via foveated, fast, arbitrary style transfer. The method achieves the same goal as an earlier approach (Freeman & Simoncelli 2011): locally texturizing images in pooling regions that increase with eccentricity, but is orders of magnitude faster. The authors perform a psychophysical evaluation to test how (in)discriminable their synthesized images are amongst each other and compared with originals. Their experiment replicates the result of Freeman & Simoncelli of a V2-like critical scaling in the synth vs. synth condition, but shows that V1-like or smaller scaling is necessary for the original vs. synth condition.

I reviewed an earlier version of this paper for a different venue, where I recommended rejection. The authors have since addressed some of my concerns, which is why I am more positive about the paper now.

Strengths:
+ The motivation for the work is clear and the implementation straightforward, combining existing tools from style transfer in a novel way.
+ It's fast. Rendering speed is indeed a bottleneck in existing methods, so a fast method is useful.
+ The perceptual quality of the rendered images is quantified by psychophysical testing.
+ The role of the scaling factor for the pooling regions is investigated and the key result of Freeman & Simoncelli (pooling regions scale with 0.5*eccentricity) is replicated with the new method. In addition, the result of Wallis et al. (2018) that lower scale factors are required for original vs. synth is replicated as well.


Weaknesses:
- Compared with earlier work, an additional fudge parameter (alpha) is introduced. It is not clear why it is necessary and it complicates interpretation.
- The paper contains a number of sections with obscure mathiness and figures that I can't follow and whose significance is unclear.


Conclusion:
The work is well motivated, the method holds up to its promise of being fast and is empirically validated. However, it feels quite ad-hoc and the writing of the paper is very obscure at various places, which leaves room for improvement.


Details:

- The motivation for introducing alpha not clear to me. Wasn't the idea of F&S that you can reduce the image to its summary statistics within a pooling region whose size scales with eccentricity? Why do you need to retain some content information in the first place? How do images with alpha=1 (i.e. keep only texture) look?

- Related to above, why does alpha need to change with eccentricity? Experiment 1 seems to suggest that changing alpha leads to similar SSIM differences between synths and originals as F&S does, but what's the evidence that SSIM is a useful/important metric here?

- Again related to above, why do you not use the same approach of blending pooling regions like F&S did instead of introducing alpha?

- I would like to know some details about the inference of the critical scaling. It seems surprisingly spot on 0.5 as in F&S for synth vs. synth, but looking at the data in Fig. 12 (rightmost panel), I find the value 0.5 highly surprising given that all the blue points lie more or less on a straight line and the point at a scaling factor of 0.5 is clearly above chance level. Similarly, the fit for original vs. synth does not seem to fit the data all that well and a substantially shallower slope seems equally plausible given the data. How reliable are these estimates, what are the confidence intervals, and was a lapse rate included in the fits (see Wichmann & Hill 2001)?

- I don't get the point of Figs. 4, 13 and 14. I think they could as well be removed without the paper losing anything. Similarly, I don't think sections 2.1 and the lengthy discussion (section 5) are useful at all. Moreover, section 3 seems bogus. I don't understand the arguments made here, especially because the obvious options (alpha=1 or overlapping pooling regions; see above) are not even mentioned.

- How is the model trained? Do the authors use the pre-trained model of Huang & Belongie or is the training different in the context of the proposed method? I could only find the statement that the decoder is trained to invert the encoder, but that doesn't seem to be what Huang & Belongie's model does and the paper does not say anything about how it's trained to invert. Please clarify.

- At various places the writing is somewhat sloppy (missing words, commas, broken sentences), which could have been avoided by carefully proof-reading the paper.",7
"This paper applies a reparameterization trick to estimate the gradients objectives encountered in variational autoencoder based frameworks with continuous latent variables.  Especially the authors use this double reparameterization trick on Importance Weighted Auto-Encoder (IWAE) and Reweighted Wake-Sleep (RWS)  methods. Compared to IWAE, the developed method's SNR does not go to zero with increasing the number of particles.

Overall, I think the idea is nice and the results are encouraging. I checked all the derivations, and they seem to be correct. Thus I recommend this paper to be accepted in its current form.",7
"The paper observes the gradient of multiple objective such as IWAE, RWS, JVI are in the form of some “reward” multiplied with score function which can be calculated with one more reparameterization step to reduce the variance. The whole paper is written in a clean way and the method is effective.

I have following comments/questions:

1. The conclusion in Eq(5) is correct but the derivation in Sec. 8.1. may be arguable. Writing \phi and \tilde{\phi} at the first place sets the partial derivative of \tilde{\phi}  to \phi as 0. But the choice of \tilde{\phi} in the end is chosen as \phi. If plugging  \phi to \tilde{\phi}, the derivation will change. The better way may be calculating both the reparameterization and reinforce gradient without redefining a \tilde{\phi}.

2. How does the variance of gradient calculated where the gradient is a vector? And how does the SNR defined in the experiments?

3. How does the variance reduction from DReG changes with different value of K?

4. Is there any more detailed analysis or intuition why the right hand side of Eq(5) has lower variance than the left hand side?",7
"Overall:
This paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.
The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising.

Clarity:
The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand.

Significance:
I think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score.
The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.

Question and minor comments:
In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians.
In this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL.
Faced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method?  
To clarify the usefulness of this method, I think the additional experimental comparisons are needed.

About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased ""experimentally"".

Followings are minor comments.
In experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance.
I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator.
However, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper.

In Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure?
(Same for Figure 4)",6
"The paper introduces RCPO, a model-free deep RL algorithm for learning optimal policies that satisfy some per-state constraint on expectation. The derivation of the algorithm is quite straightforward, starts from the definition of constrained optimization problem, and proceed by forming and optimizing the Lagrangian. Additionally, a value function for the constraint is learned. The algorithm is only compared to a baseline optimizing the Lagrangian directly using Monte-Carlo sampling.

The paper has two major problems. First, while the derivation of the method makes intuitively sense, it is supported by vaguely stated theorems, which mixes rigorous guarantees with practical approximations. For example, Equation 4 assumes strong duality. How would the result change if weak duality was used instead? The main result in Theorem 1 makes the assumption that dual variable is constant with respect the policy, which might be true in practice, but it is not obvious how the approximation affects the theory. Further, instead of simply referring to prior convergence results, I would strongly suggest including the exact prior theorems and assumptions in the appendix.

The second problem is the empirical validation, which is incomplete and misleading. Constrained policy optimization is not a new topic (e.g. work by Achiam et al.), so it is important to compare to the prior works. It is stated in the paper that the prior methods cannot be used to handle mean value constraints. However, it would be important to include experiments that can be solved with prior methods too, for example the experiments in Achiam at al. for proper comparison. The results in Table 2 are confusing: what makes the bolded results better than the others? If the criterion is highest return and torque < 25%, then \lambda=0.1 should be chosen for Hopper-v2. Also, The results seem to have high variance, and judging based on Table 2 and Figure 3, it is not obvious how well RCPO actually works.

To summarize, while the topic is undoubtedly important, the paper would need be improved in terms of better differentiating the theory from practice, and by including a rigorous comparison to prior work.

Minor points:
- What is exactly the difference between discounted sum constraint and mean value constraint?
- Could consider use colors in Table 1.
- Section 4.1.: What does “... enables training using a finite number of samples” exactly mean in this case?
- Table 2: The grid for \lambda is too sparse. 
- Proof of Theorem 1: What does it mean \theta to be stable?
- Proof of Theorem 2: “Theorem C” -> “Theorem 1”
",6
"This paper proposed a general framework for policy optimization of constrained MDP. Compared with the traditional methods, such as Lagrangian multiplier methods and trust region approach, this method shows better empirical results and theoretical merits. 

Major concerns: 
My major concern is about the time-scales. RCPO algorithm, by essence, is multi-scale, which usually has a stringent requirement on the stepsizes and is difficult to tune in practice to obtain the optimal performance. The reviewer would like to see how robust the algorithm is if the multi-time-scale condition is violated, aka, is the algorithm's performance robust to the stepsizes? 

My second concern is the algorithm claims to be the first one to handle mean-value constraint without reward shaping. I did not get the reason why (Dalal et al. 2018) and (Achiam et al., 2017) cannot handle this case. Can the authors explain the reason more clearly? 

Some minor points: 
The experiments are somewhat weak. The author is suggested to compare with more baseline methods. Mujoco domain is not a very difficult domain in general, and the authors are suggested to compare the performance on some other benchmark domains. 

This paper needs to consider the cases where the constraints are the squares of returns, such as the variance. In that case, computing the solution often involves double sampling problem. Double sampling problem is usually solved by adding an extra variable at an extra time scale (if gradient or semi-gradient methods are applied), such as in many risk-sensitive papers.​",6
"This work tackles the difficult problem of solving Constrained Markov Decision Processes. It proposes the RCPO algorithm as a way to solving CMDP. The benefits of RCPO is that it can handle general constraints, it is reward agnostic and doesn't require prior knowledge. The key is that RCPO trains the actor and critic using an alternative penalty guiding signal.

Pros:
- the motivations for the work are clearly explained and highly relevant
- comprehensive overview of related work 
- clear and consistent structure and notations throughout
- convergence proof is provided under certain assumptions


Cons:
- no intuition is given as to why RCPO isn't able to outperform reward shaping in the Walker2d-v2 domain

minor remarks:
- in Table 2, it would be good if the torque threshold value appeared somewhere 
- in Figure 3, the variable of the x-axis should appear either in the plots or in the legend
- in appendix B, r_s should be r_step and r_T should be r_goal to stay consistent with notation in 5.1.1",7
"This paper shows that a class of deep neural networks have no spurious local valleys –--implying no strict local-minima. The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks. 

Pros: The flexibility of the network structure is an interesting point.
Cons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong). 
  The simulation part is not that clear, and I have a few questions that I hope the authors can answer. 

Some comments/suggestions:
1) Training error needs to be discussed.
   Page 8 says “This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error”. This relation is not justified. The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error. Showing training error is the only way to connect to Thm 3.3. I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD). 
    This paper has no theory on generalization, thus if a whole section is just about “investigating generalization error”, then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising).   

2) Data augmentation.
  “Note that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part.” Why? 
   With data augmentation, is M still larger than N? If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2. 

3)It may be better to mention explicitly that ""it is possible to have bad local min"" –perhaps in abstract and/or introduction. 
  --Although “no sub-optimal strict local minima” is mentioned, readers, especially non-optimizers, might not notice ""strict"".
  --In fact, in the 1st round read, I do not have a strong impression of ""strict"". Later I realized it. Mentioning this can be helpful. 

4) Some references I suggest to include:
   [R1] Yu, X. and Chen, G. On the local minima free condition of backpropagation learning. 1995.  --related work. 
   [R2] Lu, H., Kawaguchi, K. Depth creates no bad local minima. 2017. --also deep nets.
   [R3] Liang, S., Sun, R., Li, Y., & Srikant, R. ""Understanding the loss surface of neural networks for binary classification."" 2018. --Also study SoftPlus neurons.
   [R4] Nouiehed, M., & Razaviyayn, M. Learning Deep Models: Critical Points and Local Openness. 2018. --also deep nets. 

Minor questions:
  --Exact 10% test accuracy for a few cases. Why exact 10%?
",7
"This paper presents a class of neural networks that does not have bad local valleys. The “no bad local valleys” implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn’t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.

The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
* adding skip connections doesn’t harm the generalization.
* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.
* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.

However, from a theoretical point of view, I would say the contribution of this work doesn’t seem to be very significant, for the following reasons:
* In the first place, figuring out “why existing models work” would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.
* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally “equivalent” to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17’) it is easy to attain global minima.
* I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive.

Below, I’ll list specific comments/questions about the paper.
* Assumption 3.1.2 doesn’t make sense. Assumption 3.1.2 says “there exists N neurons satisfying…” and then the first bullet point says “for all j = 1, …, M”. Also, the statement “one of the following conditions” is unclear. Does it mean that we must have either “N satisfying the first bullet” or “N satisfying the second bullet”, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?
* The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.
* Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses.
* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it’s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.
* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn’t necessarily satisfy the assumptions?
* Can you show the “improvement” of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.

Minor points
* In the Assumption 3.1.3, the $N$ in $r \neq s \in N$ means $[N]$?
* In the introduction, there is a sentence “potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),” which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18’ and Yun et al. 18’).
* Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as “for example, in the fully connected network case, this means that all data points are distinct.”",6
"The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. 

Overall I really enjoy reading the paper. 
The assumptions to aid the proof are very natural and much softer than the existing literature. As far as I’m concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. 
The presentation of the paper is intuitive and easy to follow. I’ve also checked all the proof and think it’s brilliantly and elegantly written. 

My only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesn’t influence my recommendation to accept the paper.


Minor issues:
I think it’s better to formally define “bad local valley” somewhere in the paper. From what I read, the definition of “bad local valley” is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. 
In proof number 4 (of Theorem 3.3), the statement should be “any *principle* submatrices of negative semi-definite matrices are also NSD”, and it’s not true otherwise. But this typo doesn’t influence the proof. 
Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your “bad local valley”. 
It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations?
",8
"This papers presents an unsupervised domain adaptation algorithm for semantic segmentation. A generative adversarial network is envisaged to carry out synthetic-to-real image translation. In doing so, depth information extracted from a simulator is used as privileged information (PI) to boost the transfer on the target domain, regularizing the model and ensuring a better generalization. 

*Quality*
The paper addresses a relevant problem, which is the adaptation of methods from simulated data to real ones. The authors devise a convincing method which takes advantage of state-of-the art generative adversarial architectures and privileged information. 

*Clarity*
The paper is sufficiently well written. In general, the main idea and proposed method are clear and easy to follow. The only problem is that some background concepts (such as privileged information or unsupervised domain adaptation) are given for granted, compromising the readability for someone not familiar with those topics. 
On a more technical side, for reproducibility purposes, the following aspects have to be clarified:
1.	Details about the validation set used for grid search. Is the validation set extracted from the target domain? (In principled labels from the target domain should not be used during learning).
2.	Number of iterations before convergence: is the training of the network numerically stable? Are there issues in convergence of some of the sub-modules? Which one is leading the learning?
3.	Comments about the relative magnitudes of losses. This will maybe give some intuitions about the values used for the hyper-parameters (e.g., the L_PI is only weighted by 0.1).

*Originality*
The way authors take advantage of depth information extracted from a simulator as privileged information is novel in the sense that, with respect to the original student-teacher paradigm of the paper by Vapnik & Vashist, here the idea of privileged information is interpreted as a regularizer to boost the training stage. 

*Significance*
The application of semantic segmentation in urban scenes for navigation tasks is relevant. The scored results are on pair with/ superior to state-of-the-art in unsupervised domain adaptation. 
However, the ablation study could be more extensive in order to understand the contribution of the several components, besides the PI network. In fact, it would be interesting to analyze the contribution of the perceptual loss (and others). Also, one could include the target-only result (as done in original LSD paper) to provide an upper bound on the best accuracy that is achievable.

*Pros*
1. The applicative setting of semantic segmentation in urban scenes for navigation is relevant. 
2. Using privileged information from simulators seems novel and well presented in this paper.
3. Strong experimental results achieved in challenging benchmarks.

*Cons*
1. The regularization effect of the PI network could be supported by a more extensive ablation study of the model, for example by ablating the several losses used (in particular, the perceptual loss).
2. A quite relevant amount of hyper-parameters need to be cross-validated. Is the method robust against different parameters’ configuration?
3. Missing citations [1, 2]: there are works in the literature that can hallucinate a missing modality during testing. Although such works approach a different problem, authors should cite them.

[1] Judy Hoffman, Saurabh Gupta, Trevor Darrell - Learning with Side Information through Modality Hallucination – CVPR 2016
[2] Nuno Garcia, Pietro Morerio, Vittorio Murino - Modality Distillation with Multiple Stream Networks for Action Recognition – ECCV 2018

*Final Evaluation*
The authors face the challenging synthetic-to-real adaptation setup, with an interesting usage of z-buffer from a simulator as privileged information. Overall, the work is fine, apart from the following points.
1.	In addition to a few missing citations [1, 2], an ablation study on the perceptual loss is necessary to dissect the impact of each component of the pipeline. 
2.	The clarity of the paper can be improved by adding some background material on unsupervised domain adaptation and learning with privileged information (PI), as to better highlight the technical novelty of using PI within a L1 regularizer. 
3.	The training stage of all submodules could have better investigated, for instance, by providing some convergence plots of the loss functions across iterations.
4.	How to do grid search for parameters in a domain adaptation setting is always a delicate aspect and authors seem elusive on that respect. 
5.	Again about hyper-parameters. Due to their high number, some sensitivity analysis should have provided.
As it is, the paper’s strengths slightly outperform the weaknesses, leading to an overall borderline-accept. If authors implement the suggested modification, a full acceptance will be feasible.

[COMMENTS AFTER AUTHORS' RESPONSE]
After the rebuttal provided by authors, all raised questions and criticisms have been fully solved. Therefore, I recommend for a full acceptance.",7
"The paper focuses on the problem of semantic segmentation across domains. The most standard setting for this task involves real world street images as target and synthetic domains as sources with images produced by simulators of photo-realistic hurban scenes.  This work proposes to leverage further depth information which is actually produced by the simulator together with the source images but which is in general not taken into consideration.
The used deep architecture is a GAN where the generator learning is guided by three components: (1) the standard discriminator loss (2) the cross entropy loss for image segmentation that evaluates the correct label assignment to each image pixel (3) an  l1-based loss which evaluates the correct prediction of the depth values in the original and generated image. A further perceptual regularizer is introduced to support the learning.

+ overall the paper is well organized and easy to read
+ the proposed idea is smart: when starting from a synthetic domain there may be several hidden extra information that are generally neglected but that can instead support the learning task
+ the experimental results seem promising 

Still, I have some concerns

- if the main advantage of the proposed approach is in the introduction of the priviledged information, I would expect that disactivating the related PI loss we should get back to results analogous of those obtained by other competing methods. However from Table 2 it seems that SPIGAN-no-PI is already much better than the  FCN Source baseline in the Cityscape case and much worse in the Vistas case. This should be better clarified -- are the basic structure of SPIGAN and FCN analogous?  

- the ablation does not cover an analysis on the role of the perceptual regularizer. This is also related to the point above: the use of a perceptual loss may introduce a basic difference with respect to competing methods. It should be better discussed.

- section 4.1 mentions the use of a validation set. More details should be provided about it and on how the hyperparameters were chosen.
A possible analysis on the robustness of the method to those parameters could provide some further intuition about the network stability.
It might be also interesting to check if the  the loss weights provide some intuition  about the relative importance of the losses in the learning process.

- the negative transfer rate is another way to measure the advantage of using the PI with respect to not using it. However, since it is not evaluated for the competing methods its value does not add much information and indeed it is only quickly mentioned in the text. It should be better discussed.

- some recent papers have shown better results than those considered here as baseline:
[Learning to Adapt Structured Output Space for Semantic Segmentation, CVPR 2018]
[Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training, ECCV 2018]
they should be included as related work and considered as reference for the experimental results.

Overall I think that the proposed idea is valuable but the paper should better clarify the points mentioned above.

",6
"This article addresses the problem of domain adaptation of semantic segmentation methods from autonomous vehicle simulators to the real world. The key contribution of this paper is the use of privileged information for performing the adaptation. The method is of those called unsupervised domain adaptation as no labels from the target domain are used for the adaptation. The method is based on a GAN with: a) A generator that transforms the simulation images to real appearance; b) A discriminator that distinguish between real and fake images;  c) a privileged network that learns to perform depth estimation; and d) the task networks that learns to perform semantic segmentation. Privileged information is very few exploited in simulations and I consider it an important way of further exploit these simulators.

The article is clear, short, well written and very easy to understand. The method is effective as it is able to perform domain adaptation and improve over the compared methods. There is also an ablation study to evaluate the contribution of each module. This ablation study shows that the privileged information used helps to better perform the adaptation. The state of the art is comprehensive and the formulation seams correct.  The datasets used for the experiments (Synthia, Cityscapes and Vistas) is very adequate as they are the standard ones.

Some minor concerns:
 - The use of 360x640 as resolution
 - The use of FCN8 instead of something based on Resnet or densenet

I would like some more details on what is happening with Vistas dataset. SPIGAN-no-PI underperforms the source model. By looking at Figure 4 we can observe that the transformation of the images is not working properly as many artifacts appear. In SPIGAN those artifacts does not appear and then the adaptation works better. Could it be a problem in the training?


",7
"This paper proposes an alternative training paradigm for DNIs: instead of training an auxiliary module to approximate the gradient provided by the following modules of the original model, they train it to approximate directly the final output of the original model. Although the approach does not seem to improve significantly, if at all, over Sobolev training of these modules (denoted 'critic' in the paper), this method seems simpler and offer side benefits which seem to be the main contribution of this paper. 
Indeed Table 4 and 5 show for example how they can, after training the full model, extract a submodel requiring significantly less computation and parameters with little loss in the performance. Alternatively, they also propose a method to do progressive inference for similar reasons. 
The ensembling result is interesting, but the figure 4 is not necessarily clear on the significance of this result, especially since the standard deviation is not shown in this figure.
The idea proposed in this paper is interesting, however, the experiments are restricted on relatively small and simple architectures and limited on two very similar datasets (CIFAR-10 and CIFAR-100), making the argument less compelling. ",6
"This paper describes a method of training neural networks without update locking. The idea is a small modification on top of Czarnecki et al. Critic training, where instead of using final loss as a critic target, one bootstraps from critics on other layers. In particular, if only one module is present, these two approaches are actually identical. To be more precise, the only difference between these two methods is that (7) in Critic training would change to l(L_i, L_N). As a consequence, method becomes forward unlocked too. It is worth noting, that in the appendix of Czarnecki et al. it is shown that this particular method (critic training) under simple conditions actually ""degenerates"" to deep supervision (which is forward unlocked too). Consequently unlocking property as such is not a big contribution of the proposed method. Rest of the paper includes following elements:
- empirical evaluation showing improvement over critic training by 0.4% in CIFAR10 and 0.9% in CIFAR100 when using 3 splits.
- expansion on using the model for progressive inference.

Given standard deviation of errors in Table 1. it is not clear how significant these improvements are. How many samples were used to estimate these quantities? It is worth noting, that Critic training was showed to be outperformed by Sobolev Training in the same paper authors cite, but its performance is not reported despite looking like a well defined baseline. In particular, can these two methods be combined? 

I believe that this is an interesting research direction, however paper in its current form seems as a small incremental improvement over sota, and could be significantly improved by for example:
- providing more comprehensive evaluation (including estimating accuracy to lower std errors)
- adding other baseline solutions (such as Sobolev training, cDNI, or deep supervision)
- considering any form of convergence/dynamics analysis of the proposed approach



",7
"This manuscript presents a new method that can conduct local training of a deep neural network. 
Briefly speaking, the proposed method first cuts a very deep network into a few groups and then train the parameters of each group almost independent of the other groups. The main idea is to attach a local critic module to each group and the error gradient is back propagated to each group from its local critic module instead of the last layer of the whole network.
Such an idea seems to work well and the resultant performance decrease is acceptable when the original network is not very large. However, when the original network is complex, the performance decrease may be relatively big. 

Another benefit of this local critic training is that in addition to the main model, it can also produce several submodels that can be used for ensemble inference and progressive classification. 

In summary, this manuscript proposes an interesting idea, but not sure empirically how useful it will be since for a complex network, this method may result in relatively big performance decrease. For a simple network, although the performance decrease is small, but there is no need to use the proposed training method for a simplex network.",6
"The paper studies the credit assignment problem in multi-agent domain (soccer playing as a concrete application). The paper itself is well-written. I hope that the author could use a better methodology to make the evaluation part stronger.

I have a few questions to the author:

1. Why features like which team has ball possession (a boolean) is represented as a channel in the image? What if you concatenate this bit with the output of the last layer of convolution?

2. Compared to the proposed algorithm, the evaluation part is somewhat weaker. 
 i) I understand that it might be hard to get ground truth of the credit assignment, but is there any way to get a more competitive baseline model.
ii) the dataset you used only contains 6 games from the same team in one tournament.  This seems biased and not a reliable dataset itself.

Minor typos:
* section 4. We extract all episodes of open play (where) one team",5
"The paper proposes to learn a policy and action value over continuous next movement from professional soccer game. The policy is modeled as a gmm and the value of the mean of each component is learned jointly. 

The experiment section seems a bit light. The dataset seems quite small, would be good to get learning curves on training / test sets separately to see of the model is overfitting. Also a breakdown of results with bigger / deeper convnets rather than just number of mixture components would be interesting. There is not much comparison with other ML / RL types methods. 

p3: why not ' instead of + to denote next state ?

f+i = fi + mi : why no f^{i+1}

Nevertheless, the movement of
the ball is a part of the environment and is modeled as random movement. Therefore, the movement
distribution is governed by a stochastic policy. 

Even if the ball movement was deterministic, the movement could still be stochastic.

Eq (2) right hand side, inconsistency between p(m | s, m) with p(s+ | s, m) defined above ?

Eq 6: I guess a softmax over a grid centered around player position would also work ? Possibly convolutional all the way? It wouldnt have much more parameters than the proposed arch?
",4
"This paper presents a method for learning the Q function from multi-agent demonstrations, such as trajectories of soccer players in a soccer game.  The basic ideas are:

-- The policy class pi(m|s) maps states s to actions m probabilistically.  In particular, the probability class is a mixture of Gaussians, with the mean/covariance functions and the mixture function all being instantiated via neural nets.  The behavior of the individual players are assumed to be conditionally independent given the current state.

-- A hand-designed reward function is used, such as the ball being in a ""strike zone"" of some kind for the team on offense. 

-- The Q function class is also a neural net.

-- The policy pi is learned through maximum likelihood of the state/action pairs of the the pre-collected demonstrations.  This is essentially probabilistic behavioral cloning over the policy class.

-- The Q function is learned via Q-fitting over the demonstration data, i.e., minimizing squared error on the Bellman residual.  This requires the pre-specified reward function, the policy pi, and some details regarding the state transitions (e.g., the non-deterministic transitions of the ball). 

-- The policy pi and the Q function are trained jointly by adding the two objectives.

-- The main empirical evaluation is comparing the quality of the Q function estimation against a naive baseline (random movements) and a non-mixture policy class (i.e., a single mode, rather than a mixture of Gaussians).  The results against the non-mixture policy class do not seem to show much difference in performance.  

-- Some qualitative evaluations are also presented, but it's unclear what insight one is expected to gain from looking at them.


**Clarity**
The paper is reasonably well written.  Some aspects of the logical flow could be improved, but overall it's fine.  Detailed comments are:

-- The proposed approach is essentially imitation learning and not, strictly speaking, reinforcement learning.  Yet the work is positioned as reinforcement learning.  Perhaps it's just a difference in norms of terminology usage.

--  There are more multi-agent RL & IL works than what was discussed in the related works section.  Of course, an exhaustive discussion is not expected, but the writing makes it sound like there hasn't been much work at all.  Examples include:
https://arxiv.org/abs/1807.09936
https://arxiv.org/abs/1706.02275
http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/rashidicml18.pdf

-- The authors comment in the related work that team sports cannot yet be simulated.  This statement is sort of true, although it's not clear how true. For instance, the physics engines of many sports video games are very realistic.  Moreover, it doesn't strike me as the most prominent point of contrast with [Hausknecht & Stone 2016].  For instance, a more prominent point of contrast is simply imitation learning vs reinforcement learning.  

-- It's not clear what the authors mean specifically when they refer to ""average performance"" contrasting with [Le et al. 2017b].  Also, desn't that work also condition on game context and situation? 

-- Section 4.1 is written in a somewhat disorganized way.  The exposition alternates between discussing baselines and the evaluation methodology in a way that is confusing. 

-- One of the baselines is described but then left to the appendix for analysis.  This doesn't make sense from a narrative perspective.


**Originality**
From a technical perspective, it's not clear that there's much novelty in this approach.  All the ingredients are pretty standard, and the ingredients are put together in a standard way. 

From an applications perspective, there might be some novelty here but it's not clear.  The application of imitation learning to sports data is not new, but also not saturated either.  The specific idea of learning a better rating system is interesting, but it's not clear how fleshed out this application is in the paper (more comments on this below).


**Significance**
For me, the key question for significance boils down to: ""Does this paper change the way people think about doing X?"" for the most interesting instantiation of X possible.  In this case, it seems X should be computing a rating system for multi-agent spatial multi-agent behavior (as suggested by the title).  But the limitations in the model and the experiments make this a questionable proposition.  

The model class all but ignores the multi-agent aspect (the agents are all independent conditioned on the states). Moreover there was no comparison against a model class that made a worse assumption on how to handle the multi-agent aspect.  So it's not clear how the multi-agent aspect is significant.

The evaluation of the rating approach is not convincing.  The approach essentially performed as well as a single-mixture baseline.  Furthermore, the direction V(s) learning approach is deferred to the appendix, and I'd like to see it included in the main paper.  Finally, it would be nice if there was some attempt to compare with a non-MDP based approach.  For instance, people directly compute things like P(goal|state).  I'd like to see evidence that this style of approach will outperform those approaches, which are the de facto standard in sports analytics right now.


**Overall Quality**
Based on the above comments, it is my opinion that this paper has too many holes in it to be ready for publication at a venue such as ICLR.  As a primarily applications paper, the burden is on the authors to demonstrate reasonably convincing evidence that this line of approach is valuable to the application.",4
"This paper proposes a spontaneous and self-organizing communication learning scheme in multi-agent RL setup. The problem is interesting. I mainly have one concern regarding its originality.

From a technical perspective, it's not clear to me that there's much novelty in this approach.  I guess it might be the case the focus of the paper is to propose a framework or scheme. However, almost all the ingredients/components are standard.

Regarding clarity, it's not clear to me:
* how the structure from Figure 2 can be reproduced. 
* how statistically significant the evaluation results are. ",4
"Summary: 
This paper expands on the work on 'emergent communication' with 2 innovations: 
- The architecture has a separate 'message channel' that processes the incoming and outgoing messages mostly independently of the hidden state of the agent. There are also dedicated architecture elements for the interaction between the hidden state and the message stream. 
- The outgoing message is gated with a 'speak' action: only when the agent takes the speak-action at time step t is a message sent out at timestep t+1. 

Comments for improvement: 
-The paper proposes a rather complicated architecture, with many moving part. In the paper's current form it is extremely hard to see which part of this architecture contribute to the success of the method. A set of ablation studies on the different components would indeed be very helpful. 
-Using the word 'thought' to describe the hidden state of the agent is rather distracting.
-Equation (1): This just seems to be the policy gradient term for a factorised action space across 'environment action' and 'communication action'. The only obvious difference is that the policy here is shown to condition on the state representation s_t, rather than on the input. Is that intended?
-The paper suffers from a lot of undefined notation, e.g. the s_t above. Please clarify.
-In Figure 2b) the MCU is shown to produce the action a_t as an output. That seems like a mistake. 
-Figure 4): The results seem to be extremely unstable, which is a well known issue for independent learning. Recent work (MADDPG, COMA) has shown that centralised critics can drastically avoid these instabilities and improve final performance. Did you compare against using a centralised critic, V(central state), rather the V(observation)? Also, using a single seed on this kind of unstable learning process renders the results highly non-conclusive. 
-In Figure (5), what are the red-arrows? Do these correspond to the actual actions taken by the agents or are they simply annotations? It would be good to see how far the communication range is by comparison. Also, why is there a blob of 'communicating' agents far from the enemy? 
-Are different methods in the large scale battle task trained in self-play and then pitched against other methods in a round-robin tournament after training has finished or are they trained against each other? 
-In Figure 6 (a), why are average rewards changing over the course of training? I would expect this to be a zero-sum setting in self-play. 
-I couldn't find any supplementary material referenced in the text for the details. Instead the paper seems to have another copy of the paper itself attached in the pdf. This makes it hard to evaluate the paper given that few details around training are provided in the main text. 

Overall I am concerned that the learning method used in the paper (independent baseline) is known to be unstable and to produce poor results in the multi-agent setting (see COMA and MADDPG). This raises the concern that the communication channel is mostly useful for overcoming the issues introduced from having a decentralised critic.",5
"The paper presents a study on multi-agent communication. The main innovation from previous work is the introduction of an explicit Speak binary action that controls whether or not an agent will emit a message. The proposed model is test on two tasks, multi-camera surveillance and battle tasks with a large number of population.

Overall, this paper is clear (although model details are missing, the authors point at the Appendix, but the Appendix is missing) and the authors compare to a number of baselines. I appreciate the use of multi-agent communication in cases where the number of agents very large,  as this is a very good stress test for current algorithms and can potentially help identifying novel challenges. 

My main concern is that in a collaborative setting I don't see why we should expect that occluding information is better than revealing information? Isn't always revealing everything the best strategy? When only 3 cameras, I really cannot think of why a model would get better performance by choosing to not reveal information. Figure 4b somewhat confirms that as there seems to be a lot of variance on the Ssoc. Have you checked how stable are your results across runs? I could believe that occluding information can be beneficial if the number of agents is very big and there is redundancy. But in the limit, not revealing information should only facilitate training --  and indeed this seems to be happening in 6a as Ssoc is learning faster but meanfield is catching up. Could there be an ablation experiment in which everything stays the same in the model but the agents always activate the Speak action? This would answer the question of how crucial this main Speak feature is for Ssoc.

Can you elaborate on this? Moreover, since the Appendix appears to be missing, can you comment on stable results were across runs?

",5
"This paper presents methods for (1) adding inductive bias to a classifier through coarse-to-fine prediction along a class hierarchy and (2) learning a memory-based KNN classifier through an intuitive procedure that keeps track of mislabeled instances during learning. Further, the paper motivates focused work on the many class / few shot classification scenario and creates new benchmark datasets from subsets of imagenet and omniglot that match this scenario. Experimental results show gains over popular competing methods on these benchmarks.
Overall, I like the motivation that this paper provides for many class / few shot and find some of the methods proposed interesting. Yet there are issues with clarity of presentation that made it somewhat difficult to fully understand the exact procedures that were implemented. The model figure is useful, but could be refined to add additional clarity -- particularly in the case of the KNN learning procedure. 
I'm not entirely familiar with recent work in this sub-field, so it is difficult for me to judge the novelty of the proposed procedure. Is it really true that class-hierarchies have never been used to perform coarse-to-fine inference in past work? If so, this should be state clearly. If not, related work should be mentioned and compared against. Finally, while the procedures are intuitive -- the takeaway of this paper could be substantially improved if even simple theoretical analysis were provided. For example, in the limit of infinite data, does the memory-based KNN learning procedure actually produce the right classifier? 
Misc comments questions:
-The paper says at least twice that coarse classification will be performed with an MLP, while fine classification will use a KNN -- yet, the model section also state that both coarse and fine use both MLP and KNN. It is unclear to me which model setup was used in experiments. 
-Can anything theoretical be shown about the class hierarchy based classification technique? Intuitively, it does add inductive bias through a manually defined taxonomy, but can something more precise be said about how it restricts the hypothesis space? This procedure is simple enough that I would be surprised if similar techniques had not be studied thoroughly in the statistical learning theory literature. 
-The procedure for updating the KNN memory is intuitive, but can anything more be said about it? In isolation, is the KNN learning procedure at least consistent -- i.e. in the limit of large data does it converge to the correct classifier? Maybe this is trivial to prove, but is worth including. 
",5
"This paper try to formulate many-class-few-shot classification problem from 2 perspectives: supervised learning and meta-learning. Although solving this problem with class hierarchy is trivial,  combing MLP and KNN in these two ways seems interesting to me. I still have several questions:

1) How the class hierarchy is got , manually set or automatically generate?  Whether the ideas still work if some coarse classes share same fine class?
2) The so-called attention module is just classic KNN operations, please don't naming it attention just because the concept ""attention"" is hot.
3) Why different ""attention"" operations are used for supervised learning and meta-learning?
4) How to get the pre-trained models for supervised learning?
5) What will happen if alternatively apply supervised learning and meta-learning?
6) In Table 4, why MahiNet(Mem-2) w/o Attention and Hierarchy performs better than the one w/o attention?
7) The authors just compare storing the average features and all features, I think results of different prototype number should be given, since one of their claim to apply KNN is to maintain a small memory.

 ",6
"This study explores the class hierarchy to solve many-class few-short learning problem in both traditional supervised learning and meta-learning. The model integrates both the coarse-class and fine-class label as the supervision information to train the DNN, which aims to leverage coarse-class label to assist fine-class prediction. The core part in the DNN is memory-augmented attention model that includes at KNN classifier and Memory Update mechanism. The re-writable memory slots in KNN classifier aim to maintain multiple prototypes used to describe the data sub-distribution within a class, which is insured by designing the memory utility rate, cache and clustering component in Memory Update mechanism. This study presents a relatively complex system that combines the idea of matching networks and prototypical networks.

One of the contributions is that the study puts forward a concept of the many-class few-short learning problem in both supervised learning and meta-learning scenarios, and uses a dataset to describe this problem.

Using the memory-augmented mechanism to maintain multiple prototypes is a good idea. It may be more interesting if its effectiveness can be proved or justified theoretically. Furthermore, it is better to offer some discussion about the learned memory slots in the view of “diverse and representative feature”.

The experiment results in Table 4 and Table 5 compare the MahiNet with Prototypical Net on the mcfsImageNet and mcfsOmniglot dataset. It is better to compare MahiNet with other state-of-the-art works, such as the Relation Network whose performance is higher than Prototypical Net. In addition, if more  challenging datasets  can be further evaluated in the experiments,  the paper  might be more convincing.

In my opinion, the hierarchy information provides the guidance to fine-gained classification, which not only can be added to MahiNet but also the other models. Therefore, to prove its effectiveness, it is better to add hierarchy information to other models for comparison. In addition, regarding the results on the column of 50-5 and 50-10 in Table 4, when the number of class increase to 50, the results are just slightly higher than prototypical network. Considering that the memory update mechanism is of the high resource consumption and complexity, it is better to provide more details about clustering, and training and testing time.
",5
"In this paper a new task namely CoDraw is introduced. In CoDraw, there is a teller who describes a scene and a drawer who tries to select clip art component and place them on a canvas to draw the description. The drawing environment contains simple objects and a fixed background scene all in cartoon style. The describing language thus does not have sophisticated components and phrases. A metric based on the presence of the components in the original image and the generated image is coined to compute similarity which is used in learning and evaluation.  Authors mention that in order to gain better performance they needed to train the teller and drawer separately on disjoint subsets of the training data which they call it a cross talk.

Comments about the task:
The introduced task seems to be very simplistic with very limited number of simple objects. From the explanations and examples the dialogs between the teller and drawer are not natural. As explained the teller will always tell ‘ok’ in some of the scenarios. How is this different with a system that generates clip art images based on a “full description”? Generating clip arts based on descriptions is a task that was introduced in the original clip art paper by Zitnick and Parikh 2013. This paper does not clarify how they are different than monologs of generating scenes based on a description.  

Comments about the method:
I couldn’t find anything particularly novel about the method. The network is a combination of a feed forward model and an LSTM and the learning is done with a combination of imitation learning and REINFORCE. 


Comments about the experimental results:
It is hard to evaluate whether the obtained results are satisfying or not. The task is somehow simplistic since there a limited number of clip art objects and the scenes are very abstract which does not have complications of natural images and accordingly the dialogs are also very simplistic. All the baselines are based on nearest neighbors. 

Comments about presentation:
The writing of this paper needs to be improved. The current draft is not coherent and it is hard to navigate between different components of the method and different design choices. Some of the design choices are not experimentally proved to be effective: they are mentioned to be observed to be good design choices. It would be more effective to show the effect of these design choices by some ablation study. 
There are many details about the method which are not fully explained: what are the details of your imitation learning method? Can you formalize your RL fine-tuning part with the use of some formulations? With the current format, the technical part of the paper is not fully understandable.",4
"The paper proposes a game of collaborative drawing where a teller is
to communicate a picture to a drawer via natural language.  The picture
allows only a small number of components and a fixed and limited set
of detailed variations of such components.

Pros:

The work contributed a dataset where the task has relatively objective
criteria for success.  The dataset itself is a valuable contribution
to the community interested in the subject.   It may be useful for
purposes beyond those it was designed for.

The task is interesting and its visual nature allows for easy inspection
of the reasons for successes or failures.  It provides reasonable grounding
for the dialog.  By restricting the scope of variations through the options
and parameters, some detailed aspects of the conversation could be explored
with carefully controlled experiments.

The authors identified the need for and proposed a ""crosstalk"" protocol
that they believe can prevent leakage via common training data and
the development of non-language, shared codebooks that defeat the purpose
of focusing on the natural language dialog.

The set up allows for pairing of human and human, machine and machine,
and human and machine for the two roles, which enables comparison to
human performance baselines in several perspectives.

The figures give useful examples that are of great help to the readers.

Cons.:

Despite the restriction of the task context to creating a picture with
severely limited components, the scenario of the dialogs still has many
details to keep track of, and many important facets are missing in the
descriptions, especially on the data.

There is no analysis of the errors.  The presentation of
experimental results stops at the summary metrics, leaving many
doubts on why they are as such.

The work feels somewhat pre-mature in its exploration of the models
and the conclusions to warrant publication.  At times it feels like the
authors do not understand enough why the algorithms behave as they do.
However if this is considered as a dataset description paper and
the right expectation is set in the openings, it may still be acceptable.

The completed work warrants a longer report when more solid conclusions
can be drawn about the model behavior.

The writing is not organized enough and it takes many back-and-forth rounds
of checking during reading to find out about certain details that are given
long after their first references in other contexts.  Some examples are
included in the followings.

Misc.

Section 3.2, datasets of 9993 dialogs:
Are they done by humans?   Later it is understood from further descriptions.
It is useful to be more explicit at the first mention of this data collection effort.
The way they relate to the 10020 scenes is mentioned as ""one per scene"", with a footnote on some being removed.
Does it mean that no scene is described by two different people?  Does this
limit the usefulness of the data in understanding inter-personal differences?

Later in the descriptions (e.g. 4.1 on baseline methods) the notion of
training set is mentioned, but up to then there is no mentioning of how
training and testing (novel scenes) data are created.
It is also not clear what training data include: scenes only?
Dialogs associated with specific scenes?  Drawer actions?

Section 4.1, what is a drawer action?  How many possibilities are there?
From the description of ""rule-based nearest-neighbor drawer"" they seem to be
corresponding to ""teller utterance"".
However it is not clear where they come from.  What is an example of a drawer action?
Are the draw actions represented using the feature vectors discussed in the later sections?

Section 5.1, the need for the crosstalk protocol is an interesting observation,
however based on the description here, a reader may not be able to understand
the problem.  What do you mean by ""only limited generalization has taken place""?  Any examples?

Section 5, near the end: the description of the dataset splits is too cryptic.
What are being split?  How is val used in this context?

All in all the data preparation and partitioning descriptions need substantial clarification.

Section 6:  Besides reporting averaged similarity scores, it will be useful to report some error analysis.
What are the very good or very bad cases?  Why did that happen?
Are the bad scenes constructed by humans the same as those bad scenes
constructed by machines?  Do humans and machines tend to make different errors?
",6
"This paper presents CoDraw, a grounded and goal-driven dialogue environment for collaborative drawing. The authors argue convincingly that an interactive and grounded evaluation environment helps us better measure how well NLG/NLU agents actually understand and use their language — rather than evaluating against arbitrary ground-truth examples of what humans say, we can evaluate the objective end-to-end performance of a system in a well-specified nonlinguistic task. They collect a novel dataset in this grounded and goal-driven communication paradigm, define a success metric for the collaborative drawing task, and present models for maximizing that metric.

This is a very interesting task and the dataset/models are a very useful contribution to the community. I have just a few comments below:

1. Results:
1a. I’m not sure how impressed I should be by these results. The human–human similarity score is pretty far above those of the best models, even though MTurkers are not optimized (and likely not as motivated as an NN) to solve this task. You might be able to convince me more if you had a stronger baseline — e.g. a bag-of-words Drawer model which works off of the average of the word embeddings in a scripted Teller input. Have you tried baselines like these?
1b. Please provide variance measures on your results (within model configuration, across scene examples). Are the machine–machine pairs consistently performing well together? Are the humans? Depending on those variance numbers you might also consider doing a statistical test to argue that the auxiliary loss function and and RL fine-tuning offer certain improvement over the Scene2seq base model.

2. Framing: there is a lot of work in collaborative / multi-agent dialogue models which you have missed — see refs below to start. You should link to this literature (mostly in NLP) and contrast your task/model with theirs.

References
Vogel & Jurafsky (2010). Learning to follow navigational directions.
He et al. (2017). Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.
Fried et al. (2018). Unified pragmatic models for generating and following instructions.
Fried et al. (2018). Speaker-follower models for vision-and-language navigation.
Lazaridou et al. (2016). The red one!: On learning to refer to things based on their discriminative properties.
",7
"The authors introduce a Graph Matching Network for retrieval and matching of graph structured objects. The proposed methods demonstrates improvements compared to baseline methods. However, I have have three main concerns: 
1) Unconvining experiments.
	a) Experiments in Sec4.1. The experiments seem not convincing. Firstly, no details of dataset split is given. Secondly, I am suspicious the proposed model is overfitted, although proposed GSL models seem to bring some improvements on the WL kernel method. As shown in Tab.3, performance of GSL models dramatically decreases when adapting to graphs with more nodes or edges. Besides, performance of the proposed GSLs also drops when adapting to different combines of k_p and k_n as pointed in Sec.B.1. However, the baseline WL kernel method demonstrates favourable generalization ability.

	b）Experiments in Sec4.2. Only holding out 10% data into the testing set is not a good experiment setting and easily results in overfitting. The authors are suggested to hold more data out for testing. Besides, I wonder the generalization ability of the proposed model. The authors are suggested to test on the small unrar dataset mentioned in Sec.B.2 with the proposed model trained on the ffmpeg dataset in Sec4.2.

2) Generalization ability. The proposed model seems sensitive to the size and edge density of the graphs. The authors is suggested to add experiments mentioned in (1).

3) Inference time and model size. Although the proposed model seems to achieve increasing improvements with the increasing propagation layers. I wonder the cost of inference time and model size compared to baselines methods. ",6
"Graph matching is a classic and import problem in computer vision, data mining, and other sub-areas of machine learning. Previously, the graph matching problems are often modeled as combinatorial optimization problems, e.g. Quadratic Assignment Problems. While these optimization problems are often NP-hard, researchers often focus on improving the efficiency of the solvers. The authors attack the problem in another way. They proposed an extension of graph embedding networks, which can embed a pair of graphs to a pair of vector representations, then the similarity between two graphs can be computed via computing the similarities of the pair of vector representations. The proposed model is able to match graphs in graph level as it can predict the similarities of the two graphs.

Compare to Graph Embedding Networks (GNN), the authors proposed a new model, in which a new matching module accepts the hidden vector of nodes from two graphs and maps them to a hidden matching variable, then the hidden matching variable serves as messages as in Graph Embedding Networks. This is the main contribution of the paper compared to GNN.

The main problem of the paper is that it is not clear where the performance improvement comes from. The authors proposed a cross-graph attention-based matching module. However, it is not clear whether the performance improvement comes from the cross-graph interaction, or the attention module is also important. It would be nice if the author can do some ablation study on the structure of the new matching module.

In graph matching, we not only care about the overall similarity of two graphs but also are interested in finding the correspondence between the nodes of two graphs, which requires the similarities between vertexes of two graphs. Compared to another [1] deep learning based graph matching model, the author did not show that the proposed are able to give the matching constraints. For example, while the authors show that it is possible to use GMN to learn graph edit distances, is it possible to use the GMN to help to the exact editing?



[1] Zanfir, Andrei, and Cristian Sminchisescu. ""Deep Learning of Graph Matching."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.",5
"The authors present two methods for learning a similarity score between pairs of graphs. They first is to use a shared GNN for each graph to produce independent graph embeddings on which a similarity score is computed. The authors improve this model using pairs of graphs as input and utilizing a cross-graph attention-mechanism in combination with graph convolution. The proposed approach is evaluated on synthetic and real world tasks. It is clearly shown that the proposed approach of cross-graph attention is useful for the given task (at the cost of extra computation).

A main contribution of the article is that ideas from graph matching are introduced to graph neural networks and it is clearly shown that this is beneficial. However, in my opinion the intuition, effect and limitations of the cross-graph attention mechanism should be described in more detail. I like the visualizations of the cross-graph attention, which gives the impression that the process converges to a bijection between the nodes. However, this is not the case for graphs with symmetries (automorphisms); consider, e.g., two star graphs. A discussion of such examples would be helpful and would make the concept of cross-graph attention clearer.

The experimental comparison is largely convincing. However, the proposed approach is motivated by graph matching and a connection to the graph edit distance is implied. However, in the experimental comparison graph kernels are used as baseline. I would like to suggest to also use a simple heuristics for the graph edit distance as a baseline (Riesen, Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision Computing, 27(7), 2009).


There are several other questions that have not been sufficiently addressed in the article.

* In Eq. 3, self-attention is used to compute graph level representations to ""only focus on important nodes in the graph"". How can this be reconciled with the idea of measuring similarities across the whole graph? Can you give more insights in how the attention coefficients vary for positive as well as negative examples? How much does the self-attention affects the performance of the model in contrast to mean or sum aggregation?
* Why do you chose the cross-graph similarity to be non-trainable? Might there be any benefits in doing so?
* The note on page 5 is misleading because two isomorphic graphs will lead to identical representations even if communication is not reduced to zero vectors (this happens neither theoretically nor in practice).
* Although theoretical complexity of the proposed approach is mentioned, how much slower is the proposed approach in practice? As similarity is computed for every pair of nodes across two graphs, the proposed approach, as you said, will not scale. In practice, how would one solve this problem given two very large graphs which do not fit into GPU memory? To what extent can sampling strategies be used (e.g., from GraphSAGE)? Some discussion on this would be very fruitful.


In summary, I think that this is an interesting article, which can be accepted for ICLR provided that the cross-graph attention mechanism is discussed in more detail.


Minor remarks:

* p3: The references provided for the graph edit distance in fact consider the (more specific) maximum common subgraph problem.",6
"Summary
=========
The authors present an extension to the VAE model by exploring the possibility of using the label space to create a new embedding space, which they call Probabilistic Semantic Embedding (PSE). 
They present two different extension of PSE, PSE and PSE*.
The idea of additionally supporting the latent embedding, created by a VAE, by using available textual descriptors seems promising. 
The proposed model was evaluated on two tasks, label-to-image generation and image annotation.
Although the work is interesting, there are a few questions that I am not clear about and have several comments. 

Questions
=========
1. How was the word2vec model trained? Did you use an existing pretrained model (e.g. available as download) or did you train the embedding model yourself? If so, on what data? 
2. The major novelty of this approach is the use of annotations supporting images and textual (pretrained) embedding spaces, but no related work regarding Wes was neither introduced in the Related Work section nor was it clearly explained in the text.
3. Why did the authors focus on the w2v model instead of more promising approaches as fastText or ELMo? 
4. How does your model deal with OOV word(s) as input? For example, when used as Image Generator.
5. Table 1 shows results achieved on MNIST but not Fashion-MNIST; was the evaluation performed on MNIST only? 
6. Table 2 presents the impact of the use of pretrained embeddings (word2vec) instead of one-hot vectors for labels. Which one do the models presented in Table 1 use? 
7. Could you explain the small difference between using one-hot vs pretrained label encodings, presented in Table 2? 
8. Also, can you explain how the numbers in table 2 were achieved (e.g. sum over all, average of all, etc.). When comparing the values presented here to the values of the same measure in table 1, one does notice the big difference between them. 

Comments
=========
1. Section 2, page2: “As derived in the original paper…” references which paper (i.e. Kingma et al)? 
2. VAE or beta-VAE model is not referenced (mentioned on page 5);
3. Authors do agree that the corpora used is not optimal for the adequate evaluation of the proposed model. It would be interesting to see the use of this approach on a more realistic data set;
4. Unclear sentence: “Compared with the VAE, latent codes where images with the same labels are clustered.”;
5. The authors claim that one of the results of this work is the possibility to generalize for unseen cases (zero-shot learning). It would be interesting to see the performance of this model compared to SOTA in CV in terms of the zero-shot learning task;
6. Figure 2 visualizes proposed embedding space (2D) but it shows VAE and beta-VAE models and omits to show PSE. VAE and beta-VAE are neither introduced nor referenced in text;
7. Table 1: mark best performing with bold. It does, however, outperform other evaluated models when using 20D embedding space;
8. Page 7: in text you mention generation accuracy and in Table 1 the same value is defined as Generation Correctness (%). ",7
"

This paper presents a VAE model that jointly models images and their labels. Specifically, the following the VAE framework, the proposed model encodes an image into a latent variable, whose prior is conditioned on the labels of the image, and the latent variable is used to reconstruct the image. The paper also presents a variant which also reconstructs the labels 


My comments are as follows:

1. About the significance and originality, although to my knowledge, there seems to be no the exact match in the existing approaches of the idea of incorporating labels into the prior of the latent variable of VAE, the idea seems a little bit trivial and less of technical depth. Moreover, in terms of performance, it seems that the proposed model is not significantly better than the previous models. Therefore, my major concern of this paper goes to the significance.

2. In terms of experiments, I am not convinced that using 20D of PSE VS 10D of CVAE is a fair comparison, although the CVAE will use another 10 dimensions to encode 10 labels with one-hot form. Moreover, using the same settings for all the models in comparison may not be perfect because different models may have different best settings. It would be better if the validation set is used to tune the settings a little bit. 

3. Does the proposed model use the word embeddings of the labels? It could be better to report both results of word embeddings and one-hot encoding of the labels, to see if word embeddings help. ",4
"This paper proposes to replace the traditional KL term of VAE with the KL between two conditional distributions, which is to equip the model with the ability to address multimodal data. Moreover, the paper also extend the model to add an additional network to predict the label from the reconstructed image to enhance the decoder with supervised information. 

In general, the model is contrived and the  novelty of the paper is incremental. Is Eq. 2 the ELBO of the new model? If so, the authors should provide the derivation of the ELBO. If not, can you prove the objective is the correct one to be optimized?
Moreover, the model is just an trivial extension of VAE and all key techniques are borrowed from existing work (Chen et al. 2016).

The experiments are only conducted on MNIST and Fashion-MNIST, which is not sufficient. CIFAT10 should be at least added, and other more challenging benchmarks should also be considered to make the model more solid.
",4
"This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. 

The proposed method is closely related to 1) “Learning Montezuma’s Revenge from a Single Demonstration” a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma’s revenge game 2) “Reverse Curriculum Generation for Reinforcement Learning” by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. 

The approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. 

The Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. 

Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). 

I commend the authors for honestly reporting their method’s shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form.
",5
"The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. As training progresses, the initial state distribution is incrementally shifted towards earlier steps in the demonstration, until the agent is trained starting from the original initial state. The authors further provide an analysis of the sample complexity of this method on a simple MDP. The method is demonstrated on a maze navigation task and a challenging game Pommerman.

The method is simple and sensible, but not particularly novel. As the authors pointed out, a very similar strategy for using demonstrations was previously presented in an OpenAI blogpost, Learning Montezuma’s Revenge from a Single Demonstration. However since that work was not published, it should not be held against this paper. That being said, sampling initial states from demonstrations is a tried-and-true strategy in RL, and the manually designed curriculum is also not particularly novel. Therefore the method is mainly a minor tweak to longstanding techniques. The paper has also acknowledges these previous works. As such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. Florensa et al. 2017 and Aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.

This work can also benefit from a more diverse set of tasks to better evaluate the effectiveness of the method, and provide more insight on when such a strategy is beneficial. The experiments were conducted only on discrete grid world tasks, and additional experiments in continuous domains could be valuable. In the maze task, Backplay is not significantly better than uniform. Pommerman is a much more compelling task and shows more promising improvements from backflip. However, training seems to have been terminated fairly early, before the performance for most policies have converged. In particular, the standard dense policy in figure 3c seems to be doing pretty well, will it catch up to the backplay policy with more training? It is also pretty unexpected that the uniform policies are doing so poorly, worse than the standard policy for the Pommerman experiments. Do the authors have any intuition on why this might be the case?

In figure 3, what is the initial state distribution used to evaluate the various methods? Are all policies initialized to the original initial state of a task, or are initial states sampled from the demonstrations? Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. If that is the case, it might not be a fair comparison for the other policies, especially for the standard policies, which are trained under different initial states.

As detailed in the appendix, the sliding windows for the curriculum do not seem to have a lot of overlap. This might be a potentially problematic design decision, since the sudden change in the initial state distribution, may cause the policy to “forget” about strategies learned for previous initial states. Has the authors experimented with other more gradual transition strategies?

I think this paper in its current form does not yet meet the bar for ICLR. But this line of work could be a potentially promising direction. More thorough evaluation, better baselines, and more diverse tasks can help to strengthen this work. Further analysis on the effects of different initialization strategies could also make for a compelling contribution.",5
"Thanks for your submission.

The  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The authors show the technique reaches an upper bound on sample complexity especially in sparse reward environments. The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. The biggest limitation of the method as with other vanilla model free RL is the lack of generalization. 

A bit more motivation on the simplified assumption that function approximation would have been better. Although, such a simplification seems to be a natural candidate to be upper bounded by the longest shortest path from v_0 to v_*; consideration of such simplicaton on the neighbourhood structure of the graph with respect to the maximum vertex degree seems to be missing or cliques. Although, the authors comment about the strong assumptions being made to aid the analysis. 

The authors explain the analysis in a very precise and the analysis seems to work. Although, the part of the analysis where connections are drawn to the reciprocal spectral gap is not very clear. 

The authors discuss the limitation of the analysis in the case of the binary tree, that follows from the arguments before.

It will be great to see a more systematic approach to deciding how fast/slow the window should be updated to unify some of the findings from the empirical experiments as that seems to affect the way the agent trains using Backplay.",5
"This paper gives provable recovery guarantees for a class of neural networks which have high-threshold activation in the first layer, followed by a ""well-behaved"" polynomial, under Gaussian input. The algorithm is based on the approach by Ge et al. (2017), as well as an iterative refinement method.

While this could be an interesting result, I have several concerns regarding the assumptions, correctness, and writing.

1) It is required that the threshold is at least sqrt{log d} (Thm. 1), where d is the number of hidden neurons in the first layer. It seems that this essentially zeros out almost all the neurons, since the maximum among d Gaussian random variables is roughly sqrt{log d}. The authors should explain what exactly this model is doing, i.e., what kind of functions it can compute, in order to justify why this is an interesting model.

Furthermore, the authors claim that the studied model is a ""deep"" neural network, but I disagree. As I understand, the difference between this model and two-layer networks is that the second layer here is a polynomial instead of a linear function. This doesn't make it a deep network since the (polynomial) part above the first layer is not modeled in a layer-wise fashion, not to mention that under the setting considered in the paper the polynomial behaves similar to a linear function.

2) It is stated at the end of Section 2 that the angle can be reduced by a factor of 1-1/d ***with constant probability***. How does this ensure you can succeed after O(d log(1/nu)) iterations? As far as I see you need the success probability in one iteration to be at least something like 1-1/Omega(d) so that you can apply a union bound.

3) Even if the issues of motivation and correctness are clarified, I find it very difficult to understand the overall intuition and main technical contributions in this paper. The writing needs to be significantly improved to reach the level of a top conference.",4
"This paper considers the problem of recovering the lowest layer of a deep neural network whose architecture is ReLU or sign function followed by a polynomial. This paper relies on three assumptions: 1) the lowest layer has a high threshold (\Omleg(\sqrt{d})), 2) the polynomial has 1/poly(d) lower bouned and O(1) upper bounded linear terms and is monotone 3) the input is Gaussian. Under these assumptions, this paper shows it is possible to learn the lowest layer in precision \eps in poly(1/eps, d) time.

The proposed algorithm has two steps. The first step is based on the landscape design approach proposed by Ge et al. (2017) and the second step is based on checking the correlation. 

Provably learning a neural network is a major problem in theoretical machine learning. The assumptions made in this paper are fine for me and I think this paper indeed has some new interesting observation. My major concern is the writing. There are several components of the algorithm. However, it is hard to digest the intuition behind each component and how the assumptions are used. I suggest authors providing a high-level and non-technical description of the whole algorithm at the beginning. If authors can significantly improve the writing, I am happy to re-evaluate my comments and increase my rating.",5
"This paper gives a new algorithm for learning parameters of neural network under several assumptions: 1. the threshold for the first layer is very high; 2. the future layers of the neural network can be approximated by a polynomial. 3. The input distribution is Gaussian.

It is unclear why any of these assumptions are true. For 1, the thresholds in neural networks are certainly not as high as required in the algorithm (for the threshold in the paper after the first layer the neurons will be super sparse/often even just equal to 0, this is not really observed in real neural networks). For 2, there are no general results showing neural networks can be effectively approximated by low degree polynomials, and, if the future networks can be approximated, what prevents you from just assuming the entire neural network is a low degree polynomial? People have tried fitting polynomials and that does not perform nearly as well as neural networks.

The proof of the paper makes the problem even more clear because the paper shows that with this high threshold in the first layer, the future layers just behave linearly. This is again very far from true in any real neural networks.

Overall I'm OK with making some strong assumptions in order to prove some results for neural networks - after all it is a very difficult problem. However, this paper makes too many unrealistic assumptions. It's OK to make one of these assumptions, maybe 2, but 3 is too much for me.",4
"The authors present a multi-agent communication architecture where, agents can use targeted communication and can perform multiple communication steps. The paper is well written and easy to follow.

Comments:

1) The idea of multi-stage communication is great, but the paper doesn't have a strong point to support this contribution. Could the authors illustrate the benefit of multi-stage e.g. vs. the communication channel width?

2) In DIAL, the authors introduce a ""null"" action, what is the difference of that and multi-stage?

3) It is not clear to the reader what is the contribution of targeted communication vs. non-targeted as it looks a solution to the mean-pooling. Could the authors include at least one more experiment with on an architecture that doesn't use mean pooling. From an architecture perspective there is a scalability benefit of using pooling, but if that's the only one it has to be made more clear.

4) Following (3) based on Reddit there was a recent code release in python https://github.com/minqi/learning-to-communicate-pytorch. An alternative would be to evaluate TarMAC to one of the test beds, but the paper misses baselines.",6
"The authors propose a new architecture for learning communication protocols. In this architecture each message consists of a key and a value. When receiving the message the listener produces an attention key that is used to selectively attend to some messages more than other using soft attention. This differs from the typical 'broadcasting' protocols learned in literature. 

Questions / Comments: 
- Eqn (4) looks like a vanilla RNN. Did you experience any issues around exploding or vanishing gradients when doing multiple rounds of communication? Why not use a gated architecture here? 
- ""Centralized Critic"" section: This equation is from the COMA paper, ie. a centralised critic with policy gradients rather than DDPG. What did you use for the variance reduction baseline to estimate the advantage? Also, did you try conditioning the critic on the central state rather than the concat of observations? Formally this is required for the algorithm to be convergent. 
- How many independent seeds are the results averaged over? 
- The attention mechanism seems to provide very little value across all experiments: 
-- 84.9% vs 82.7% 
-- 89.5% vs 89.6% 
-- 64.3% vs 68.9% 
Did you check if any of these numbers are significant? This is my single biggest concern with the paper. Currently it's unclear whether attention is required at all in the settings presented. It would be good to see eg. the TarMAC 2-stage on the traffic junction (97.1%) ablated without attention.",6
"The authors present a study on multi-agent communication.
Specifically, they adapt communication to be targeted and multi-staged.
Experiments on  2 synthetic datasets and 1 3D visual dataset confirm that both additions are beneficial

Overall, this paper was somewhat clear and more importantly includes experiments on House3D, a more realistic dataset.

My main concern is the following: the method is not about targeting, but about selectively hearing.
If agents are sharing the reward then why should targeted communication be beneficial at all? Isn't the optimal strategy to just communicate everything to everyone? I understand that they should be selective at the listening side to properly integrate only the relevant information (so, attend over all received messages), but why should we expect the speaker to apriori know who this message should go to? Moreover, I don't really understand how targeted communication can even work (in the way the authors explain it) since the agents have partial information (e.g., in shapes they only see 5x5 around them), so they don't really know who is where --  but I could potentially see this working should the agents put information about their own identity and location.  So, given the positive results that the authors get, my understanding is that the signature doesn't have information about who should the recipient of the information be but more about what where the properties of the sender of this information.  So, based on my understanding, I don't feel that the flow of the story quite matches what is really happening and this might be very confusing for prospective readers. Can the authors elaborate on this, aim i getting things wrong?

There is literally no information about model size (or at least I wasn't able to find any). Is there any weight-sharing across agents? Do you obtain CommNets by using the implementations of the authors or by ablating the signature-part of your model? Moreover, why do agents have a limited view window on the SHAPES -- is (targeted) communication redundant when agents have full observability? The part about how multi-staged communication is implemented is quite cryptic at the moment -- is multi-staged the fact that the message is out-putted by processing with a recurrent unit? The messages is factorized into two parts k and u leading to a vector of size D -- what happens should we have one message of size D (rather than factorizing into 2), something like this would control for any improvements obtained from increases the parameters of the model.

Finally,  if the premises of the paper is to define more effective communication protocols, evident in the use of continuous communication, (rather than studying what form can multi-agent communication etc etc), a necessary baseline  (especially in cases where agents share reward), is to communicate the full observation (rather than a function of it).  This baseline is not presented here and it's absolutely necessary.
",6
"Brief Summary:
The authors present a novel adversarial attack on node embedding method based on random walks. They focus on perturbing the structure of the network. Because the bi-level optimization problem can be highly challenging, they refer to factorize a random walk matrix which is proved equivalent to DeepWalk. The experimental results show that their approach can effectively mislead the node embedding from multi-classification. 

Quality:
This paper is well-written except for some minor spelling mistakes

Clarity:
This paper is quite clear and easy to follow.

Originality:
This work follow the proposal Qiu et al.(WSDM'18)'s proof and present a novel approach to calculate the loss when the network changes by A'.

Pros:
1. Detailed proofs presented in appendix
2. They present 6 questions and answer them with effective experiments.
3. They present a new way to attack node embedding by factorizing a equivalent matrix.

Cons:
1. I have noticed that Zügner et al.(KDD'18) present an adversarial attack method on GCN for graph data. I think it is reachable by the day the authors submitted this paper. This is opposite to the first sentence ""Since this is the first work considering adversarial attacks on node embeddings there are no known
baselines"" said in Section 4.
2. The author present the time analysis of their approach but the efficiency result of their approach is not presented.
3. To enforce misclassification of the target node t, the author set the candidate flip edges as edges around t. Does this mean only the node's local edges can mislead the target node from downstream tasks? I think the authors should consider more candidate edges but this may lead to larger time complexity.
4. Figure. 4 tells that low-degree nodes are easier to mis-classify. If the baseline method B_{rnd} randomly select edges to flip among the local area of node t, I think the result should be similar to the proposed approach on low-degree nodes because the flipped edges should be the same. 

== I have read the rebuttal. Thanks for the response.",6
"The topic of this paper is interesting; however, the significance of the work can be improved.  I recommend that the authors test the vulnerability of node embeddings on various random graph models.  Examples of random graph models include Erdos-Renyi, Stochastic Kronecker Graph, Configuration Model with power-law degree distribution, Barabasi-Albert, Watts-Strogatz, Hyperbolic Graphs, Block Two-level Erdos-Renyi, etc.  That way we can learn what types of networks are more susceptible to attacks on random-walk based node embeddings and perhaps look into why some are more vulnerable than others.",5
"This paper is a timely work on poisoning random walk based graph embedding methods. In particular, it shows how to derive a surrogate loss function for DeepWalk. Even though the analysis method and the algorithm proposed is somewhat loose, I think this paper do a good contribution towards the adversarial attack problem for important models.

There are still some room to improve in this paper. One problem is that since the paper proposes a surrogate loss L_{DW3}, it would be natural to analysis its gap from L_{DW1}. In this paper I can only see some empirical results on this issue. Another issue is that the algorithm the paper used is another approximation towards L_{DW3} by an additional sampling method. And the overall strategy can be far away from the true optimal solution for maximizing L_{DW3}. Still there's no analysis on that issue. A potential drawback for the method proposed is that its complexity is O(NE), which can be quite expensive when the graph is big, and # edges is \Omega(NlogN).

The experiments in this paper is convincing. It seems that the method proposed is way better than its competitors when removing edges. Is there any further intuition on that? Why the method is not so good when we add edges? Moreover, the black-box attack scenario requires more justification. What is the relative performance gain for A_{DW3} against other attacks in the black-box setting?

Overall, this paper targets on an important issue in machine learning. My main concern is that it leaves too many questions behind their algorithms. Still some effort is required to improve the paper.",6
"The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. I have three major concerns:

(1) The optimization problem for complete orthogonal dictionary learning in this paper is very different from overcomplete dictionary learning in practice. It is actually more similar to tensor decomposition-type problems, especially after smoothing. From this point of view, it is not as interesting as the optimization problem.

Arora et al. Simple, Efficient, and Neural Algorithms for Sparse Coding, 2015

(2) Some recent works focus on analyzing gradient descent for phase retrieval and matrix sensing. These obtained results are significantly improved and near-optimal. However, the convergence rate in this paper is very loose. Besides, the paper even does not look into the last phase of gradient descent, when there exists restricted strong convexity. Thus, only sublinear convergence rate is presented.

Chen et al. Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval, 2018

The quality of this paper could be improved, if the author could sharpen their analysis.

(3) The analysis for the manifold gradient methods is something new, but not very significant. There have already been some works on manifold gradient methods. For example, the following paper has established convergence rates to second order optimal solutions for general nonconvex function over manifold.

Boumal et al. Global rates of convergence for nonconvex optimization on manifolds. 2016.

The following paper has established the asymptotic convergence to second order optimal solutions for general nonconvex function over manifold.

Lee et al. First-order Methods Almost Always Avoid Saddle Points, 2017.

",5
"The authors analyze the convergence performance of Riemannian gradient descent based algorithm for the dictionary learning problem with orthogonal dictionary and sparse factors. They demonstrate a polynomial time convergence from a random initialization for a smooth surrogate objective for the original non-smooth one. The problem and the analysis are of interest, but I have several questions regarding the paper as follows.

My first concern is that the analysis is on a smooth surrogate of the non-smooth sparse minimization for solving the dictionary learning problem, so it is not clear what is relationship between the global minimizer of the smooth problem to the underlying true dictionary. More specifically, how far is the global minimizer of problem (1) or (2) to the true dictionary parameter, and whether they share (approximately) the space or components regarding the sparse factors. Without clarifying this, it is not well motivated why we are interested in studying the problem considered in this paper at the beginning. Intuitively, since the recovered factors are not sparse anymore, it will impact the dictionary accordingly due to the linear mapping, which may lead to a very different set of dictionary components. Thus, explicit explanation is necessary here to avoid such degenerate case.

My second concern is the eligibility of assuming the dictionary A to be an identity matrix and extending it to the general orthogonal matrix case. The analysis uses the fact that rows of A are canonical basis, i.e., each row only has one non-zero entry. I do not see a straightforward extension by replacing A to be an orthogonal matrix as the authors claimed on page 3, since then the inner product of one row of A and one column of X is not just the corresponding entry of the column of X. It will be helpful if the authors can explain this explicitly or adjust the analysis accordingly to make this valid.

Another issue is the clarity of the paper. Some statements in the paper are not very clear. Form example, on page 3, third paragraph of Section 3, what does row(Y) = row(X_0) mean? Also, in eqn (1), y_k means k-th column of Y, and in eqn (2), q_i means i-th entry of q? Since both are bold lower case letters, clear distinction will help. Moreover, the reference use (. ) instead [ .], which can be confusing sometimes. 
",4
"This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The analysis relies on the negative curvature in the directions normal to the stable manifolds of all critical points that are not global minimizer.

Exploring the surface of a function and analyzing the structure of the negative curvature normal to the stable manifolds is an interesting idea. However, I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. See details below. Therefore, I have reservations about the paper.

*) The terminology ""stable manifolds"" is used from the first page, while its formal definition is given on page 4.
*) P3, the dictionary learning problem is not formally given. It is stated in the paper that the task is to find A and X, given Y. However, what optimization problem does the author consider? Is it \min_{A, X} \|Y - A X\|_F^2? assuming both dictionary A and sparse code X are unknown or \min_{A} \|Y - A X\|_F^2 assuming only dictionary is unknown?
*) P3, second paragraph in Section 3: what is the variable q? It is not defined before.
*) P3, third paragraph in Section 3: What is the function row()? Why does row(Y) equal row(X)?
*) P3: How does the dictionary learning problem reformulate into the problem in the third paragraph of Section 3? If I understand correctly, the task is to find A, X such that A^* Y = X since A is orthogonal. Consider the first column in A and denote it by q. Then the first column of X is approximated by q^* Y. Since X is sparse, the task is to find q so that q^* Y as sparse as possible. But how about the other columns in matrix $A$? 
*) The Riemannian gradient algorithm is not stated in this paper.


",5
"Updated after reading author revisions:
I appreciate the clarifications, the response answered almost all of my small technical questions.  That plus the new error analysis increases my opinion about the paper, and I'm no longer concerned that the rule templates are hand-generated given their generality and small number.  I am still concerned that we don't actually know how well the methods work, because the test sets are small and the performance differences between the methods (in Table 1) are quite close.  I will raise my score one point.

The authors might try to evaluate using k-fold cross-validation with the training set, to obtain more examples for evaluation.

Original review:

The paper presents a technique for using prolog along with neural representations and Open IE to perform reasoning with weak unification.

I like the basic direction of trying to combine prolog with neural models, and the weak unification notion.  The approach seems sufficiently novel, and the GRL is a reasonable heuristic.

I do, however, have significant concerns about the experiments.  The data sets are selected subsets of other standard benchmarks, rather than the entire benchmarks, and the test sets are quite small (e.g., the ""developer"" column where the NLProlog approach shows some of the larger wins -- when ensembled with previous techniques -- is based on a test set of only 29 examples).  Given the hand-annotated nature of much of the input knowledge (the rule templates), this introduces an important concern that the experimental wins will not be robust in more realistic settings where different knowledge may be required.

Minor comments/questions
page 2: ""without the need to transforming""
I did not understand how individual symbols, predicates and entities, have embeddings that come from sentence vectors (Section 4.1).
The learning objective in Section 4.2 seems reasonable, but I did not understand how ""evolution"" was part of the strategy there.
The example rule template for transitivity isn't actually transivity unless p_i=p_j for all i,j, I found that a little confusing.
Where are ""t-norms"" (mentioned at the top of page 6) used?  I did not see this.
""candidates entities"" -> ""candidate entities""
",5
"Update:
I appreciate the through error analysis the authors have done in the revision, which addressed my major previous concerns. I've updated my score accordingly.

This paper presents an approach to combine Prolog-like reasoning with distributional semantics. First, extracted fact triples are unified (i.e. mapped to) predicates and entities. Next, reasoning is performed with rule templates, where predicates and entities are abstracted. Since the reasoning process is non-differentiable, zero-oder optimization is used to fine-tune the predicate / entity embeddings.

The general idea of combining logical reasoning with neural models is quite appealing. A sketch of the algorithm is to first build structured knowledge from the text, then do reasoning over it to answer queries. In this work, the first step is completely relied on an off-the-shelf tool, Open-IE. It would be useful to see whether this step is the bottle neck of such approaches. One possibility is to apply the model to knowledge graph reasoning, which would remove any noise introduced from the knowledge extraction step, and solely focus on evaluating reasoning.

The results are a bit restricted, as in only a subset of the datasets are evaluated. I suspect part of the reason is that most of the QA datasets which claims to require multi-step reasoning don't really need much reasoning... However, it would be useful to do some simple (perhaps qualitative) analysis on the data quality, and make sure that it indeed tests what it intended to. For the ensemble results in Table 1, usually even ensembling same models trained with different seed would show improvements, so I'm not completely convinced that BiDAF and NLProlog are complementary - would be nice to see error analysis here.

Question:
What is the size of hand-coded predicates and rules? What's the coverage of these rules on the datasets, i.e. are there questions unanswerable by the provided rules?

Overall, while the results are limited, the approach is interesting, and hopefully will spur more work towards interpretable models with explicit reasoning.",7
"

UPDATE: Given the authors' rebuttal and the clear improvements to their paper, I've increased my rating of the work.

=======================

This paper presents NLProlog, a method that combines logical rules with distributed representations for reasoning on natural language statements. Natural language statements (first converted to logical triples) and templated logical rules are embedded in a vector space using a pretrained sentence encoder. These embedded ""symbols"" can be compared in vector space, and their similarity used in a theorem prover (Prolog) modified to support weak unification. The theorem prover determines the answer to a natural language query by constructing a proof according to its logical rules.

Training through the non-differentiable theorem prover occurs via an ""evolutionary strategy,"" which enables the model to fine-tune its sentence encoders and learn domain-specific logic rules directly from text. The authors also propose a Gradual Rule Learning (GRL) algorithm that seems necessary for the optimization process to converge on good solutions.

Despite the model's complexity, the paper was fairly clear to me.

Although the proposed model is a conglomeration of pre-existing parts, the combination is original to my knowledge. The use of Open Information Extraction to transform natural language statements to logical statements, which amenable to theorem provers, is novel and also circumvents the complicated preprocessing required by previous related works.

The authors evaluate the proposed approach on subsets of the Wikihop dataset and BABI-1k. NLProlog performs competitively with neural models, similarly augmented with Sent2Vec but lacking explicit logical rules, only on the 'country' subset of Wikihop. It does not compete with or clearly outperform these models in general. As the authors state, it further ""struggles to find meaningful rules for the predicates 'developer' and 'publisher'.""

NLProlog demonstrates strong performance on a subset of problems labelled unanimously by annotators to require multi-hop reasoning. Unfortunately, this is only done for the ""country"" subset of Wikihop, on which the model was already shown to have the strongest performance. I'd find this more convincing if similar improvements were shown on the other subsets (publisher, developer).

Taking into account also that the BABI subset was used only for ablation, the limited results call into question the significance of the work. It would definitely benefit from more extensive experimental validation. On the other hand, it's very positive to see that NLProlog seems to succeed where the neural models fail, and vice versa, so that the two approaches can be combined in an ensemble to achieve state-of-the-art results. This suggests that the paper's line of research has something to add to the community and should be pursued further. I'd find this result more interesting if an error analysis elucidated some characteristics of the examples that each approach does well/poorly on.

I'd like to see more analysis in general, that answers questions including:
- How reliable is the Open IE system and how does its performance impact the end task?
- How well-specified must the a priori rule structures be to achieve good performance? Further, how does the number and structure of the rules (a hyperparameter in this work) affect performance?
- What is the run-time/complexity of the exhaustive proof search during training?
- Relatedly, you state that you limit the rule complexity to two body atoms in the rule templates for BABI. Can you estimate what rule complexity is required in the Wikihop tasks?

I would like to recommend this work more confidently because it tackles such an important problem and does so in an interesting, well-conceived way. My reluctance arises from the limited experimental validation and analysis. Given more analysis details and experimental evidence from the authors, I'm happy to raise my recommendation.

Pros:
- the method complements standard deep QA models to achieve state-of-the-art results in an ensemble.
- unifying neural representations with logical/symbolic formalisms is an important research direction.
- a code release is planned.

Cons:
- a very complex model, whose details are occasionally unclear; the algorithms in Appendix A are helpful but they are not in the main text.
- the model expresses only a limited subset of first order logic; dynamically changing world states are not supported (yet).
- limited experimental validation.
- it's good to be able to incorporate prior knowledge, but it seems like it's quite necessary to pre-specify rules (in template form).

Minor quibble: Evolutionary learning strategies, such as genetic algorithms, go back a long way. It's strange using only a reference from 2017 to introduce them.",7
"This paper provides a principled way to examine the compression phrase, i.e, I(X;T) in deep neural networks. To achieve this, the authors provides an theoretical sounding entropy estimator to estimate mutual information.  Empirically, the paper did observe this compression phrase across both synthetic and real-world data and relates this compression behavior with geometric clustering. 

Pros:
- The paper is well-written and easy to understand.
- The framework for analyzing the mutual information in DNNs is theoretically sounding and robust.
- The finding of connecting clustering with compression is novel and inspiring. 

Questions:
- The main concern of the paper is its conclusion. While the experiments in the paper did show the mutual information goes down as the clustering effect enhanced, it only means `clustering` and `compression` are correlated; but the paper claims `clustering` is the source of `compression`, i.e., `clustering` leads to `compression`. This conclusion is problematic. For example, looking at Figure 5(a), as the mutual information goes down from epoch 28 to epoch 8796, not only the clustering gets enhanced, but also the loss is going down. Thus, alternatively, one can also argue the loss (i.e., `relevance`) is the cause of `compression` instead of `clustering`. From another aspect, the effect of `clustering` is also related to the loss, i.e., it is the loss function that pushes the points of the same class to be closer; then, even if the direct cause of `compression` is `clustering`, the root cause might still be the loss (i.e., `relevance`). 
- In Figure 5(a). Why the mutual information increases from epoch 80 - epoch 541? Also, it seems that the test loss increases as the I(X;T) decreases from epoch 541 to epoch 8796. This seems to be counter-intuitive to the claim that ""lower I(X;T) implies higher generalization ability"". Can you explain this phenomenon?

[UPDATE] the authors address my concerns in a detailed way, and the updated revision is rather robust, therefore, I decide to change my score to accept.",7
"Response to author comments:

I would like to thank the authors for answering my questions and addressing the issues in their paper. I believe the edits and newly added comments improve the paper. 

I found the response regarding the use of your convergence bound very clear. It is a very reasonable use of the bound and now I see how you take advantage of it in your experimental work. However, I believe the description in the paper, in particular, the last two sentences of Remark 1, could still be improved and better explain how a reasonable and computationally feasible n was chosen.

To clarify one of my questions, you correctly assumed that I meant to write the true label, and not the output of the network.


***********

The paper revises the techniques used in Tishby’s and Saxe et al. work to measure mutual information between the data and a hidden layer of a neural network. The authors point out that these previous papers’ measures of mutual information are not meaningful due to lack of clear theoretical assumptions on the randomness that arises in DNNs.

The authors propose to study a perturbed version of a neural network to turn it into a noisy channel making the mutual information estimation meaningful. The perturbed network has isotropic Gaussian noise added to each layer nodes. The authors then propose a method to estimate the mutual information of interest. They suggest that the mutual information describes how distinguishable the hidden representation values are after a Gaussian perturbation (which is equivalent to estimating the means of a mixture of Gaussians). Data clustering per class is identified as the source of compression.

In addition to proposing a way to estimate a mutual information of a stochastic network, the authors analyze the compression that occurs in stochastic neural networks. 

It seems that the contribution is empirical, rather than theoretical, as the theoretical result cited is going to appear in a different article. After reading that the authors “develop sample propagation (SP) estimator”, I expected to see a novel approach/algorithm. However, unless I missed something, the proposed method for estimating MI for this Gaussian channel is just doing MC estimation (and no guarantees are established in this paper). The convergence bounds for the SP estimator are presented(Theorem 1), however, the result is cited from another article of the authors, so it is not a contribution of this submission. 

Since the authors have this convergence  bound stated in Theorem 1, it would be great to see it being used - how many samples are needed/being used in the experiments? What should the error bars be around mutual information estimates in the experiments? If the bound is too loose for a reasonable number of samples, then what’s the use of it?

The authors perform two types of experiments on MNIST. The first experiment demonstrates that no compression is observed per layer and the mutual information only increases during training (as measured by the binning approach, which is supposed to track the mutual information of the stochastic version of the network). The second experiments demonstrates that deeper layers perform more clustering. 

Regarding the first experiment, could the authors clarify how per unit and per entire layer compression estimation differs?

Also, in my opinion, more clustered representations seem to indicate that the mutual information with the output increases. Could the authors comment on how the noise levels in this particular version of a stochastic network affects the mutual information with the output and the clustering? Do more clustered representations lead to increased mutual information of the layer with the output?

I found it fairly difficult to summarize the experimental contribution after the first read. I think the presentation and summary after each experiment could be improved and made more reader friendly. For example, the authors could include a short section before the experiments stating their hypothesis and pointing to the experiment/figure number supporting their hypothesis.",7
"This paper studied the information bottleneck principle for deep learning. In the paper by (Schwatz-Ziv & Tishby 17'), it is empirically shown that the mutual information I(X;T) between input X and internal layers T decreases, which is called a compression phase. In this paper, the author found that the compression phase is not always happening and the shape of the curve of I(X;T) highly depends on the ""bining size"" which is used for estimating mutual information by (Schwatz-Ziv & Tishby 17'). Then the authors proposed to use a noisy DNN to make sure the map X->T is stochastic, then proposed a guaranteed mutual information estimator. Then some empirical results are shown.

I think the problem in (Schwatz-Ziv & Tishby 17') do exist and their result is highly questionable. However, I have some major question about this paper.

1. In this paper a noisy DNN was proposed. However, how do you choose the noise level \beta? If I understand correctly, the noise level plays a similar role of the bining size in (Schwatz-Ziv & Tishby 17'). Noise level goes to zero is similar to bining size goes to zero. I wish to see a figure about how different \beta affects the curve of I(X;T) (similar to Figure 1 but let \bet change). 

    In Figure 4(d) there is a plot showing different \beta will affect the mutual information, but the x-axis is ""weight"". I wonder that how the curve of mutual information change w.r.t \beta, if the x-axis is training epochs. Do your statement stable about \beta? 

2. I think Section 3 and Theorem 1 are interesting and insightful. But I notice that in Section 10 you mentioned that this will be a separate paper. Is it OK to put them together in this paper?

3. The paper by (Schwatz-Ziv & Tishby 17') has not pass a peer-review process and it is still a preprint. This paper is nothing but only saying some deficiencies of (Schwatz-Ziv & Tishby 17') (except Section 3 and Theorem 1 which I think should be an independent paper). I think such a paper should not be published as a conference paper before (Schwatz-Ziv & Tishby 17') pass a peer-review process.

So totally I think this paper should not be accepted by ICLR at this point. I think Section 3 and Theorem 1 should become an independent paper, and the DNN approach can be an application of the mutual information estimator.",4
"This paper proposes a meta-learning algorithm for reinforcement learning that incorporates expert demonstrations. The goal is to reduce the sample complexity of meta-RL algorithms in the validation phase. The paper provides a good discussion of the background literature. Experimental results are provided on multi-goal planning problems for three prototypical simulated systems, namely, a 2D point-mass robot, a 7-DOF manipulator and a quadruped crawler.

The theoretical and practical contributions of this paper are minor. The authors propose a straight-forward combination of MAML, importance-weighted policy gradients across the inner-outer loop and off-policy supervised learning of the expert demonstrations, all standard techniques in reinforcement learning and meta-learning. The experimental section is unconvincing and lacking in details. I however find the approach well-motivated and pertinent. Demonstrations on a real robotic platform, where the improved sample complexity is essential, would make this paper much more impressive.

Detailed comments:

1. Can you compare the different algorithms using the number of rollouts as the X-axis? The narrative provides this information but it is difficult to judge the performance based on Figs. 2 and 3.
2. It is unfair to compare MRI (the approach of this paper) with MAML which does not have access to expert demonstrations for validation tasks. The improved sample complexity is thus directly coming from demonstrations. It is difficult to compare MRI and MAML-RL/Imitation on an equal footing. Perhaps the validation tasks could be significantly harder, e.g., sub-goals in the planning problems, or one could consider a large number of validation tasks.
3. It seems the improvement over MAML-RL/Imitation in Fig. 3 is minor. Why is this so?",4
"The paper presents a meta-RL method extends previous work on meta-RL by including an imitation learning step. It is mentioned that the behaviour closing part of this extended algorithm can come from a teacher or some other source. Since this extension is the major contribution, it must be discussed in more detail. I also don't understand why 

My second problem with the paper is reproducibility. The purpose of OpenAI is comparability and reproducibility of algorithms. It is not sufficient to simply state that you have used TensorFlow. We need information about the architecture, etc so that the results can be reproduced. Also, since you use OpenAI, the score for each experiment should be compared to the best scores known from the website so that the performance of the new algorithm can be compared to others.",3
"This work addresses the problem of learning a policy-learning-procedure, through meta-learning, that can adapt quickly to new tasks. This work uses MAML for meta-learning, and with this choice, the problem can be broken down into two loops: 

1) inner loop: adapting a policy \pi_phi based on unseen rollouts, where initial parameters phi were provided by the meta-trainer in the outer loop 
2) outer loop: the meta-trainer tries to learn parameters phi on batches of tasks that provide good initial parameters 

In prior work on meta-reinforcement learning via MAML, both the outer as well as inner objective attempt to minimize a RL objective, leading to an algorithm that has very high sample-complexity. This work uses imitation learning for the outer loop procedure, to significantly decrease sample-complexity.

Technical Contribution:
-----------------------
The idea of using imitation learning for reinforcement learning is well explored in the literature, and so using this idea in itself is not real contribution. There are several issues with the presentation of this work, that make it incredibly difficult to identify a technical contribution:

1. overreaching statements without details to backup: you are writing the paper as if you are learning a ""RL algorithm"" that can be used to quickly learn new tasks. your manuscript does not really provide a description for this ""algorithm"". After re-reading several other papers I concluded that what you mean is that you learn an initial set of policy parameters that can quickly adapt to new related tasks and an update rule with which you update these parameters. However, standard MAML uses SGD as an update rule so there is really nothing to be learned here. Unfortunately, your paper provides zero detail on these claims of learning a ""RL procedure"", so for now I have to assume that you are simply learning a good initial set of policy parameters through meta-learning. If that is the case, then using imitation learning in this setting is really not novel, this has been done by a lot of other people before (you're just using MAML to learn ""better"" initial parameters).
2. you're technical section (section 4) provides some details on the technical challenges of using demonstrations to perform the outer loop optimization step. Unfortunately, you are not putting your work in the context of existing work ([1], [2]), that discuss and address the importance/issue of sampling in meta-rl with MAML. So it's impossible to know whether there is any new insight here

Experimental Evaluation:
-------------------------
The experimental evaluation is very ""thin"", other than the original MAML-RL and pure imitation learning no other more recent baselines ([1], [2]) have been compared to. And only 2 relatively simple simulation settings are tested. 

Summary:
-----------
Very minor contribution, a manuscript that is lacking important details and does not relate it's technical section to existing work, with very thin evaluation. 


[1] The Importance of Sampling in Meta-Reinforcement Learning, NIPS 2018
[2] CONTINUOUS ADAPTATION VIA META-LEARNING IN NONSTATIONARY AND COMPETITIVE ENVIRONMENTS, ICLR 2018",2
"This paper describes a meta-RL algorithm through imitation on RL policies. While the paper builds nicely up to the core part, I find essential details missing about the imitation setup. By glancing at previous BC papers (some of which are cited), the quantity for supervised imitations, etc., were clearly defined. 

It will be useful for this reviewer if the authors can provide more clarity in explaining the BC task involved in their algorithm.",5
"This paper proposes combining the Gumbel-max trick and ""direct loss optimization"" for variance reduction in VAEs with discrete latent variables. This is a natural combination (in hindsight), since the Gumbel-max trick turns sampling into non-differentiable optimization, and direct loss optimization provides a way to optimize the expected value of a non-differentiable loss. The paper is well-written for the most part and is backed by good experimental results. However it like some of the mathematical details and some of the exposition could be greatly improved.

I think there are several mistakes in the reasoning presented in the proof of Theorem 1 (see detailed comments below). Theorem 1 in the current paper seems to me to be a special case of Theorem 1 in (Song 2016), where the expectation over data is replaced by an expectation over the Gumbel variable gamma. If I've understood correctly, it seems like it would be more correct and concise to simply cite that paper with some explanatory comments.

The word ""direct"" occurs quite a lot in the paper. It sometimes seemed misplaced. For example for ""The direct differentiation of the resulting expectation"" in the introduction, in what sense is the differentiation direct, and what would non-direct differentiation be?

In section 3, that's not the meaning of the term ""exponential family"".

The re-use of theta and psi as both model parameters and the log probability density / distribution is unnecessarily confusing.

A small point, but in ""the challenge in generative learning is to reparameterize and optimize (2)"", the authors assume that q has analytic expression for the second KL term in (1). That's often the case but definitely not always. Also, even if this KL term has an analytic expression, it is not always better to use it (see Duvenaud ""Sticking the landing..."").

In (3), the usual notation is P(x = i) where x is the random variable and i is its possible value, whereas in (3) the random variable z^{\phi + \gamma} appears on the right of the equals sign.

The first paragraph of section 4.1 and (4) and (5) are just a simple application of the law of total expectation, and it would be simpler and clearer to state that.

""gradient of the decoder"" should be ""gradient of the decoder log probability"" (or log prob density depending on preference). Similarly with ""the decoder is a smooth function"". The decoder is a conditional probability distribution (at least according to my understanding of conventional usage).

In the first equation in the proof of Theorem 1, it seems as though the authors are using the standard change of variables formula for integrals. However the new variable \hat{\gamma} depends on \hat{z} through \theta, so I don't see how it's valid to ignore the max in the way the present paper does. One way to see that something is wrong is the fact that the integrand on LHS has \hat{z} as a bound variable only, whereas the integrand on RHS has \hat{z} as both a bound variable (inside the max) and a free variable (since \theta depends on \hat{z}, though strangely that is not written in the equation). What is the value of \hat{z} used for \theta on RHS?

There's a missing [] after \partial_\epsilon in the third line of the paragraph starting ""We turn to prove Equation (8)"".

In the same line, I don't see why the two expectations are equal. It seems to me that the differentiation w.r.t. epsilon ignores the fact that changing epsilon occasionally changes z^{\epsilon \theta + \phi_v + \gamma} in a discontinuous way. The term being differentiated has both a continuous-in-epsilon component and a piecewise-constant-in-epsilon component, and the latter appears to have been ignored. While the gradient of a piecewise constant function is zero almost everywhere, the occasional large changes (which could be thought of as delta functions) still can make a large contribution to the overall expression once we take the expectation. To look at it another way, if the reasoning here is correct, why can't the same argument be used on the RHS of (8), first to take the derivative inside the expectation and subsequently to compute the derivative as zero, since the inner term is a piecewise constant function of v? Yet clearly the RHS of (8) is not always zero.

Around ""However when we approach the limit, the variance of the estimate increases..."", I think it would be extremely helpful to explain that for small epsilon, we occasionally obtain a large gradient (and otherwise zero), while for large epsilon we often obtain a moderate non-zero gradient. That gives some insight into the effect of epsilon, and why the variance is larger for small epsilon.

Any reason not to plot the bias in right Figure 1, which is ostensibly about the bias-variance trade-off?

I didn't follow the meaning of the diagram or caption for left Figure 1.

In (11), I wasn't sure whether S included the supervised examples or not (i.e. whether S_1 was disjoint from or a subset of S). If disjoint, shouldn't the KL term be included, or the expectation-over-gamma term be changed to use ground truth z? I guess I was unclear on the form of loss used for the supervised data, and unclear on the motivation for this choice.

In the last sentence of section 5.1, should ""chain rule"" be ""variance reparameterization trick""?

In section 5.2, it would be helpful to mention what mean field means in terms of the variational distribution q (namely q(z | x) = \prod_i q(z_i | x) ). Also, the term ""mean field"" is not conventionally used for general distributions (such as the decoder here) as far as I'm aware, only for variational distributions. ""Conditionally independent"" might be clearer.

What does ""for which we can approximate z^{...} efficiently"" refer to?

In section 6.1, what is the annealing rate? Also, the minimal epsilon is set to 0.1. Is epsilon changed as training progresses according to some schedule?

""The main advantage of our framework is that it seamlessly integrates semi-supervised learning"" seems like an overstatement. Wouldn't semi-supervised learning be relatively straightforward to incorporate into any form of VAE? And why not just use log p(x, z) for updating the decoder parameters and log q(z | x) for updating the encoder parameters?

How many labeled examples were used for the CelebA semi-supervised learning?

Some bibliography typos. For example, no capitalization throughout (e.g. ""gumbel"" instead of ""Gumbel""). Also lots of arxiv preprints cited when published papers exist (e.g. Jang 2017 should be ICLR 2017 not arxiv preprint).


",7
"This work proposes a new (biased) gradient estimator to learn a discrete auto-encoders. Similarly to the gumbel-softmax estimator this paper proposes to use the gumbel-max trick and the reparametrization trick but instead of relaxing the argmax by a softmax, the authors derive a formula for the gradient based on direct loss optimization to compute the gradient through the argmax.

Pros:
- The approach is well motivated and the proof of theorem 1 which gives the formula of the new gradient estimator seems correct.

Cons:
- The principle downside of the proposed approach is that it requires to compute the value of the objective for several values of z, which makes it more computationally expensive than gumbel-softmax. Could the author compare the different estimators in terms of running time instead of epoch for example in fig2. it seems like RELAX would perform similarly or better in terms of wall-clock time.

- [1] also proposed an estimator that requires evaluating the objective for different values of z, and showed that it is unbiased and optimal (lowest variance). I think the authors should mention this related work and how their approach differs. I also think the author should compare their work to [1].

- Since both gumbel-softmax and the proposed approach are biased, could the authors give some intuitions on why they believe their approach is better.

- I believe the expectation of the right-hand side of equation (9) can be computed in closed form by using a formula similar to eq (4) and (5), which replace the expectation by a sum over the possible values of z. This will lead to a gradient estimator with no variance, can the author comment on this ?

- I think the bias induced by the mean-field approximation of the decoder should be investigated more thoroughly. Could the authors plot the gap as a function of n for example ? What happens if we also increase the number of category ? (there is a typo in this section it should be k^n instead of n^k) ? Can they compare to gumbel-softmax, is there a threshold at which gumbel-softmax becomes better ?

- It's not clear on what setting is the variance plotted in fig 1. is computed ? Is it computed on the discrete VAE experiment ? if so how many latent variables and category ? Could the bias also be provided ? Could it be compared to gumbel-softmax with varying temperature ?

- The experiments are a bit toyish, it's not clear what happens when the task are more complex, the architecture for the encoder and decoder are deeper or the latent space is bigger. In particular the authors only consider linear encoder and decoder when comparing the ELBO of different methods.

- In the semi-supervised settings what happens if we don't set the perturbed level to the true label ?

Conclusion:
The experiments are quite toyish and the approach is more computationally expensive than gumbel-softmax. More experiments should be done to clearly show the advantage of this method compared to gumbel-softmax.",5
"The authors propose a method to apply the reparametrization trick when the random variables of interest are discrete. Their technique is based on a formulation of the objective function in terms of Gumbel-Max operators. They propose a derivation of the gradient in terms of an auxiliary variable \epsilon, such that the resulting gradient estimate is biased but the bias is reduced as \epsilon approaches zero, at the cost of increasing variance. Experiments are performed with VAE including discrete latent variable models. The authors show how their method converges faster than other baselines formed by estimators of the gradient given by the REBAR, RELAX and Gumbel-soft-max methods. In experiments with semi-supervised VAEs, their method outperforms the Gumbel softmax method in terms of accuracy and objective function.

Quality:

The theoretical derivations seem rigorous and the experiments performed clearly indicate that the proposed method can outperform existing baselines.

Clarity:

The paper is clearly written and easy to read. I found that the network architecture shown in the left of Figure 1 a bit confusing and needs to be explained more clearly.

Significance:

The experimental results clearly show that the proposed method can outperform existing baselines and that the proposed contribution is significant.

Novelty:

The proposed method is novel up to my knowledge. This is the first time I have seen the proposed theoretical derivations, which are significantly different from previous approaches.
",7
"This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. 

It is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? 

Arguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. 

Another way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?
",7
"In this interesting study, the authors propose a score (BrainScore) to (1) compare neural representations of an ANN trained on imagenet with primate neural activity in V4 and IT, and (2) test whether ANN and primate make the same mistakes on image classification.  They also create a shallow recurrent neural network (Cornet) that performs well according to their score and also reasonably well on imagenet classification task given its shallow architecture.

The analyses are rigorous and the idea of such a score as a tool for guiding neuroscientists building models of the visual system is novel and interesting.

Major drawbacks:

1. Uncertain contribution to ML: it remains unclear whether architectures guided by the brain score will indeed generalize better to other tasks, as the authors suggest.

2. Uncertain contribution to neuroscience: it remains unclear whether finding the ANN resembling the real visual system most among a collection of models will inform us about the inner working of the brain.


The article would also benefit from the following clarifications:

3. Are the recurrent connections helping performance of Cornet on imagenet and/or on BrainScore?

4. Did you find a correlation between the neural predictivity score and behavioral predictivity score across networks tested? If yes, it would be interesting to mention.

5. When comparing neural predictivity score across models, is a model with more neurons artificially advantaged by the simple fact that there is more likely a linear combination of neurons that map to primate neural activations? Is cross-validation enough to control for this potential bias?

6. Fig1: what are the gray dots?

7. “but it also does not make any assumptions about significant differences in the scores, which would be present in ranking. “
What does this mean?

8. How does Cornet compare to this other recent work: https://arxiv.org/abs/1807.00053 (June 20 2018) ?

Conclusion:
This study presents an interesting attempt at bridging the gap between machine learning and neuroscience. Although the impact that this score will have in both ML and Neuroscience fields remains uncertain, the work is sufficiently novel and interesting to be published at ICLR. I am fairly confident in my evaluation as I work at the intersection of deep learning and neuroscience.
",7
"Please consider this rubric when writing your review:
1. Briefly establish your personal expertise in the field of the paper.
2. Concisely summarize the contributions of the paper.
3. Evaluate the quality and composition of the work.
4. Place the work in context of prior work, and evaluate this work's novelty.
5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.
6. Provide a summary judgment if the work is significant and of interest to the community.

1. I am a researcher working at the intersection of machine learning and
biological vision.  I have experience with neural network models and
visual neurophysiology.

2. This paper makes two contributions: 1) It develops Brain-Score - a
dataset and error metric for animal visual single-cell recordings.  2)
It develops (and brain-scores) a new shallow(ish) recurrent network
that performs well on ImageNet and scores highly on brain-score. 

3. The development of Brain-Score is a useful invention for the field.  A nice
aspect of Brain-Score is that responses in both V4 and IT as well as behavioral
responses are provided.   I think it could be more useful if the temporal dynamics (instead of the
mean number of spikes) was included.    This would allow to compare temporal
responses in order to compare ""brain-like"" matches.

4. This general idea is somewhat similar to a June 2018 Arxiv paper
(Task-Driven Convolutional Recurrent Models of the Visual System)
https://arxiv.org/abs/1807.00053
but this is a novel contribution as it is uses the Brain-Score dataset.

One limitation of this approach relative to the June 2018 ArXiv paper
is that the Brain-Score method is just representing the mean neural
response to each image - The Arxiv paper shows that different models
can have different temporal responses that can also be used to decide
which is a closer match to the brain.

5. More analysis of why CORNET-S is best among compact models would greatly
strengthen this paper.  What do the receptive fields look like?  How do they compare
to the other models.  What about the other high performing networks (e.g. DenseNet-169)?
How sensitive are the results to each type of weight in the network?   What about feedback connections
(instead of local recurrent connections)? 

6. This paper makes a significant contribution, in part due to the
development and open-sourcing of Brain-Score.  The significance of the
contribution of the CORnet-S architecture is limited by the
lack of analysis into what aspects make it better than other models.



",5
"- The writing and structure of the paper can be improved. It is difficult to read without first reading Gygli et al. 2017, and this paper should be more self-contained. There are also many parts that are not clear: 
  1. What is the model structure of G? Is it another neural network, or other structured prediction approaches such as graphical models? 
  2. The GAN-based approaches listed in the experiments section are originally designed for learning generative models. What are the adaptations required to turn them into structured prediction models? This is not clear at all. 

- The convergence of GAN training is an ongoing research problem and in practice also affects the quality of results. Yet in this paper I don't see any details on how these adversarial networks are trained jointly (e.g., heuristics to balance the progress on G and D). The authors should give more details on these.   

- The main difference of this paper compared to Gygli et al. 2017 seems to be the joint learning of a prediction model G. Instead of relying only on the valuation network D and starts iterative gradient ascent on the initial prediction of a vector y^(0) of all zeros, the authors start the iterative gradient ascent with the prediction from G (equation 7). Otherwise the paper looks very similar to Gygli et al. 2017, including the training sample generation methods. So to me the main message of this paper is that you can improve deep value networks by providing a better starting point in inference with G. The improvement is somewhat small though, between 1-2% on the datasets shown in the experiments section. 

- Overall this paper gives a useful but incremental improvement over the deep value network proposed by Gygli et al. 2017. However, the writing should be substantially improved to make the paper more self-contained and to include missing experiment details. 

=== after rebuttal ===

The authors explain some of their model choices in the rebuttal, but I am still not convinced about the difference with Gygli et al. 2017 is significant enough. 

",5
"Building on the work of (Gygli et al., 2017), this paper introduces a training algorithm for energy-based models for structured prediction. Similar to Gygli et al. (2017), they train an energy-based discriminator, which matches the energy value of structured outputs with their target values assigned by a value function.
The authors present the learning algorithm in an adversarial learning framework by describing a structured prediction model G and a discriminator D. However, it is very confusing for me to understand the proposed formulation as an adversarial framework. In an adversarial framework, at the equilibrium, G could be used as a final prediction model, however, the predicted output of G are still low quality. For example, considering Table 1, the performance of G is exactly the same as NN baseline, which suggests that only the first term of Eq. 6  participates in the training of G (because L_g(G, y*) is the exact objective of the NN baseline).
What that I can easily relate to, however, is that this training algorithm is similar to Gygli et al. (2017), but uses G to get an initial point for gradient-based inference. We want this initial point to be close to the target value Eq. 6. We use the initial value (prediction of G) and the ground truth as the matching constraints (Eq. 5) (as well as the other samples that construct Eq. 10). This actually describes why D can refine the output of G (because it looks at it as an initial point that needs refinement), but the discriminators in the other adversarial frameworks can't refine that much (since they have reached the equilibrium). I would love to hear authors comments on my concern regarding the proposed adversarial framework.

Other comments:
1) Lg is a surrogate loss, not the task-loss. Task-loss could be F1, IOU, BLEU, etc, which is the ultimate performance measure on a task. 
2) The authors refer to G as a structured prediction model but starting from Section 4, they have switched to call it a classifier, which is confusing.
3) ""Gygli et al. (2017) found that the key to learning energy-based models is generating proper training data."":  Is this a general statement for every energy-based model? I understand its effect when matching values, but is it still true for other training algorithms such as structural SVM training (Belanger and McCallum, 2016)? Do you have evidence to support it?
4) ""In the experiment, we adopt the fully convolutional network (FCN) (Long et al., 2015) baseline model proposed in (Gygli et al., 2017) as our segmentation network. It consists of three 5 × 5 convolutional layers and two deconvolution layer."": The text from Gygli et al. (2017) says three convolutional layers and two fully connected layers. Are you using the same architecture? If not, can you describe the architecture in more details? 
5) The qualitative results for Gygli et al. (2017) appear in https://gyglim.github.io/deep-value-net/. The reported output for DVN row is significantly worse than the segmentation results of the same horses specifically for columns 4 and 8, while the overall reported IOU in Table 2 is exactly the same. Can you describe the source of this disagreement?
6) Is having a continuous domain for value function v essential for the proposed training algorithm? 

=== After rebuttal ===
I am not convinced that the improved performance is because of the adversarial training. I trained a simple MLP and with the right amount of regularization it gets 42.0% f1 score on Bibtex, so I am not sure that the adversarial training is very essential here.
 

 ",5
"Summary:
This paper effectively learns a variant of a Deep Value Network (Gygli et al 2017), a model consisting of an energy network that assigns scores to input-output tuples that is trained to mimic a task-specific loss. The primary differences between the model presented in this work (titled LDRSP) and DVNs are twofold: first, the initial label prediction used at test time for inference is the output of a model rather than being initialized to all zeros. Second, a GAN-inspired loss is used to train both the scoring function and the initial prediction estimator. This new setup is compared against a variety of recent structured prediction methods on the tasks of multilabel classification, semantic segmentation, and 3-class face segmentation.

Comments:
I think the ideas presented in this paper are interesting, but I think their presentation could be a bit clearer. As mentioned in the summary, what you’re presenting is still more or less a deep-value network with some additions - however, you don’t refer to it as such in the body of the paper anywhere I saw. The first addition is the use of a learned model to produce the initial prediction; this is a natural extension to Deep Value Networks, and on its own is somewhat incremental in nature. I do not think you adequately explained why you chose to use a GAN-like loss to learn these models. Another baseline that would have helped justify its use would be to train your G model to predict structured outputs in the standard way (max-margin or cross-entropy loss) and then train your energy function in the DVN way. 

The experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine. You make appropriate comparisons against previous structured prediction models as well as against different types of GAN-like losses. But, as I mentioned before, I think you needed to have more comparisons against different ways of training these networks that do not follow a GAN-inspired framework. 

Overall, I like the new ideas in this paper but I think a few more experimental settings are required before they should be published.

=== after rebuttal ===

I appreciate the response, but I still think further analysis of the model is needed to understand where the gains in performance are coming from. The claim is that this is due to the adversarial loss used, but without further ablations I feel this is too strong a claim to be making given the current evidence.
",5
"This paper presents a theoretical study of different learning rate schedules. Its main result are statistical minimax lower bounds for both polynomial and constant-and-cut schemes.

I enjoyed reading the paper and I think the contributions in it shed some light in step size schedules that have shown to be useful in practice. I do have however some concerns that I hope the authors can address in their rebuttal. My initial rating is marginally below acceptance but I will gladly increase this rating if my concerns are addressed.


# Pros

* The paper is written in a way that's both clear and accessible.

* The Theoretical contributions are important, as they address the choice of step size in one of the most used optimization methods machine learning and are novel to the best of my knowledge.

* Due to time constraints, I only skimmed through the proofs, but results seem correct.


# Concerns


My biggest concern is that its unclear how realistic is their noise model. The authors assume that the noise in the stochastic gradients e verifies E[e e^T ] = \sigma H. While they claim that this is verified for problems like least squares, it is not clear to me that this is indeed the case. Related work like (Moulines and Bach, 2013) and (Flammarion and Bach, 2015) take the same setting but can only assume that the covariance of the noise is _bounded_ by a matrix of sigma times H. How do the authors obtain a much stronger condition on the noise covariance with the same assumption? I would be much more convinced with a proof in appendix clearly showing that the assumptions in footnote 8 imply the aforementioned covariance of the noise and a paragraph comparing their noise model with that of related literature like the aforementioned references (I'm not affiliated with any of that work). 

Also, the authors claim that their results hold for an arbitrary noise covariance matrix but the proofs are all done with the specific \sigma H matrix. I don't think its OK to say ""our results hold for a more general setting"" without proof. If they do hold for a more general setting then the proofs should be done in the general setting. If not, it should only be mentioned as future work. Please edit that remark accordingly.

* The paper does not compare or discuss against constant step size with averaging, which has been shown to be theoretically optimal in some scenarios (see aforementioned papers). This should at least be mentioned, and ideally also included in experiments.


# Presentation issues

Clarity of the proofs can be improved. For example, in Theorem 1, the formula for v_T(1) and v_T(d) follow from a recurrence that is stated _below_ the formula, needing several passes to understand. The proofs could benefit from a pass on them to improve the flow.


It is never clear whether expectations are taken with respect to the full randomness of the algorithm or conditioned on previous randomness. The E in Eq. just-before-section-4 (please add equation numbers) is a full expectation while the E[e] should be conditioned on previous randomness. The expectation in footnote 8 is also unclear if its wrt to the stochasticity of the algorithm or the randomness in the data generating process.


No equation numbers  makes it difficult to reference equations. Please add equation numbers so that reviewing is not more difficult than it should (and others can reference your work more precisely).

Other minor presentation issues include:

  * Page 1: Why l-BFGS and not L-BFGS? the lowercase l makes it look like a 1.
  * Page 2: There important -> There *are* important.
  * Page 2: In fact, at least ... (missing parenthesis around Omega tilde).
  * Page 4: ""a stochastic gradient oracle which gives us"" the second w should also be boldface.
  * Page 4: I would have appreciated

  * Page 11: ""variance in the i-th"" direction. It would be more correct to say in the i-th coordinate as otherwise it can be mistaken with the i-th update direction.

Update:
  I am satisfied with the answers and have upgraded my rating.",6
"The paper studies the effect of learning-rate choices for stochastic optimization, focusing on least-mean-squares with decaying stepsizes. The main result is showing that exponentially decaying stepsizes can yield improved rates of convergence of the final iterate in terms of dependence on the condition number. The proposed learning rate schedule depends on the condition number and the number of iterations. This positive result is complemented by showing that without prior knowledge of the time horizon, any stepsize sequence will frequently yield suboptimal solutions.

I have mixed feelings about the paper. On the positive side, the particular observation that exponential learning-rate schedules lead to faster convergence for SGD in linear least-squares problems indeed seems to be a novel result, and the lower bound also appears to be new and interesting. The analysis seems to be technically correct as well. 

On the other hand, I have several concerns about the presentation of the results:

- The abstract and the introduction sets up a misleading narrative around the results: the authors seem to suggest that their work somehow explains why certain learning-rate schedules work better than others for deep learning applications / non-convex optimization, although the actual results exclusively concern the classical problem of linear least-squares regression. This presentation is completely uncalled for as the authors themselves admit that it is unclear how the results would generalize to other convex optimization settings, let alone non-convex optimization. Also, I think that this presentation style is rather harmful as it suggests that learning-theory results concerning classical setups are somehow embarrassing, so they need to be sold through some made-up connections to trendy topics in deep learning. I would suggest that the authors completely ""rethink"" the presentation of the paper and write it in a style that is consistent with the actual results: as a learning theory paper, without the irrelevant deep learning experiments (that only show well-known phenomena anyway).

- The paper misrepresents a large body of work on stochastic/online optimization. Specifically, the authors suggest that the stochastic optimization literature exclusively suggests the use of polynomially decaying stepsizes. This picture is grossly inaccurate for multiple reasons:
*** It has been known for a while that the de facto optimal tuning of SGD for least squares involves a large constant stepsize and iterate averaging (see, e.g., Bach and Moulines, NIPS 2013). This approach is only mentioned in passing without any discussion, even though it yields convergence rates that do not involve *any* dependence on the condition number in the leading term---thus achieving a much more significant improvement than the learning-rate schedule studied in this paper. In light of these results, learning-rate schedules are already being ""re-thought"" as we speak, and studying the behavior of the last iterate has received less attention in the past couple of years. If anything, the present paper only provides further evidence (through the negative result) that the individual iterates are ill-behaved in general and it is better to average the iterates instead. I would consider this negative result as an interesting addition to the stochastic-optimization literature, had it been presented in a completely different narrative (e.g., augmenting the discussion in ""Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes"" by Shamir and Zhang, 2013).
*** Exponentially decaying (or ""constant-and-cut"", as they are called here) schedules have actually been studied before in the paper ""Beyond the Regret Minimization Barrier: Optimal Algorithms for Stochastic Strongly-Convex Optimization"" by Hazan and Kale (JMLR 2014). This significantly weakens the main intended selling point of the paper which was being the first-ever study of such learning-rate schedules. The results in said paper are of a somewhat different nature, but they have arguably as little to do with deep learning as the results of the present paper has. Notably, both the present paper and the cited work rely on *strong convexity* of the objective (through assuming prior knowledge of the condition number), so I would expect that none of these results would explain anything in the context of deep learning.

On the technical side, the proofs appear to be correct but presented somewhat sloppily, with most of the notation appearing without proper definitions. For instance, the proof of Theorem 2 seems to import notation from the proof of Theorem 1, although without explicitly mentioning that the covariance matrix is assumed to be diagonal(ized). The proof of Theorem 3 then seems to again replace this previously (non-)established notation by another one (e.g., v becomes err and \eta becomes \gamma). The proofs also involve long sequences of inequalities without explanation, and only bound the variances (w_k-w^*_k)^2 without mentioning how this quantity is related to the excess risk. (The relation is well-known but not obvious at all for first-time readers of such proofs.)

One technical limitation of the results is that they assume a simple additive-noise model for the gradients, which the authors conveniently call ""fairly natural"" and incorrectly claim to hold for linear regression with well-specified models (footnote 8). In reality, the gradient noise in this setting also depends on the current iterate w_t, which makes analysis significantly harder. (To see the difference, just compare the complexity of the proofs of Lemma 1 and Theorem 2 that correspond to these different settings in ""Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression"" by Dieleveut, Flammarion and Bach, 2017.)

Overall, I don't think that this paper is fit for publication in its present form. Once again, I would suggest that in a future version, the authors focus solely on discussing the actual results without attempting to draw disproportionate conclusions from them.

Detailed comments
=================
- pp.1, abstract: the first half of the abstract is completely irrelevant to the rest of the paper, so I'd suggest removing it.
- pp.1, ""learning-rate schedules for SGD is a rather enigmatic topic""---""enigmatic"" feels like a bit of a strong adjective here, given that there are many aspects of learning-rate tuning that are actually pretty well-understood.
- pp.2: The second paragraph on page 2 is again irrelevant to the actual technical content of the paper.
- pp.2, ""all the works in stochastic approximation try to bound the error of each iterate of SGD""---This is simply not true, given the growing literature concerning the behavior of the *averaged iterates*.
- pp.4, first display: poor typesetting.
- pp.6, Eqs. 1--3: ditto.
- pp.8, last paragraph: Singling out the particular setting of gradient-norm minimization feels arbitrary and poorly justified.
- pp.11: the first and second displays should be switched for better readability (otherwise the first one comes without explanation). Also note that this form is not just due to the algorithm design, but also to the simplified noise model.
- pp.12, App B: 
*** It appears that you forgot to mention here that you're working in the coordinate system induced by the eigenvectors, and also forgot to define the eigenvalues, etc. 
*** The indices (1) and (k) are incoherent in the first display. 
*** Although you promise you'll prove the inequality in the second display, you eventually prove something else.
*** It is not very clear on first sight that \ell^* actually exists and falls within the scope of \ell---you should explain that it exists due to the choice of the number of phases. (Which, by the way, should be rounded up to allow this property?)
*** The sequence of inequalities in the last display seems correct but unnecessarily hard to verify due to the lack of explanations.",4
"This work provides theoretical insights on recent learning rate proposals such as Cyclical Learning Rates (Smith et al.). The authors focus on stochastic approximation i.e. how large is the SGD loss as a function of condition number and horizon. The critical contribution is the theoretical benefit of oscillating learning rates over more traditional learning rate schemes. Authors provide novel upper/lower bounds to establish benefit of oscillating LR, support their theory with experiments and provide insights on finite horizon learning rate selection. An important drawback is that results only apply to linear regression which is a fairly simple setup.

I have two important comments regarding this work:
1) I believe proof of Theorem 3 has a bug. In the proof, authors use the inequality
(1-gamma_t lambda^k)^2 < exp(-2lambda^k gamma_t).
Obviously this can only be correct for gamma_t lambda^k<1. However, checking the setup of the problem, it can be seen that for largest eigenvalue and gamma_0, ignoring log factors:

gamma_0L = L/(mu T_e)=kappa / T_e=kappa/T.

Since, no restriction is imposed on T, gamma_0L can be as large as O(kappa) and invalidates the above inequality. So T should be T>O(kappa). I am not sure if this affects the overall statement or the remaining argument.

2) The paper can benefit from more detailed experiments (e.g. Figs 1 and 2). Arguably the most obvious baseline is ""constant learning rate"". However, authors compare to 1/T or 1/sqrt(T) learning rates. It is not at all clear from current experiments, if the proposed approach beats a good constant LR choice.

I am happy to increase my score if the comments above are addressed.",6
"# Summary

This paper proposes search-guided training for structured prediction energy networks (SPENs). SPENs are structured predictors that learn an input-dependent, non-linear energy function that scores candidate output structures. Many methods have recently been proposed for training SPENs. One in particular, rank-based training, has the advantage of supporting training from weak supervision in the form of a reward function. By performing gradient descent on this reward function, rank-based training generates output, improved output pairs that become margin-based constraints on the learning objective. Each constraint specifies a pair of outputs for a given input, and penalizes the current weights if the improved output is not scored higher than the other output by a certain margin.

This paper addresses a limitation of rank-based training, that this gradient descent procedure for finding output pairs may get stuck in plateaus. In search-guided training, truncated randomized searches are performed starting at an initial output to find an improved output. The paper says that the random search procedure is informed by the reward function, but it is not specific. Are steps in the search space performed uniformly at random? The paper only says that the returned improved example must score higher in the reward function by some margin \delta that is ""based on the features of the reward function (range, plateaus, jumps)"" but it is not discussed how to identify these features of the reward function or how to set \delta accordingly.

Experiments are conducted multi-label classification, citation field extraction, and shape parsing. On multi-label classification search-guided SPENs (SG-SPENs) outperform structural SVM training of SPENs. Why is it not compared with rank-based training (R-SPENs)? On citation field extraction, SG-SPENs improves accuracy by two percentage points over R-SPENs. On shape parsing, R-SPENs fail because it cannot produce valid parsing programs as improved outputs. SG-SPENs perform well relative to other methods like iterative beam search and neural shape parsing.

# Strengths

SG-SPENs are better across the experiments than other SPEN training methods, though I do not know why they are not compared against R-SPENs on multi-label classification.

# Weaknesses

The work seems incremental without any major new insights beyond the work on R-SPENs. The idea seems to reduce to doing random search instead of gradient descent on a reward function in order to produce output pairs.

As mentioned above, the paper is also light on details about how the experiments were conducted, such as setting \delta and creating the space of operators to use when searching for improved outputs.",5
"Summary:
This paper discusses a method to train SPENs when strong supervision is not provided. Instead, training feedback comes in the form of a scalar-valued scoring function for a provided input as well as a prediction. The approach taken here is similar to that described in [1] in that score-violating pairs are found using some procedure, which are then used to update the parameters of the model. The primary difference here is that a random search procedure is used to find score violations rather than the test-time inference procedure; this is justified by noting that the gradient descent procedure may become stuck in flat areas of the optimization surface and thus not encounter high-reward areas. Experiments are run on multilabel classification, citation field extraction, and shape parsing tasks to demonstrate the validity of this approach.

Comments:
Overall, this paper is very nicely written and presents its ideas very clearly. The base approach is the same as presented in [1], but the changes to the learning procedure are adequately justified (and the experiments corroborate this). Furthermore, everything is explained in sufficient detail to be easy to follow. The main detail that I didn’t notice anywhere was a sentence or two describing the random search procedure used - adding this would further clarify your approach.

The tasks chosen to evaluate these methods are diverse and indicate that this approach is broadly useful in situations where strong supervision may be hard to come by. I think it would have been interesting to see how the model performs in a semi-supervised task (i.e. where some small fraction of the data has labels), but perhaps this is better suited for future work. The one question I have regarding your results is the following: you include the average reward for the citation-field extraction task in your results table, but don’t seem to comment on this anywhere. Are there any conclusions that you think these results imply?

This paper is an excellent addition to the field of structured prediction, and thus I think it should be accepted.

[1] Rooshenas, A., Kamath, A., & McCallum, A. (2018). Training Structured Prediction Energy Networks with Indirect Supervision. NAACL HLT 2018
",7
"The paper proposes to use a reward function to guide the learning of energy-based models for structured prediction. The idea is to update the energy function based on a random search algorithm guided by a reward function. At each iteration, the SPEN proposes a solution, then a better one is found by the search algorithm, and the energy function is updated accordingly.  Experiments are made on three use-cases and show that this method is able to outperform other training algorithms for SPENs. 

In term of model, the proposed algorithm is interesting since it can allow us to learn from weakly supervised datasets (i.e a reward function is enough). Note that in Section 3, the reward function R is never properly defined which would be nice. The algorithm is quite simple and well presented in the paper. The fact that it is based on a margin could be discussed a little bit more since the effect of the margin is not clear in the paper (the value of alpha). Moreover, the structured prediction problem has already been handled as the maximization of a reward function using RL techniques (see works by H. Daume, and works by F. Maes) and the interest of this approach w.r.t these papers is not clear to me. A clear discussion on that point (and experimental comparison) would be nice. 

The experimental section could be improved. First, the experiments on multi-label classification do not provide any comparison with SoTA methods while the two other use-cases provide some comparisons. Moreover, as far as I understand, the different use-cases could be fully supervised, and different reward functions could be defined. So investigating more deeply the consequences of the nature of the supervision/reward on these use-cases could be interesting and strengthen the paper.  Moreover, training sets are very small and it is difficult to know if this method can work on large-scale problems. 

Pro:
* interesting algorithm for structured prediction (base on reward)
* interesting results on some (toy) use-cases

Cons:
* Lack of discussion on the positive/negative point of the approach w.r.t SoTA, and on the influence of the reward function
* Lack of experimental comparisons 
* Only toy (but complicated) problems with limited training sets
",4
"This paper studies the geometry of adversarial examples under the assumption that dataset encountered in practice exhibit lower dimensional structure despite being embedded in very high dimensional input spaces. Under the proposed framework, the authors analyze several interesting phenomena and give theoretical results related to the necessary number of samples needed to achieves robustness. However, the theory in this paper is not very deep.

Pros:

The logic of this paper is very clear and easy to follow. Definitions and theories are illustrated with well-designed figures.

This paper shows the tradeoff between robustness under two norm and infinity norm for the case when the manifolds of two classes of data are concentric spheres.

When data are distributed on a hypercube in a k dimensional subspace, the authors show that balls with radius \delta centered at data samples only covers a small part of the ‘\delta neighborhood’ of the manifold. 

General theoretical results on robustness and minimum training set to guarantee robustness are given for nearest neighbor classifiers and other classifiers.

Cons:

Most of the theoretical results in this paper are not very general. The tradeoff between robustness in different norms are only shown for concentric spheres; the ‘X^\epsilon is a poor model of \mathcal{M}^\epsilon’ section is only shown for hypercubes in low dimensional subspaces. 

Section 5 is not very convincing. As is discussed later in the paper, although $X^\delta$ only covers a small part of \mathcal{M}^\delta, robustness can be achieved by using balls centered at samples with larger radius.

Most of the analysis is based on the assumption that samples are perfectly distributed to achieve the best possible robustness result. A more interesting case is probably when samples are generated on the manifold following some probabilistic distributions. 

Theorems given in Section 6 are reasonable, but not very significant. It is not very surprising that nearest neighbor classifier is more robust than ‘x^\epsilon based’ algorithms, especially when the samples are perfectly distributed. 
",5
"This paper gives a theoretical analysis of adversarial examples, showing that (i) there exists a tradeoff between robustness in different norms, (ii) adversarial training is sample inefficient, and (iii) the nearest neighbor classifier can be robust under certain conditions. The biggest weakness of the paper is that theoretical analysis is done on a very synthetic dataset, whereas real datasets can hardly be conceived to exhibit similar properties. Furthermore, the authors do not give a bound on the probability that the sampling conditions for the robust nearest neighbor classifier (Theorem 1) will be satisfied, leading to potentially vacuous results.

While I certainly agree that theoretical analysis of the adversarial example phenomenon is challenging, there have been prior work on both analyzing the robustness of k-NN classifiers (Wang et al., 2018 - http://proceedings.mlr.press/v80/wang18c/wang18c.pdf) and on demonstrating the curse of dimensionality as a major contributing factor to adversarial examples (Shafahi et al., 2018 - https://arxiv.org/abs/1809.02104, concurrent submission to ICLR). I am very much in favor of the field moving in these directions, but I do not think this submission is demonstrating any meaningful progress.

Pros:
- Rigorous theoretical analysis.

Cons:
- Results are proven for particular settings rather than relying on realistic data distribution assumptions.
- Paper is poorly written. The authors use unnecessarily complicated jargon to explain simple concepts and the proofs are written to confuse the reader. This is especially a problem since the paper exceeds the suggested page limit of 8 pages.
- While it is certain that nearest neighbor classifiers are robust to adversarial examples, their application is limited to only very simple datasets. This makes the robustness result lacking in applicability.
- Weak experimental validation. The authors make repeat use of synthetic datasets and only validate their claim on MNIST as a real dataset.",3
"This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency.
 
In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\del and \pi^\del) and vol \pi^\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. 
I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. 

Minor concerns:
1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary.
2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1.
3. In Page 7, “Figure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.”  I think, the subfigure shows that the ratio approaches 1 when d and k are all increased.
",6
"Summary:
This paper incorporates Generalized Additive Models (GAMs) with model distillation to provide global explanations of neural nets (fully-connected nets as black-box in the paper). It is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of black-box models. However, it lacks novelty and is limited to tabular data as presented.

Pros:
- The paper is well written.
- The experiments are detailed and thorough with both synthetic and real data.

Cons:
- The novelty is limited. The core consists of GAMs well studied in the literature, e.g. Caruana et al 2015. Admittedly, this work also tries to incorporate model distillation to explain black-box models globally. The concept of student models approximating teacher models is not new either. The originality seems incremental in both directions.
- The scope is limited. The paper only presents applications in tabular data. Also, it would be better to experiment with black-box models beside simple fully-connected nets.
- The interpretability is not convincing. It is not sufficient to demonstrate the interpretability of the proposed method, or the expressive advantage of feature shapes. It is encouraged to include studies with human subjective to compare against other existing interpretable approaches.

Specifics:
- With Figure 3, it is not convincing that the student model actually explains the teacher model, so the paper tries to elaborate more with Table 1. I think Table 1 also needs more details to help, such as the significance of error difference and '-' elements.
- Many figures are hard to read mostly because of font, color, and overlap.
",6
"Summary: This paper makes an interesting contribution of providing global explanations of black box models (such as neural nets) using a special class of models called generalized additive models.  While the paper is well written and experiments are quite detailed, I have some problems with some of the basic premises of this work. 
1. The concept of using simpler models to approximate other complex models (model distillation) is not new and has been explored quite a bit already in ML literature. The only new proposition of this work is to use generalized additive models to approximate other complex models. This seems rather incremental. 
2. The premise behind using generalized additive models (GAMs) to explain other complex models is that GAMs are interpretable. I am not convinced about this premise. While I can intuitively see that GAMs might be able to better approximate complex models compared to rules and trees, I highly doubt if they are even interpretable. 

Pros:
1. The paper is well written
2. Experiments are very detailed and thorough

Cons:
1. The proposed approach lacks novelty
2. Experimentation lacks a user study which helps understand if and when GAMs are at least as interpretable as rule-based approaches.

Detailed Comments: 
I actually like the way this paper is written and executed. The writing is very clear and experiments are quite thorough. But, as discussed earlier, I have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that GAMs are interpretable. I would encourage the authors to discuss these two aspects in their rebuttal. I would strongly encourage the authors to carry out at least a simple user study which compares the interpretability aspect of GAMs with rule-based or prototype based approaches. 
",4
"This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense.

Good things:
- It is a very elegant method. It is also very simple (in a good way).
- The paper is really well written.
- The experiments are carefully conducted and are indeed showing what the authors describe.
- I think the method is potentially of practical use.

Problems:
- I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though.
- The font is too small in many figures. It is impossible to read it. 
- I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network.
- Only makes sense to apply this to fully connected networks.",6
"i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.

The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. 

The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.

Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty.

",6
"This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. Under a set of conditions, the authors proved convergence of the proposed attack algorithm. My main concern about this paper is why this algorithm has a better performance than CW attack? I would suggest comparing with CW attack under different sets of hyper-parameters.

Minor comment:
The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.",5
"The authors propose a new method for constructing adversarial examples called MarginAttack. The method is inspired by Rosen's algorithm, a classical algorithm in constrained optimization. At its core, Rosen's algorithm (instantiated for adversarial examples) alternates between moving towards the set of misclassified points and moving towards the original data point (while ensuring that we do not move too far away from the set of misclassified points). The authors provide theoretical guarantees (local convergence) and a broad set of experiments. The experiments show that MarginAttack finds adversarial examples with small distortion (as good as the baselines or slightly better), and that the algorithm runs faster than the Carlini-Wagner (CW) baseline (but slower than other methods).

The authors make a distinction between ""fixed perturbation"" attacks and ""zero confidence"" attacks. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Method such as projected gradient descent fall into the ""fixed perturbation"" category, while MarginAttack and CW belong to the ""zero confidence"" category. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Indeed, their results show that MarginAttack is 3x - 5x faster than CW and sometimes achieves smaller perturbations.

First of all, I would like to emphasize that the authors conducted a thorough experimental study on multiple datasets using multiple baseline algorithms. Unfortunately, the comparison to CW and PGD still leaves some questions in my opinion:

- The authors state that CW does an internal binary search over the Lagrangian multiplier, and that this search goes for up to 10 steps. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters. To the best of my knowledge, the CW implementation in Cleverhans is specifically set up so that the user does not need to tune a large number of hyperparameters (the implementation accepts a running time overhead to achieve this). Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search.

- The authors explicitly state that the step sizes for CW were tuned for best performance, but do not mention this for PGD. For a fair comparison, the step sizes used for PGD should also be (approximately) tuned. Moreover, it is not clear why PGD is only used for an l_inf comparison and not a l_2 comparison.

- In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above).

I would be grateful if the authors could provide their view on these points. Until then, I will assign a rating of 5 since tuning the parameters of optimization algorithms is crucial for a fair comparison.


Additional comments:

- In the introduction, the authors equate white-box attacks with access to gradient information. But generally a white-box attack is understood as an attack that has arbitrary access to the target network. It may be helpful for the reader to clarify this.

- In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. So it is not clear that there is a large gap in difficulty. Moreover, the authors state that fixed perturbation attacks often come with theoretical guarantees. But to the best of my knowledge, there is no comprehensive theory that describes when a fixed perturbation attack should be expected to succeed in attacking a commonly used neural network.

- On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. Why is that?

- The authors state that JSMA (Papernot et al., 2016) is one of the earliest works that use gradient information for constructing adversarial examples. However, L-BFGS as employed by Szegedy et al., 2013 also uses gradient information. Moreover, the authors may want to cite the work of Biggio et al. from 2013 (see the survey https://arxiv.org/abs/1712.03141).

- Since all distances referred to by d(x, y) seem to be norms (and the paper relies on the existence of dual norms), it may be more clear for the reader to use the norm notation || . || from the beginning.",5
"This paper proposes a new architecture for learning dynamics models in 2D Atari-like game words. The architecture includes multiple layers of abstraction: a “motion detection” level, which looks at which pixels change over time in order to guess at which parts of the image are in the foreground or not; a “instance segmentation” level, which segments the foreground into regions and instances; and a “dynamics learning” level, which learns the dynamics of object instances using a interaction network-style approach.

Pros:
- Impressive-looking dynamics predictions in Atari-like games.
- An object-based prediction model, which could enable predictions about specific entities in the scene rather than holistic frame predictions.

Cons:
- Very complicated and difficult-to-understand architecture.
- No ablation studies to validate different components of the architecture.
- No validation in a model-based RL or control setting.
- Experiments are only done on one-step predictions, rather than long-term rollouts.

Quality
---------

The quality of the predictions seems quite high (based on Figure 6 and the results tables), though there are a number of opportunities to further strengthen the evaluation and analysis:

- I wish that there were more than a single figure of qualitative results to go on. I highly recommend that a revision include a link to a video showing more predictions over time for each environment, ideally with comparisons to the other baselines as well.
- The introduction of the paper motivates the learning of the model in terms of model-based RL, however, the model is not actually used in a model-based RL setting. It would be nice to see at least a simple validation that the model can be used with an off-the-shelf planner to solve one of the games which are evaluated in the paper. If it cannot, then that limits the significance of the model.
- As far as I can tell, all the results reported in the tables are based on one-step predictions only. While it is great to show that even in this regime the other models struggle, it would be even better if results could be reported for longer rollouts (i.e., taking the model outputs and feeding it back in as input, and repeating this procedure say 50 steps into the future). Models are not particularly useful in a MBRL setting if they can only be used to predict a single timestep, so it is important to validate that longer-term predictions can be made as well.

Overall the literature review is reasonably solid, but I am not sure the citations in the opening sentence are quite appropriate as model-based DRL has been around for longer than 2017 (see for example [1-3]). Moreover, Chiappa et al (2017) only learns a model and does not use it for planning, so I am not sure it is quite appropriate as a citation for MBRL. 


Clarity
--------

Unfortunately, I had a very hard time understanding how exactly the architecture works and I felt like there were a lot of details missing. I am not confident that I would be able to reproduce the architecture from reading the paper alone. Below, I will list some of the specific points where I was confused, but I think overall the paper needs to be substantially reorganized in order to be clearer as to how the architecture actually works.

More broadly, I think some of my confusion stems from the fact that there are very similar computations occurring across the three levels of abstraction but the paper does not really make it clear how these computations relate to one another or how they are similar/different. For example, in the “dynamics learning” level there are modules for performing object detection and instance localization. But then in the “instance segmentation” level, there are similarly modules for detecting and masking out instances. It is not clear to me why this needs to be done twice? 

In general, I would *strongly* recommend including at least in the appendix an algorithm box that sketches out the computational graph for the whole architecture (not in as much detail as the existing algorithm boxes, but in more detail than what is given in Figure 1).

Specific places where I was confused:

- Where do the region proposals (P) come from?
- If I’m understanding correctly, the variable M is used multiple times in multiple different ways. It seems to be produced from the “instance localization” module in the “dynamics learning” level, but also from the “dynamic instance segmentation network” in the “instance segmentation” level. Are these M different or the same?
- Where does F_foreground^(t) come from?


Originality
-------------

Overall, the idea of learning object-based transition models is not really new (and there are a few citations missing regarding prior work in this regard, e.g. [4-6]). However, there is yet to be an accepted solution for actually learning object-based models robustly and the present work seems to result in the cleanest separation between dynamic objects and background that I have seen so far, and is therefore quite original in that regard.

This paper appears to be quite similar to Zhu & Zhang (2018), with the main difference being additional functionality to handle multiple dynamic objects in a scene rather than just a single dynamic object. This is a fairly significant difference and the improvement over Zhu & Zhang (2018) seems quite large, so even though the papers seem quite similar on the surface I think the difference is actually quite substantial.

Significance
----------------

If it were clearer how to reproduce this paper, and if it could be shown to apply to a wider range of environments (e.g. the Atari suite, or even better the Sonic domains from the OpenAI Retro contest), then I believe this paper could be quite significant as it would open up new avenues for model-based learning in these domains. Unfortunately, however, it is not clear to me as the paper is currently written how well it would do on other 2D environments, thus limiting the significance. If the model only works on Monster Kong and Flappy Bird---neither of which are commonly used in the RL literature---then it has limited applicability to the rest of the model-based RL community. Similarly, as stated above, it is not clear how well the model will work with longer rollouts or in actual in MBRL settings, thus limiting its significance.

References
---------------

[1] Heess, Wayne, Silver, Lillicrap, Tassa, & Erez (2015). Learning Continuous Control Policies by Stochastic Value Gradients. NIPS 2015.
[2] Gu, Lillicrap, Sutskever, & Levine (2016). Continuous Deep Q-Learning with Model-based Acceleration. ICML 2016.
[3] Schmidhuber (2015). On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models. arXiv 2015.
[4] Wu, Yildirim, Lim, Freeman, & Tenenbaum (2015). Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning. NIPS 2015.
[5] Fragkiadaki, Agrawal, Levine, & Malik (2016). Learning visual predictive models of physics for playing billiards. ICLR 2016.
[6] Kansky, Silver, Mely, Eldawy, Lazaro-Gredilla, Lou, Dorfman, Sido, Phoenix, & George (2017). Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. ICML 2017.",4
"This paper proposes a novel architecture, coined Multi-Level Abstraction Object-Oriented Predictor, MAOP. This architeture is composed of 3 parts, a Dynamics model, an object segmentation model, and a motion detection module.

While some parts of the model use handcrafted algorithms to extract data (e.g. the motion detection), most parts are learned and can be trained without much additional supervision, as the objectives are mostly unsupervised objectives.

The proposed model is interesting, and certainly ""solves"" the two tasks it is trained on. On the other hand, this model seems to be specifically tailored to solve these two tasks. It assumes a static background, very local newtonian-like physics, a very strong notion of object and object class. It is not clear to me if any of the improvements seen in this paper are valuable, reusable methods, or just good engineering work.
As such, I do not think that this paper fits ICLR. There has been a growing number of works that aim to find learning algorithms that learn to discover and disentangle object-like representations without having so much prior put into the model, but rather through some general purpose objective. The current paper seems like a decent applications paper, but it explores improvements orthogonal to this trend that IMO is what preoccupies the ICLR audience.

The writing of this paper makes it a bit hard to understand what the novel contributions of this paper are, and how the proposed method should go beyond the two problems that it solves. In general, there are many phrasings that would benefit from being rewritten more concisely; it would help with clarity, since the proposed model has a multitude of different parts with sometimes long names.

Experimentally, there are many parts to the proposed model, and while it is clear what each of them achieves, it is unclear how necessary each of the parts are, and how sensitive the model is to any part being (possibly slightly) incorrect.

The proposed method is tested on, presumably, RL environments; yet, no RL experiments are performed, so there is no way of knowing if the proposed model is actually useful for planning (there are instances of model-based methods learning acceptable models that are just wrong enough to *not* be useful to actually do RL or e.g. MCTS planning).

Overall, this paper tackles its tasks in an interesting but maybe too specific way; in addition, it could be improved in a variety of ways, both in terms of presentation and content. While the work is novel, I am not convinced that it is relevant to the interests of the ICLR audience.


Comments:
- When running your experiments, do you report results averaged over multiple runs?
- Figure 4+C7: why does the x-axis start at 2000?
- I don't think Figure 5 is really necessary
- All figures: your captions could be improved by giving more information about what their figure presents. E.g. in Figure C7 I have no idea what the curves correspond to. Sure it's accuracy, but for which task? How many runs? Is it a running average? Etc.
- Where are the test curves? Or are all curves test curves?
- Your usage of \citep and \citet, (Author, year) vs Author (year), is often inconsistent with how the citation is used.

",4
"In this paper, the novel MAOP model is described for self-supervised learning on past video game frames to predict future frames. The presented results indicate that the method is capable of discovering semantically important visual components, and their relation and dynamics, in frames from arcade-style video games. Key to the approach is a multi-level self-learning approach:  more abstract stages focus on simpler problems that are easier to learn, which in turn guide the learning process at the more complex stages.
A downside is that it the method is complex, consisting of many specific sub-components and algorithms, which in turn have again other sub-components. This makes the paper a long read with a lot of repetition, and various times the paper refers to the names of sub-components that are only explained later. Other methodological details that are relevant to understand how the method operates are described in the Appendices. I expect that if the paper would be better structured, it would be easier to understanding how all the parts fit together. Another downside of this complexity is that the method seems designed for particular types of video game frames, with static backgrounds, a fixed set of objects or agents. It is unclear how the method would perform on other types of games, or on real-world videos. While the method therefore avoids the need for manual annotation, it instead encodes a lot of domain knowledge in its design and components.
I also didn't fully understand how the self-supervised model is used for Reinforcement Learning in the experiments. Is the MAOP first trained, and the fixed to perform RL with the learned agent models, or is the MOAP learned end-to-end during RL?

Pros:
+ MAOP seems successful on the tested games in the experiments
+ Demonstrates that, with a sufficiently engineered method, self-supervised learning can be used to discover different types of objects, and their dynamics.

Cons:
- writing could be improved, as the methodology currently reads as a summation of facts, and some parts are written out of order, resulting in various forward references to components that only become clear later. Several times, the paper states that some novel algorithm is used, but then provides no further explanation in the text as all description of this novelty is deferred to an appendix. 
- method does not seem generic, hence it is unclear how relevant this architecture it is to other use cases
- many hyperparameters for the individual components, algorithms. Unclear how these parameter setting affect the results

Below are more detailed comments and questions:

General comments:
* The proposed MOAP method consists of many subalgorithms, resulting in various (hyper)parameters which may impact the results (e.g. see Appendix A, B). Appendix D lists several used hyperparameter settings, though various parameters for the algorithms are still missing (e.g. thresholds alpha, beta in Algo.2). Were the used parameters optimized? How are these hyperparameters set in practice? How does changing them impact your results?
* Methods seems particularly designed for 'video games', where the object and background structures have well defined sizes, appearance, etc. How will the MOAP fair in more realistic situations with noisy observations, occluded objects, changing appearances and lighting conditions, etc.?
* How about changing appearance of an agent during an action, e.g. a 'walking animation' ? Can your method learn the sequence of sprites to accurately predict the next image? Is that even part of the objective?
* Appendix D has important implementation details, but is never mentioned in the text I believe! Didn't realize it existed on first read through.

* Introduction:
	* What prediction horizon are you targeting? 1 step, T steps into the future, 1 to T steps in the future simultaneously?
	What are you trying to predict? Object motion? Future observations?
	* ""... which includes a CNN-based Relation Net to ... "", the names Relation Net, Inertia Net, etc.. are used as if the reader is expected to know what these are already. If these networks were introduced in related work already, please add citations. Otherwise please rephrase to clarify that these are networks themselves are part of your novel design.

* Section 3.1
	* ""It takes multiple-frame video images ... and produce the predictions of raw visual observations."". As I understand from this, the self-supervised approach basically performs supervised learning to predict a future frame (target output) given past frames (input). I do not understand how this relates to Reinforcement Learning (RL) as mentioned in the introduction and Related Work. Is there still some reward function in play when learning the MAOP parameters? Or is the idea to first self-supervised learn the MAOP, and afterwards fix its parameters and use it in separate a RL framework? I believe RL is not mentioned anymore until Section 4.2. This connection between self-supervised and reinforcement learning should be clarified, or otherwise the related work should be adjusted to include other (self-supervised) work on predicting future image frames.
	* ""An object mask describes the spatial distribution of an object ..."" Does the distribution capture uncertainty on the object's location, or does it capture the spread of the object's extent ('mass distribution') ?
	* ""Note that Object Detector uses the same CNN architecture with OODP"". What does OODP stand for? Add citation here. (first mention of OODP is in Experiments section)
	* ""(similar with Section 3.2)"" → ""similar to"". Also, I find it a confusing to say something is similar to what will be done in a future section, which has not yet been introduced. Can you not explain the procedure here, and in Section 3.2 say that the procedure is ""similar to Section 3.1"" instead?
	* ""to show the detailed structure of the Effect Net module."" First time I see the name 'Effect Net', what is it? This whole paragraph different nets are named, with a rough indication of their relation, such as ""Dynamic Net"", ""Relation Net"" and ""Inertia Net"". Is ""Effect Net"" a different name for any of the three previous nets? The paper requires the reader to puzzle from Fig.2 that Relation Net and Inertia Net are parts of Effect Net, which in turn is part of Dynamics Net. This wasn't clear from the text at all.

* Section 3.2:
	* p7.: ""Since DISN leans"" → ""Since DISN learns"" ?
	* There are many losses throughout the paper, but I only see at the end of Section 3.1 some mentioning that multiple losses are combined. How is this done for the other components, .e.g is the total loss for DISN a weighted sum of L_foreground and L_instance ? Are the losses for all three three MAOP levels weighted for full end-to-end learning?
	* This section states various times ""we propose a novel [method]"", for which then no explanation is given, and all details are explained in the Appendix. While the Appendix can hold important implementation details, I would still expect that novelties of the paper are clearly explained in the paper itself. As it stands, the appendix is used as an extension of the methodological section of an already lengthy paper.
	* ""Conversely, the inverse function is ... "" M has a mask for each of the n_o ""object classes"", hence the ""Instance Localization Module"" earlier to split out instances from the class masks. So how can there be a single motion vector STN^-1(M,M') if there are multiple instances for an object mask? How will STN^-1 deal with different amount of instances in M and M' ?

* Section 3.3:
	* What is the output of this level? I expect some mathematical formulation as in the previous sections, resulting in some symbol, that is then used in Section 3.2. E.g. is the output ""foreground masks F"" (found in Appendix A) ?  This paper is a bit of a puzzle through the pages for the reader.

* Section 4: 
	* ""We compare MAOP with state-of-the-art action-conditioned dynamics learning baselines, ..."" Please re-iterate how these methods differ in assumptions, what they model, with respect to your novel method? For instance, is the main difference your ""novel region proposal method"" and such? Is the overall architecture different? E.g. explain here already the AC Model uses ""pixel-level inference"", and that OODP has ""lacks knowledge on object-to-object relations"" to underline their difference to your approach, and provide context for your conclusions in Section 4.1.

* Appendix A:
	* Algorithm 1, line 7: ""sample a pixel coordinate"" → is this non-deterministically sampling?
",6
"This paper presents a new adversarial training defense whereby an ensemble of models is trained against both benign and adversarial examples. The authors demonstrate on the CIFAR-10 dataset that the ensemble has improved robustness against a wide variety of white-box and transfer-based black-box attacks compared to other adversarial training techniques. The results appear significant but would greatly benefit from more thorough experiments.

Pros:
- Conceptually simple and intuitive.
- Thorough baselines and attack methods.

Cons:
- Limited novelty.
- Needs more experimental validation against other datasets (e.g. ImageNet) and models (e.g. Inception-v3).
- Table 1 shows that the clean accuracy of adversarially trained models is significantly worse, which suggests that some aspect of training was done improperly.

-----------------------------

I would like to clarify regarding the listed cons:

- Limited novelty: Adversarial training has been well-established as a viable defense against adversarial examples, as well as training a single model against an ensemble of adversarial examples crafted on different networks (Tramer et al. https://arxiv.org/pdf/1705.07204.pdf). While this work is sufficiently different from prior methods, its novelty is insignificant.
- More experimental validation: Due to the limited novelty, it is crucial that the authors validate their result more thoroughly to eliminate any doubt on applicability, especially against more challenging datasets such as ImageNet. While the experiments on CIFAR-10 are certainly sufficient, it is dangerous, particularly in works on defenses against adversarial examples, to restrict only to a relatively simple dataset.
- Tramer et al. (https://arxiv.org/pdf/1705.07204.pdf) publicly released their ensemble adversarially trained Inception-v3 model that has the same top-1 and top-5 clean accuracy on ImageNet as the base model. This serves as evidence that it is certainly possible to adversarially train a model without compromise to clean accuracy, especially on a simpler dataset such as CIFAR-10.",5
"The paper proposes to train an ensemble of models jointly, where the coupling lies in that at each time step, a set of examples that are adversarial for the ensemble itself is incorporated in the learning.

The experiments are thorough and compare multiple types of attacks, although they are all based on gradients (while the paper does mention recent attacks that do not rely on gradients so much). The results are rather convincing and show a clear difference between the proposed method and independently training the models of the ensemble (even if each one is training with examples adversarial to itself).

The paper is clear and well-written.

Pros:
- The superior performance of the proposed method
- The method is simple and thus could have a practical impact
- Clear and thorough analysis

Cons:
- Only gradient based attacks (which are somewhat criticized in the introduction) 
- Novelty may be a bit limited: this is a rather small variation on existing stuff (but it works rather well)

Remarks:
- Fig 2c could use the same line styles and order as Fig 2a/2b
- ""a gap 7 accuracy""?",6
"Summary. The paper considers the robustness of neural nets against adversarial attacks. More precisely, the authors experimentally investigate the robustness of ensembles of neural nets. They empirically show that adversarially trained ensembles of 2 neural nets are more robust than ensembles of 2 adversarially trained neural nets.

Pros.
* Robustness of neural nets is a challenging problem of interest for ICLR
* The paper is easy to read
* Experimental results compare different algorithms for 2 neural nets

Cons.
* The study is experimental
* It is limited to gradient-based attacks
* It is limited to ensembles of size 2
* The Ensemble2Adv is a single NN model and not an ensemble model. 

Evaluation.
The problem is significant and the use of ensemble methods for robustness against adversarial attacks is a promising line of research. The experimental study in this paper opens new lines of research in this direction. But, in my opinion, the paper is not ready for publication at ICLR. Detailed comments follow but the study is limited to k=2; the main finding is limited to the comparison between bagging two adversarially trained neural nets (SeparateEnsemble2Adv) and learning adversarially the average of two neural nets (Ensemble2adv). In my opinion, Ensemble2adv is a single model of double size and not an ensemble model thus somehow contradicting the main claim of the paper.

Detailed comments.
* Introduction, end of §2, it is said that non-gradient based attacks are still effective. But in the sequel you only consider gradient-based attacks and never discussed this question.
* Introduction, contributions, it should be made clear at the beginning of the paper that you will consider ensembles of size 2 and only gradient-based attacks.
* Section 2. The momentum-based attack should be cited and could be considered. ""Boosting adversarial attacks with momentum, Dong et al, CVPR18""
* Section 3, §2, the discussion on ensemble methods is unprecise. Ensemble methods have different objectives. For instance, Bagging-like methods  aim at reducing the generalization error while others as Boosting aim at augmenting the capacity of individual models.
* Section 3. Here is my main concern on this paper. The classical method would be bagging of neural nets with different initializations. The neural nets could be adversarially trained. This would lead to the so-called SeparateEnsemble2Adv. Here, the authors consider another method. Their method can be viewed as k(=2) copies of the same neural network with different initializations and an additional layer computing the average of the k(=2) outputs. Then adversarially learn the obtained model which leads to the so-called Ensemble2Adv algorithm. This algorithm is not an ensemble method as such. In my opinion for k=2, it is equivalent to doubling the size of a neural net, adding averaging of the outputs, and adversarially training the obtained neural net.
* Note recent advances in ensemble NNs with papers such as Averaging weights leads to wider optima ..., Izmailov et al, UAI18; Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Garipov et al, arXiv:1802.10026
* Section 4.1. Here comes the limitation k=2. The case k=4 is considered in table 1 but is not discussed elsewhere in the paper.
* Section 4.1. I am not convinced by DoubleAdv. It is one way of doubling the size of a neural net but I am not convinced that this is the more efficient. As said before, in my opinion, Ensemble2Adv is another way for doubling the size. And many more should exist.
* Section 5. In my opinion, the main comparisons should concern SeparateEnsemble2Adv and Ensemble2Adv. Also other methods doubling the size should be considered. 
* Section 5. For k greater than 2, SeparateEnsemblekAdv should be the better method because the adversarial learning phase could be easily parallelized.
* I am not convinced by the discussion in Section 6.
* Typos. and -> an l-13, p5; IFGSM5, l-19 p6; then l-6 p7; to due l-6 p9
* Biblio. Please give complete references
",4
"This paper proposed to add an additional label for detecting OOD samples and adversarial examples in CNN models. This research direction seems interesting, however, the idea of using an extra label for OODs is not new and was previously explored in different domains. I would expect the describe how their method is different, and keep the research from that point.
Additionally, there are several claims in this paper which I'm not convinced are true, such as the over-generalization of CNNs, the choice of OODs (recent studies have shown NNs are not well calibrated, so using softmax as the confidence might not be the best idea), etc.
Reg. the results, did the authors compare their method to existing adv. example detection methods, such as Ma, Xingjun, et al. ICLR (2018) ""Characterizing adversarial subspaces using local intrinsic dimensionality."" ? or some other method? 
Moreover, in Table 2. I'm not sure what should I conclude from the ""Naive Model Error"" on OOD samples.
",4
"The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored. In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use. Some of the claims in the paper can be further substantiated or explored. For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one. Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax. In general it is very unlikely that you will be able to choose every variation of out-distribution cases. Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution. 

However, the studies in this paper are still valuable and I strongly recommend continuing on the same direction. ",4
"The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.
In summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable.",3
"The current paper proposes using Graph Convolutional Networks (GCN) to explicitly represent and use relational data in dialog modeling, as well an attention mechanism for combining information from multiple sources (dialog history, knowledge base, current utterance). The work assumes that the knowledge base associated with the dialog task has en entity-to-entity-relationship format and can be naturally expressed as a graph. The dependency tree of dialog utterances can also be expressed as a graph, and the dialog history as a set of graphs. To utilize this structure, the proposed method uses GCNs whose lowest layer embeddings are initialized with the entity embeddings or via outputs of standard RNN-like models. The main claim is that the proposed model outperforms the current state-of-the-art on a goal-oriented dialog task.

The idea of explicitly modeling the relational structure via GCNs is interesting. However, the use of GCNs independently per sentence and per knowledge-base is a bit disappointing, since it does not couple these sources of information in a structured way. Instead, from my current understanding, the approach merely obtains better representations for each of these sources of information, in the same way it is done in the related language tasks. For instance, have you considered passing information across the trees in the history as well? Or aligning the parsed query elements with the KB elements?

The results are very good. That said, a source of concern is that the model is only evaluated as a whole, without showing which modification brought the improvements. The comparison between using/not using RNNs to initiate the first GCN layer is promising, but why not compare to using only RNN also? Why not compare the various encoders within an established framework (e.g. without the newly introduced attention mechanism)? Finally, the attention mechanism, stated as a contribution, is not motivated well.

Clarity:
The notation is described well, but it's not terribly intuitive (the query embedding is denoted by c, the history embedding by a, etc.), making section 4.4. hard to follow. A figure would have made things easier to follow, esp. due to the complexity of the model. A clearer parallel with previous methods would also improve the paper: is the proposed approach adding GCN on top of an established pipeline? Why not?

More discussion on code-mixed language, e.g. in section 4.6, would also improve clarity a bit (make the paper more self-contained). While the concept is clear from the context, it would be helpful to describe the level of structure in the mixed language. For instance, can dependency trees not be obtained code-mixed languages? Is there any research in this direction? (or is the concept very new?) Maybe I am just missing the background here, but it seems helpful in order to asses how appropriate the selected heuristic (based on the co-occurence matrix) is.

Relevant Reference:
Learning Graphical State Transitions, Johnson, ICLR 2017 also uses graph representations in question answering, though in a somewhat different setting.

Typos:
Section 4: ""a model with following components""
Section 5: ""the various hyperparameters that we conisdered""",5
"The paper proposes a Graph Convolutional Network-based encoder-decoder model with sequential attention for goal-oriented dialogue systems, with the purpose of exploiting the graph structures in KB and sentences in conversation. The model consists of three encoders for a query, dialogue history, and KB, respectively, and a decoder with a sequential attention mechanism. The proposed model attains state-of-the-art performance on the modified DSTC2 dataset of (Bordes et al., 2017). For the experiments with graphs constructed from word co-occurrence matrix, code-mixed versions of modified DSTC2 released by (Banerjee et al., 2018) are used.

Pros and Cons
(+) SOTA performance on the DSTC2 dataset.
(+) Without dependency parser when it is not possible
(-) Limited novelty
(-) Limited convincing the advantage of GCN itself

Detailed comments
The paper incorporates the graph structures in sentences and KB to make richer representations of conversation and achieves a state-of-the-art performance on the DSTC2 dataset. The paper is clearly written, and the results seem promising. However, as the paper combines existing mechanisms to design a model for dialog, the novelty seems to be relatively weak.
In particular, I felt that some experimental results are required to verify some of the arguments put forward by the authors. We listed two issues as below.

1. Effects of GCN
The authors show that RNN-GCN-SeA can make state-of-the-art performance, but not how much GCN makes effects on improving the performance on the dialog task. 
I think the authors need to compare the results of RNN-GCN-SeA with a model without GCN (i.e. RNN-SeA) in order to show that exploiting the structural information of dependency and contextual graphs do play an important role.
The random graph experiments (Table 3) show the effect of good structure in GCN, but I felt that it is not enough to demonstrate an improvement by GCNs. 

2. Comparative Experiments
I think that some experiments, which is reported in the previous papers (including Mem2Seq), would make the author’s experimental argument strong.
- Entity F1 score for the modified DSTC2 dataset
- Results on bAbI dialog dataset (task1~5 and its OOV variants) and In-Car Assistant dataset

Minor issues
1. Authors described that Mem2Seq is one of the state-of-the-art models in this field, including in the abstract. However, Mem2Seq does not outperform seq2seq model in all experiments. From what point of view is this model state-of-the-art? 
2. Recent studies have focused on copy mechanism in task-oriented dialog systems. Could you explain how the copy mechanism could be incorporated into the proposed model? I am also interested in the comparative results between seq2seq + attn + copy (per-resp-acc of 47.3) and its entity F1 measure (Eric and Manning, 2017; Madotto et al. 2018).

",6
"This is a well-written paper (especially the introduction) with fairly extensive experimentation section. It'a very possitive for me that you resort to more than one set of figures of merit.

My concerns are: 

You mention that GCNs have been used for question-anwering already. It would be infomative to furhter describe this work and clearly state how you handle things differenclty, since a Q&A system is quite close to a dialogue one.

There are some parts that could be made more clear. For example, when you mention that you collectively represent all trees as a single graph. How do you do that?

The model has a great number of parameters. It is not clear to me how you concluded to the specific parameter values. 

It would be nice to add the complexity of the model and also be more specific about how you choose the parameter values.

My proposals are:

I think that the paper would greatly benefit if you additionally to the equations you also presented the model in a graphical way as well. Additionally, although the paper is very well mathematically defined, is not so easy to follow from a practical perspective. For example, regarding section 5.3 I would prefer to see the 3 models you present in a graphical way as well.

Maybe add the links to the datasets you are using? On a related subject, would your models be transferable accross datasets?

Minor issues
PPMI abreviation is first used and then defined.
There are also some typos, like conisdered (that I suppose was meant to be considered, for example)",7
"This paper describes a signal separation method called neural egg separation (NES).
The separation problem is tackled in a semi-supervised setting where the observed mixture contains a target signal and a background noise, with access to the distributions of target and mixture signals.

The strength of the paper is that it describes the importance of the problem setup for practical use with some motivating examples. 
However, some unclear notations weaken the claim of the paper.

Specific comments follow.
* The loss in (1) is unclear.
Assuming latex grammar, \| \| is usually used to denote a vector norm, but (1) has two values inside. 
I would write \ell(T(y_i), b_i) to show a loss function, instead of the \| \| style.
More importantly, the loss should be explicitly defined. Does this mean the l2 error?

* The iterative separation process of (2) is even unclear.
Does T^m(b_j + x_i^m) share the parameter of that from previous iterations like T^{m-1}?
Or are the parameters fixed throughout the iterations?

* Use of \cdot.
There may be a confusion between the inner product and element-wise product with the \cdot operator.
Right after (5), there is an inequality z \cdot z \leq 1, which is meant to be the inner product.
On the other hand, the use of \cdot in (8) looks like the element-wise product to describe a masking operation.

Clarifying the objective and overall procedures is necessary for presenting the proposed method.

=================================
EDIT: I confirmed the revisions regarding the notation issues, but there still have confusing parts.
* Definitions of norm operator \| \| is unclear.
  * L_1 is mentioned below (1), and used other parts (3) or Algorithm 1. Equation (12) in Appendix uses |W1|_1^2, which looks like the l1 norm as well. Use consistent notations.
  * Equations (12, 13, 14) uses \|\|_2 or \|\|_1 to specify the type of norm, whereas (5), (6), (7) and other parts after (15) use \|\|. This confuses me. What do you mean by \|\| without subscript?
  * \|\| operator taking to symbols is a weird notation for me. Usually, norm is defined for a single vector (or a matrix). For example in (5), I would write \| b - G(z_b) \|, if you want to measure the difference between b and G(z_b).

The experimental result is impressive, as the other reviewers mention. I strongly recommend clarifying the notation to better deliver the method.",5
"This article presents an interesting if heuristic approach to source separation, NES, buttressed by the use of GLO masking for initialization, with promising results on data generated from synthetic source mixing.

The paper is well written and on the whole clear. My main concern with the work is the empirical nature of the NES iterative procedure. As far as I can tell there is no guarantee of convergence (nor discussion concerning this point). Since i am not familiar with the tasks, it is hard for me to judge the quality of the empirical results -- though the results do seem promising.

re: Bags & shoes task / table 1: ""...  Finetuning from GLOM, helped NES achieve stronger performance, nearly identical to the fully-supervised upper bound. It performed better than finetuning from AM (which achieved 22.5/0.85 and 22.7/0.86)"": I can't place the first number in the table, therefore i'm not quite sure what is being pointed out here.

re: Music task / table 3: ""... GLOM was much better than AM initialization (that achieved 0.9 and 2.9)"": I don't see either number in the table. I'd assumed that GLOM was used to fine-tune NES, so I was expecting to see the 2.9 under ""FT"". 

== 

I think the authors' response is reasonable. They have added clarifying material to the paper addressing my concerns. I have raised my rating from a 5 to a 6.",6
"This paper presents an iterative approach to separate unobserved distribution signal from a mixture with observed distribution. The proposed approach looks reasonable to me, however, the experiment and analysis are insufficient.
1. At test time, does the input also go through the same number of iterations (10)? I would like to see how the separated results evolve over iterations.
2. It is not clear what is the quality of samples generated by GLO. In the image separation task, GLOM performs better than GAN, but worse in other tasks. Analysis is needed here.
3.  I noticed that only in the music separation task, finetuning is significantly better than vanilla NES. Is it because generative models can synthesize more realistic data samples? For example, would the generator learn to synthesize X+B with temporal synchronization? More analysis is also needed here.

============================

I think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6.",6
"1. The paper introduces the idea of some existing hand-crafted features into the deep learning framework, which is a smart way for building light weighted convolutional neural networks.

2. I have noticed that binary patterns used in the paper are trainable, which means that these binary patterns can be seen as learned convolution filters with extremely space and computational complexity. Thus, the proposed method can also be recognized as a kind of binary network. 

3.  The baseline BCNN has a different architecture to the network using the proposed method. Thus, comparisons shown in Table 3 and Table 4 are somewhat unfair.

4. The capability of the proposed method was only verified on character recognition datasets. Does it can be easily used for other tasks such as face recognition or object detection on some relatively large datasets?",5
"In this work, a neural network that uses local binary patterns instead of kernel convolutions is introduced. Using binary patterns has two advantages: a) it reduces the network definition to a set of binary patterns (which requires much less storage than the floating point descriptions of the kernel weights used in CNNs) and b) allows for fast implementations relying only on logical operations (particularly fast on dedicated hardware).

This work is mostly descriptive of a proposed technique with no particular theoretical performance guarantees, so its value hinges mostly on its practical performance on real data. In that sense, its evaluation is relatively limited, since only figures for MNIST and SVHN are provided.

A list of additional datasets is provided in Table 5, but only the performance metric is listed, which is meaningless if it is not accompanied with figures for size, latency and speedup. The only takeway about the additional datasets is that the proposed LBPNet can match or outperform a weak CNN baseline, but we don't know if the latter achieves state-of-the-art performance (previous figures of the baseline CNN suggest it doesn't) and we don't know if there's significant gain in speed or size.

Regarding MNIST and SVHN, which are tested in some more detail, again, we are interested in the performance-speed (or size) tradeoff, and it is unclear that the current proposal is superior. The baseline CNN does not achieve state of the art performance (particularly in SVHN, for which the state-of-the-art is 1.7% and the baseline CNN achieves 6.8%). For SVHN, BCNN has a much better performance-speed tradeoff than the baseline, since it is both faster and higher performance. Then, the proposed method, LBPNet, has much higher speed, but lower performance than BCNN. It is unclear how LBPNet's and BCNN's speeds would compare if we were to match their performances. For this reason, it is unclear to me that LBPNet is superior to BCNN on SVHN.

Also the numbers in boldface are confusing, aren't they just incorrect for both the Latency and Error in MNIST? Same for the Latency in SVHN.

The description of the approach is reasonably clear and clarifying diagrams are provided. The backpropagation section seems a bit superficial and could be improved. For instance, backpropagation is computed wrt the binary sampling points, as if these were continuous, but they have been defined as discrete before. The appendix contains a bit more detail, where it seems that backpropagation is alternated with rounding. It's not justified why this is a valid gradient descent algorithm.

Also how the scaling k of the tanh is set is not explained clearly. Do you mean that with more sampling points k should be larger to keep the outputs of the approximate comparison operator close to 0 and 1?

Minor:

What exactly in this method makes it specific to character recognition? Since you are trying to capture both high-level and low-level frequencies, it seems you'd be capturing all the relevant information. SVHN data are color images with objects (digits) in it, what is the reason that makes other objects not be detectable with this approach?

English errors are pervasive throughout the paper. A non-exhaustive list:

Fig 4.b: X2 should be Y2
particuarly
""to a binary digits""
""In most case""
""0.5 possibility""
""please refer to Sec ..""
""FORWARD PROPATATION""",6
"This paper proposed a LBPNet for character recognition, which introduces the LBP feature extraction into deep learning. Personally I think that this idea is interesting for improving the efficiency of CNNs, as traditionally LBP has been demonstrated its good performance and efficiency in some vision tasks such as face recognition or pedestrian detection. However, I do have the following concerns about the paper:

1. Calculation/Implementation of Eq. 4: I do not quite understand how it derived, and how to use Eq. 3 in calculation. I suggest the authors to explain more details, as this is the key for implementation of LBP layers.

2. Effects of several factors on performance in the experiments are missing: (1) random projection map in Fig. 5, (2) $k$ in Eq. 2, and (3) the order of images for computing RHS of Eq. 3. In order to better demonstrate LBPNet, I suggest to add such experiments, plus training/testing behavior comparison of different networks. 

3. Does this network work with more much deeper?

4. Data: The datasets used in the experiments are all well-aligned. This makes me feel that the RHS of Eq. 3 does make sense, because it will capture the spatial difference among data, like temporal difference in videos. How will the network behave on the dataset that is not aligned well, like affnist dataset?

5. How will this network behave for the applications such as face recognition or pedestrian detection where traditionally LBP is applied?
",5
"This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. This work was very difficult to follow, so it is somewhat unclear what were the main contributions (since much of this seems to be covered by other works as referenced within the paper and as related to similar unreferenced works below). Moreover, regarding the experiments, many things were unclear (some of the issues are outlined below). While the overall idea of using logic in this way to help with skill composition is interesting and exciting, I believe several things must be addressed with this work. This includes: situating this work more clearly against existing similar works which use logic in this way, clearly defining the novel contributions of this work as compared to those and others, overall making the methodology more clear and specific (including experimental methodology), and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., HRL multi-task learning, meta-learning) to emphasize the need/importance of this work — I am aware that at least 1 HRL work is mentioned, but this work is not really contrasted against it to help situate it.

Questions/Concerns about Experiments:

+ Does Figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step, or 5 episodes, each from a separate run averaged together? It is a bit unclear especially because the main text and the figure caption slightly differ. Also, average discounted return is somewhat different than average return,  suggest updating the label to be clear also with the discount factor used.
+ What were the standard deviations for this across experiments? Even with averaging it seems that these runs are very high variance, would be good to understand what variance bounds to expect if using this method.
+ Why were average discounted returns reported in Figure 5 and not in Table 1?
+  What were the standard deviations on success rate and training time? Also what about sample complexity? 
+ To my understanding the benefit here is reusability of learned skills via the automata methods described here. It would have made sense to compare against other HRL or multi-task learning methods in addition to just SQL or learning from scratch. For example how would MAML compare to this?
+ It is also unclear whether the presented results in Table 1 and Figure 5 are on the real robot or in simulation. The main text says, “All of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning.” So does this mean that Figure 5 is simulated results and Table 1 is on the real robot?



Citations that should likely be made:

+ Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. ""Reinforcement Learning for LTLf/LDLf Goals."" arXiv preprint arXiv:1807.06333 (2018). 
+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Decision-making with non-markovian rewards: From LTL to automata-based reward shaping.""  In Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017. 
+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. ""Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping."" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017. 


Typos/Suggested grammar edits:

“Skills learned through (deep) reinforcement learning often generalizes poorly across tasks and re-training is necessary when presented with a new task.” —> Often generalize poorly

“We present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for convenient specification of complex temporal dependent tasks with logical expressions and construction of new skills from existing ones with no additional exploration.” —> Sentence kind of difficult to parse and is a run-on

“Policies learned using reinforcement learning aim to maximize the given reward function and is often difficult to transfer to other problem domains.” —> ..and are often..

“by authors of (Todorov, 2009) and (Da Silva et al., 2009)” —> by Todorov (2009) and Da Silva et al. (2009) Also several other places where you can use \citet instead of \cite",5
"The contribution of the paper is to set up an automaton from scTLTL formulas, then corresponding MDP that satisfies the formulas is obtained by augmenting the state space with the automaton state and zeroing out transitions that do not satisfy the formula. This approach seems really useful for establishing safety properties or ensuring that constraints are satisfied, and it is a really nice algorithmic framework. The RL algorithm for solving the problem is entropy-regularized MDPs. The approach “stitches” policies using AND and OR operators, obtaining the overall optimal policy over the aggregate. Proofs just follow definitions, so they are straightforward, but I think this is a quality. The approach is quite appealing because it provides composition automatically. The paper is very well written.  The main problem I see with the work is that composition can explode the number of states in the new automaton and hence the new MDP. It would be interesting in future work to do “soft” ruling out of transitions rather than the ""hard"" approach used in the paper. The manipulation task provided is quite appealing, as the robot arm is of high dimensionality but the FSAs obtainedare discrete. Overall, the paper provides a very good contribution.

Small comments:
Equation equation in Def 3 also proof of Theorem 2
In section,  -> In this section
are it has -> and it has",7
"This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks. This method provides a structured solution for reusing learned skills (with scTLTL formulas), and can also help when new skills need to be involved in original tasks. The topic of the composition of skills is interesting. However, the joining of LTL and RL has been developed previously. The main contribution of this work is limited to the application of the previous techniques.

The proposed approach also has some limitations. 
Will this method work on composing scTLTL formula with temporal operators other than disjunction and conjunction?
Can this approach deal with continuous state space and actions? This paper describes a discretization way, which, however, can introduce inaccuracies. 
The design of the skills is by hand, which restricts badly its usability.
The experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? 
  ",6
"This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies. This idea is motivated by the desirability of compositional policies. I find the idea compelling, but I am not sure the proposed method is a useful solution. Overall, the description of the method is difficult to follow. With more explanations (perhaps an algorithm box?), I would consider increasing my score.

The experiments demonstrate that this method can outperform SQL at skill composition. However, it is unclear how much prior knowledge is used to define the automaton. If prior knowledge is used to construct the FSA, then a missing comparison would be to first find the optimal path through the FSA and then optimize a controller to accomplish it. As the paper is not very clear, that might be the method in the paper. 

Questions:
- How do you obtain the number of automaton states? 
- In Figure 1, are the state transitions learned or handcoded? Are they part of the policy's action space?
- In section 3.2, you state  s_{t:t+k} |= f(s)<c ⇔ f(s_t)<c    What does s without a timestep subscript refer to? Why does this statement hold?

Can you specify more clearly what you assume known in the experiments? What is learned in the automata? In Figure 5, does SQL have access to the same information as Automata Guided Composition?",5
"TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.

* Summary

The manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\lambda\in (0,1)$ (sampled from a $\mathrm{Beta}(\alpha,\alpha)$ distribution).

A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.

A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.

I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.

* Major remarks

- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.
- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.
- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.

* Minor issues

- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.
- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.",8
"The tone of the paper is notably scientific, as the authors clearly state the assumptions and all observations, whether positive or negative. That said, the approach itself can be seen as a direct extension of the earlier advanced 'mixup' scheme. In addition to performing data augmentation solely in the input space, their method proposes to train the networks on the convex combinations of the hidden state representations by learning to map them to the convex combinations of their one-hot ground truth encodings. 

The results are competitive, in most cases exceeding the current state-of-art. However, the scheme has only been tested on low-res datasets such as MNIST, CIFAR and SVHN while the predecessor (plain 'mixup') also demonstrated improvement over the much larger and high-res ImageNet dataset.

Although their work is not extremely novel, the experiments and observations could serve as a useful extension to this line of research. 

Suggestions:
1.  The results on ImageNet would be a useful add-on to really drive home the benefit of their method when we talk of real-world large-scale datasets. 
2. The associated functions represented by 'f',  'g' and 'h' change meaning between sec. 2 and sec. 3. It would be more smooth if some consistency in notations was maintained.",6
"The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. 

The paper is well written and easy to follow. Various experiments are conducted to support the contributions of the paper.  Nevertheless, the technical novelty seems a bit weak to me. The method basically moves the interpolating process from input space as in MixUp to randomly selected hidden states. More importantly, some of the paper’s claims are not very convincing to me in its current form.

Major remarks:

1.	The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. 
The authors theoretically prove that with the proposed training cost in Manifold Mixup, the representation for each class will lie on a subspace of dimension dim (h) –d +1 (h and d are the hidden dimension and number of classes, respectively). I did not get the idea of how such dimension reduction relates to the ‘’flattening’’ of the manifold and in particular how such representations (representations for each class “concentrating into local regions”) can avoid the class collision issues as that in Mixup.
Experimentally, from Figures 3 and 4, it seems the class collision issue could be worse than that of Mixup. For example, for mixing ratio of 0.6 (meaning the created image has almost half labels from the two original images), MixUp clearly shows, for instance in the second row, that there are two overlapped images (Horse and Plane), but Manifold Mixup seems to have only the Plane in the mixed image with a soft label. 

2.	The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning. In this way, the paper would be much more convincing because the proposed method is so close to MixUp and the observation here is contradictive.
3.	I wonder how sensitive is the parameter Alpha in Manifold Mixup. For example, how the mixing rate Alpha impacts the results for NLL and Semi-supervised learning in section 5.2? 
4.	It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper.

Minor remarks:

1.	In Table2, the result from AdaMix seems missed.
2.	Why not using Cifar100, but with a new dataset SVHN for the semi-supervised learning in section 5.2?
3.	In related work, regarding regularizing deep networks by perturbing the hidden states, the proposed method may relate to AgrLearn (Guo et al., Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks) as well.
",4
"



Review

This paper discusses invariances in ReLU networks. The discussion is anchored around the observation that while the spectral norm of neural networks layers (their product bounds the Lipschitz constant) has been investigated as a measure of robustness of nets, the smallest singular values are also of interest as these indicate directions of invariance. 

The paper consists mostly of a theoretical analysis with little empirical support, focusing on a property of matrices called omnidirectIonality. The definition given seems weird — an A \in R^{m \times n} is omnidirectional if there exists a unique x \in R^n such that Ax \leq 0. 

If there is a *unique* x then that x must be 0. Else if there were a nonzero x for which Ax \leq 0, then A(cx) also \leq 0 for any positive scalar 0 and thus x is not unique. Moreover if x must be equal to 0 Ax \leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright? Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0? Also perhaps better to use the curly sign for vector inequality. 

Overall the paper, while interesting is unacceptably messy. 
The first two pages have no paragraph breaks!!! This means either that the author are separating paragraphs with \\ \noindent or that they have modified the style file to remove paragraph breaks to save space. Either choice is unreadable and unacceptable. The paper is also littered with typos and vague statements (many enumerated below under *small issues*). In this case, they add up to make a big issue. 


The notation at the top of page 4 — see (1) and (2) — comes out of nowhere and requires explanation. |_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?

Ultimately this paper focuses on the question on whether the pre-image of a ReLU layer can be concluded (based on the post-image) to be a singleton,  a compact polytope, or if it has infinite volume. The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?). 

Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score. 


Small issues

The following is a *very* incomplete list of small bugs found in the paper:

“From a high-level perspective both of these approaches” --> missing comma after “perspective”

""as well as the gradient correspond to the highest
possible responds for a given perturbation"" --> incomprehensible ""corresponding?"" ""possible responds?"" do you mean ""response"", and if so what is the precise technical meaning here?

""analyzing the lowest possible response"" what does ""response' mean here?

""We provide upper bounds on the smallest singular value"" -- the singular value of what? This hasn't been stated yet.

""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.

""we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights"" -- what does ""mechanisms"" mean here?

Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets. 

""realated""
",6
"The paper has two distinct parts. In the first part (section 2) it studies the volume of preimage of a ReLU network’s activation at a certain layer as being singular, finite, or infinite. This part is an extension of the work in the study of (Carlsson et al. 2017). The second part (section 3) builds on the piecewise linearity of a ReLU network’s forward function. As a result, each point in the input space is in a polytope where the model acts linearly. In that respect, it studies the stability of the linearized model at a point in the input space. The study involves looking at the singular values of the linear mapping. 

The findings of the paper are non-trivial and the implications potentially interesting. However, I have some concerns about the study.

There is a key concern about the feasibility of the numerical analysis for the first part. That is, a layer-by-layer study can have a computational problem where the preimage is finite at each layer but can become infinite by the mapping of the preceding layers. In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.

As for the second part, the authors mention the increase in the dimensionality of the latent space in the current deep networks. However, this observation views convolutional networks as MLPs. However, there is more structure in a convolutional layer’s mapping function. The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.

All in all, while there are some concerns and the contributions are not entirely novel, the reviewer believes the findings of the paper is generally non-trivial and shed more light on the inner workings of the ReLU networks and is thus a valuable contribution to the field.",6
"This paper presents an analysis of the inverse invariance of ReLU networks. It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments. They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope. They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.

The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together. The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting. The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network. However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.

I have several questions for the authors:
- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability. Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?

In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space. The experiments are not very convincing or illustrative of the theoretical results in my opinion. It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.",7
"Summary: the paper is interested in parsing compound questions for querying on knowledge graph, e.g. MetaQA by Zhang et al. (2017). The paper proposes to have two modules, one that segments the question into partitions (up to three) and the other that looks at each segment to get the relation. The relations are merged to obtain a single KG path, which is queried to obtain the answer. Since the segmentation is a non-differentiable process, the paper uses reinforcement learning to propagate gradient to the segmentation model. The segmentation is a process of classifying each word for which partition it should be tied to. Answering is a process of classifying the partition into one of the possible relation edges. The model shows expected results in a synthetic arithmetic dataset, and obtains the state of the art in MetaQA, improving nearly 5% over the baseline. The model especially does much better on 3-hop questions, with nearly 20% improvement.

Strengths: the paper is well-written. The model is simple yet effective and is a novel contribution to compound question answering on KG. Especially, the improvement on 3-hop category is nearly 20%, which is substantial and quite impressive. 

Weaknesses: My biggest concern is the lack of discussions on its relevance to  (Iyyer et al., 2016), which also proposed to decompose question into simpler ones for WIkiTableQuestions. Also, I think it would be good to mention Semantic Role Labeling as related literature, which is about tagging each word with its role in the sentence. The partition index can be somewhat considered as a “role” in the sentence.

Questions:
1. How do you obtain x^(k)? Is it the last state of the LSTM?
2. Why did you have to augment “NO_OP” relation in the MetaQA dataset?
3. Why +1 reward has lower variance than probabilistic reward? Explanation or citation would be needed.
4. What if two partitions need to share a word? The current setup necessitates that a word participates in only one partition. Wouldn’t this be problematic?
5. I am a bit confused about how the simple question answering module is trained. Is it directly trained by the gold relation label?

Typos and Suggestions:
- Second paragraph of 2.1: in stead -> instead
- Third paragraph of 2.1: research. -> research
- c_t + h_t: would be good to explicitly mention that the circled plus sign is concatenation.
- Last paragraph on page 4: “leave to be”?
- Second last paragraph of 4.1: he -> The
- Second paragraph of 4.2: “if exists a proper meaning”?
- First paragraph of page 7: be either assume -> either assume
- Last paragraph of Section 5: generalizing -> generalize
- I think you should not put acknowledgment in a double-blind submission.

M Iyyer, W Yih, MW Chang. Answering complicated question intents expressed in decomposed question sequences. 2016 (https://arxiv.org/abs/1611.01242)
",6
"This paper proposes a knowledge-based QA system that learns to decompose compound questions into simple ones. The decomposition is modeled by assigning each token in the input question to one of the partitions and receiving reward signal based on the final gold answer. The model achieves the state-of-the-art performance on the MetaQA dataset. 

My main complaint about the paper is its lack of technical details and analysis of empirical results. Parts of the paper seem quite unclear, for example:

In the last paragraph of Section 3.1, it says “We do not assume that nay question should be divided into exactly three parts. … See section 4 for case study.” Does this mean that the model can have <=3 partitions, but not more? How is this number decided?

Section 3.2 describes the simple-question answer. From Eq (4), it seems that the answerer only uses the current partition, is that the case? Moreover, how is the gold relation r obtained?

It would be nice to add more explanation to the caption of Figure 4 to make it self-contained.

The case study section (4.3) only contains a single example. It would be very helpful to include more examples of question partitions (there is enough space). Error analysis would also be helpful to understand, for example, why the proposed model is worse than VRN (Zhang et al. 2017) on 1- and 2-hop questions.
",5
"This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.

The proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.

I like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between ""compound questions"" instead of ""multi-hop questions"". In my opinion, ""multi-hop questions"" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.

My only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:

(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).

(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include ""Search-based Neural Structured Learning for Sequential Question Answering"" and ""ComplexWebQuestions"". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.",5
"This paper provides an in-depth study of the quantization error in low-precision training and gives consequent bounds on the low-precision SGD (LP-SGD) algorithm for convex problems under various generic quantization schemes. 

[pros]
This paper provides a lot of novel insights in low-precision training, for example, a convergence bound in terms of the L1 gradient Lipschitzness can potentially be better than its L2 counterpart (which is experimentally verified on specially designed problems). 

I also liked the discussions about non-linear quantization, how they can give a convergence bound, and even how one could optimally choose the quantization parameters, or the number of {exponent, significance} bits in floating-point style quantization, in order to minimize the convergence bound.

The restriction to convex problems is fine for me, because otherwise essentially there is not a lot interesting things to say (for quantized problems it does not make sense to talk about “stationary points” as points are isolated.)

This paper is very well-written and I enjoyed reading it. The authors are very precise and unpretentious about their contributions and have insightful discussions throughout the entire paper.

[cons]
My main concern is that of the significance: while it is certainly of interest to minimize the quantization error with a given number of bits as the budget (and that’s very important for the deployment side), it is unclear if such a *loss-unaware* theory really helps explain the success of low-precision training in practice.

An alternative belief is that the success comes in a *loss-aware* fashion, that is, efficient feature extraction and supervised learning in general can be achieved by low-precision models, but the good quantization scheme comes in a way that depends on the particular problem which varies case by case. Admittedly, this is a more vague statement which may be harder to analyze or empirically study, but it sounds to me more reasonable for explaining successful low-precision training than the fact that we have certain tight bounds for quantized convex optimization. 

[a technical question]
In the discussions following Theorem 2, the authors claim that the quantization parameters can be optimized to push the dependence on \sigma_1 into a log term -- this sounds a bit magical to me, because there is the assumption that \zeta < 1/\kappa, which restricts setting \zeta to be too large (and thus restricts the “acceleration” of strides from being too large) . I imagine the optimal bound only holds when the optimal choice of \zeta is indeed blow 1/\kappa?",6
"This paper discusses conditions under which  the convergence of training models with low-precision weights do not rely on model dimension. Extensions to two kinds of non-linear quantization methods are also provided. The dimension-free bound of the this paper is achieved through a tighter bound on the variance of the quantized gradients.  Experiments are performed on synthetic sparse data and small-scale image classification dataset MNIST.

The paper is generally well-written and structure clearly. However, the bound for linear quantization is not fundamentally superior than previous bounds as the ""dimension-free"" bound in this paper is achieved by replacing the bound in other papers using l2 norm with l1 norm. Note that l1 norm is related to the l2 norm as: \|v\|_1 <= \sqrt{d}\|v\|_2, the bound can still be dependent on  dimension, thus the title may be misleading. Moreover, the assumptions  1 and 2 are much stronger than previous works, making the universality of the theory limited. The analysis on non-linear quantization is interesting, which can really theoretically improve the bound. It would be nice to see some more empirical results on substantial networks and  larger datasets which can better illustrate the efficacy of the proposed non-linear quantization.

Some minor issues:
1. What is HALP in the second contribution before Section 2?
2. What is LP-SVRG in Theorem 1?
3. What is \tilde{w} in Theorem 2?",6
"The paper considers the problem of low precision stochastic gradient descent. Specifically, they study updates of the form x_{t + 1} = Q (x_t - alpha * g_t), where g_t is a stochastic gradient, and Q is a quantization function. The goal is to produce quantization functions that simultaneously increase the convergence rate as little as possible, while also requiring few bits to represent. This is motivated by the desire to perform SGD on low precision machines.

The paper shows that under a set of somewhat nonstandard assumptions, previously studied quantization functions as well as other low precision training algorithms are able to match the performance of non-quantized SGD, specifically, losing no additional dimension factors. Previous papers, to the best of my knowledge, did not prove such bounds, except under strong sparsity conditions on the gradients. I did not check their proofs line-by-line however they seem correct at a high level.

I think the main discussion about the paper should be about the assumptions made in the analysis.  As the authors point out, besides the standard smoothness and variance conditions on the functions, some additional assumptions about the function must be made for such dimension independent bounds to hold. Therefore I believe the main contribution of this paper is to identify a set of conditions under which these sorts of bounds can be proven. 

Specifically, I wish to highlight Assumption 2, namely, that the ell_1 smoothness of the gradients can be controlled by the ell_2 difference between the points, and Assumption 4, which states that each individual function (not just the overall average), has gradients with bounded ell_2 and ell_1 norm at the optimal point. I believe that Assumption 2 is a natural condition to consider, although it does already pose some limitations on the applicability of the analysis. I am less sold on Assumption 4; it is unclear how natural this bound is, or how necessary it is to the analysis. 

The main pros of these assumptions are that they are quite natural conditions from a theoretical perspective (at least, Assumption 2 is). For instance, as the authors point out, this gives very good results for sparse updates. Given these assumptions, I don’t think it’s surprising that such bounds can be proven, although it appears somewhat nontrivial.  The main downside is that these assumptions are somewhat limiting, and don’t seem to be able to explain why quantization works well for neural network training. If I understand Figure 4b correctly, the bound is quite loose for even logistic regression on MNIST. However, despite this, I think formalizing these assumptions is a solid contribution.

The paper is generally well-written (at least the first 8 pages) but the supplementary material has various minor issues.

Smaller comments / questions:

- While I understand it is somewhat standard in optimization, I find the term “dimension-independent“ here somewhat misleading, as in many cases in practice (for instance, vanilla SGD on deep nets), the parameters L and kappa (not to mention L_1 and kappa_1) will grow with the dimension.

- Do these assumptions hold with good constants for training neural networks? I would be somewhat surprised if they did.

- Can one get dimension independent bounds for quantized gradients under these assumptions?

- The proofs after page 22 are all italicized.

- The brackets around expectations are too small in comparison to the rest of the expressions.",6
"The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments.  First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. 

In section 3, the authors then introduce the ""max-marginal regularization"" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word ""marginal"" instead of ""margin"" seems quite wrong to say the least.  Secondly, the stated definition seems to be wrong. In the definition the range of values for \gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \gamma not equals 0) below and show that both have issues:
(a) \gamma > 0: This seems to imply that when the log-likelihood of ground-truth is already \gamma better than the log-likelihood of the random negative, the loss comes to life. Strange!
(b) \gamma < 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. 
I'm assuming the authors swapped y with y^{-} in the ""regularization"" part.
Anyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well.  

Coming to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two ""Where is your location?"" outputs.  The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. 

A lot of typos/wrong phrasing/wrong claims and here are some of them:
(a) Page 1, ""lead to the misrecognition of those common replies as grammatically corrected patterns""? - No idea what the authors meant.
(b) Page 1, ""unconsciously preferred"" - I would avoid attaching consciousness before AGI strikes us.
(c) Page 1, ""Above characters"" -> ""Above characteristics""
(d) Page 1, ""most historical"" -> ""most previous""
(e) Page 2, ""rest replies"" -> ""rest of the replies""
(f) Page 3, ""variational upper bound"" -> Not sure what's variational about the bound
(g) ""Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance""? - No idea what the authors meant.",3
"The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. The authors show that the approach yields better results in the dataset considered using various measures and human evaluation.

Improvement Points
- the explanation for low ROGUE measure due to the method favoring non-repetitive words sounds like it can be supported using numerical statistics, than hand-waiving argument
- for the timing, how much time was taken to tune the additional parameters (how the # of negative responses sampled for each positive response was chosen as four via uniform sampling)
- some description about
a) how many users are there, what type of conversation/active users/topics etc.
b) what time frame was used during data collection (this may have implications for lemma asserting zipf)
- it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies)",7
"This paper presents a framework for understanding why seq2seq neural response generators prefer ""universal""/generic replies. To do so the paper breaks down the response generation probability into the probability of selecting the set of tokens (reflecting the topic of the output) and then selecting an ordering of the tokens (reflecting the syntax of the output.)

The results presented in this paper are not technically sound. E.g the derivation in Eq(2) derive a meaningless bound. Here is why:
1. The first equality assumes that the words in a set are independent which is not true.
2. In the second equality, the authors incorrectly replace the summation of word probability in each sentence with the summation of word probabilities over all unique words (the set) overall sentences. This is simply not true if there are common words shared between sentences.
3. Perhaps the biggest issue is the incorrect application of Jensen's lemma. JL is often used as 
log(\sum_i a_i x_i) > \sum_i a_i log x_i if \sum_i a_i = 1. Instead what authors have used is 
log (\sum_i x_i) > \sum_i log(x_i), which is not always true, and is trivially true for all x_i < 1. In fact, this bound is not even tight (unlike Jensen's lemma) and the *worst* part is that the LHS increases if we add more x_i (<1) and the RHS decreases. This means this bound is far from being meaningful and as such should be summarily ignored.

Similarly, in section 2.3, the technical content is quite poor. Why is this true -- ""the amount of possible queries M of y... 1 << M \propto N""? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in ""non-universal"" replies? I am not going more into the details in this section because I think the problems with section 2.2 are themselves dealbreakers.

Overall, given the problems this work is not technically sound to be accepted.",1
"Clarity: Paper is generally well written; however, certain theoretical statements (e.g. Theorem 1) are not very precise.

Originality: Contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.

Significance: Unclear whether the techniques significantly advance the state of the art.

Quality: Overall, I think this is a promising direction but the idea might not have fully fleshed out.

----
Summary: 
the paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, CoSaMP and IHT, but presumably other similar methods can also be used here). The key idea is that if the measurement matrix is normalized, then the k-sparse thresholding of the gradient update can be viewed as solving a k-nearest neighbor problem. Therefore, one can presumably use fast k-NN methods instead of exact NN methods. Specifically the authors propose to use the prioritized DCI method of Li and Malik.

Pros: 
reasonable idea to use fast (sublinear) NN techniques in the k-sparse projection step.

Cons: 
* It appears that the running time improvement over the baseline IHT (which has Otilde(mn) complexity) heavily depends on the intrinsic dimensionality of A. However, the authors do not characterize this.
* The authors neglect to mention in the paper that prioritized DCI has a pre-processing time of O(mn), so the final algorithm isn't really asymptotically faster.
* I cannot parse Theorem 1 (especially, the second sentence). Is epsilon the failure probability of DCI?
* Experimental results are far too synthetic. In real-life problems k itself is big, so there may be other bottlenecks (least squares, gradient updates, etc) and not necessarily the hard thresholding step.
",4
"The paper is very well-written, readable, with the ideas and derivations clearly explained. 

The literature review is comprehensive and informative. I do feel however that the review could be improved, for example, by discussing the recent papers by Chinmay Hegde and Piotr Indyk on ""head"" and ""tail"" approximate projections to speed up recovery algorithms. The problem under study is indeed important and the contribution is interesting. 

My biggest concern is that the technical contribution is too modest. Theorem 1 serves more as a decorative technical result (the assumption ""And for any vector v..."" seems out of the blue and too convenient) and the paper does not answer the many questions that come to mind here. For example, what is the intrinsic dimension of common random measurement matrices? Or how do any wrongly detected nearest neighbours propagate through the iterations of the algorithm? How does the measurement noise change the intrinsic dimension? We should intuitively lose stability in return for faster recovery. How would this be quantified in what you've proposed.

",5
"The paper proposes a greedy-like algorithm for sparse recovery that uses nearest neighbors algorithms to efficiently identify candidates for the support estimates obtained at each iteration of a greedy algorithm. It assumes that the norms of the columns of the matrix A are one to be able to change the project-and-sort step into a nearest neighbors search.

It is not clear what the value of Fact 1 is, given that none of the sparse recovery algorithms discussed here actually performs ell0 norm minimization. Additionally, it is common in theoretical analysis of sparse recovery to assume that the columns of the matrix A have unit norm. In fact, the RIP implies that the columns of the matrix must have norm within delta of 1. Nonetheless, it would be useful to have a discussion of the effect that having non-unit column norms would have on the proposed approach.

Similarly, Fact 2 is almost self-evident; I suggest to discard the proof.

The equivalence of Definition 1 and the statement involving ps and qs needs to be shown more clearly. The statement in Definition 1 is given in terms of distances (ball radiuses), not counts of neighbors.

I suggest swapping the use of CoSaMP and AIHT - the theoretical results of the paper refer to AIHT, so it is not clear why the algorithm itself is relegated to the supplementary material.

It is not clear how d0 is to be computed to implement Accelerated AIHT.

For Theorem 1, the authors should comment on when the assumption ""xtilde(t) converges linearly to a k-sparse signal with rate c"".

In Figures 1 and 2, does ""residual"" refer to the difference between x and xtilde, or b and Axtilde? 

Minor comments:
Typo in page 5 ""¿""
Grammar error in page 6 ""characterizing of the difficulty"".",5
"This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.

1. Stochastic Line Search

Determining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors’ attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.

First, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so I’m on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. 

How are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?

The theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?

The algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?

In deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\min(1, \xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?

The theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\hat{p}_k^T \hat{g}_k] = E[\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.

All the proofs also depend on a linear Taylor approximation that is not well-explained, and I’m wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\hat{f}_{z’} (x + \alpha \hat{p}_z) = \hat{f}_{z’} + \alpha \hat{p}_z’ \hat{g}_z(x + \bar{\alpha} \hat{p}_z)$, where $\bar{\alpha} \in [0, \alpha]$. How does this affect the proof?

Lastly, I would propose for the authors to change the name of their condition to the “Armijo condition” rather than using the term “1st Wolfe condition” since the Wolfe condition is typically associated with the curvature condition (p_k’ g_new >= c_2 p_k’ g_k), hence referring to a very different line search. 

2. Design of the Quasi-Newton Matrix

The authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because “it is not obvious that enforced symmetry necessarily produces a better search direction” and “treating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor”. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the “true” gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.

Additionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.

The matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.

Lastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?

All of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.

3. Numerical Experiments

As written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)

In addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and it’s not clear if either of these are done in the paper.

Typos/Grammatical Errors:
- Pg. 1: Commas are needed in some sentences, i.e. “Firstly, for large scale problems, it is…”; “…compute the cost function and its gradients, the result is…”
- Pg. 2: “Interestingly, most SG algorithms…”
- Pg 3: Remove “at least a” in second line
- Pg. 3: suboptimal, not sup-optimal
- Pg. 3: “Such a solution”, not “Such at solution”
- Pg. 3: Capitalize Lemma
- Pg. 4: fulfillment, not fulfilment
- Pg. 7: Capitalize Lemma
- Pg. 11: Before (42), Cov \hat{g} = \sigma_g^2 I
- Pg. 11: Capitalize Lemma

Summary:

In summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.

References:
[1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. ""A multi-batch l-bfgs method for machine learning."" Advances in Neural Information Processing Systems. 2016.
[2] Byrd, Richard H., et al. ""A stochastic quasi-Newton method for large-scale optimization."" SIAM Journal on Optimization 26.2 (2016): 1008-1031.
[3] Schraudolph, Nicol N., Jin Yu, and Simon Günter. ""A stochastic quasi-Newton method for online convex optimization."" Artificial Intelligence and Statistics. 2007.",4
"The authors present an interesting variation of the standard QN methods. Their main point of departure from LBFGS/SR1 is in constructing a simpler Hessian inverse approximation. Recall that SR1 and LBFGS updates all satisfy the secant equation for each of the `m` previous gradient differences stored in memory. The authors choose to get ""close"" to satisfying the equations by solving an l_2 penalization of the secant equations. 

The resulting algorithm is interesting, but it is not clear from the paper what the claimed advantage of doing this is. The LBFGS and SR1 unrolled update rules for H (Hessian inverse approximation) is O(m^2 d) (Sec 7.2 NW 2006),  and this seems to be the same for the authors' method, where the main matrix R_k that forms H has the same order. (BTW, did you mean 'd' in place of 'n'  the computational order discussion preceding Sec  4.2?)

The experiments show that this method's performance is impressive compared to an LBFGS implementation provided by Bollapragada 2018 , but as I recall that paper presented a variable/increasing batch method, while the authors' method uses fixed batches (as far as I can tell) so it is not clear that comparison on time alone is sufficient. The advantage over LBFGS and SGD seen in MNIST seems to go away by the CIFAR example, so it is unclear what might happen in larger problems like ImageNet. 

I am also not able to see the difference between the 'stochastic' line search presented here and the standard backtracking method as applied to mini-batch evaluated estimates. What is different, and new that brings in consideration for the noise? I recall that bollapragada 2018 had an additional variance based rule to check. Some more conservative values are chosen for the step length, but I do not see the justification presented in the appendix, esp Eq 47 : p is not independent from g here, being calucated as p=Hg,  so E[p^t g] is not equal to the product of  the individual expectations.

Some key points were left out in the discussion of the experiments. This is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like mini-batches sizes , value of \lambda in the H derivation, how one calculates the \sigma^2_g within the algorithm presented in the Appendix. The last must include
an extra computational cost, or are you using Adam style online variance estimator?

The MSE error alone seems insufficient in the results. Please publish the test mis-classification results too. Also, why is the MSE loss used with the softmax in CIFAR? Shouldn't cross-entropy, better justified theoretically, be better justified?
",5
"This paper presents a new quasi-Newton type method for stochastic optimization problems. The primary contributions of the paper include a new stochastic linesearch method as well as a novel way to incorporate second order information which is different from existing approaches such as BFGS or L-BFGS. 

In terms of the clarity, I think this is a very well-written paper with nice organization. The paper does have some typos, though. 

In terms of significance, how to incorporate second-order information in stochastic optimization has long been an important research topic. Most existing stochastic quasi-Newton methods use L-BFGS method to incorporate second order information and choose a fixed, small stepsize, with the only differences being how to compute the curvature pair (s_k, y_k). Therefore, this paper is addressing a very important question and has made respectable attempt to use mechanisms other than L-BFGS method and to incorporate a linesearch scheme. 

Specifically, the paper relaxes the secant equation, which is natural for the stochastic settings because the difference in gradients y_k is computed from stochastic gradients, and the true Hessian only satisfies the secant equation in expectation. I believe replacing the secant equation is an important and promising direction.

However, there are concerns about the new approach proposed in this paper:

1.	The resulting Hessian inverse approximation in (14) is no longer symmetric, or guaranteed to be positive definite. While the underling true Hessian might not be positive definite because of the nonconvexity, it is always symmetric. Is it possible to impose symmetricity as a constraint in (12)?

2.	What is the correct way to choose regularization parameter λ in (14)?

The paper also proposes a stochastic linesearch algorithm. For this part, there are several concerns as well:

1.	The assumption that the covariance of gradient estimator is a constant multiple of identity is a strong and unrealistic assumption, which is never satisfied in machine learning. 

2.	The algorithm performs a backtracking linesearch, with the initial trial stepsize decreaing as O(1/k), which means that the stepsize used is always decreasing at least as fast as O(1/k). This is in general in stark contrast with the intuition that a O(1) stepsize should be used for a quasi-Newton method. 

3.	Satisfying the Armijo condition in expectation does not lead to any useful convergence guarantee. 

The paper also presented some numerical experiments. While the numerical results look promising, I would appreciate some clarification about what method they are really comparing against. For example, for LBFGS the authors cite R. Bollapragada et al. “A progressive batching L-BFGS method for machine learning”. Is the paper comparing against progressive batching L-BFGS? The results of LBFGS here seem to be very different from the paper cited. 

Finally, the paper could certainly benefit by making some mathematical statement more rigorous. For example, Lemma 1 and 2 are stated in expectation; however, since the algorithm is a stochastic algorithm, the whole sequence {x_k} generated is a stochastic process, and the expectation in the lemmas are conditional expectations. It is important to clarify w.r.t. what the conditional expectation is taken.

In summary, I believe that this paper has made a novel contribution. However, the author should address the concerns above.",5
"This paper studies the signal denoting problem. The theoretical results are nice, and supported by numerical experiments. I have the following two major concerns:

(1) Using deep neural network as a prior in signal denoising is definitely an important and also challenging problem, only when the neural network is learnt from data. However, this paper assumes that the weight matrices of the neural network prior are i.i.d. Gaussian ensemble and independent on the signal. This assumption is oversimplified, and makes the theoretical results become quite expected and delicate. One can hardly get any insights of the practical signal denoising.

(2) The paper has a significant overlap with HV:COLT18:""Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk"". HV:COLT18 consider a RIP-type linear operator, and this paper considers the identity operator, which is actually easier. Dealing with the additive noise is new, but somehow incremental.

~~~~~After Rebuttal~~~~~~

The rebuttal still cannot justified such a random deep prior well. I keep my rating unchanged.",5
"The paper analyzes the recovery accuracy of a ""tweaked"" gradient descent algorithm for imaging denoising and compressive sensing under deep generative priors. In particular, when assuming Gaussian randomness of the network weights and extremely stringent conditions of network sizes, they demonstrate a specific denoising rate of O(k/n), with k and n being the input and output dimension of the generative network. This is seemingly optimal in terms of the dependence on the latent code dimensionality and the signal dimensionality and is the first result of this kind. 

Two papers are closely related, but are not sufficiently discussed in the introduction. [Bora et al., 2017] does not require Gaussian randomness of the network weights, but achieves only O(1) error bound assuming the empirical risk minimization problem can be solved to optimality.  [Hand & Voroninski, 2018] showed that under same assumptions as in this paper, the nonconvex empirical risk minimization problem exhibits a nice geometric landscape - no spurious stationary points. This implies that virtually anything reasonable would converge to global optimum. Combing both facts, it is not surprising to arrive at the results in this paper.

While the paper makes some novel theoretical contributions, two concerns stand out. First, there is a lack of intuition or justification of the tweak in gradient descent - flipping the sign of the iterate at times.  The author argued that around approximately -x*, the loss function is larger than around the optimum x* . So simple gradient descent is likely to get stuck in this region, so the negation check is needed. I am not so convinced by the argument. There could be other critical points that are not necessarily in the negative regime of true optimal, right?  So why would this be sufficient or necessary for global convergence? Second, even ignoring the unrealistic Gaussian assumption on the network weights, the theorem requires very narrow regimes for the expansivity condition and the noise variance bound. It's hard to verify whether these conditions can be satisfied at all. 

The experiment on denoising with learned prior from MNIST data is interesting, as it suggests that the theoretical assumptions are not necessary in practice to observe the optimal recovery rate. It would be more convincing if more experiments are provided, especially for the compressive sensing application. 
",6
"The paper studies the standard denoising problem under the assumption that the unknown n-dimensional signal can be written as the output of a known d-layer neural network G mapping k dimensions to n dimensions. The paper specifies an algorithm to perform this denoising and the algorithm is based on a variant of the usual gradient method. Then, under additional assumptions on the neural network G, the paper proves that their algorithm produces a denoised signal that achieves a mean squared accuracy of k/n. Because the input signal has ""effective"" dimensionality k (as it can be written as G(x) for some k-dimensional x), it is nice that it can be recovered at the accuracy k/n by Gradient Descent despite the complicated nature of G. In this respect, the result is quite interesting. However, the underlying assumptions are too strong in my opinion as described below: 

1. It is assumed that the Weights of the neural network G are all Gaussian (and also specific Gaussians with mean zero and variances determined by the layer dimensions). This of course is highly impractical. In practice, these network weights are pre-learned (say based on similar datasets) and there is hardly any reason to believe that they will satisfy the Gaussian assumption. 
2. It is assumed that the network is expansive in some sense with an expansivity constant \epsilon. This \epsilon then gets into the accuracy bound which basically means that \epsilon has to be set very small. Unfortunately, this leads to the expansivity condition being quite stringent which will further lead to k being very small (especially if d is large). It is unrealistic to believe that real-world signals will come from a neural network with small k. 

Given that there do not seem to be other such results for the accuracy of neural network denoising, the paper might still be considered interesting despite the above shortcomings. However, I believe that the theoretical result has near-zero relevance to a practical neural network denoiser.

Another concern is that the paper seems to borrow quite a lot of ideas from the paper ""Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk"" by Hand and Voroninski. It will be good if the authors can explain the essential differences between the present paper and this earlier paper. ",6
"The authors argue that graph neural networks based on the message passing frameworks are not able to infer the topological structure of graphs. Therefore, they propose to use the node embedding features from DeepWalk as (additional) input for the graph convolution. Moreover, a graph pooling operator is proposed, which clusters node pairs in a greedy fashion based on the l2-distances between feature vectors. The proposed methods are evaluated on seven common benchmark datasets and achieve better or comparable results to existing methods. Moreover, the method is evaluated using synthetic toy examples, showing that the proposed extensions help to infer topological structures.

A main point of criticism is that the authors claim that graph convolution is not able to infer the topological structure of a graph when no labels are present. In fact the graph convolution operator is closely related to the Weisfeiler-Lehman heuristic for graph isomorphism testing and can distinguish most graphs in many practical application. Therefore, it is not clear why DeepWalk features would increase the expressive power of graph convolution. It should be stated clearly which structural properties can be distinguished using DeepWalk features, but no with mere graph convolution.
The example on page 4 provides only a weak motivation for the approach: The nodes v_1 and v_2 should be indistinguishable since they are generated using the same generator. Thus, the problem is the mean/max pooling, and not the graph convolution. When using the sum-aggregation and global add pooling, graphs with two clusters and graphs with three clusters are distinguishable again. Further insights how DeepWalk helps to learn more ""meaningful"" topological features are required to justify its use.

Clustering nodes that are close in feature space for pooling is a reasonable idea. However, this contradicts the intuition of clustering neighboring nodes in the graph. A short discussion of this phenomenon would strengthen the paper in my opinion.

There are several other questions that not been answered adequately in the article.

* The 10-fold cross validation is usually performed using an additional validation set. What kind of stopping criteria has bee use? * It would be helpful to provide standard deviations on these small datasets (although a lot of papers sadly dismiss them).
* I really like the use of synthetic data to show superior expressive power, but I am unsure whether this can be contributed to DeepWalk or the use of the proposed pooling operator (or both). Please divide the results for these toy experiments in ""GEO-DEEP"" and ""GEO-deep no pooling"". As far as I understand, node features in different clusters should be indistinguishable from each other (even when using DeepWalk), so I contribute this success to the proposed pooling operator.
* A visualization of the graphs obtained by the proposed pooling operator would be helpful. How do the coarsened graphs look like? Given that any nodes can end up in the same cluster, and the neighborhood is defined to be the union of the neighboring nodes of the node pairs, I guess coarsened graphs are quite dense.
* DiffPool (NIPS 2018, Ying et al.) learns assignment matrices based on a simple GCN model (and thus infers topological structure from message passing). How is the proposed pooling approach related to DiffPool (except that its non-differentiable)? How does it perform when using only the features generated by a GCN? How does it compare to other pooling approaches commonly used, e.g., Graclus? At the moment, it is quite hard to judge the benefits of the proposed pooling operator in comparison to others.


In summary, the paper presents promising experimental results, but lacks a theoretical justification or convincing intuition for the proposed approach. Therefore, at this point I cannot recommend its acceptance.


Minor remarks:

* p2: The definition of ""neighbour set"" is needless in its current form.
* p2: The discussion of graph kernels neglects the fact that many graph kernels compute feature vectors that can be used with linear SVMs.

-----------
Update:
The comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly.",6
"This paper proposes a deep GNN network for graph classification problems using their adaptive graph pooling layer. It turns the graph down-sampling problem into a column sampling problem. The approach is applied to several benchmark datasets and achieves good results.

Weakness

1.	This paper is poorly written and hard to follow. There are lots of typos even in the abstract. It should be at least proofread by an English-proficient person before submitted. For example, in the last paragraph before Section 3. “In Ying et al. ……. In Ying et al.”
2.	In paragraph 1 of Section 3, there should be 9 pixels around the center pixel including itself in regular 3x3 convolution layers.
3.	The definition of W in Eq(2) is vague. Is this W shared across all nodes? If so, what’s the difference between this and regular GNN layers except for replacing summation with a max function?
4.	The network proposed in this paper is just a simple CNN. GNN can adopt such kind of architectures as well. And I didn’t get numbers of first block in Figure 1. The input d is 64?
5.	The algorithm described in Algorithm 1 is hard to follow. There are some latex tools for coding the algorithms.
6.	The authors claim that improvements on several datasets are strong. But I think the improvement is not that big. For some datasets, the network without pooling layers even performs better at one dataset. The authors didn’t provide enough analysis on these parts.

Strength:
1.	The idea used in this paper for graph nodes sampling is interesting. But it needs more experimental studies to support this idea. 
",4
"The authors propose a method for learning representations for graphs. The main purpose is the classification of graphs.

The topic is timely and should be of interest to the ICLR community. 

The proposed approach consists of four parts: 

Initial feature transformation
Local features aggregation
Graph pooling
Final aggregator

Unfortunately, each of the part is poorly explained and/or a method that has already been used before. For instance, the local feature aggregation is more or less identical to a GCN as introduced by Kipf and Welling. There are now numerous flavors of GCNs and the proposed aggregation function in (2) is not novel. 

Graph pooling is also a relatively well-established idea and has been investigated in several papers before. The authors should provide more details on their approach and compare it to existing graph pooling approaches. 

Neither (1) nor (4) are novel contributions. 

The experiments look OK but are not ground-breaking and are not enough to make this paper more than a mere combination of existing methods. 

The experiments do not provide standard deviation. Graph classification problems usually exhibit a large variance of the means. Hence, it is well possible that the difference in mean is not statistically significant. 

The paper could also benefit from a clearer explanation of the method. The explanation of the core parts (e.g., the graph pooling) are difficult to understand and could be made much clearer. 
",3
"This paper focuses on the problem of unsupervised feature selection, and proposes a method by exploring the locally linear embedding. Experiments are conducted to show the performance of the proposed locally linear unsupervised feature selection method. There are some concerns to be addressed.

First, the novelty and motivation of this paper is not clear. This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data. Why uses LLE rather than other methods such as LE? What are the advantages?

Second, in Section 3.3, authors state that the method might be biased due to the redundancy of the initial features. To my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of f Li et al. (2012) ""Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control"". 

Third, how about the computational complexity of the proposed method? It is better to analyze it theoretically and empirically.

Finally, the equation above Eq. 8 may be wrong.
",4
"In this paper, the authors presented Locally Linear Unsupervised Feature Selection (LLUFS), where a dimensionality reduction is first performed to extract data patterns, which are used to evaluate compliance of features to the patterns, applying the idea of Locally Linear Embedding.

1. This work basically assumes that the dataset is (well) clustered. This might be true for most real world dataset, but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. For example, if most data points are concentrated on a particular area not being well clustered, how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.

2. For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD? Theoretical and experimental comparison should be interesting and useful.

3. This paper is well-written, clearly explaining the idea mathematically. It is also good to mention limitation and future direction of this work. It is also good to cover a corner case (XOR problem) in details.

4. Minor comments:
 - Bold face is recommended for vectors and matrices. For instance, 1 = [1, 1, ..., 1]^T, where we usually denote the left-hand 1 in bold-face.
 - It seems x_j is missing in Johnson-Lindenstrauss Lemma formula. As it is, \sum_j W_{i,j} is subject to be 1, so the formula does not make sense.",6
"Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.

Comments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.

Second, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?

Finally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?

Minor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?

Overall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.

[1] Venna et al. ""Information retrieval perspective to nonlinear dimensionality reduction for data visualization."" Journal of Machine Learning Research 11, no. Feb (2010): 451-490.",3
"Note: This is an emergency review. I managed not to look at existing comments/ratings for this paper before writing my review.

Summary
---

This paper studies deep multi-agent RL in settings where all of the agents must cooperate to accomplish a task (e.g., search and rescue, multi-player video games). It uses simple cooperative weighted voting games 1) to study the efficacy of deep RL in theoretically hard environments and 2) to compare solutions found by deep RL to a fair solution concept known in the literature on cooperative game theory.

In a weighted voting game each agent is given a weight and the agents attempt to form teams. The first team whose total weights exceed a known threshold get the total reward, which is distributed amongst the team members. Given such a game, the __shapely value__ of an agent measures the importance of that agent. How much does it contribute to a team from this set of agents? How much payoff should it get? These have existed in the literature for over 60 years and appear to be widely known and used.

All of this is agnostic to how the agents communicate to form teams: i.e., the communication protocol or the actions available in the environment. The protocol matters because it can allow certain teams to form more or less easily than others, even though the same team would get the same reward regardless of protocol. This can make an agent more or less effective under different protocols. Here two protocols are considered - one where agents suggest proposed teams directly and another where they suggest teams by congregating on a 2d plane. Both protocols result in games whose Nash equilibria are computationally intractable.

The paper shows 4 results:
1) It considers a hand-designed bot similar to models from the game theory literature. Relative to a group of RL agents, an additional RL bot will outputperform a hand-designed bot in terms of average reward it receives.

2) The average reward of a bot is strongly correlated with that bots shapely value.

3) In the negotiation by congregation environment, a bot's spatial position can affect its ability to negotiate.

4) Shapely values can be predicted quite accurately from the weights and threshold that define a cooperative voting game, though these predictions have high variance.

The paper concludes that deep RL is effective at learning agents for cooperative games in multiple ways:
1) Deep agents are better than a hand-designed agent.

2) Deep agents easily extend across negotiation protocols (something hand-designed agents don't do).

3) A popular result in cooperative game theory predicts how effective agents should be. Deep agents are just about that effective.

Strengths
---

* The paper does a pretty good job of reviewing relevant work from game theory.

* Some of the organization is nice (e.g., the list of reasons classic game theory doesn't extend to practice; one section per experiment).

Weaknesses mentioned in individual sections...

Quality
---
Overall, things were well thought through, but I would have liked more out of the experiment 4 section and I think a few minor details might have been missed.

Details:

Section 4.5/Experiment 4: The Shapely value comparison is the most important part of the paper.  This section is important because it tries to explain those results, but it seems like there's more work to be done here. I'm not sure capacity is eliminated as a concern, and there might be other concerns not listed like optimization error.

* I'm not sure what conclusion to take from experiment 4. Shapely values can be computed from the cooperative games directly, independent of protocol. We're interested in __policies__ that get exactly the shapley values as their average reward. Policies depend on the protocol. Does being able to predict shapley values mean that a model with similar capacity can learn a policy that will have the desired shapley value? Was that the desired conclusion?

Other comments:

* The current hand-designed baseline uses weights to form a probability distribution. There should be another baseline that uses Shapley values instead of weights.

* It's not clear exactly what the spatial nature of the Team Patches environment adds. It is good to try another environment just to have an additional notion of generalization.

Clarity
---
Overall, the motivation could be clearer. Is the point to do work on cooperative games or to compare to Shapley values?

Presentation details:

* The paper does not get to specific examples of agents acting in environments until about page 4. Providing a simple, brief example which leaves out some details at the beginning would go a long way toward aiding intuitions about the abstract concepts discussed. Here are some clarity issues I had that might have been helped with an example:
    * What exactly is it about a task which requires agents to form teams? How necessary are those teams?
    * What exactly is a negotiation protocol?
    * What does it mean to distribute/share a reward across agents?

* When talking about shapely values, fairness seems to be emphasized somewhat often, but no concrete intuition about what fairness means in this setting is provided.

* Intro para 4: What does the human data measure? And thus how might it be useful?

* Intro para 7: People in the ICLR community will be more familiar with this work. What is the difference between communication and team forming?

* The section on Shapley values should provide more intuition about what they're thought about as measuring. (An agent's importance or what payoff it should expect, according to wikipedia.)

* Instead of measuring correlation to Shapley values, the paper measures whether average reward approximates Shapley values. It seems like the two are on a different scale. Average reward is unbounded and Shapley values are in [0, 1]. How are they comparable?

* The paper mentions how results vary over different types of boards (ones with higher and lower variance in the sampled weights). It does not show results to support this discussion. A conditional analysis of performance would be interesting and relevant, perhaps conditional versions of Fig. 3.

Originality
---
I do not know much about game theory and I'm only somewhat familiar with multi-agent deep RL, so I am not in a great position to judge novelty. Nonetheless, Given existing work in multi-agent RL, it is unsurprising that deep RL agents learn reasonable policies in these environments.

As far as I know, the comparison of average reward to shapely values has not been done before. 


Significance
---
Most work in multi-agent RL evaluates by 1) comparing to baselines or 2) measuring some environment/task-specific metric. The best thing about this work is that it evaluates by comparing actual performance to some external theory that suggests how well an agent should be able to do, falling into a 3rd category.  It's not alone in this category (e.g., paper compare to theoretically optimal baselines if they can), but it is interesting to see another example of this kind of evaluation.

The community might possibly start to focus more on cooperative games because of this paper. A more interesting result would occur if others are inspired to implement more comparisons to how agents __should__ perform in theory.


Justification for Final Rating
---

I am unsure about novelty. As described above, the paper is lacking in clarity and quality (esp. section 4.5), but I don't think these concerns would invalidate the main result. I think the contribution is significant because of the kind of evaluation, but I'm not sure it will ultimately have a large impact. Thus I think some of the concerns above should be addressed before publication, but I would not be very disappointed if it were published as is.

",5
"This is an emergency review, so apologies for the briefness.

The paper introduces an approach to learning negotiation strategies using reinforcement learning. The authors propose a new setup in which self-interested agents must cooperatively form teams to achieve a reward. They explore two ways of proposing agreements: one involving a random agent proposing an agreement symbolically, and another in which agents form teams by moving to the same location. Results show that RL-trained models outperform simple rule-based bots, and correlate with game-theoretic predictions. I think the paper is very well clearly presented, and tackles an interesting an important problem.

One issue I have is that as I understand it, the results are only reported for training games. Could the agents just be memorizing a good outcome for that specific environment, rather than actually learning to negotiate? Why not evaluate on held out games?

The experiments are pretty interesting, and I appreciated the last one showing that limitations are due to the difficulty of RL, rather than expressive power of the network. However, I think there are some other natural questions that could be explored, including: what kind of strategies are the models learning? Could we change the environment in such a way that the proposed approach is not sufficient? Is the choice of RL approach crucial, or does anything work? I think further experiments would strengthen the paper.",6
"This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings.  The authors evaluate their approach on two games against optimal solutions given by the Shapley value.

The work builds upon a substantial and growing literature on reinforcement learning for multiagent competitive and cooperative games. The most novel component of the work is a focus on the process of negotiation within cooperative coalition games. The two game environments studied examine a ""propose-accept"" negotiation process and a spatial negotiation process.

The main contribution of the work is the introduction of a reinforcement learning approach for negotiation that can be used in cases where unlimited training simulations are available.  This approach is a fairly straightforward application of RL to coalition games, but could be of interest to researchers studying negotiation or multiagent reinforcement learning, and the authors demonstrate the success of RL compared to a normative standard.

My primary concerns are:
- The authors advertise the work as requiring no assumptions about the specific negotiation protocol, but the learning algorithms used are different in the two cases studied, so the approach does require fine-tuning to particular cases.
- Maybe I missed it, but how many training games are required?
- In what real applications do we expect this learning algorithm to be useful?  
- The experiments where the RL agents are matched against bots include training against those specific bot types. How does the trained algorithm perform when matched against agents using rules outside its training set?  
- Since the Shapley value is easily computable in both cases studied.  If the bots are all being trained together, why wouldn't the bots just use that to achieve the optimal solution?
- Why are only 20 game boards used, with the same boards used for training and testing?  How do the algorithms perform on boards outside the training set?

Overall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal.
",5
"    The paper authors provide a good overview of the related work to Private/Fair Representation Learning (PRL). Well written, The theoretical approach is extensively explained and the first sections of the paper are easy to follow. The authors demonstrate the model performance on or the GMM, the comparison between theoretical and data driven performance is a good case study to understand the PRL.

We usually expect to see related work in the first sections, in this case it's has been put just before the conclusion. It can be still justified by the need o introduce the  PRL concepts before comparing with other works.
The GMM study case is interesting, but incorporates strong assumptions. Moreover, for a 4 or 8 dimensional GM, 20K data points are more than enough to infer the correct parameter. It would have been more useful if it was used to comapre between the mentioned methods in ""Related Work"".

There seems to be important parts of the paper that has been put in the appendices: how to solve the constrained problem, Algorithm.... Similarly, some technical details were expanded in the paper body (Network structure).

The authors mentioned the similarities with other works and their model choices that set theirs apart from other. Yet, the paper doesn't provide performance ( accuracy, MI) comparison to other works. There seems to be a strong similarity with Censoring representations with an adversary, Harrison Edwards and Amos Storke (link: https://arxiv.org/abs/1511.05897). Difference : distortion instead of H divergence, non-generative autoencoders.

Consequently, I question the novelty of the paper's contribution. Without extensive comparison with other methods and especially to similar ones mentioned in the related work, there is little to say about the ""state-of-the-artness"". Yet, it is important to acknowledge the visible effort behind the paper and how the author(s) managed to leverage the simplicity and power of GANs.

On a lighter note:
A)- the paper mention ""state-of-the-art CNNs, state-of-the-art entropy estimators, MI, generative models"", for the Machine Learning community, many of these elements have been around for a while now.
B)- ""Observe that the hard constraint in equation 2 makes our minimax problem different from what is extensively studied in the machine learning community"": I would argue it's not an objective statement.
 


",4
"This paper present an adversarial-based approach for private and fair representations. This is done by learned distortion of data that minimises the dependency on sensitive variable while the degree of distortion is constrained. This problem is important, and the analysis from game-theory and information theory perspectives is interesting. However, the approach itself is similar to Edwards & Storkey 2015, and I find the presentation of this paper confusing at a few points. 

First, while both the title and abstract suggest it is about learning representation, the approach might be better considered as data-augmentation. As described a bit later: ""...modifying the training data is the most appropriate and the focus of this work"". This contradiction with more commonly accepted meaning of representation learning (learning abstract/high level representation of data) is confusing.

Although the authours argued this work is different from Edwards & Storkey 2015, I think they are quite similar. The presented method is almost a special case of this previous work: it seems that one can obtain this model by modifying Edwards & Storkey's model as follows (referring to the equations in Edwards & Storkey's paper): (1) removing the task (Y) dependent loss in eq. 9. (2) assume the encoder transforms X to the same data space so the decoder can be removed, so eq. 7 become equivalent to the distortion measure in this paper. There are other small differences, such as adding noise and the exact way to impose constraint, but I doubt whether the novelty is significant in this case.

Other places that are unclear include: proposition 1 -- what does ""demographic parity subject to the distortion constraint"" mean? demographic parity was defined earlier as complete independence on sensitive variable, so how can ""complete independence"" subject to a constraint? In addition, it would be helpful introduce S is binary. This information was delayed to section 3 after the cross-entropy loss that assumes binary S was presented.

Overall, I think this paper is interesting, and the analysis offers insights into related areas. However, the novelty is not enough for acceptance at ICLR, and the presentation can be improved.",4
"The authors describe a framework of how to learn a ""fair"" (demographic parity) representation that can be used to train certain classifiers, in their case facial expression and activity recognition. The method describes an adversarial framework with a constraint that bounds the distortion of the learned representation compared to the original input.

Clarity:
The paper is well written and easy to follow. The appendix is rather extensive though and contains some important parts of the paper, though the paper can be understood w/o it.

I didn't quite follow Sec 3. It is a bit sparse on the details and the final conclusion isn't entirely clear. It also isn't clear to me how general the conclusions drawn from the Gaussian mixture model are for more complex cases.

Novelty:
Adversarial fairness methods are not new, but in my opinion the authors do a good job of summarizing the literature and formalizing the problem. I am not fully familiar with the space to judge if this is enough novelty.

Using the distortion constraint is interesting and seems to work according to the experiments. Generally though, I think that distortion can be a very restrictive constraint. One could imagine representations with a very high distortion (e.g. by completely removing the sensitive attribute) and predictive qualities equivalent to the original representation. Some further discussion of this would be good.

Experiments:
The experiments are somewhat limited, but show the expected correlations (e.g. distortion vs predictiveness). 

Overall, I do believe that this work is in the right direction in this more and more popular area of great importance. I also think that contributions compared to other works could be made more clear, as well as additional experiments and discussions of the shortcomings of this approach may be added.",7
"This manuscript presents a method to prune deep neural networks while training. The main idea is to use some regularization to force some parameters to have small values, which will then be subject to pruning. 
Overall, the proposed method is not very interesting. More importantly, the manuscript only lists the percentage of pruned parameters, but did not compare the actual running time before and after pruning. ",5
"

==Major comments==

You need to better explain how the regularization path is obtained for your method. It is not clear to me at all why the iterates from lines 5-8 in Alg 2 provide a valid regularization path. 

I am very confused by section 3.4. Is the pruning strategy introduced in this section specific to LBI? In other words, is there some property of LBI where the regularization path can be obtained by sorting the parameters by weight? It seems like it's not specific to LBI, since you use this pruning strategy for other models in your experiments. Is this the right baseline? Surely there are other sparsification strategies.


How/why did you select 5e-4 for lambda? You should have tuned for performance on a validation set. Also, you should have tuned separately for each of the baselines. There is no reason that they should all use the same lambda value.

Can you say anything about the suboptimality of the support sets obtained by your regularization paths vs. if you had trained things independently with different regularization penalties?

I am very concerned by this statement: 
""However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers.""
This is important. Do you have idea of why it is true? This instability seems like an obstacle for large-scale deployment.

Are your baselines state of the art? Is there anything discussed in the related work section that you should also be comparing against?


==Minor comments==
The difference between Alg 2 and 3 is mechanical and should be obvious to readers. I'd remove it, as the notation is complex and it doesn't add to the exposition quality. Instead, you should provide an algorithm box that explains how you postprocess W to obtain a sparse network.

Your citation format is incorrect. You should either have something along the lines of ""foo, which does X, was introduced in author_name et al. (2010)"" or ""foo does X (author_name, 2010)."" Perhaps you're using \citet instead of \citep in natbib.

Algorithm box 1 is not necessary. It is a very standard concept in machine learning. 

On the left hand of (5), shouldn't Prox be subscripted by L instead of P?





",5
"This paper introduces an approach to pruning while training a network. This is interesting and experiments show interesting results in several datasets including ResNet18

Here are a few comments:

 - Pruning or regularization for compression is not new. Alvarez and Wen have used group lasso types as suggested in the paper and some others such as Alvarez and Salzmann (Compression aware training NIPS 2017) and Wen (Coordinating filters ICCV2017) have used low-rank type of while training. How is this different to those? They also do not need any sort of fine tuning and more importantly, they show this can scale to large networks and datasets. 

- These last two works I mentioned promote redundancy, similarly to what is suggested in the paper. Would be good to get them cited and compared. Important from those is the training methodology to avoid relevant overheads. How is that happening in the current approach


- While I like the approach, would be nice to see how this scale.  All for methods above (and others related) do work on full imagenet to show performance.  For ResNet, cleaning the network is not really trivial (near the block), is that a limitation?
- Why limiting experiments to small networks and datasets? Time wise, how does this impact the training time?
- Why limiting the experiments to at most 4 layers? 
- I am certainly not impressed by results on fully connected layers in MNIST. While the experiment is interesting does not seem to be of value as most networks do not have those layers anymore.

- Main properties of this approach are selecting right filters while training without compromising accuracy or needing fine tuning. While that is of interest, i do not see the difference with other related works (such as those I cited above)

- As there is enough space, I would like to see top-1 results for comprehensive comparison. 

- I think tables need better captions for being self-contained. I do not really understand what i see in table 5 for instance. 
- Droping 13% of top5 accuracy does not seem negligible, what is the purpose there? Would also be interesting to compare then with any other network with that performance. 
- What about flops and forward time? Does this pruning strategy help there?

",4
"The system is explained thoroughly, and with the help of nice looking graphics the network architecture and its function is clearly described. The paper validates the results against baselines and shows clearly the benefit of double  domain learning. The paper is carefully written and  follows the steps required for good scientific work.

Personally, I do not find this particularly original, even with the addition of the zero-shot learning component. 

As a side note, the task here does not seem to need a multitask solution. Adding the text input as subtitles to the video gives essentially the same information that is used in the setup. The resulting inclusion of text could utilise the image attention models in a similar manner as the GRU is used in the manuscript for the text. In this case the problem stated in the could be mapped  to a ""DeepMind Atari"" type of RL solution, with text as a natural component, but added as visual clue to the game play. Hence, I am not convinced that the dual attention unit is essential to the performance the system.

In addition, there are studies (https://arxiv.org/abs/1804.03160) where sound and video are , in unsupervised manner, correlated together. This contains analogous dual attention structure as the manuscript describes, but without reinforcement learning component.

I would recommend this as a poster.
",7
"The authors propose a multitask model using a novel “dual-attention” unit for embodied question answering (EQA) and semantic goal navigation (SGN) in the virtual game environment ViZDoom. They outperform a number of baseline models originally developed for EQA and SGN (but trained and evaluated in this multitask paradigm).

Comments and questions on the model and evaluation follow.

1. Zero-shot transfer claim:
1a. This is not really zero-shot transfer, is it? You need to derive object detectors for the meanings of the novel words (“red” and “pillar” from the example in the paper). It seems like this behavior is supported directly in the structure of the model, which is great — but I don’t think it can be called “zero-shot” inference. Let me know if I’ve misunderstood!
1b. Why is this evaluated only for SGN and not for EQA?

2. Dual attention module:
2a. The gated attention model only makes sense for inputs in which objects or properties (the things picked out by convolutional filters) are cued by single words. Are there examples in the dataset where this constraint hold (e.g. negated properties like “not red”)? How does the model do? (How do you expect to scale this model to more naturalistic datasets with this strong constraint?)
2b. A critical claim of the paper is that the model learns to “align the words in both the tasks and transfer knowledge across tasks.” (Earlier in the paper, the claim is that “This forces the convolutional network to encode all the information required with respect to a certain word in the corresponding output channel.”) I was expecting you would show some gated-attention visualizations (not spatial-attention visualizations, which are downstream) to back up this claim. Can you show me visualizations of the gated-attention weights (especially when trained on the No-Aux task) which demonstrate that words and objects/properties in the images have been properly aligned? Show that e.g. the filter at index i only picks out objects/properties cued by word i?

3. Auxiliary objective: it seems like this objective solves most of the language understanding problem relevant in this task. Can you motivate why it is necessary? What is missing in the No-Aux condition, exactly? Is it just an issue with PPO optimization? Can you do error analysis on No-Aux to motivate the use of the Aux task?

4. Minor notes:
4a. In appendix A, the action is labeled “Turn Left” but the frames seem to suggest that the agent is turning right.
4b. How are the shaded regions estimated in figs. 7, 8? They are barely visible — are your models indeed that consistent across training runs? (This isn’t what I’d expect from an RL model! This is true even for No-Aux..?)
4c. Can you make it clear (via bolding or coloring, perhaps) which words are out-of-vocabulary in Table 3? (I assume “largest” and “smallest” aren’t OOV, for example?)
",5
"This work proposes to train an RL-based agent to simultaneously learn Embodied Question Answering and Semantic Goal Navigation on the ViZDoom dataset. The proposed model incorporates visual attention over the input frames, and also further supervises the attention mechanism by incorporating an auxiliary task for detecting objects and attributes.

Pros:
-Paper was easy to follow and well motivated
-Design choices were extensively tested via ablation
-Results demonstrate successful transfer between SGN, EQA, and the auxiliary detection task

Cons:
-With the exception of the 2nd round of feature gating in equation (3), I fail to see how the proposed gating -> spatial attention scheme is any different from the common inner-product based spatial attention used in a large number of prior works, including  [1], [2], and [3] and many more.
-The use of attribute and object recognition as an auxiliary task for zero-shot transfer has been previously explored in [3]


Overall, while I like the results demonstrating successful inductive transfer across tasks, I did not find the ideas presented in this work to be sufficiently novel or new.

[1] Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, Huijuan Xu, Kate Saenko
[2] Drew A. Hudson, Christopher D. Manning, Compositional Attention Networks for Machine Reasoning
[3] Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks, Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem",5
"**Summary**

The paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: 
1. learning technique for high fidelity one-shot imitation at test time. 
2. Policies to improve the expert performance through RL.  

The main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. 


** Comments **
- The novelty of algorithm block
The main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. 

1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? 
If the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. 

2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. 

* Improved Comparisons
- Compare with One-Shot Performance
Since this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. 

This comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018)

- Compare High-Fidelity Performance
It is used as a differentiator of this method but without experimental evidence.
The results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation.

Furthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful.

- Compare Policy Learning Performance
In addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. 


* Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work!


- ""Diverse Novel Skills"" 
The experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help.

- Bigger networks
""In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms"" -- but Fig 3 is a network architecture diagram. It is probably fig 6!

- The authors are commended for presenting a broad overview of imitation based methods in table 2

** Questions **

1.  How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. 

2. How is the local context considered in action generation in sec 2.1. 
The authors reset the simulation environment to o_1 = d_1. 
Then actions are generated with  \pi_{theta} (o_t, d_{t+1}). 
a. Is the environment reset every time step?
b. If not how is the deviation of the trajectory handled over time? 
c. how is the time horizon for this open loop roll out chosen. 

3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. 

4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model?

Suggestion:
The whole set of experiments are in a simulation. 
The authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. 

References:
1. Neural Task Programming, Xu et al. 2018 (https://arxiv.org/abs/1710.01813)
2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453)
3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674)
4. A survey of robot learning from demonstration, Argall et al. 2009
",4
"Summary

This work porposes a approach for one-shot imitation with high accuracy, called ""high fidelity imitation learning"" by the authors. Furthermore, the work addresses the common problem of exploration in imitation learning, which would help to rescue from off-policy states.

Review

In my opinion, the main claims of this paper are not validated sufficiently in the experiments. I would expect the experiments to be designed specifically to support the claims made, but little evidence is provided:

- The authors claim that the method allows one-shot generalization to an unknown trajectory. To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes. I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation.
Until then I see no evidence for a ""one-shot"" imitation capability of the proposed method.

- That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states: This is never experimentally validated. This hypothesis could easiliy be validated with an ablation study, were the results of early would not be added to the replay buffer.

- High fidelity imitation: In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos. Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations.
Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated.

Mainly due to the missing experimental validation of the claims made I recommend to reject the paper.",4
"This paper presents an RL method for learning from video demonstration without access to expert actions. The agent first learn to imitate the expert demonstration (observed image sequence and proprioceptive information) by producing a sequence of actions that will lead to the similar observations (require a renderer that takes actions and outputs images). The imitation loss is a similarity metric. Next, the agent explores the environment with both the imitation policy and task policy being learned; an off-policy RL algorithm D4PG is used for policy learning. Experiments are conducted on a simulated robot block stacking task.

The paper is really clearly written, but presenting the approach as ""high-fidelity"", ""one-shot"" learning is a bit confusing. First, it's not clear what's the motivation for high-fidelity. To me this is an artifact due to having to imitate the visual observation instead of the actions, which is a legitimate constraint, but not the original goal. Second, the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person; both for the same task. Usually one-shot learning tests on slightly different tasks or environments, whereas here the goal is to generalize to novel demonstrations. It's not clear why do we care imitation per se in addition to the task reward.

What I find interesting is the proposed approach for learning for video demonstration without action labels. Currently this requires an executor to render the actions to images, what if we don't have such an executor or only have a noisy / approximate renderer? In the real world it's probably hard to find a good renderer, it would be interesting to see how this constraint can be relaxed.

Questions:
- While the authors have shown the average rewards of the two sets are different, I wonder what's the variance of each person's demonstration. 
- In Fig 5, on the validation set, in terms of imitation loss there aren't that much difference between the policies, but in terms of task reward, the 'red' policy goes to zero while others policies' rewards are still similar. Any intuition for why seemingly okay imitation doesn't translate to task reward?

Overall, I enjoyed reading the paper and the experiments are comprehensive. The current presentation angle seems a bit off though.",5
"
Summary:
This paper proposes MetaMimic, an algorithm that does the following:
(i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task.
(ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task.

Overall Evaluation:
This is a good paper. In my opinion however, it does not pass the bar for ICLR.

Pros:
- The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive.
- The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful.
- Interesting pipeline of learning policies that can use demonstrations without actions.
- The results on the simulated robot arm (block stacking task with two blocks) are good.

Cons:
- The abstracts oversells the contribution a bit when saying that MetaMimic can learn ""policies for high-fidelity one-shot imitation of diverse novel skills"". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better.
- Experimental results are shown only for one task; block stacking with a robot arm in simulation.
- Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks.

Questions:
- Where does the ""task stochasticity"" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic?
- The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time?
- It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.)
- The x-axis in the figures says ""time (hours)"" - is that computation time or simulated time?

Other Comments:
- In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? 
- An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum.

Nitpick (no influence on score):
[1. Introduction]
- I find the first sentence, ""One-shot imitation is a powerful way to show agents how to solve a task"" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like ""Expert demonstrations are a powerful way to show agents how to solve a task."" works better?
- Second sentence, the chosen example is ""manufacturing"" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations.
- Add note that with ""unconditional policy"" you mean not conditioned on a demonstration.
[2. MetaMimic]
- [2.1] Third paragraph: write ""Figure 2, Algorithm 1"" or split the algorithm and figure up so you can refer to them separately.
- [2.1] Last paragraph, second line: remove second ""to""",5
"Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers.  While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was.  For example, how the optimization was done is not discussed at all.  At the same time, I'm not an expert in group theory, so it's possible that the paper has technical novelty or significance which I did not appreciate.  

Strengths: 

 -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful.  

Weaknesses: 
  
-I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3).  

-The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references.  

  -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I'm missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper).  In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset.  

  -It's interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better.  For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it's important to confirm).  


Questions: 

-Section 3.1 makes reference to ""learning parameters"".  I assume that this is done in the usual way with backpropagation and then SGD/Adam or something?  

-How is it guaranteed that W is orthogonal in the learning procedure?  
",6
"This paper proposes autoencoder architectures based on Cohen-Welling bases for learning rotation-equivariant image representations. The models are evaluated by reconstruction error and classification in the space of the resulting basis on rotated-MNIST, showing performance improvements with small numbers of parameters and samples.

I found most of this submission difficult to read and digest. I did not understand much of the exposition. I’ll freely admit I haven’t followed this line of work closely, and have little background in group theory, but I doubt I’m much of an outlier among the ICLR audience in that regard. The “Preliminaries” section is very dense and provides little hand-holding for the reader in the form of context, intuition, or motivation for each definition and remark it enumerates. I can't tell how much of the section is connected to the proposed models. (For comparison, I skimmed the prior work that this submission primarily builds upon (Cohen & Welling, 2014) and found it relatively unintimidating. It gently introduces each concept in terms that most readers familiar with common machine learning conventions would be comfortable with. It's possible to follow the overall argument and get the ""gist"" of the paper without understanding every detail.)

All that being said, I don’t doubt this paper makes some interesting and important contributions -- I just don’t understand what they are.

Here are some specific comments and questions, mostly on the proposed approaches and experiments:

* What actually is the “tensor (product) nonlinearity”? Given that this is in the title and is repeatedly emphasized in the text, I expected that it would be presented much more prominently. But after reading the entire paper I’m still not 100% sure what “tensor nonlinearity” refers to.

* Experiments: all models are described in long-form prose. It’s very difficult to read and follow. This could be made much clearer with an algorithm box or similar.

* The motivation for the “Coupled Autoencoder” model isn’t clear. What, intuitively, is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa? The empirical gains are marginal.

* Experiments: the structure of the section is hard to follow. (1) and (2) are descriptions of two different models to do the same thing (autoencoding); then (3) (bootstrapping) is another step done on top of (1), and finally (4) is a classifier, trained on top of (1) or (2). This could benefit from restructuring.

* There are long lists of integer multiplicities a_i and b_i: these seem to come out of nowhere, with no explanation of how or why they were chosen -- just that they result in “learn[ing] a really sharp W_28”. Why not learn them?

* How are the models optimized? (Which optimizer, hyperparameters, etc.?)

* The baseline methods should also be run on the smaller numbers of examples (500 or 12K) that the proposed approach is run on.

* A planar CNN baseline should be considered for the autoencoder experiments.

* Validating on MNIST alone (rotated, spherical, or otherwise) isn’t good enough in 2018. The conclusions section mentions testing the models with deeper nets on CIFAR, but the results are not reported -- only hinting that it doesn’t work well. This doesn’t inspire much confidence.

* Why are Spherical CNNs (Cohen et al., 2018) a good baseline for this dataset? The MNIST-rot data is not spherical.

* Table 1: The method labels (Ours, 28/14 Tensor, and 28/14 Scale) are not very clear (though they are described in the text)

* Table 1: Why not include the classification results for the standard AE? (They are in the Fig. 6 plot, but not the table.)

* Conclusions: “We believe our classifiers built from bases learnt in a CAE architecture should be robust to noise” -- Why? No reasons are given for this belief.

* There are many typos and grammatical errors and odd/inconsistent formatting (e.g., underlined subsection headers) throughout the paper that should be revised.",3
"Recently there has been a spate of work on generalized CNNs that are equivariant to various symmetry groups, such a 2D and 3D rotations, the corresponding Euclidean groups (comprising not just rotations but also translations) and so on. The approach taken in most of the recent papers is to explicitly build in these equivariances by using the appropriate generalization of convolution. In the case of nontrivial groups this effectively means working in Fourier space, i.e., transforming to a basis that is adapted to the group action. This requires some considerations from represntation theory. 

Earlier, however, there was some less recognized work by Cohen and Welling on actually learning the correct basis itself from data. The present paper takes this second approach, and shows for a simple task like rotated MNIST, the basis can be learned from a remarkably small amount of data, and actually performs even better than some of the fixed basis methods. There is one major caveat: the nonlinearity itself has to be rotation-covariant, and for this purpose they use the recently introduced tensor product nonlinearities. 

The paper is a little rough around the edges. In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose: SO(2) is a really simple commutative group, so the way that ""tensor product"" representations reduce to irreducibles could be summed up in the formula  $e^{-2\pi i k_1 x}e^{-2\pi i k_2 x}=e^{-2\pi i (k_1+k_2) x}$. I am not sure why the authors choose to use real representations (maybe because complex numbers are not supported in PyTorch, but this could easily be hacked) and I find that the real representations make things unnecessarily complicated. I suspect that at the end of the day the algorithm does something very simple (please clarify if working with 
real representations is somehow crucial). 

But this is exactly the beauty of the approach. The whole algorithm is very rough, there are only two layers (!), no effort to carefully implement nice exact group convolutions, and still the network is as good as the competition. Another significant point is that this network is only equivariant to rotations and not translations. 

Naturally, the question arises why one would want to learn the group adapted basis, when one could just compute it explicitly. There are two interesting lessons here that the authors could emphasize more:

1. Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted (Cohen-Welling) basis. This is interesting because Fourier space (""tensor"") nonlinearities are a relatively new idea in the literature. This finding suggests that the nonlinearity might actually be more important than the basis.

2. The images that the authors work on are not functions on R^2, but just on a 28x28 grid. Rotating a rasterized image with eg. scikit-rotate introduces various artifacts. Similarly, going back and forth between a rasterized and polar coordinate based representation (which is effectively what would be required for ""Harmonic Networks"" and other Fourier methods) introduces messy interpolation issues. Not to mention downsampling, which is actually addressed in the paper. If a network can figure out how to best handle these issues from data, that makes things easier.

The experiments are admittedly very small scale, although some of the other publications in this field also only have small experiments. At the very least it would be nice to have standard deviations on the results and some measure of statistical significance. It would be even nicer to have some visualization of the learned bases/filters, and a bare bones matrix-level very simple description of the algorith. Again, what is impressive here is that such a small network can learn to do this task reasonably well.

Suggestions: 

1. Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities.

2. Clean up the formatting. ""This leads us to the following"" in a line by itself looks strange. Similarly ""Classification ising the learned CW-basis"". I think something went wrong with \itemize in Section 3.1. 

",7
"The paper introduces a new loss function for training a deep neural network which can abstain.
The paper was easy to read, and they had thorough experiments and looked at their model performance in different angles (in existence of structured noise, in existence of unstructured noise and open world detection).  However, I think this paper has some issues which are listed below:


1)  Although there are very few works regarding abstaining in DNN, I would like to see what the paper offers that is not addressed by the existing literature. Right now, in the experiment, there is no comparison to the previous work, and in the introduction, the difference is not clear. I think having an extra related work section regarding comparison would be useful.

2) The experiment section was thorough, and the authors look at the performance of DAC at different angles; however, as far as I understand one of the significant contributions of the paper is to define abstain class during training instead of post-processing (e.g., abstaining on all examples where the network has low confidence). Therefore, I would like to see a better comparison to a network that has soft-max score cut-off rather than plain DNN. In figure 1-d the comparison is not clear since you did not report the coverage. I think it would be great if you can compare either with related work or tune a softmax-score on a validation set and then compare with your method. 

3) There are some typos, misuse of \citet instead of \citep spacing between parenthesis; especially in figures, texts overlap, the spacing is not correct, some figures don’t have a caption, etc.
",6
"This manuscript introduces deep abstaining classifiers (DAC) which modifies the multiclass cross-entropy loss with an abstention loss, which is then applied to perturbed image classification tasks.  The authors report improved classification performance at a number of tasks.

Quality
+ The formulation, while simple, appears justified, and the authors provide guidance on setting/auto-tuning the hyperparameter.
+ Several different settings were used to demonstrate their modification.
- There are no comparisons against other rejection/abstention classifiers or approaches.  Post-learning calibration and abstaining on scores that represent uncertainty are mentioned and it would strengthen the argument of the paper since this is probably the most straightforward altnerative approach, i.e., learn a NN, calibrate predictions, have it abstain where uncertain.
- The comparison against the baseline NN should also include the performance of the baseline NN on the samples where DAC chose not to abstain, so that accuracies between NN and DAC are comparable. E.g. in Table 1, (74.81, coverage 1.000) and (80.09, coverage 0.895) have accuracies based on different test sets (partially overlapping).
- The last set of experiments adds smudging to the out-of-set (open set) classification tasks.  It is somewhat unclear why smudging needs to be combined with this task.

Clarity
- The paper could be better organized with additional signposting to guide the reader. 

Originality
+ Material is original to my knowledge.

Significance
+ The method does appear to work reasonably and the authors provide detail in several use cases.
- However, there are no direct comparison against other abstainers and the perturbations are somewhat artificial.",5
"This paper formulates a new deep method called deep abstaining classifer. Their main idea is to introduce a new modified loss function that utilizes an absention output allowing the DNN to learn when abstention is a better option. The core idea resemble KWIK framework [1], which has been theoretical justified.

Pros:

1. The authors find a new direction for learning with noisy labels. Based on Eq. (1) (the modified loss), the propose \alpha auto-tuning algorithm, which is relatively novel. 

2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.
For example, they conduct experiments on CIFAR-10 and CIFAR-100. Besides, they conduct experiments on open-world detection dataset.

Cons:

We have three questions in the following.

1. Clarity: in Section 3, the author claim real-world data is corrupted in some non-arbitrary manner. However, in practice, it is really hard to reason the corrpution procedure for agnostic noisy dataset like Clothing1M [2]. The authors are encouraged to explain this point more.

2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [3], estimating noise transition matrix [4,5], and explicit and implicit regularization [6]. I would appreciate if the authors can survey and compare more baselines in their paper.

3. Experiment: 
3.1 Baselines: For noisy labels, the author should compare with [7] directly, which is highly related to your work. Namely, designing new loss function can overcome the issue of noisy labels. Without this comparison, the reported result has less impact. Moreover, the authors should add MentorNet [2] as a baseline https://github.com/google/mentornet

3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.

References:

[1] L. Li, M. Littman, and T. Walsh. Knows what it knows: a framework for self-aware learning. In ICML, 2008.

[2] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.

[3] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.

[7] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NIPS, 2018.",5
"The paper tries to make a connection between the functionality of Gaussian noise to adversarial examples. It shows that data augmentation with added Gaussian noise could also improve the model robustness. 
Meanwhile, it shows that in a high dimensional space, even with a small (error) set, its bounding ball of \epsilon l_p distance could be large. It explains why even with a small test error, the model could still be vulnerable to adversarial examples. 

Although the paper has some good intuitions and some nice experiments, I find the main conclusion of this paper not very interesting. 
Although I partially buy the second point about the high dimensional geometry, this is an obvious observation and does not give rise to much meaningful result for the future work. It would be more interesting to see the different geometry structure of robust versus not robust models. 

Meanwhile, though a formal definition of error set is not presented in the paper, it seems the authors are simply dividing the data space to the set where the model gives a correct label, and the “error set” the other way around. However, since the paper is considering a data distribution (q) rather than a dataset, the separation could be more complicated than that. For instance, a noise image should also in your data space, but does it belong to an error set or not? It isn’t necessarily attached to any labels. Or does your model only consider meaningful images? But what if adding noise simply get you out of the space? It’s better to make this concept clearer. 


It is not a surprising result that there is one randomly chosen direction mimicking the performance of adversarial examples as in Figure 2. By “carefully crafted imperceptible noise”, I assume it means choosing one random sample that will change the model output the most. This is exactly a way of choosing an adversarial example. Since even in high dimensional space, out of a lot of random vectors , one could approximate a target (adversarial) direction.

Similar explanations also apply to the training with error part. How much more data do you use for the data augmentation? If you use much more data with Gaussian noise than what your use for adversarial training, it is not surprising at all to get a more robust network, with a similar argument as above.


minor issue: Section 3 should not be an isolated section.
",4
"This paper propose an alternative view for adversarial examples in high dimension spaces by considering the ""error rate"" in a Gaussian distribution centered at each test point. However, as mentioned in the related work, adversarial examples through the lens of isoperimetric inequality is not new to this paper; the implication of adversarial sensitivity by error rate in the test-sample-centered-Gaussian in general non-linear case is rather weak; and the empirical results does not show advantage over simple adversarial training against lp constrained adversarial attacks.

Here are some more detailed comments and feedbacks:

The clarity could be improved by making clear use of notations and define some key terms explicitly:

1. For example, the error rate sometimes refer to the test error, sometimes refer to the error rate under a special distribution centered at a particular test example.

2. Similarly, the distribution q sometimes refer to the original (unknown) input data distribution, but the same notation is also used to refer to this Gaussian distribution centered on each test example. Although the paper says that ""q need not be restricted to the distribution from which the training set was sampled"", it could potentially confuse the reader less if there is a symbol for the ""usual"" test error and a different one for this test-error-under-Gaussian-centered-at-a-particular-test-example.

3. It would be good if the paper could make a formal definition of the problem being studied and explicitly specify the assumptions on the existence of a deterministic target function (concept) and explicitly define the error set E.

4. It seems to assume the input distribution is continuous everywhere but not stated. If for example, the original data is supported on disconnected manifolds separated with low density or even zero-density margins, then the Gaussian distribution centered on test example argument will need to be modified to talk about the intersection of the Gaussian with the data manifold instead. If the paper does decide to make this kind of assumption, some empirical study on the data to verify the fidelity of the assumptions would be great.

5. The paper does not mention how measurement against the error set E. Under the original data distribution, it is natural to measure the error rate with the provided training or test data with labels. However, under each newly formed Gaussian distribution centered at each test point, the labels for the newly sampled examples from this Gaussian is unknown, and since there is no ""ground truth classifier"" for MNIST or CIFAR10 available, it seems impossible to ""calculate"" the true label for those samples, which are needed to calculate the error rate. It is not very clear from the paper how this issue is solved. I'm guess it uses the label from the Gaussian center for all the samples from the Gaussian. While this might be reasonable assumption for Gaussian with tiny variances, it is less clear how reasonable it is for the large variance Gaussian distributions considered in the paper. If this assumption is made, please state it explicitly and empirically or theoretically study how reasonable this assumption is in the regime of variances considered in this paper. If my guessing is wrong, please also explicitly what approach is used to get around this issue.

The followings are some feedbacks on the contents and ideas of the paper:

6. I think one sentence in the text summarize a large part of the paper very well: ""to measure adversarial robustness is to ask whether or not there are any errors in the linf ball, ... and to measure test error in noise is to measure the volume of the error set in the defined noise distribution"". However, this looks like a rather roundabout approach to attack another problem (measuring volume of an unknown set in a very high dimension space) in order to solve the original problem, while the implication is rather weak (a precise implication can only be obtained for linear separating hyper-plane, while for non-linear classifiers it is much less clear).

Note while exact adversarial robustness is NP-hard, volume estimation in high dimension is not easy (if not harder). For cifar-10, the inputs are in dimension 32x32x3 = 3072, the 1,000 samples used in the paper to estimate the volume of a set in this high dimension seem to be quite inaccurate. I would appreciate if variances could be reported in those studies to show the confidence of the estimations. For imagenets, the inputs are in even larger spaces.

Given the difficulty (in terms of sample complexity) to estimate the ""error-in-noise"", it might not be very surprising that the noise augmentation does not show advantages to lp constrained adversarial attacks (comparing to adversarial training).

7. In the conclusion, the paper states ""we proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations"". I believe a formal proof is only given to the case of existence of small adversarial perturbations to examples to the ""noisy examples"" from the Gaussian distribution, and in this case, it is a rather direct corollary from the Gaussian Isoperimetric Inequality. For the more ""practical case"" (in the sense that is more related to the usual notion of adversarial examples) of existence of small adversarial examples, it seems only the case of the linear classifier is formally discussed.

Moreover, I'm a little bit worried some important pieces might be lost and create potentially misleading or seemly strong conclusion. Maybe it would be helpful if a concrete example could be given in the paper that shows the full path from the error-in-noise to existence of adversarial example, by showing all the constants involved. I'm a bit confused here because (in order for the isoperimetric inequality to have favorable bounds?) the Gaussian distributions used in error-in-noise seem to have rather large variance. As mentioned in the paper, the majority of the mass in the Gaussian distribution considered will be in a thin sphere of radius sigma * sqrt(n) centered at the test example x. If sigma = 0.1 and dimension n = 3072 (cifar-10), then the radius is around 5.5 (in l2 distance) which is probably quite far from the test point x (is it?). It is then less clear how ""a large majority of this thin sphere far away from x is epsilon close to the error set"" could tightly imply properties of adversarial robustness of x itself in its close vicinity. Maybe a specific example with all the numerical constants spelled out would help illustrate this.

In summary, I think this paper takes an interesting but roundabout perspective to adversarial robustness, and the implication is weak in the non-linear case. (Potentially because of the weak implication), the suggested approach for defenses by augmenting with noises does not show advantage over adversarial training.",5
"The paper suggests a connection between training with noisy images and the adversarial training. The observation is original to me. It has several cons.

1. The paper is hard to follow because of  too many vague descriptions and unnecessary contrast clauses. Here are some.
 1) The title of section 4: ERRORS IN NOISE IMPLY ADVERSARIAL EXAMPLES FOR NOISY IMAGE.  
 2) ""The discussion of high-dimensional geometry suggests that adversarial examples may actually not be in contradiction to high generalization performance. Indeed, high generalization performance does not mean perfect generalization"" The uncertain tone ""may not"" and the vague statement ""does not mean perfect generalization"" make readers hard to get the solid understanding about what the paper tries to say. 
 3)  ""Adversarial training on small perturbations on CIFAR-10 also improved generalization in noise, and training on noise improved robustness to small perturbations."" 
Many other sentences like the above make the paper not technically sound.

2. It is problematic that the paper uses Euclidean l2 distance to measure the error set and its surface as it is believed that the dataset lives on low-dimensional manifold. Moreover, the adversarial examples often constructed by moving the legal images towards a specific direction rather than adding the Gaussian isotropic noise. 
3. The advocates that using test error in noise as a measure of adversarial robustness  is misleading  as test error in noise has a large number of different combinations: noise type, noise amplitude. One may find one type of test error in noise coinciding with adversarial robustness but  in general it is not a good measure for adversarial robustness because of its varying nature.
4. Several terms are referred without definitions: errors in noise,  adversarial robustness. From the definition of E_epsilon, it should include the interior of E.
",4
"Model Compression is used to reduce the computational and memory complexity of DL models without significantly affecting accuracy. Existing works focused on pruning and regularization based approaches where as this paper explores structured sparsity on RNNs, using predefined compact structures.

They replace matrix-vector multiplications which is the building computational block part of RNNs, with localized group projections (LGP). where LGP divides the input and output vectors into groups where the elements of the output group is computed as a linear combination of those from the corresponding input group. Moreover, they use a permutation matrix or a dense-square matrix to combine outputs across groups. They also combine LGP with low-rank matrix decomposition in order to further reduce the computations. 

Strong points: 

Paper shows how combining the SVD and LGP can reduce computation. In particular in matrix-vector multiplications Ax, low rank reduces the computation by factorizing A into smaller matrices P and Q, while LGP reduces computation by sparsifying these matrices without changing their dimensions.

The paper discussed that their model target labels alone does not generalize well on test data and they showed teacher-student training helps greatly on retaining accuracy. They use the original uncompressed model as the teacher, and train the compressed model(student) to imitate the output distribution of the teacher, in addition to training on the target labels.

Paper is well written and easy to follow. 

This paper would be much stronger if it compared against quantization and latest pruning techniques. 

This paper replace matrix-vector multiplications with the lowRank-LGP, but they only consider RNN networks. I am wondering how it affects other models given the fact that matrix-vector multiplications is the core of many deep learning models. It is not clear why their approach should only work for RNNs.

Table 1 shows the reduction in computation and model size over the original matrix-vector multiplications Ax. I think in this analysis the computation of the those approaches are neglected. For example running the SVD alone on A (n by m matrix) takes O(m^2 n+n^3). That is true that if P and Q are given, then the cost would be n(m+n)/r. However, finding P and Q takes O(m^2 n+n^3) that could be very expensive when matrices are large.

Table 2 only shows the LGB-shuffle resuts. What about the combined SVD and LGP? Similarly in Table 4, what is the performance of the LGB-Dense?

",6
"This paper proposed to use sparse low-ranking compression modules to reduce both computation and memory complexity of RNN models. And the model is trained using knowledge distillation. 
clarity:
I think Fig1a can be improved. Initially I don't understand how the shuffle part works. It will be more clear if the mx1 vectors have the same length and the two (m x1) labels are in the same height.
originality:
The method is quite interesting and should be interesting to many people. 
pros:
1) The method reduces computation and memory complexity at the same time.
2) The result looks impressive.  
cons:
1) Is the training of AntMan models done on GPU or CPU? How is the training time. It seems efficient implementation of the model on GPU can be challenging. 
2) It seems the modules can be used to replace any dense matrix in the neural networks. I'm not sure why it is applied on RNN only.
3) I think another baseline is needed for comparison, a directly designed small RNN model trained using knowledge distillation. In this way, we can see if the sparse low-rank compression provides new values. ",5
"This paper presents a network compression method based on block-diagonal sparse structure for RNN. Two kinds of group mixing methods are discussed. Experiments on PTB and SQUAD have shown its superiority over ISS.
The idea present is interesting, and this paper is easy to follow. However, this paper can be improved from the following perspectives.
1.	The method of balancing the quantity of different parts in knowledge distillation is trivial. It is quite general trick.
2.	Details of experimental setup were unclear. For example, the optimization method used, the block size, and the hyper-parameters were unclear. In addition, it is also unclear how the block diagonal structure was used for the input-to-hidden weight matrix only or all weights. 
3.	In addition, the proposed method was compared with ISS only. Since there are many methods of compressing RNNs, comparison with other competitors (e.g., those presented in Related work) are necessary.  Moreover, more experiments with other tasks in addition to NLP will be better.  
4.	In Table 2, the comparison with ISS seems be unfair. The proposed methods, i.e., LGP-shuffle was obtained based on the distillation. However, ISS was trained without distillation. From Table 3, when Cmse and Ckl were set to zero, the result was much worse. The reviewer was wondering that how does ISS with distillation perform. 
",5
"The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. The authors define timbre transfer as applying parts of the auditory properties of a musical instrument onto another. It replaces the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective. By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.
Some detailed comments are listed as follow,
1 The implementation steps of the proposed method (MoVE) are not clear. Some details are missing, which is hardly reproduced by the other researchers.
2 The experimental settings are not reasonable. The current experimental settings are not matched with the practice environment. 
3 The proposed method can transfer the positive knowledge. However, some negative knowledge information can be also transferred. So how to avoid the negative transferring? 
4 For the model, the optimization details or inferring details are missing, which are important for the proposed model.
",5
"Summary
-------
This paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.
The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres).
The method is evaluated on a collection of individual note-level recordings from 12 instruments, grouped into four families which are used as domains.
The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy.
The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy.
Qualitative investigation demonstrates that the learned representation can approximate several coarse spectral descriptors of the target domains.


High-level comments
-------------------
Overall, this paper is well written, and the various design choices seem well-motivated.

The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.  Specifically, the authors introduced an extra input (control) to encode the pitch class and octave information during encoding.  I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.

While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.  Some of this comes down to incomplete definition of the metrics (see detailed comments below).
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer). The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?  What is the criteria for bolding here?  It would be helpful if these scores could be calibrated in some way, e.g., with reference to
MMD/KNN scores of random partitions of the target domain samples.

Since the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD.  This could provide a slightly stronger version of the comparison in Figure 3, which shows that the overall distribution of spectral centroids is approximated by transfer, but does not demonstrate per-sample correspondence.



Detailed comments
-----------------
At several points in the manuscript, the authors refer to ""invertible"" representations (e.g., page 4, just after eq. 1), but it seems like what they mean is approximately invertible or decodable.  It would be better if the authors were a little more careful in their use of terminology here.

In the definition of the RBF kernel (page 4), why is there a summation? 
 What does this index? How are the kernel bandwidths defined?

How exactly are reconstruction errors calculated: using the NSGT magnitude representation, or after resynthesis in the time domain?
",5
"This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.

Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.

I have several further concerns about this work:

* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.

* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?

* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.

I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.

Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.



Other comments:

* In the introduction, an adversarial criterion is referred to as a ""discriminative objective"", but ""adversarial"" (i.e. featuring a discriminator) and ""discriminative"" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.

* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.

* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.

* Introduction, top of page 2: should read ""does not learn"" instead of ""do not learns"".

* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.

* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.

* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.

* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").

* Section 3.1, ""amounts to optimizing"" instead of ""amounts to optimize""

* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to ""Generating Sentences from a Continuous Space"" by Bowman et al. (2016). This should probably be cited instead.

* ""circle-consistency"" should read ""cycle-consistency"" everywhere.

* MMD losses in the context of GANs have also been studied in the following papers:
- ""Training generative neural networks via Maximum Mean Discrepancy optimization"", Dziugaite et al. (2015)
- ""Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"", Sutherland et al. (2016)
- ""MMD GAN: Towards Deeper Understanding of Moment Matching Network"", Li et al. (2017)

* The model name ""FILM-poi"" is only used in the ""implementation details"" section, it doesn't seem to be referred to anywhere else. Is this a typo?

* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?

* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.",3
"In this paper, the authors propose a method for pruning the convolutional filters. This method first separates the filters into clusters based on similarities defined with both Activation Maximization (AM) and back-propagation gradients. Then pruning is conducted based on the clustering results, and the contribution index that is calculated based on backward-propagation gradients. The proposed method is compared with a baseline method in the experiments. 

I consider the proposed method as novel, since I do not know any filter pruning methods that adopt a similar strategy. Based on my understanding of the proposed method, it might be useful in convolutional filter pruning.

It seems that ""interpretable"" might not be the most proper word to summarize the method. It looks like that the key concept of this paper, including smilarity defined in Equation (3), and the contribution index defined in Equation (7) are not directly relevant to interpretability. Therefore, I would consider change the title of the paper, for example, to ""Convolutional Filter Pruning Based on Functionality "". 

In terms of writing, I have difficulty understanding some details about the method. 

In filter clustering, how can one run k-means based on pair-wise similarity matrix $S_D$? Do you run kernel k-means, or you  apply PCA to $S_D$ before k-means? What is the criterion of choosing the number of clusters in the process of grid search? 

Are filter level pruning, are cluster level pruning and layer level pruning three pruning strategies in the algorithm? It seems to me that you just apply one pruning strategy based on the clusters and contribution index, as shown in Figure 3. 

In the subsubsection ""Cluster Level Pruning"", by ""cluster volume size"", denoted with$length(C^l_c)$, do you mean the size of cluster, i.e., the number of elements in each cluster? This is the first time I see the term ""volume size"". I assume the adaptive pruning rate, denoted by $R_{clt}^{(c,l)}$, is a fraction. But it looks to me that $length(C^l_c)$ is an integer. So how can it be true that $R_{clt}^{(c,l)} = length(C^l_c)$?

In the subsubsection ""Layer Level Pruning"", how is the value of $r$ determined?

The authors have conducted several experiments. These experiments help me understand the advantages of the proposed method. However, in the experiments, the proposed method is compared to only one baseline method. In recent years, a large number of convolutional filter pruning methods have been proposed, as mentioned in the related work section. I am not convinced that the proposed method is one of the best methods among all these existing methods. I would suggest the authors provide more experimental comparison, or explain why comparing with these existing methods is irrelevant. 

Since the proposed method is heuristic, I would also like the authors to illustrate that each component of the method is important, via experiment. How would the performance of the proposed method be affected, if we define the similarity $S_D$ in Equation (3) using only $V$ or $\gamma$, rather than both $V$ and $\gamma$? How would the performance of the proposed method be affected, if we prune randomly, rather than prune based on the contribution index?

In summary, I think the method proposed in this paper might be reasonable. But I do not suggest acceptance, unless the author can improve the writing and include more experimental results.

",4
"This paper proposes a new method to prune filters of convolutional nets based on a metric which consider functional similarities between filters. Those similarities are computed based on Activation Maximization and gradient information. The proposed method is better than L1 and activation-based methods in terms of accuracy after pruning at the same pruning ratio. The visualization of pruned filters (Fig. 3) shows the effectiveness of the method intuitively. 

Overall, the idea in the paper is pretty intuitive and makes sense. The experimental results support the ideas. I think this paper could be accepted if it is improved on the followings:

1. The paper is not very easy to read although the idea is simple.  

The equations could be updated and simplified. For example, I'm not sure if S_D in Eq. (3) wants to take V(F_i^(c,l)) and V(F_k^(c,l)) as the arguments. Layer L_l could be just l. 

Algorithm 1 is hard to read. At least, one line should correspond to one processing. k is not initialized. It is difficult to understand what each variable represents.

The terms used in Section 4.2 may not be very accurate. First of all, I'm not sure if it is a hierarchical method. It does not perform pruning at multiple levels such as filters, clusters, and layers. Rather, it considers information from multiple levels to determine if a filter should be pruned or not. In that sense, everything is filter level pruning and distinguishing (filter|cluster|layer) level pruning just confuse readers. I'd recommend to simplify the section and describe simply what you do.

2. Comparisons with more recent papers

The proposed method was compared with methods from 2015 and 2016. Model compression is an active area of research and there are a lot of papers. Probably, it makes sense to compare the proposed method against some state-of-the-art methods. Especially, it is interesting to see comparisons against methods with direct optimization of loss function such as (Liu et al. ICCV 2017). We might not need to even consider functionality with such methods.

Liu et al. ICCV 2017: https://arxiv.org/pdf/1708.06519.pdf


* Some other thoughts

** If you look at Figure 3 (a), it looks that there are still a lot of redundant filters. Actually, except the last row, I'm not sure if we can visually find any important difference between (a) and (b). I wonder if the most important thing is that you do not prune unique filters (ones which are not clustered with others). It might be interesting to see a result of the L1-based pruning which does not prunes such filters. If you see an interesting result from that, it could add some value to the paper.

** I'd recommend another proofread.",4
"This paper claims to have shown some insights about the filters in a neural network. However, it has little contributions that are justifiable to be published and it missed way too many references.

The visualization of filters is hardly any contribution over [1]. The claim that AM is the best visualization tool is a weird statement given that there are many recent references on visualization, such as [2-4], which the authors all missed.

The proposed filter pruning is a simplistic approach that bears little technical novelty, and there has been zero comparison against any filter pruning approach/network compression approach, among the cited references and numerous references that the paper didn't cite, e.g. [5-6]. In this form I cannot accept this paper.

[1] D Bau, B Zhou, A Khosla, A Oliva, and A Torralba. Network Dissection: Quantifying the Intepretability of Deep Visual Representations. In CVPR 2017.
[2] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh Dhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. ICCV 2017
[3] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff. Top-down Neural Attention by Excitation Backprop. ECCV 2016
[4] Ruth Fong and Andrea Vedaldi. Interpretable Explanations of Black Box Algorithms by Meaningful Perturbation. ICCV 2017
[5] Y. Guo, A. Yao and Y. Chen. Dynamic Network Surgery for Efficient DNNs. NIPS 2016
[6] T.-J. Yang, Y.-H. Chen, V. Sze. Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning. CVPR 2017",3
"The authors present the interesting and important direction in searching better network architectures using the genetic algorithm. Performance on the benchmark datasets seems solid. Moreover, the learned insights described in Section 4.4 would be very helpful for many researchers.

However, the overall paper needs to be polished more. There are two many typos and errors that imply that the manuscript is not carefully polished. Explanations about some terms like growth rate, population, etc. are necessary for broader audience. 

More importantly, while some of step jumps in Figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in Section 4.2. Thee clear explanation about that phenomena is required.

* Details
- Please represent the blocks (e.g. 1*1conv) better. Current representation is quite confusing to read. Maybe proper spacing and different style of fonts may help.
- In Page 5, ""C_{m}ax"" is a typo. It should be ""C_{max}"".
- Regarding the C_max, does sum(C_max) represent (D * W)^2 where D is the total depth and W is the total indicies in each layer? If so, specifying it will help. Otherwise, please explain its meaning clearly.
- In Figure 4(a), it would be better if we reuse M_{d,w} notation instead of Module {d_w}.
- Please briefly explain or provide references to the terms like ""growth rate"", ""population"", and ""individuals"". 
- Different mutations may favor different hyper-parameters. How the authors control the hyperparameters other than the number of epochs will be useful to know.
- Even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated. They need to be presented to verify the hypothesis that the authors claim. 
",5
"The problem is of increasing practical interest and importance. 

The ablation study on the contribution and effects of each constituent  part is a strong part of the experiment section and the paper. 

One major concern is about the novelty of the work. There are many similar works under the umbrella of Neural Architecture search who are trying to connect different building blocks (modules) to build larger CNNs. One example that explicitly makes sparse connections between them is [1]. Other examples of very similar works are [2,3,4].

The presentation of the paper can be improved a lot. In the current setup it’s very similar to a collection of ideas and tricks and techniques combined together. 

There are some typos and errors in the writing. A thorough grammatical  proofreading is necessary. 

In conclusion there is a claim about tackling overfitting. It’s not well supported or discussed in the experiments. 

[1] Shazeer, Noam, et al. ""Outrageously large neural networks: The sparsely-gated mixture-of-experts layer."" arXiv preprint arXiv:1701.06538 (2017).
[2] Xie, Lingxi, and Alan L. Yuille. ""Genetic CNN."" ICCV. 2017.
[3] Real, Esteban, et al. ""Large-scale evolution of image classifiers."" arXiv preprint arXiv:1703.01041 (2017).
[4] Liu, Hanxiao, et al. ""Hierarchical representations for efficient architecture search."" arXiv preprint arXiv:1711.00436 (2017).
",5
"The authors bridge two components (density of CNNs and sparsity structures) by proposing a new network structure with locally dense yet externally sparse connections. 

+ Combination of being dense and sparse is an interesting area.
- Although experiment results demonstrate evolving sparse connection could reach competitive results, it would be interesting to show how separating a network into several small networks is useful, for example, interpretablity of deep neural network. There is an interesting work: ""Using deep learning to model the hierarchical structure and function of a cell"" https://www.nature.com/articles/nmeth.4627
",6
"This paper evaluates language models for tasks that involve ""commonsense knowledge"" such as the Winograd Schema Challenge (WSC), Pronoun Disambiguation Problems (PDP), and commonsense knowledge base completion (KBC). 

Pros:

The approach is relatively simple in that it boils down to just applying language models. 

The results outperform prior work, in some cases by pretty large margins. 

The language models are quite large and it appears that this is the first time that large-scale language models have been applied seriously to the Winograd Schema Challenge (rather than, say, to the NLI version of it in GLUE, to which it is hard to compare these results). 

Some of the additional and ablation experiments are interesting. 


Cons:

While this paper has some nice results, there are some aspects of it that concern me, specifically related to hyperparameter tuning and experimental rigor:

There are three methods given for using an LM to make a prediction: full, full-normalized, and partial. For PDP, full (or perhaps full-normalized?) works best, while for WSC, partial works best. The differences among methods, at least for WSC, are quite large: from 2% to 10% based on Figure 3. I don't see a numerical comparison for PDP, so I'm not sure how these methods compare on it. Since the datasets are so small, there is no train/dev/test split, so how were these decisions made? They seem to be oracle decisions. This is concerning to me, as there is not much explanation given for why one method is better than another method. 

My guess is that the reason why partial works better than full for WSC is because the WSC sentences were constructed such that the words up to and including the ambiguous pronoun were written such that it would be difficult to identify the antecedent of the pronoun. The rest of the sentence would be needed to identify the antecedent. I'll assume for this discussion that the sentence can be divided into three parts x, y, and z, where x is the part before the pronoun, y is the phrase that replaces the pronoun, and z is the part after the pronoun. Then p(z|xy), which is partial scoring, corresponds to p(xyz)/p(xy), which can be viewed as ""discounting"" or ""normalizing for"" the probability of putting y in place of the pronoun given the context x. For WSC, I think one of the goals in writing the instances is to make the ""true"" p(xy) approximately equal for both values of y. The language model will not naturally have this be the case (i.e., that p(xy) is the same for both antecedents), so dividing by p(xy) causes the resulting partial score to account for the natural differences in p(xy) for different antecedents. This could be explored empirically. For example, the authors could compute p(xy) for both alternatives for all PDP and WSC instances and see if the difference (|p(xy_1) - p(xy_2)|, where y_1 and y_2 are the two alternatives) is systematically different between WSC and PDP. Or one could see if p(xy) is greater for the antecedent that is closer to the pronoun position or if it is triggered by some other effects. It could be the case that the PDP instances are not as carefully controlled as the WSC instances and therefore some of the PDP instances may exhibit the situation where the prediction can be made partially based on p(xy). The paper does not give an explanation for why full scoring works better for PDP and chalks it up to noise from the small size of PDP, but I wonder if there could be a good reason for the difference.

The results on KBC are positive, but not super convincing. The method involves fine-tuning pretrained LMs on the KBC training data, the same training data used by prior work. The new result is better than prior work (compared to the ""Factorized"", the finetuned LM is 2.1% better on the full test set, and 0.3% better on the novelty-based test set), but also uses a lot more unlabeled data than the prior work (if I understand the prior work correctly). It would be more impressive if the LM could use far fewer than the 100K examples for fine-tuning. Also, when discussing that task, the paper says: ""During evaluation, a threshold is used to classify low-perplexity and high-perlexity instances as fact and non-fact."" How was this threshold chosen?

I also have a concern about the framing of the overall significance of the results. While the results show roughly a 9% absolute improvement on WSC, the accuracies are still far from human performance on the WSC task. The accuracy for the best pretrained ensemble of LMs in this paper is 61.5%, and when training on WSC-oriented training data, it goes up to nearly 64%. But humans get at least 92% on this task. This doesn't mean that the results shouldn't be taken seriously, but it does suggest that we still have a long way to go and that language models may only be learning a fraction of what is needed to solve this task. This, along with my concerns about the experimental rigor expressed above, limits the potential impact of the paper.


Minor issues/questions:

In Sec. 3.1: Why refer to the full scoring strategy as ""naive""? Is there some non-empirical reason to choose partial over full?

The use of SQuAD for language modeling data was surprising to me. Why SQuAD? It's only 536 articles from Wikipedia. Why not use all of Wikipedia? Or, if you're concerned about some of the overly-specific language in more domain-specific Wikipedia articles, then you could restrict the dataset to be the 100K most frequently-visited Wikipedia articles or something like that. 

I think it would be helpful to give an example from PDP-60.

Sec. 5.1: How is F_1(n) defined?  I also don't see how a perfect score is 1.0, but maybe it's because I don't understand how F_1(n) is defined.

Sec. 6.1: Why would t range from 1 to n for full scoring? Positions before k are unchanged, right? So q_1 through q_{k-1} would be the same for both, right?

In the final example in Figure 2, I don't understand why ""yelled at"" is the keyword, rather than ""upset"". Who determined the special keywords?

I was confused about the keyword detection/retrieval evaluation. How are multi-word keywords handled, like the final example in Figure 2? The caption of Table 5 mentions ""retrieving top-2 tokens"". But after getting the top 2 tokens, how is the evaluation done?

Sec. 6.3 says: ""This normalization indeed fixes full scoring in 9 out of 10 tested LMs on PDP-60."" Are those results reported somewhere in the paper? Was that normalization used for the results in Table 2?

Sec. 6.3 says: ""On WSC-273, the observation is again confirmed as partial scoring, which ignores c [the candidate] altogether, strongly outperforms the other two scorings in all cases"" -- What is meant by ""which ignores c altogether""?  c is still being conditioned on and it must not be ignored or else partial scoring would be meaningless (because c is the only part that differs between the two options). 


Typos and minor issues:

Be consistent about ""common sense"" vs. ""commonsense"".

Be consistent about ""Deepnet"" vs. ""DeepNet"" (Tables 2-3).

Sec. 1:
""even best"" --> ""even the best""
""such as Winograd"" --> ""such as the Winograd""
""a few hundreds"" --> ""a few hundred""
""this type of questions"" --> ""this type of question""
""does not present"" --> ""is not present""
""non-facts tuples"" --> ""non-fact tuples""

Sec. 2:
""solving Winograd"" --> ""solving the Winograd""
""Store Cloze"" --> ""Story Cloze""
""constructed by human"" --> ""constructed by humans""

Sec. 4:
What is ""LM-1-Billion""?
Why SQuAD?
""Another test set in included"" --> ""Another test set is included""

Sec. 5.2:
Check margin in loss_new

""high-perlexity"" --> ""high-perplexity""

Sec. 6:
Figure 2 caption: ""keyword appear"" --> ""keyword appears""

Sec. 6.2:
""for correct answer"" --> ""for the correct answer""

Appendix A:
""acitvation"" --> ""activation""
Appendix B:
Figure 4 caption: ""is of"" --> ""is""
The right part of Figure 4 has some odd spacing and hyphenation.
",5
"This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.

This paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. 

The paper also suffers from methodological problems:
-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. 
-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.

Finally, the paper suffers clarity issues. 
-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). 
-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).
-- The authors do not address hyper-parameter tuning. 
-- What is the gold standard for the ""special word retrieved"" data? how is it computed?


Other comments: 
-- Page 2: ""In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning."": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.
-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. 
-- Page 5: how is F1 defined?

Minor comments: 
-- Page 2: "" ... despite the small training data size (100K instances)."": 100K is typically not considered a small training set (for most tasks at least)
-- Page 5: ""... most of the constituent documents ..."": was this validated in any way? how?
-- The word ""extremely"" is used throughout the paper without justification in most cases.


Typos and such:
page 1: ""... a relevant knowledge to the above Winograd Schema example, **does** not present ... "": should be ""is""
page 5: ""In the previous sections, we ***show*** ..."": showed
page 7: ""For example, with the ***test*** ..."": ""test instance""
",4
"This paper uses a language model for scoring of question answer candidates in the Winograd schema dataset, as well as introduces a heuristic for scoring common-sense knowledge triples.

Quality:
Pros: The paper shows improvements over previous papers for two tasks related to common-sense knowledge. They both mainly utilise simple language models, which is impressive. The second one uses an additional supervised collaborative filtering-style model. The authors further perform a detailed error analysis and ablation study.
Cons: The paper isn't very well-written. It contains quite a few spelling mistakes and is unclear in places. The Winograd Scheme Challenge isn't a very interesting dataset and isn't widely used. In fact, this is evidenced by the fact that most cited papers on that datasets are preprints and technical reports.

Clarity:
The paper is confusing in places. It should really be introduced in the abstract what is meant by ""common sense"". Details of the language model are missing. It is only clear towards the end of the introduction that the paper explores two loosely-related tasks using language models.

Originality:
Pros: The suggested model outperforms others on two datasets.
Cons: The suggested models are novel in themselves. As the authors also acknowledge, using language models for scoring candidates is a simple baseline in multiple-choice QA and merely hasn't been tested for the Winograd schema dataset.

Significance:
Other researchers within the common-sense reasoning community might cite this paper. The significance of this paper to a larger representation learning audience is rather small.",4
"This paper first proposes an intuition that when training, neural network weights tend to be correlated. It then suggests to make this correlation more specific, and does so using a heuristic algorithm based on approximate empirical Bayes.

The paper is generally well written, in the sense that everything that is done is clearly stated. However why things are done the way they are is less clear.

-	The correlation intuition given in the introduction isn’t quite right (if the training does it anyway, why try to push it even more?). The better way to think about the benefit of correlations is to say that some weak form of weight sharing may help.

-	The approximate notion of empirical Bayes is not well justified.  The arguments given are all about concentration and essentially unimodality. The successive MAP perspective is interesting, but again not clear why we the joint maximization of (4) won’t place us far from the smoothed version of (1).

-	Not only is the AEB principle a heuristic, but the implementation via Algorithm 1 is itself a heuristic to this heuristic. This is because the optimization of the model parameters still remains approximate (due to the local search aspect), even if the optimization of the hyperparameters is shown to be exactly solvable. In the end we end up with a stationary point of the AEB objective, for which we have even less theoretical insight, even if the empirical evidence is promising.

-	A couple of comments about algorithm 1. First, the \theta notation is flipped to refer to the model parameters, instead of the hyperparameters, and it should be fixed. Second, the role of the constant u and v is a bit unclear. On the surface, they provide extra regularization to the hyperparameters. But then, how do we choose them?

-	The experimental results are a bit oversold. First, we have SGD methods that do thrive in smaller batch size regimes, and it’s disingenuous to handicap them with larger batch sizes, when they were performing comparable at smaller ones. It *is* interesting that AED makes the learning batch-size insensitive, and I wish that was elaborated more, to see if it’s a prevalent property in other data sets too. Also, the authors define better local optima is by saying that they reach a lower value of training loss. This is claimed and shown. Usually we think of them as those leading to better generalization. This is claimed, but not shown that these are indeed those that generalize better. Figure 3 (which I assume shows training cross-entropy) shows one outcome among many (the many that are averaged in the other plots), and it could very well be that this outcome did not generalize as well.

Despite these shortcomings, I believe this paper is another welcome push to introducing empirical Bayes ideas into neural networks (though it’s not the first), and the empirical evidence seems to indicate that there is indeed something there to investigate further, so I give it a weak accept.",6
"Summary: The submission proposes a method to learn Kronecker factors of the covariance of a matrix-variate normal distribution over neural network weights. Given a setting of the neural network weights, the Kronecker factors can be found in closed form by solving a convex optimization problem. 

Strengths:
+ The paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical Bayes setup.

Weaknesses:
- The novelty of the method is overstated.
- The method is claimed to be efficient, but each iteration requires an inner loop of solving a MAP problem (via gradient descent on the negative log likelihood with an extra regularization term), which is at least as expensive as a standard training run.
- The submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.
- The experimental evaluation is not strong.

Major comments:
- The use of empirical Bayes is not novel in the context of neural networks despite the submission's claim that ""Existing studies on
parametric empirical Bayes methods focus on the setting where the likelihood function and the prior are assumed to have specific forms"" (pg. 1). In particular, see e.g., https://papers.nips.cc/paper/6864-an-empirical-bayes-approach-to-optimizing-machine-learning-algorithms.pdf, https://arxiv.org/abs/1801.08930, https://arxiv.org/abs/1807.01613 for the use of non-conjugate likelihoods in empirical Bayes.
- The cited motivation for the use of a matrix-variate normal prior over the weights of a neural network is weak. In particular, one iteration of credit assignment via backpropagation in a one-layer neural network does not adequately describe the complex interactions between parameters of a nonlinear model over the course of an optimization procedure such as the one used in line 2 of Algorithm 1. In addition, a learned prior can be used to introduce additional correlations between parameters, so it is strange to describe the learned prior as ""capturing correlations"" resulting from a single weight update.
- The submission lacks clarity on the assumptions entailed by using the proposed methodology, namely that the Kronecker factors are assumed positive definite. For instance, the logdet function is defined for positive definite (PD) matrices, the results in the paragraph titled ""Approximate Volume Minimization"" hold only for PD matrices, and the InvThresholding procedure is valid for only the same class. It should additionally be noted that this assumption is *not* required for a Kronecker factorization to be defined.
- The submission makes heavy use of Kronecker factorization, but neglects to cite works that use a similar factorization of the covariance matrices for neural network applications (e.g., https://arxiv.org/abs/1503.05671, https://arxiv.org/abs/1712.02390). Furthermore, the method bears a strong similarity to https://arxiv.org/abs/1506.02117 in learning a Kronecker-factored covariance structure between parameters of a neural network. Can the authors comment on the similarities and differences?
- Results are reported on a simple regression task (SARCOS) and multiclass classification problems (MNIST & CIFAR10) using a neural network with a single hidden layer. Moreover, ""in all the experiments, the AEB algorithm is performed on the softmax layer"" (pg. 7) and the justification for this in the ""Ablations"" section is opaque to me. Was a similar restriction used for L2 weight decay regularization? I can't interpret how thorough the evaluation is without such details. It is also not clear that the approach is extensible to more complex architectures, or that there would be a significant empirical benefit if this is done. 
- Figure 5 does not really exhibit interesting learned structure in the correlation matrix. Why not plot a visualization of the learned prior, rather than the weights?

Minor comments:
- The submission needs to be checked for English grammar and style.
- abstract: ""Learning deep neural networks could be understood as the combination of representation learning and learning halfspaces."" This is unclear.
- pg. 2: ""Empirically, we show that the proposed method helps the network
converge to better local optima that also generalize better...""  What is a ""better"" local optimum?
- Section 3.1 describes empirical Bayes with point estimates. Please make it clearer that this methodology is not itself a contribution of paper by citing prior work.
- pg. 5: ""Alg. 1 terminates when a stationary point is found."" What exactly is the stopping criterion?
- pg. 6: The labels in Figure 2 are extremely small. Moreover, please keep the y-axis range constant.
- pg. 6, Figure 2 caption: ""AEB improves generalization under both minibatch settings and is most beneficial when training set is small."" Do the CIFAR10 results not show the opposite effect, that the regularization is most beneficial when the training set is large?
- pg. 7: ""Batch Normalization suffers from large batch size in CIFAR10"" weird wording
- pg. 8: ""One future direction is to develop a better approximate solution to optimize the two covariance matrices from the marginal log-likelihood function."" This is unclear.",3
"Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.

The current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.

 Strengths:

 This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.

 Minor comments:

 * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.
 * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.",7
"Summary: 
This paper presents a new interpretable prediction framework which combines rule based learning, prototype learning, and NNs. The method is particularly applicable to longitudinal data. While the idea of bringing together rules, prototypes, and NNs is definitely novel, the method itself has some unclear design choices. Furthermore, the experiments seem pretty rudimentary and the presentation can be significantly improved. 

Detailed Comments: 
1. In Section 2, the authors seem to define rule list as a set of independent if-then rules. Please note that rule lists have an ""else if"" clause which creates a dependency between the rules. Please refer to ""Interpretable decision sets"" by Lakkaraju et. al. for understanding the differences between rule lists and rule sets. 
2. Section 3.1 is quite confusing. It would be good to give an intuition as to how the various pieces are being combined and in why it makes sense to combine them in this way. The data reweighting process seems a bit adhoc to me. What other choices for reweighting were considered?
3. I would strongly encourage the authors to carry out at least a simple user study before claiming that the proposed method is more interpretable than existing rule lists. Adding both prototypes and rules, in fact, adds to the cognitive burden of an end user - it would be interesting to see when and how having both prototypes and rules will help an end user. 

Pros:
1. First approach to combine NNs, rule learning, prototype learning
2. Provides an interpretable method for predictions on longitudinal medical data
3. Experimental results seem to suggest that the proposed approach is resulting in accurate and interpretable models.

Cons:
1. The various pieces in the method (rule learning, prototype, NNs, data reweighting) seem to be somewhat haphazardly connected. Section 3.1 does not give me a good idea about how the different pieces are resulting in an accurate and interpretable model
2. The paper makes claims such as ""Experimental results also show the resulting interpretation
of PEARL is simpler than the standard rule learning."" without actually doing any significant user studies. Furthermore, any other synthetic data experiments which could demonstrate the various facets of accuracy-interpretability tradeoffs are missing
3. The presentation of the paper is quite unclear. See detailed comments above. 
",5
"This paper aims at tackling the lack of interpretability of deep learning models, which is especially problematic in a healthcare setting --the focus of this research paper. Specifically, the authors propose Prototype lEArning via Rule Lists (PEARL), which combines rule learning and prototype learning to achieve more accurate classification and better predictive power than either method independently and which the authors claim makes the task of interpretability simpler.  
The authors present an interesting and novel architecture in PEARL. Combining the two approaches of rule lists and prototype learning. However, my main concern with the paper and with the architecture in general is the lack of clarity upfront regarding what the authors perceive as the criteria for interpretability. This seems to be one of the chief aims of the paper, however, the authors don’t reach this point until Section 4 of the paper. Given that this is one of the main strengths of the paper as proposed by the authors, this needs to be given more prominence and also needs to be made more explicit what the authors mean by this. The authors define interpretability as measured by the number of rules and number of protoypes identified by a particular model, without, providing an argument, justification, or a citation of previous work which justifies these criterion. Especially since this is one of the main points of the paper, this needs to be better argued and the authors should either elaborate on this point, or restrain on making claims that these models are more interpretable.
The model architecture of Section 3.1 was quite obscure both from the intuitive and implementation level. It’s not clear how the different modules (prototype learning, rule lists) link together in practice, nor how these come together to create an interpretable model.
Generally, the paper is quite poorly structured and there were several grammatical errors which made the paper quite hard to follow. Although the problems articulated are important, the paper did not do sufficient justice to addressing these problems. 
",3
"

Review Summary
--------------
The paper presents a combination of rule lists, prototypes, and deep representation learning to fit classifiers that are said to be simultaneously ""accurate"" and ""interpretable"". While the topic is interesting and the direction seems novel, I don't think the work is quite polished or competitive enough to be accepted without significant revision. The major issues include non-competitive evaluation of what ""interpretability"" means, ROC AUC numbers that are indistinguishable from standard deep learning (RCNN) pipelines that use many fewer parameters, and many unjustified choices inside the method itself. The paper itself could also benefit from revision to improve flow and introduce technical ideas to be more accessible to readers.


Paper Summary
-------------
The paper presents a new method called ""PEARL"" (Prototype Learning via Rule Lists), which produces a rule list, a set of prototypes, and a deep feed-forward neural network that can embed any input data into a low-dimensional feature space. The primary intended application is classifying subjects into a finite set of possible disorders given longitudinal electronic health records with categorical features observed at T irregular time intervals. 

The paper suggests learning a representation for each subject's data by feeding the EHR time series into a recurrent convolutional NN. The input data is a 2 x T array, with one row representing observed data and second row giving time delay between successive observations. The vector output of an initial convolutional RNN is then fed into a highway network to produce a final vector denoted ""h"". 

Given an encoder to produce feature vectors, and a fixed rule list learned from data itself, the paper suggests obtaining a prototype for each rule by computing the average vector of all data that matches the given rule. The quality of these prototypes and related neural networks (for computing features and predicting labels from features) is then assessed via their loss function in Eq. 1: a weighted combination of how well the prototypes match the learned embeddings (distance to closest prototype) and how well the classifier predicts labels.  The core idea is that the embedding is learned to classify well while creating a latent space that looks like the prototypes of the rule list.

After training an embedding and NN classifier on a fixed rule list, it seems the data is reweighted according to some heuristic procedure to obtain better properties, then a new rule list is trained and the process repeats again. (I admit the reweight procedure's purpose was never clear to me).

Experiments are done on a proprietary heart failure EHR dataset and on a subset of MIMIC data. 

Strengths
---------
* Seems original: I'm unaware of any other method connecting rule lists AND prototypes AND NNs
* Neat applications to healthcare

Limitations
-----------
* Interpretability evaluation seems weak: no human subject experiments, no quantiative metrics, unclear if rule-lists shown is an apples-to-apples comparison
* Prototypes themselves never evaluated 
* Many design choices inside method not justified with experiments -- why highway networks + RCNNs?

Major Issues with Method
------------------------

## M1: Not clear that AUC difference between PEARL and baselines is significant

The major issue is that the presented approach does not seem significantly different in predictive performance than the baseline Recurrent CNN. Comparing ROC AUC, we have PEARL's 0.688 to RCNN'S 0.682 with stddev of 0.009 on the proprietary heart failure dataset, and PEARL's 0.769 to RCNN's 0.766 with stddev of 0.009. When AUCs match this closely, I struggle to believe one model is definitively better, especially given that the RCNN has 2x *fewer* parameters (8.4k to 18.4k). 

If the counterargument is that the resulting ""deep model"" is not ""interpretable"", one should at least compare to a post-processing step where the decision boundary of the RCNN is the reference to which a rule list or decision tree is trained.

## M2: Interpretability evaluation not clear.

Isn't the maximum number of rules set in advance? 

Additionally, prototypes are a key part of this work, but the learned prototypes are not evaluated at all in any figure (except to track avg. distance from prototype while training). If prototypes are so central to this work, I would like to see a formal evaluation of whether the learned prototypes are indeed better (in terms of distance, or inspection of values by an expert, or something else) than alternatives like Li et al.

## M3: Missing a good synthetic/small dataset experiment

Neither of the presented data tasks is particularly easy to understand for non-experts. I'd suggest creating an additional experiment where the audience of ML readers is likely to easily grasp whether a set of rule lists is ""good"" for the problem at hand... maybe create your own synthetic task or a UCI dataset or something, or even use the stop-and-frisk crime dataset from the Angelino et al. 2018 paper. Then you can compare against just a few relevant baselines (rule lists only or prototypes only). I think a better illustrative experiment will help readers grasp differences between methods. 

## M4: How crucial is feature selection?

In each iteration, Algo. 1 performs feature selection before learning rules. Are any other baselines (trees, rule lists) allowed feature selection before the classifier is learned? What would happen to PEARL without feature selection? What method is used for selection? (A search of the document only has 'feature selection' occur once, in the Alg. itself, so it seems explanation is missing).

## M5: Why are multiple algorithm iterations needed?

Won't steps 3 and 4 of Alg. 1 result in the same rules every time? It's not clear then why on subsequent iterations the algorithm would improve. Perhaps it's just the reweighting of data that causes these steps to change?

Minor issues
------------

## Loss function notation confusing

Doesn't the rule list classifier s_R take the data itself X? Not the learned embedding h(X)? Please fix or clarify Eq. 1. I think you might clarify notation by just writing yhat(h(X)) if you mean the predicted label of some example as done by your NNs. Using ""R"" makes folks think the rule list is involved.

## Not clear why per-example reweighting is required

None of the experiments assess why per-example reweighting (lines 6-9 of Algo. 1) is required. Readers would like to see a comparison of performance with and without this step.

## Not clear or justified when ""averaged"" prototypes are acceptable

Are your ""averaged"" prototypes guaranteed to satisfy the rule they represent? Is taking the average of vectors that match a rule always guaranteed to also match the rule? I don't think this is necessarily true. Consider a rule that says ""if x[0] == 0 or x[1] == 0, then ___"".  Suppose the only matching vectors are x_A = [0 1] and x_B = [1 0]. The average vector is [0.5 0.5] which doesn't work.

## Several different measures of distance used without careful justification 

Why use two different distances -- Euclidean distance to assess distance to prototypes for prototype assignment, and then cosine similarity when deciding which examples to upweight or downweight? Why not just use Euclidean distance for both (appropriately transformed to a similarity)?

Comments on Presentation
------------------------
Overall I think every section of the paper needs significant revision to improve a reader's ability to understand main ideas. Notation could be introduced slowly (explain purpose and dimension of every variable), assumptions could be clearly stated (e.g. each individual rule can have ANDs but not ORs), and design choices justified. You might try the test of giving the paper to a colleague and having them explain back the ideas of each section to you... currently I do not believe this version passes this test.

The introduction claims that ""clinicians are often unwilling to accept algorithm recommendations without clarity as to the underlying reasoning"", but I would be careful in blindly asserting this without evidence. For a nice argument about avoiding blind assumptions about what doctor's will and won't accept, see Lipton's 2017 paper ""The Doctor Just Won't Accept That"" (https://arxiv.org/abs/1711.08037)

Additionally, the authors should clarify more precisely what definition of interpretability is needed for their applications. Is it simplicity? Is it conceptual alignment with known medical facts? Is it the ability to transparently list the rules in plain English?

Line-by-line details
--------------------

## Sec. 2

When introducing p_j, should clarify this this is one prototype vector of many.

When defining p_j = f_j(X), can you clarify what dimensionality p_j has? Is it always the same size as each example's data vector x_i?


## Sec. 3

Fig. 2: I don't find this figure very easy-to-understand. It's clear that after embedding raw features to a new space, the learned rules are *different*, but it's not clear they are *better*.  None of the illustrated rules perfectly segments the different colors, for example. I guess the point is all the red dots are within one rule? But they aren't alone (there are blue and orange dots too), so it's still not clear this would be a better classifier.

For EHR datasets, are you assuming that events are always categorical? And that outcomes ""y"" are always discrete (one-of-L) variables? Or could y be real-valued?

Eq. 1: You should make notation clearly indicate which terms depend on \theta. Currently it seems that nothing is a function of \theta.

Eq. 1: Do you also find the prototype set P that minimizes this objective? Or is there another way to obtain P given parameters \theta? This is confusing just from reading the eqn.

What size is the learned representation h(X)? Is it a vector?

Eq. 6: Do you really need a ""network"" to compute the distance to each of the K prototypes? Can't you just compute these distances directly?

## Sec 4

""Mac OS 1.4"" : Do you mean Mac OS version 10.4? Not clear this is relevant.

4.3 Case Study: How do I read these rules? Is this rule applied only if ALL conditions are true? or if any individual one is true (""or"")? This is unclear.",4
"The authors present a framework for creating meaning-preserving adversarial examples, and give two methods for such attacks. One is based on k-nn in the word embedding space, and another is based on character swapping. The authors further study a series of automatic metrics for determining whether semantic meaning in the input space has changed, and find that the chrF method produces scores most correlated with human judgement of semantic meaning. The authors finally give an evaluation of the two methods.


Positive:
- The authors give a framework with the explicit goal of preserving meaning in attacks.

Negative:
- Unclear novelty: previous work also gives the goal of preserving input meaning in attacks, even if the attacks themselves do not preserve meaning effectively (ie Zhao et al) 
- Unclear attack effectiveness: The chrF scores for CharSwap and kNN methods have higher chrF scores than the ""unconstrained"" method, but it is unclear what this means in context. Similarly, the RDchrF scores show that the average output changes in meaning by some amount, but the authors do not show in context what this really means in terms of meaning. 

Details of negatives:
Unclear attack effectiveness: 
- Using chrF score as a proxy for human judgement is unmotivated. There is little analysis of the distribution of chrF scores compared to human judgement - the only analysis given is that a) there is a .586 correlation on French and .497 correlation on English, and b) that :""90% of French sentence pairs to which humans gave a score of 4 or 5 in semantic similarity have a chrF > 78"". It would be good to plot the distribution of chrF score vs human judgement, so that the reader is able to tell what the chrF scores really mean in context here - a correlation score of approximately .5 is difficult to interpret.
- The chrF/RDchrF scores in the source and target spaces (respectively) as they relate to ""meaning-preservingness"" suffer from uninterpretability as a reader, both because of the point above and also because there are few examples of adversarial examples with their chrF/RDchrF scores given (only two).",4
"The authors provide a natural definition of adversarial examples for natural language transduction (meaning-preserving on source side while meaning-destroying on target side) and a human judgment task to measure it. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. Finally, they show that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting.

Overall this is a strong paper. It is well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. While the studied attack methods are fairly primitive, the empirical results are still interesting.

Comments
---------------
I wish the authors would include experiments with CharSwap where OOV is not forced as I'm not sure the assumption that OOV is more meaning-destroying in the target side is necessarily true (one could also argue that since the models are already trained with OOV words, they may be more robust to OOV words than in-vocabulary words in the wrong context).

It would be nice to add correlation for each type of constraint as well to Table 2. The result would be even stronger if the experiment was replicated in the opposite direction or for another language pair as well.

I don't understand why the adversarial output in the second example in table 4 has a RDchrF of zero (the word July is completely dropped).

From Table 6 it looks like random sampling is actually slightly better than adversarial training in terms of robustness to CharSwap attacks in the Transformer model. Moreover, the benefit of adversarial rather than random sampling is quite small in the LSTM model as well. This could be made more clear in the text.

It would be interesting to see how adversarial training with the CharSwap method fares against the unconstrained and kNN attacks in table 6.
",6
"The paper is about meaning-preserving adversarial perturbations in the context of Seq2Seq models. The paper proposes two ways of achieving that: (a) kNN - substituting word with nearest neighbors from the word embedding space, and (b) character swapping. It's debatable if character swapping is really meaning preserving since a lot of typos can really change the word. Similarly a case can be made about kNNs as well. But even if these are the best approximations we have, I have some major issues about the novelty of the work. Firstly, while the authors are trying to pitch the work in a new mold, there's major overlap with Belinkov and Bisk, 2018. The use of character swapping as an adversarial perturbation/noise and the subsequent benefits of training with adversarial noise have already been shown in Belinkov and Bisk, 2018. Secondly, the models tested are operating at word-level whereas most of the state-of-the-art systems nowadays are all using subword-level vocabularies. The character swap method presented would need to be adapted and some of the takeaways from results are hence less relevant for the current SOTA models. Coming to positives, the two real contributions for me are: (a) the result that chrF correlates better with human judgement, and (b) the measurement of adversarial perturbation's success measured via a sum that includes relative decrease in target score and the similarity of source sentence with the perturbed version. However, these are minor contributions and not enough to cover up the major flaws that I discussed above. 

Some other minor issues:
(a) Table 1: The first example has the CharSwap row missing the word ""faire"".
(b) Section 3.1.1: ""d"" is not defined when discussing time complexity. 
(c) No separate section 3.1.2 required as it can be merged with 3.1.1 and would be more easy to understand without confusing the readers that there's some context change.
(d) Table 6 entries are not clearly defined. How is robustness measured?

 ",4
"Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. Given source and target metrics for measuring similarity, an attack is deemed successful if the source difference is smaller than the relative decrease in target similarity to the reference. A first experiment measures correlation with human judgements of similarity between original and perturbed sentences, and concludes that chrF is better than BLEU and METEOR for this purpose. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. In comparisons on three language pairs from IWSLT,  the constrained attacks are found to preserve meaning and yield more successful attacks according to the current framework. The Transformer architecture was also found to deal less well with attacks under the 10-closest embedding constraint. Finally, adversarial training with the character-swap constraint confers some robustness to this attack, without degrading performance on normal text.

I think it is a good idea to formalize a method for carrying out and assessing adversarial attacks, but the framework proposed here seems too narrow, as it excludes adversarial inputs that are sensible but not a close perturbation of an existing source/reference pair, or ones that contain varying amounts of noise. It is more difficult to measure output quality for such attacks, but that doesn’t seem like a good reason for excluding them from what is intended to be a general framework. Note also that “more difficult” doesn’t mean impossible, since good attacks can produce severely degraded output that is relatively easy to detect.

I found some of the methodology questionable. Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. Unnecessary because knowing the class of perturbation already gives you a lot of information about semantic distance. Unlikely to succeed because automatic metrics are too coarse to reliably distinguish among different perturbations. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. The experiments that support the viability of automatic metrics in 4.2 do so by measuring correlation with human judgment when the number of perturbed tokens varies from 1 to 3. I think the good correlation is likely due to the metrics being able to detect that, eg, changing 3 tokens makes things worse than changing only one. To be convincing, the experiments would have to be repeated with number of perturbations fixed at 3, to match the setting in the remaining experiments. 

Apart from the interesting observation about the Transformer’s performance on embedding-neighbor attacks mentioned above, it is difficult to know what conclusions to draw from the experiments. In 4.3 it seems obvious a priori that perturbations intended to be relatively meaning preserving would indeed preserve meaning better than unconstrained ones. Similarly, it is not surprising that character swaps that by design produce an OOV token will cause more damage than choosing a near neighbor in embedding space. In 5.3, training with OOVs (resulting from character swaps) is of course not likely to hurt performance on test sets containing few OOVs, and, as is known from previous work, it will improve robustness to the same kind of noise. A final comment about the experiments is that word-based systems are not state of the art, and it isn’t clear how much we could expect any conclusions to carry over to sub-word models.

To conclude, although this is an interesting initiative, both the framework and the methodology need to be tightened up.

Details:

End of 2.1: this would be easier to interpret if you had previously specified the allowed range for s_src.

3.2 For kNN, being semantically related doesn’t imply that the relationship is synonymy, as would be required for meaning preservation. It also doesn’t imply that the substitution will be grammatical, which could jeopardize meaning preservation even if the words are synonyms.

CharSwap seems odd. If you’re just going to replace a work with an OOV symbol in any case, why go to the trouble of swapping characters? No matter what actual semantic shift is caused by the swap, the model will always see exactly the same representation.

4.1 “Following previous work on adversarial examples for seq2seq models (Belinkov & Bisk, 2018; Ebrahimi et al., 2018a)” - this is misleading: Ebrahimi et al only work with classification, and don’t use IWLST.

4.1 Should mention the size of the training sets in this section.

Table 1, first sentence, CharSwap example omits “faire”.

4.3, “Adding Constraints Helps Preserve…” last sentence: but here you need to reason in the opposite direction.

5.2 It would be good to also give absolute scores for table 6, so we can judge how much the systems actually benefited, and whether these gains were statistically significant.",3
"This paper introduces a normalization technique, which normalizes the weights of convolutional layers. The method looks like the combination of batch normalization(BN) and weight normalization. Several assumptions are provided to proceed what the authors would like to show, and some experimental results follow. 

pros)
(+)  The idea of normalizing weights looks good.

cons)
(-) The major problem is the experimental section, where the results are not convincing and cannot support the effectiveness of the proposed method. Specifically, the proposed method cannot outperform batch normalization (BN) in respect of validation/test accuracy even the authors claim that training is faster. It looks similar to the training curve when using smaller learning rates. 
(-) The proposed method (+ mixup or manifold mixup) cannot show better results over batch normalization on ImageNet dataset.
(-) The proposed method seems to have a dependency on the additional method such as mixup and manifold mixup. Furthermore, the parameter of them could determine the overall performance of it.
(-) Section 5.1 cannot fully support the conjecture which is that the proposed normalization method can concern the covariate shift effect. 

comments)
- What is the benefit of the faster convergence in early epochs? We can observe faster convergence when using smaller learning rates, but finally, the larger learning rates can show better validation/test accuracy. As shown in Figure 2,  and some results in Figure 4, the accuracy does now outperform BN.  In this light, the faster convergence does not seem to have any advantages.
- The authors should put the training curve about the first approach (shown on page 6) to clarify how the training goes differently.
- For the second approach on page 6, it looks interesting the proposed method can outperform BN on CIFAR datasets when combining with mixup or manifold mixup. However, there is no analysis of why it works.  
- The first equation would be changed without the cyclic padding assumption, and this would cause errors normalizing all weights (just following the derived equations). So, this affects not only on edge pixels but all the pixels in features.
- Updating r with ""batchsize x heightout x widthout x stide^2"" makes a tremendously big value in the earlier layers (i.e., close to the input), so it must affect the normalization results.


The paper contains an interesting approach by normalizing weights directly, but the grounds for the conjecture are not clearly addressed. A few minor things for better readability should be addressed: all the equation should have equation number so that the reader can easily follow the paper, and the subsection titles (e.g., 4.1 Full case, 4.2 Assumptions (for what?)) are quite hard to understand what the authors will tell us. ",4
"Authors present a new normalization technique called Equi-Norm and experimentally show its fast convergence properties over its popular competitors Batch-norm and Group-norm. The main idea in the paper is the EquiNorm method modifies the weights of a layer before forward propagating through it such that the contribution from positive and negative kernel weights to the output is same. In the experimental section, their approach is shown to be more accurate than Batch-norm and group norm on all datasets except, Imagenet. Is there any reasoning why this method performed slightly poorly on this particular dataset.

Quality:
Results are convincing and comprehensive; authors compare their proposed approach to major two baseline normalization frameworks and demonstrate improved performance.

Clarity:
The paper is well written for readers to understand however there are many grammar mistakes in multiple places.

Originality:
The work seems to be a logical new approach to covariate shift problem and as described above, I think the proposed method provides convincing results.

I am personally not as familiar with normalization approaches so my confidence in the assessment is low; my main experience is in the computer vision for autonomous driving and sparse coding.
",6
"This manuscript introduces a new layer-wise transform, EquiNorm, to improve upon batch normalization. As with batch normalization and related techniques, the idea is to introduce a simple linear transform at each layer to reduce the dependency of the features to the data. Unlike batch normalization, the procedure does not modify the inputs to the layers but rather the layer weights. For this purpose, a scaling factor and a shift is computed on a mini batch, separating positive and negative weights to compute easily both running estimates of shift and of spread (here in the l1 sense). The method is compared to BatchNorm and GroupNorm on several classic computer vision datasets. Empirically, the method converges faster in the beginning of the optimization, in the sense that in the first few epochs the test accuracy is higher than for BatchNorm. However, this benefit decreases with more epochs and when the results are close to peak performance the difference between methods is a small. In addition, test accuracy can decrease at the end of the optimization, which the authors interpret as a sign of increased overfit, and tackle with clever data augmentation. The paper reads well.

The strengths of the paper are the elegance of the solution proposed, and the faster convergence. To me, the main drawback of the manuscript is that the results do not show a clear benefit at convergence of EquiNorm compared to BatchNorm. It seems that as the learning rate decreases there is no consistent difference: in none of the 4 datasets does EquiNorm give a gain larger than a quartile of the distribution of scores and in 2 out of 4 it performs less well.

My advice to improve this work would be to do more experiments and to show better that the lack of performance gain is due to overfit, and can be fixed with larger data or data augmentation.


The faster initial convergence could be a real benefit of this method. However, the time to useful convergence is the same than with BatchNorm. This is probably because the learning-rate decrease schedule is the same. It might be interesting to study adapted learning-rate decrease.

To argue for increased overfit, it would have been interesting to compare test error with train error, and to plot train loss alongside with figure 2.

While the fast convergence is a benefit, it would have been interesting to also compare EquiNorm with BatchNorm with learning scale schedules (ie outside ""super convergence"" settings). While costly, such an experiment would help separating the benefits of EquiNorm from choice of learning rate and give a standard baseline to compare to. Ideally, EquiNorm achieves this baseline with less compute time.

I would like to congratulate the authors for reporting multiple runs with different RNGs and the quantile of the distribution. This is absolutely good practice to judge the significance of improvements and is seldom done.

Why does figure 2 shows only test loss, while the other experimental figures show test accuracy and loss?
",7
"The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. The algorithm can be used to implicitly maximize the likelihood of models for which the former quantity is not intractable but for which sampling is easy which is typically the case for implicit models.  The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. However, The paper lacks clarity and the experiments are not really convincing. Here are some remarks:
Experiments:
- The estimated likelihood was reported on table 1 using parson window which is known to have bad scaling behavior with the dimension of data. In the end, the table compares methods that maximize different objectives and are evaluated with an unreliable metric. Here are two possible experiments that could be more informative:
- Consider toy examples for which the likelihood can be evaluated and the MLE obtained easily and then compare with the proposed method. This would already give a good sense of how well the algorithm behaves in simple cases.
- Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form. This would allow comparing the proposed algorithm to direct likelihood maximization on more complicated datasets as done in [1].
It seems like having experiments of this nature is far more convincing than a long justification for why the results are not necessarily state-of-the-art.
- There are way too many samples on the figures so it is very hard to perform any visual assessment.

Theory:
- More discussions of the assumptions are needed, concrete examples for which these assumptions hold or not would be very useful. 
- Lemma 2 is a direct consequence of the following result: if p is continuous at x_0 then x_0 is a Lebesgue point.

General remarks on the paper:
- What complexity is the nearest neighbor algorithm? Since it is crucial for the proposed method to be scalable it is worth presenting this algorithm at a high level in the main paper.
- The discussion in section 3 could be much more concise if concrete examples and figures were provided. Most of the facts discussed in that section are generally well understood, so conciseness is very appreciated in this case.
- «  A secondary issue that is more easily solvable is that samples presented in papers are sometimes cherry-picked; as a result, they capture the maximum sample quality, but not necessarily the mean sample quality. » Could you please provide an example of such paper? I would be very interested in having a closer look.
- In the last paragraph of section 5, it is said that although the samples may not be state of the art in terms of precision, other methods which achieve better precision «  may » have less recall. It would be good to have empirical evidence to back this claim.


Revision: Although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:
- Missing experimental evidence for the efficiency of the NN search algorithm.
- Experiments are using Parzen window for estimating likelihood which  are known to be unreliable in high dimensions.  
- None of the suggested experiments were considered. In my opinion these experiments could improve the quality of this work. 
- Moreover, as mentioned by reviewer 1, Grover et al., 2017 provides evidence contrary to what the authors claim but this was never addressed so far in the paper.
- Theorem 1 makes rather strong assumptions: as pointed out by reviewer 1, assumption 3 is unlikely to hold for the distributions used in practice

For these reasons I recommend a clear reject. 

[1] I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of Maximum Likelihood and GAN-based training of Real NVPs.
",3
"Two high-level points about my review before going into the details:
1. This paper was a thoroughly enjoyable and insightful read. Kudos to the authors for attempting such a comprehensive overview of likelihood-based vs. likelihood-free learning.
2. I’ll be more than happy to revise my current rating if my concerns are addressed by the authors.

With regards to the technical assessment of this work, the idea of using a nearest neighbors objective for learning a generative model is both intriguing and appealing. What makes this work even more interesting are its connections with maximum likelihood estimation. Novelty aside, I believe there are major theoretical, algorithmic, and empirical concerns in the current work which I discuss below:

Theorem 1 
- The third condition is true for location-scale family of distributions e.g., Gaussian. But the distribution learned by a generative model p_theta is far from Gaussian or other location-scale distributions. 
- More importantly, I don’t think the upper bound is tight in practice because the likelihoods can vary significantly across the dataset. Take MNIST for example. Compare the log-likelihoods of an autoregressive model or ELBOs of a VAE across the different classes of digits. Straight digits (like 1s) have much higher log-likelihoods on average than curved digits. 

Algorithm
- While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it’s hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation.
- Similarly, I was a bit disappointed by the choice of Euclidean distance in a pixel space as the choice of distance metric. The argument that you do not want to use “auxiliary sources of labelled data or leverage domain-specific prior knowledge” is indeed necessary for fair comparisons, but also points to a limitation of the current approach.

Empirical evaluation 
- Seems too outdated both in terms of baselines and metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches.
- While it is arguably well-established that Parzen window estimates are misleading (Theis et al.), that’s the only quantitative estimate in this work (Table 1). Hard to think of any recent published work (last 1-2 years) in generative modeling that even reports these estimates.
- The baselines in Table 1 are all from 2013-15. Clearly, much has happened in the last 3 years that merit the inclusion of more recent baselines.
-  Even for sample quality, there has been a lot of research in designing and improving metrics. E.g., Inception scores, Frechet Inception Distance, Kernel Inception Distance. I am not looking for state-of-the-art numbers, showing heavily zoomed out samples without any of these metrics is slightly disingenuous.
- As mentioned before, reporting the computation time/per iteration and number of iterations for convergence for the proposed algorithm in comparison with other approaches  is important.
- Similarly, the argument about the method avoiding even the other GAN problems (e.g., vanishing gradients, stability in training) can and should be supported by empirical evidence.

Analysis and discussion
- One family of generative models that is crucially missing from this work is normalizing flow models. 
- This is somewhat debatable, but I do not agree that the tradeoff between likelihoods and sample quality is due to model capacity. As far as I can tell, the cited work of Grover et al., 2017 provides evidence contrary to what the authors claim. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. So model capacity isn't necessarily the key differentiating factor (which is same for both training algorithms in their experiments), it's more about the choice of the objective function and the optimization procedure.

Minor points for improving presentation:
- Section 3 can be made more concise and to the point. I’d be especially interested if the precision and recall discussion in this section and elsewhere can be formalized.
- Use numbered lists instead of bullets for assumptions in Theorem 1, so that the discussion of the assumptions right after the theorem statement are easy to follow.
- The citation of Grover et al. seems outdated? The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models.
- In general, avoid making somewhat hard assertions that are speculative. Some of them I’ve highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.).",4
"Summary:

This paper proposes a nearest-neighbor-based algorithm for implicit maximum likelihood.  Samples are produced by the generator network and then a nearest neighbors algorithm is run to match the samples with their nearest data point.  The generator is then updated using the Euclidean distance between samples and neighbors as the optimization objective.  Six conditions are then provided, and if they are met, then the authors show that this method is performing maximum likelihood on the implied density.  Experiments report Parzen window density estimates, samples from the model, and latent-space interpolations for MNIST, Toronto Faces, and CIFAR-10.

Pros: 

The primary contribution of this paper is an algorithm for implicit likelihood maximization with theoretic guarantees.  As far as I’m aware, this is a novel and noteworthy contribution.  Moreover, as each sample must be paired with an observation, it does seem like the algorithm would be somewhat robust to the notorious mode collapse problem.

Cons:

My primary critique of the paper is that there is very little experimental investigation of the crucial details of the algorithm.  Firstly, running the nearest neighbors algorithm seems like it could be a computational bottleneck.  The authors acknowledge this, but then say “this is no longer the case due to recent advances in nearest neighbor search algorithms (Li & Malik 2016; 2017)” (p 3-4).  No other justification is given, from what I can tell.  A simulation showing how the runtime scales with dimensionality or number of data points would be very useful for knowing the scalability and practicality of the algorithm.  In the same vein, showing that the algorithm works well even with a relaxation such as approximate neighbors or random projections would make the algorithm more attractive to adopt.  

Moreover, I found it frustrating that the paper teases a fix to several well-known GAN issues: “The proposed method could sidestep the three issues mentioned above: mode collapse, vanishing gradients and training instability” (p 3).  But the paper never experimentally investigates if the proposed approach indeed is better in these aspects.  I was disappointed since, intuitively, the algorithm does seem like it could be robust to mode collapse.  In addition to this lack of experimental focus, the only quantitative result is the Parzen window estimates in Table 1.  The proposed method does best the others but the other reported results are quite old---all from 2015 or earlier. 

Minor points: 

The paper is at 10 pages, and while it is well-written, the writing is verbose and could be use tightening.  


Evaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis.  Moreover, the theory seems not too divorced from practice (but I didn't check every detail).  However, the evaluation of this method is where the paper falters.  A big issue (that the authors note themselves) is the practicality of performing repeated nearest neighbor iterations.  No runtimes are report, nor are any approximations considered.  Rather, samples and interpolations are given the most discussion.  Furthermore, there is no demonstrations of training stability or quantitative analysis of mode collapse.  Due to these experimental deficiencies, I recommend rejection, weakly.
",5
"The mental fatigue is an important factor in road accidents. Finding a direct mapping between EEG features and reaction time is difficult and error-prone, combining the noise measurement of EEG and individual variation of RT.  The authors introduce a measure called BDrank based on partial ordering instead of regression. Formulating the measure as a MAP problem, the authors propose a generalized EM algorithm for prediction. An online extension, relying on iterative L-BFGS optimization over mini-batches. 

Figure 3 shows the indegree sequence for 4 selected subjects. What is the criterion to select these subjects? These cases seem interesting, but is it representative for the best/worst case? It could provide some information to show some of the few cases where SVR is more accurate than BDrank.
Regarding the identification of noisy channels, the 33rd channel is indicated as a non-EEG one. What is it?

Some minor questions and suggestions:
- It could be interesting to mention the performance of this measure using only a limited set of EEG channels to evaluate its robustness.
- The introduction indicates that de Naurois et al ., 2017 rely on EEG to estimate the RT, but it is not the case.
- The formulation of the assumption (2) on page 3 is unclear, as sensors are not supposed to make any emission and there is a high correlation between channels.
- The model do not consider transition between type-1 and -2 preference, could it be a problem with confidence interval",7
"The paper proposes an algorithm for mental fatigue monitoring, relating a subjects' EEG signals to their reaction time (RT) during a simulated driving task, as an ordinal regression problem. The authors argue that RTs could be heavily skewed and/or non-smooth, making traditional regression approaches unstable due to outlier values. They propose a brain dynamic ranking algorithm,  BDrank, using a generalized EM algorithm to estimate its parameters, and compare it to support vector regression and Logistic Ordinal Regression, where they show improved performance by accuracy and root mean squared error (RMSE) over a database of 44 subjects.

General comments, in no particular order:

1. There are some minor grammatical errors throughout. The paper could benefit from another read-through to correct these errors.

2. It is unclear to me how the model works at test time; as the model is essentially building a relational structure in the data, does the user have to provide multiple EEG trials at time of prediction?

3. There is notation early on in the paper that doesn't appear to be appropriately defined. For example, Equation (1) describes two sets of propositions, with M1 and M2 elements, respectively. How is M1 and M2, the total set of propositions, calculated? It appears to be all pair-wise comparisons of RTs but then it's unclear why there are two indexes associated with them. Also, what does it mean for the orderings to be significant? (i.e.: that the ""type-1 preference propositions that the orderings between the RTs are significant""). The authors then switch to a new notation x^1 and x^2 without defining them. Notational problems also persist throughout the paper, making it hard to gauge what is being done at each step.

4. The authors describe using an FFT to transform the EEG data into the frequency domain. I'm assuming they are doing the FFT on the entire 10-s interval but the paper does not make this clear. Also, the authors state using EEG power between 0-30Hz for their analysis; do they further sub-divide this range (for example, to the standard theta/alpha/beta power ranges) or just use the power across the entire 0-30Hz band?

5. I am concerned with the relatively sparse set of comparison algorithms the authors use. The authors only compare to relatively simple approaches (support vector regression and logistic ordinal regression), yet they cite many previous works in this area but do not compare against them, instead just leaving a pretty generic statement of ""The regression assumption of this method between EEG signals and RT is not correct""; they do not elaborate on this aspect.

 
Overall I think there is limited novelty in the approach; the idea to learn the structure of the data relationally instead of absolutely is pretty straight-forward, and is a standard practice for example in non-parametric statistical modeling. I am also not positive that ICLR is the best venue for this work; perhaps a better avenue for this would be in a more BCI/neural engineering-focused venue.
",4
"The paper studies fatigue monitoring of EEG driving simulator experiments using various EEG analysis algorithms, one also based on ranking. The data used was from a prior experiment. 

The paper is written in a rather confusing manner, which makes the assessment of originality and significance a hard task for the reviewer. A novel algorithm Bdrank (based on raking is defined) and compared to 2 other algorithms; unclear why with these and not with others. The paper ignores a large portion of the literature, starting with Kohlmorgen et al 2007, Blankertz group, Lee group etc. 
The results  are only somewhat interesting, no understanding of the underlying physiological processes is given. 

Overall, I consider the paper somewhat preliminary. ",2
"This paper proposes a method to use weighted Frechet Mean (wFM) for the operation on Manifold valued data for CNN. The novel point is to view wFM as a convolutional layer. Overall, this paper is mathematically well written, however, how each theory improves CNN and the model used in experiments are not clear enough. 

Pros
+ The use of wFM instead of a convolutional layer is an interesting idea. 
+ This paper is mathematically well written. 

Cos 
- It is hard to understand how each theory presented in this paper helps to improve CNN. For example, the invariance to group operation. Some experimental results would help to understand the advantage of the group invariance.

- It is also unclear why the authors constructed the invariant last layer although the inputs of the last layer are invariant under group operations. 

- In the introduction section, the authors raised the omnidirectional camera, diffusion magnetic resonance imaging, elastography as examples of manifold-valued data. However, experiments are limited to standard video sequences. 

- It is unclear how to obtain the weights {w_i} of wFM by backpropagation. 

- Since the contribution of this paper is to to use wFM instead of a convolutional layer, it is more interesting to visualize the weights {w_i}. 

- More explanation needs for the model used for experiments. Especially in dimensional reduction experiments, I could not understand how each subspace is obtained and averaged. If each frame is a subspace, by averaging frames, the reconstruction would be blurred. 
",5
"This paper introduces a generalization of convolution operations to manifold-valued data using  the computation of the weighted Frechet mean (wFM). Without applying any non-linear units and pooling layers, the paper proposes to merely stack suching generalized convolutional layers to construct a deep network for the data residing on Riemannian manifolds. The evaluations on video classification and reconstruction tasks show the advantages of the introduced network over some baselines. 

The paper’s major contribution is using wFM to generalize the traditional convolution for manifold-valued data.  Accordingly, the critical technical contribution is to estimate the wFM. The paper suggests a inductive/recursive way for the wFM approximation. To the best of my knowledge, there already exist some inductive/recursive wFM estimation methods like [1,2]. Unfortunately, the authors seem to overlook them and do not discuss them at all. Accordingly, I think the paper missed some important related works for sufficient study.

[1] Y. Lim and M. Pa ́lfia. Weighted inductive means .LAA, 453:59–83, 2014.
[2] Hesamoddin Salehian et al., An efficient recursive estimator of the Fre ́chet mean on a hypersphere with applications to Medical Image Analysis, Mathematical Foundations of Computational Anatomy. 2015.

Due to the inconsistent fonts, chaotic layout it is really hard for the readers to follow the content of the paper. I feel like the paper seems to be completed in the last minute. This brings another critic problem, it is not easy to reimplement the wFM layers, which is the core contribution of the paper. For instance, the paper claims that they used intrinsic Riemannian metric when using wFM to convolve the manifold-valued data. I guess it is involved in \Gamma_X^Y (Eq.2) which is explained as the shortest curve from X to Y. Anyway it fails to describe what kind of intrinsic metric they used for the specific manifold-valued data like SPD matrices and linear subspace. In addition, is it \Gamma_{M_{n-1}}^{X_n} rather than \Gamma_{M_{n-1}}^{X_M} for Eq.(2)? 

Another problem is the evaluations are far from sufficient. For video classification, the paper only uses the moving MNIST, which is not a challenging dataset while there are plenty of large scale video datasets. In addition, the paper is expected to compare SOTA video classification methods. To learn the advantage of the proposed network over some related manifold networks like Huang et al., 2016, Huang & Van Gool 2017, it is necessary to evaluate them in the experiments.  For video reconstruction, using 1000 frame color sample of video is also not sufficient to study the effectiveness of the proposed ManifoldNet. Furthemore, it is also expected to compare more SOTA auto-encoder based reconstruction models like VAE [3], AAE [4]  and WAE [5].

[3] Kingma et al., Auto-encoding variational bayes, 2013
[4] Makhzani et al., Adversarial autoencoders, 2015
[5] Wasserstein auto-encoders, 2017
",4
"Brief summary:
The paper considers a generalization of convolutional neural networks (CNNs) to data residing on Riemannian manifolds. The idea is to replace convolutions with weighted averages, which are implemented intrinsically on the manifold. It is shown that this operator is equivariant to isometric group actions. A related approach for dimensionality reduction is also proposed, but I think this only applies to Euclidean data, so I am a bit confused about that part. Empirical performance is reported on toy data with weak baselines.

Good points about the paper:
+ It is a relevant point that the intrinsic average in manifolds is a way to generalized convolutions to the Riemannian domain.
+ The paper is generally fairly easy to read.

Concerns with the paper:
- A key point of CNNs is that you learn filters that are small, i.e. only have non-zero weight assigned to a few neighboring pixels. As far as I can tell, the here proposed ""filters"" would be one weight per data point. 

- I am concerned about the stacking of multiple ""convolution"" (wFM) layers: since each layer computes the average of a set of points, then doesn't stacking multiple ""convolution"" (wFM) layers on top of each other correspond to computing the average of a set of averages? And can this not be computed by a single average? In other words, is a cascade of ""convolution"" (wFM) layers equivalent to a single layer? Seems like a complicated way of doing shallow learning unless I misunderstand.

- In section 2.2 it is argued that the weighted average (wFM) is a contraction mapping. While I think the proof is correct, I am concerned about the prerequisite assumption that that distance between a set of points X and Y is the *maximal* distance between points in the two sets. Usually, one would define this distance as the *minimal* distance (akin to the Hausdorff distance). It seems that under this more reasonable choice of distance, the proof no longer holds

- Large parts of section 2.1 is devoted to an efficient algorithm for computing weighted averages on manifolds.  Here the text is written such as to indicate that this is a novel contribution of the present paper, even if these results are readily available in the literature. I strongly encourage a re-writing to emphasize that this is a repetition of previous knowledge.

- Proposition 5 does not appear to come with a proof.

- Section 3.2 introduce a new dimensionality reducing layer based on the Grassmann average construction for subspace learning. I was quite confused when reading this. From what I can tell this layer is only applicable when the input data is Euclidean, and as such appears to be unrelated to the rest of the paper.  

- At times the paper is rather sloppy written, e.g. fonts are way too small in figures, the dot(.) notation is not defined (e.g. in def. 7), and the citation style is very difficult to read (please use \citet and \citep instead of \cite).

Other comments:
*) The assumption (page 3) that the data reside within a geodesic ball of a certain radius seem quite strong. It would be good to comment on this in the paper.

*) It is not clear to me that the weighted average is a particularly good way to generalize convolution. Yes, I agree, it is *one
 way to generalize, but why should I pick this one in particular?

*) In the experiment in sec. 3.1 the manifold comes from a particular way to extract features from data. In a deep learning context, we would usually learn such features instead of manually designing them. This seem like a general issue, that most often manifold come into play through a manual feature-design, which seem to be at odds with the end-to-end learning mindset. It might be good to have examples where this is not the case.

",4
"PROS:
* This is an interesting approach of assigning contribution weights to each source sample.
* Could be very helpful for tasks where we have a noisy and a (small) clean dataset.
* The method seems to be performing well for the tasks chosen, especially for the CIFAR experiments.
* Simple idea and relatively easy to implement

CONS:
* Clarity could be improved, especially in the experimental section
* The motivation for the SVHN 0-4 to MNIST 5-9 is not clear. It would make more sense to me to transfer between SVHN 0-5 to SVHN 5-9, or from the entire SVHN to the entire MNIST, but this particular transfer seems somewhat irrelevant to the claims. The two domains are particularly dissimilar and trying to select ""good"" SVHN samples according to 20 or 25 MNIST samples seems somewhat ill-posed. It is also particularly surprising to me that 25 MNIST samples were enough to train a LeNet to the point of 84% accuracy on the entire MNIST test set. (I'm referring to the target-only line) Is that really the case, or was a larger training set used for that particular line?
* There is a claim that ""SOSELETO has superior performance to all of the techniqiues which do not use unlabelled data"", however I'm not sure whether these techniques were used as prescribed and if the comparison was fair. For example, I believe domain adaptation techniques like DANN, largely assume a common label space between the domains.
* Comparison with previous re-weighting techniques would have been very informative.

QUALITY:
* The quality of the writing was overall high, with a few exceptions, including the related work and the experimental section.
* In related work, the ""bilevel optimization"" section could be a bit more descriptive, maybe some of the explanationgiven in Sect. 3 could be moved here?
* The experiments were convincing, with the exception of the SVHN to MNIST section.

CLARITY:
* I believe a better synthetic experiment could be chosen to highlight the approach: how about a truly noisy dataset that is not as separable as the ""noisy"" dataset in Figure 1? Maybe you could have the same noisy dataset but with a small portion of random points having the wrong label. For the same experiment, it should be clearly stated that your task is binary classification and what was the classifier used.
* For the CIFAR experiments, it is very good that it performs well, but it'd be informative to see if SOSELETO can perform even better with 10K samples.
* It wasn't clear to me whether the a-values of only one batch (32 samples?) at a time were affected. If so, how does this scale to really large datasets like, say, Imagenet?
* In the CIFAR experiments, it is mentioned that a target batch-size is chosen to be larger to enable more a-values to be affected. This seems like a typo, but it was confusing. (I assume that the source batch-size is chosen to be larger)
* Figure 2 could use a better caption and a legend. It would also be an easier figure to parse if the x-axis was reversed (eg. if the x-axis was the fraction of data used)
* It was not clear to me what ""true transfer learning"" means as opposed to domain adaptation.

ORIGINALITY:
* It seems that this idea has been explored before, however I'm not personally familiar with that work. I would have definitely liked to see comparisons with it though.


SIGNIFICANCE:
* This is a simple idea that seems to work well. As I wrote above, it would be great to know how it compares to other re-weighting techniques.",7
"This is an interesting paper claiming that on assumptions are made (or explicitly made) on the similarity of distributions. Traditionally, we learned the weights for transfer learning by matching the distributions. I am wondering if there are any relationships between those two methods. It is necessary to show the differences between the weighted source domain and the target domain, and compare them with the traditional matching methods.

My another concern is about the technical contribution. The model is very intuitive and simple. Some analyses are made for optimization. However, theoretical justifications are lacking, making the technical contribution weak and looks like a simple combination of two existing techniques. I would like to know if the weights are identifiable and what kinds of weights are preferred. 

By searching, I found related papers on transfer learning with label noise and learning with label noise by importance reweighting, e.g., Yu, Xiyu, et al. ""Transfer Learning with Label Noise."" arXiv preprint arXiv:1707.09724 (2017). and Liu, Tongliang, and Dacheng Tao. ""Classification with noisy labels by importance reweighting."" IEEE Transactions on pattern analysis and machine intelligence 38.3 (2016): 447-461. However, they are not discussed in the submission. It is curious to see the relationships and differences.",5
"In this paper, the authors propose a SOSELETO (source selection for target optimization) framework to transfer learning and training with noisy labels. The intuition is some source instances are more informative than the others. Specifically, source instances are weighted and the weights are learned in a bilevel optimization scheme. Experimental studies on both training with noisy label problems and transfer learning problems demonstrate the effectiveness of the proposed SOSELETO.

Overall, this paper is well-written, and easy to follow. The intuition is clear and reasonable, although it is not new. Regarding the technical section, I have the following comments:
(1)	The paper assumes that the source and target domains share the same feature representation parameters \theta. This is a widely used assumption in the existing works. However, these works usually have a specific part to align two domains to support the assumption, e.g. adversarial loss or MMD. In objective of SOSELETO, I do not see such a domain alignment part. I am wondering whether the assumption is still valid in this case. From the experimental study, I find SOSELETO achieves very good results in transfer learning problems. I am wondering whether the performance would be further improved if a domain alignment objective is added in the weighted source loss.
(2)	Each source has a weight, and thus there are n^s \alpha. As mini-batch is used in the training, I am wondering whether batches are overlapping or not. If overlapping, how to decide the final \alpha_i for x^s_i as you may obtain several \alpha_i in batches. 
(3)	Another point is abouth \lambda_p. In the contents, you omit the last term Q \alpha_m \lambda_p in eq.(4) as you use the fact that it is very small. I am not convincing on this omission as \lambda_p is also a weight for the entire derivative. Moreover, if \lambda_p is very small, the convergence would be very slow. In the experimental studies, you use different \lambda_p for different problems. Then, what’s the rule of setting \lambda_p given a new problem?

Regarding the experimental results, the experimental settings for the section 4.2 are not very clear to me. You may need to clearly state the train and test set (e.g. data size) for each method.  
",5
"The primary purpose of this paper, from what I understand, is to show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. This can potentially be useful in helping to identify artificial samples in the real world.

I think that the reviewers did an excellent job of probing into different statistical perspectives, such as looking at the continuity of the distribution of pixel intensities and the various higher moments of spectral data. I also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.

One of the first findings is that, with MNIST and CIFAR, the pixel intensities of fake samples are noticeably different when viewed from the perspective of a Kolgomorov-Smirnov Test or Jensen-Shannon Divergence comparison. This is an interesting observation, but less so than it would be if compared to something such as a variational autoencoder (VAE), which fits a KL distribution explicitly. IWGAN and LSGAN are using different metrics in their loss functions (such as Wassertein and least squares), and thus the result is not surprising or novel. I think if the authors had somehow shown how they worked their metrics into IWGAN or LSGAN to achieve better results, this could have been interesting.

Another observation the authors make is about the smoothness of the GAN distributions. This may not be so easily wrapped into the loss function, but it seems easily remedied as a post-processing step, or perhaps even a smoothing layer in the network itself. Nevertheless, this is an observation that I have not seen discussed in the literature so there is merit to at least noting the difference. It is confusing that on page 4, the authors state that they hypothesized that the smoothness was due to the pixel values themselves, and chose to alter the distribution of the original pixels in [0,1]. However, they state that in Figure 5, the smoothness remained ""as expected."" Did the authors misspeak here?

I found the music and speech experiments very interesting. The authors note that the synthetic Bach chorales, for instance, introduce many transitions that are not seen in the training and testing set of real Bach chorales. This, again, is interesting to note, but not surprising as the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized. I do not believe these observations to be paper-worthy by themselves. However, the authors I believe have a good start on creating papers in which they specifically address these issues, showing how they can create better synthetic samples by incorporating their observations.

As to the writing style, there are many places where the writing is not quite clear. I would suggest getting an additional party to help proofread to avoid grammatical mistakes. I do not believe that the mistakes are so egregious as to impede understanding. However, it could distract from the importance on the authors future innovations if not corrected.

One last note. The title of the paper is ""TequilaGAN: How to Easily Identify GAN Samples."" This makes it seem as if the authors were introducing another type of GAN, like LSGAN or DCGAN. However, they are not. As a matter of fact, nowhere else in the paper is the word ""TequilaGAN"" mentioned. This title seems a bit sensational and misleading.

In the end, although I did find this paper to be an interesting read, I cannot recommend it for publication in ICLR.

----

Edit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication.",5
"The paper proposes statistics to identify fake data generated using GANs based on simple marginal (summary) statistics or formal specifications automatically generated from real data. The proposed statistics mostly boil down to the fact that continuous-valued generator neural networks can’t adequately generate data distributions that are topologically different from the distribution in the latent z-space. The differences in the summary data/ feature statistics and statistics corresponding to formal specifications between fake and real data are of the above nature. 

This seems fairly obvious but I haven’t seen this property of GANs being exploited to distinguish between GAN generated and real-data

This property/ shortcoming of the generator is not surprising at all and has been acknowledged before. See, for example, the discussions in,

Khayatkhoei et al, Disconnected Manifold Learning for Generative Adversarial Networks, arXiv:1806.00880 (NIPS, 2018)

This has spurred various approaches to mitigate this shortcoming. See, for example,

Ben-Yosef and Weinshall, Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images, arXiv:1808.10356

Jang, Gu and Poole, Categorical Reparameterization with Gumbel-Softmax, arXiv:1611.01144 (ICLR 2017)

So, the fact that summary statistics predicated on discreteness of data or discreteness of their dependencies can distinguish GAN-generated data from real data is not surprising at all. In fact, in the paper itself, figure 10 and 11 show that for continuous data like speech, the proposed statistics are unable to distinguish between the fake and real data. 

Beyond this, even though it's interesting, there isn’t enough contribution in the paper.  It would be great if the authors can extend this observation and show if such statistics can always be found and tricks like Gumbel-Softmax/ GMM-GANs etc are doomed to fail or if certain extentions of GAN architectures can handle such statistics.

Furthermore, the paper needs to provide more clarity/ clarifications about the following:

1.	Apart from formal specifications, the rest of the statistics are ad-hoc (e.g. the spectral centroid or the spectral slope which are just borrowed from the audio domain) – why should these be good for images? 
2.	Training choices do not seem principled – GANs are trained till generated samples look like real samples. Why not use parameter settings and train to produced state of the art results with chosen architectures?
3.	Figure 1: Why does the CDF for the real data start in the middle of the figure? The figure purportedly shows bimodal 1D data for which the CDF should be a step function whereas the reference data has an inclined line. Why?
4.	Using the term ‘spectral’ (centroid and slope) for image features is misleading when spectral features are not computed.  Do these features capture spectral properties of images. How? Why are these good features?
5.	What does an “asymptotically converging activation function” mean?

6. Some typos need to be corrected. Figure # and caption (with dataset name) for Figure 6 needs to be provided, etc.
",4
"The ability to detect generated samples is a very interesting topic and has recently triggered a lot of discussion. The paper is well written, easy to follow and the authors have done an extensive evaluation in a number of different GAN applications.


Comments:

1) Methodology - The GANs being evaluated were trained ignoring the statistics that the authors use to detect generated samples, and thus it is expected that there will be a difference. Have the authors attempted to include those in the loss function? It is a fair argument to say that some are not differentiable, but there are ways one could still incorporate them e.g. using REINFORCE. What do the authors thing about that?

2) Following (1), as far as the specifications are known one could train a GAN to fake them. What do the authors think about that? Do the authors think they could detect samples from that or such statistics could be used to devise better GAN losses?

3) The authors conclude that the smoothness and the differentiability of a loss function will always result to an inductive bias. However, that's an assumption given that there are no experiments trying to fake the detection, or experimenting with a large number of different architectures.

4) In CIFAR10 the authors state that the distributions of pixels was quite different specially in the values close to -1. Another way to see neural networks is as differentiable compressors. Many times, value distortions are correlated to the amount of compression. Have the authors seen differences in e.g. larger architectures?

5) On a last note, I would change the title as there is no proof that these tests / assumptions would hold for further research in the field. It would be great to show that the statistics used to detect GAN samples cannot be tricked.

Minor comments:
1) p.1 In the context of Verified Artificial IntelligenceSeshia [...] - needs a space.
2) p.3 Spectral centroid in 2 [...] - 2 -> Figure 2.
3) p.7 Figure 8 doesn't have a caption.
4) P.7 There are some figures above SPEECH without a figure number.
5) P.7 Reference to table 9b seems to be missing.",6
"In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints

The proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved. Indeed I think that the paper is bit too hard to follow. 
The example at page 2 is not clearly explained.

In Equation 1 the relationship between constants S_i and the variables z is not clear. Is each S_i an assignment to z?

I do not understand the step from Eq. 4 to Eq. 6. Why does arg min become min?

At page 4 the authors state ""we sometimes write a predicate \phi to denote its indicator function 1_\phi"". I’m a bit confused here, when is the indicator function used in equations 1-6?

What kind of architecture is used for implementing DL2? Is a feedforward network used? How many layers does it have? How many neurons for each layer? No information about it is provided by authors.

It is not clear to me why DL2/training is implemented in PyTorch and DL2/querying in TensorFlow. Are those two separate systems? And why implementing them using different frameworks?

In conclusion, I’m a bit insecure about the rating to give to this paper, the system seems interesting, but several part are not clear to me.

[Minor comments]
It seems strange to me to use the notation L_inf instead of B_\epsilon to denote a ball.

In theorem 1. \delta is a constant, right? It seems strange to me to have a limit over a constant.",6
"Summary
-------
This paper proposes DL2, a framework for turning queries over parameters and input, output pairs to neural networks into differentiable loss functions, and an associated declarative language for specifying these queries. The motivation for this work is twofold. The first is to allow for the specification of additional domain knowledge during training. For example, if a user expects that the predicted probabilities of some output classes should be correlated for all predictions, this constraint can be enforced during weight learning. Second, it allows users to search for specific inputs that satisfy specified conditions. In this way, DL2 can capture popular applications like searching for adversarial examples by querying for inputs close to a known input of class A but that the network predicts is class B with high confidence.

The paper provides a concise specification of the query language (a mixture of logical and numeric operators) and asserts a theorem that the given procedure for constructing the query loss produces a function such that anytime the function is 0, the constraints are satisfied. No proof is given, but I cannot see a counterexample. There is also a statement about the converse relationship, that when the loss is above some threshold it implies that the query is not satisfied. 

Experiments are conducted on supervised, semi-supervised, and unsupervised computer vision tasks. I particularly liked the experiment on semi-supervised learning with CIFAR-100. By replacing labeled examples with domain knowledge about the relationships among classes in CIFAR-100, the paper demonstrates a compelling use case for DL2.

The primary technical challenge is the non-convex optimization required to search for a solution to a query. Experiments show that the loss functions created by DL2 are often solved quickly and correctly, but not always

Strengths
---------
The framework is expressive enough that many interesting use cases are clear, from specifying background knowledge during training to model inspection. The experiments cover a range of these use cases, demonstrating that the constructed optimization objectives usually work as intended.

Weaknesses
-----------
The statement in Theorem 1 regarding the converse case is unclear, because it says that the limit of \delta as \epsilon approaches zero is zero, but it is not explained what \epsilon is or how it changes. If \epsilon is the threshold that can often be used in the query, it is not obvious that every query contains exactly one \epsilon. If other cases exist, it is unclear how Theorem 1 applies.

It remains unknown how to handle the case when queries fail. AS the paper points out, if a query fails, it cannot be determined whether no solution exists or if the optimization simply failed to find a solution. Of course, this is a computationally hard in general.

Related Work
------------
There are a couple of points from related work that would be good to add to the paper.

First, the paper ""Adversarial Sets for Regularising Neural Link Predictors"" (Minervini et al., UAI17) is a prior paper that generates adversarial examples to handle restrictions on inputs which may not exist in the training set. The paper claims DL2 is the first to do this, but I believe this paper is an earlier example that does so, albeit for a particular problem. DL2 is certainly more general.

Second, the description of the limitations of rule distillation (Hu et al., ACL16), particularly in Appendix A is not fully accurate. The expressivity of PSL is greater than stated (see Bach et al., JMLR17 for a full description). In particular, the DL2 loss function for z = (1, 1) can be expressed exactly in PSL using what it calls arithmetic rules. It is not clear that this affects the findings of the semi-supervised learning experiment significantly, although I would appreciate a clarification of the authors. PSL by construction produces convex loss functions, and so the constraint that all outputs for a group of classes is either high OR low would probably not work well.",7
"
The paper tackles the interesting problem of combining logical approaches with neural networks in the form of  translating a logical formula into a non-negative loss function for a neural network. 
The approach is novel and more general than previous approaches and the math is sound. However, I feel that the method is not well presented. Sadly the introduction does not set the method into context or give a motivation. The abstract is very short and misses key information. Indeed, even the more technical parts sometimes lack clarity and assume familiarity with a wide range of methods. 

The experiments are well thought out and show the promise of the method when encoding performance measures such as entropy into the constraints. It would have been interesting to additionally see other kinds of constraints such as purely logical formulas that do not have a specific aim (robustness or performance or otherwise) but simply state preconditions that should be fulfilled. It would furthermore be interesting to inspect the corner cases of the proposed method such as what happens if two constraints are nearly opposing each other and so on. 



To conclude, the presented method is clearly novel and provides an interesting solution to a challenging problem. However the paper in the current form does not fully adhere to the standards of conferences such as ICLR. I suggest rewriting especially the abstract and the introduction and then submitting to a different venue as the approach itself seems promising. Additionally, as only very limited comparison experiments can be performed the method itself should be more thoroughly inspected by performing, for example, edge-case or time/number of constraints inspections.

Minor remarks:
Hyperparameters such as batch size not reported
Spelling mistake in line 2, page 2 “Lipschitz condition”
When mentioning “prior work” in the introduction a citation is needed.",5
"I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. 

PS: I am downgrading my confidence in my evaluation.

---

Paper 93 proposes an empirical evaluation of the memorization properties of convnets. More specifically, it evaluates three aspects:
-	First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier. The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.
-	Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model. For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions. It is shown experimentally that one can detect (even partial) leakage with such a technique.
-	Third, it evaluates whether we can detect that a single images was used to train a convnet. Two simple techniques are proposed. The first one considers that a sample is part of the training set if it correctly classified. The second one considers that a sample is part of the training set if its loss is below a threshold. It is shown experimentally that one can make such a decision with moderate accuracy.

On the positive side:
-	This is a topic that should be of broad interest to the ICLR community.
-	The paper is generally well-written.
-	The experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.

On the negative side:
-	It is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?
-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?
-	Section 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.
-	The experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy. Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3). This seems to be too low to be of practical use. This might be because the Bayes and MAT attacks are too simplistic. Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?
",6
"==============Final Evaluation================
I have gone through the other reviews as well as the author response.
Firstly, I would like to thank the authors for providing detailed responses to my questions.

In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.

Moreover, from my understanding the analysis in David McKay’s book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron). As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper). Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review). In general, I feel this section could use some tighter formalism and justifications.

I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen. The authors address this by saying 3M of the 15 M negatives have been seen. That does not seem like a small enough percentage to claim that these are “unseen” images.

In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.
==================

Summary
The paper trains classification models to classify a labeling of a subset of images (assigned with label 1) from the rest of the images (assigned with a label 0). Firstly, the paper shows that deep learning models are able to learn such classifiers and get low training loss. It then proposes to use this model to ``attack’’ task-specific models to perform membership inference, i.e. figuring out if an image provided in a set was used in training or not. 

Strengths
+ The paper thoroughly covers related work and provides context.
+ Results on confidence as a signature of a dataset are interesting.

Weaknesses

[Motivation]
1. In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization. Thus, without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct. Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)

[Capacity]
2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization. This is something which would need to be explained/ substantiated separately. (*)

3. Scenario discussed in Sec. 4 seems somewhat impractical. Given a set of m images, it is not clear that a classifier that is trained to detect between train and validation is sufficient, as one might also need to figure out if it is neither train nor val, which is a very practical scenario.

4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image ‘m’ corresponds to is useful or practical, as this seems to be a property of the set ‘m’ rather than the property of the trained classification model (f_\theta). Please clarify. On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing. (*)

6. It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model. While the baseline approaches seem to make use of the model confidence, I cannot see how the proposed approach (which uses a classifier) makes use of the original model. It is also not clear why Table. 3 does not report the Bayes baseline results. Also, does this section use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?

7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model. (*)

Minor Points
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’. In addition, the paper would also need to show that such a model does not generalize to a validation set of images. This is probably obvious given the results from Zhang et.al. but should be included as a sanity check.
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.


References:
[A]: Blier, Léonard, and Yann Ollivier. 2018. ``The Description Length of Deep Learning Models.’’ arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1802.07044.

Preliminary Evaluation
There are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization’’ capabilities of the classifier trained to remember train vs val images is not clear in general. Important points for the rebuttal are marked with (*).
",4
"Summary of the paper:


The paper has two intertwined goals. These goals are to illuminate the
generalization/memorization properties of large and deep ConvNets in
tandem with trying to develop procedures related to identifying
whether an input to a trained ConvNet has actually been used to train the
network. The latter task is generalized to detecting if a
particular dataset has been used to train a ConvNet. These goal are
tackled empirically with multiple sets of experiments on largescale
datasets such as ImageNet22k and modern deep ConvNets architectures
such as VGG and ResNet.



Paper's positive points

+ The paper has a very comprehensive set of references in the areas it
touches upon.

+ Some of the experimental results presented are quite
interesting. They show that regularization data-augmentation helps
prevent a network from explicit memorization and could be used as a
way to help make training data more anonymous.

+ Large scale experiments are reported on modern architectures.


Paper's negative points

- The paper makes use of a result from the David MacKay textbook
  which defines the capacity of a single layer network to memorize the
  labelling of $n$ inputs in $d$-dimensional space. If I'm not
  mistaken, from this result the authors extrapolate that the capacity
  of a (deep) neural network is proportional to the number of
  parameters in the network. This is true, but there are a
  couple of caveats. The first is that the coefficient of
  proportionality must depend very much on the number of layers in the
  network. Increasing the network's depth increases the efficiency of
  the representation (i.e. fewer total parameters needed to have the
  same representational power as a shallow network). And as MacKay
  also says in his book (chapter 44 quoting findings from Radford
  Neal) that for MLPs what determines the complexity of the typical
  function (once the network has a large enough width) represented by
  the MLP is the ""characteristic magnitude of the weights"". So the
  regularization technique applied is very significant in the
  controlling the effective capacity of a network. This paper
  experimentally shows that is the case multiple times as it is shown
  that with increasing degrees of regularization (figure 1, figure 2)
  it becomes harder and harder to memorize the positive training
  images. It would be great if the paper also made some attempt to
  consider these connections. Or at least comment on how these factors
  could be incorporated into a more sophisticated analysis of the
  capacity of a network.



- There is a slight oxymoron in the premise of the first set of
  experiments. The network is forced to memorize a set of
  positive examples relative to the negative set it sees during
  training. What is memorized I presume depends a lot on the negative
  set used for training (its diversity, closeness to the positive set
  and how frequently each negative example is seen during
  training). This issue is not really commented upon in the paper. Is
  there a training task which would allow one to more explicitly
  memorize the image (some sort of reconstruction task) as opposed to
  an in/out classification task?

- This paper is a slightly difficult read - not because of the
  language or the presentation of the material but more because there
  is not one main coherent argument or goal for the paper. This is
  reflected in the ""Related work"" section where 4 different
  issues/tasks are referred to. Each one of these topics is worthy of
  a paper in itself, but this paper dips into each one and then
  swiftly moves onto the next one. For example in section 3 the paper
  explores if a network can be forced to explicitly memorize a set of
  images and how the size of this set is affected by the number of
  parameters in the network and data augmentation. High-level
  conclusions are made: more parameters in the network implies more
  images can be memorized and data-augmentation makes explicit
  memorization more difficult. Then it is off to considering
  pre-trained networks and determining whether by analyzing the
  statistics of the responses at different layers one can decide if a
  set of images was used for training or not (or similar tasks). Yes
  the different sections are related but it is does not feel like they
  build upon each other to help form a clearer picture of memorization
  within neural networks.
  

- The conclusions focus on the importance of section 3 and
  the results of the experiments performed. Do the conclusions accurately
  reflect the opinions of the author? If yes, would
  it better to re-organize the paper and devote more of it to the
  material presented in section 3 and filling this out with more
  analysis and experiments to perhaps explore the issue of the
  capacity of a network in more 


Queries/ points that need some clarification

- I'm a little unclear when data-augmentation is included in the
  training phase whether the goal is to be able to also recognise
  perturbed versions of the input images at test time. In section 3 is
  a perturbed positive image considered a positive training image? And
  in the testing phase are only unperturbed versions of the positive
  images given to the ConvNet as input?

- Last paragraph page 4: ""when the accuracy gets over 60\% and again
  at 90\%"". Is this training or validation accuracy?




Typos possible errors spotted along the way:

* First paragraph page 5: ""more shallow"" --> ""shallower""
* Page 7, first paragraph of section 5.: ""is ran"" --> ""is run""
* Using ""scenarii"" for the plural of ""scenario"" I would say is pretty
  non-standard and most people would use ""scenarios""",5
"========= Summary =========

The authors propose a novel method for counterfactual inference (i.e. individual/heterogeneous treatment effect, as well as average treatment effect) with neural networks. They perform propensity score matching within each minibatch in order to match the covariate distributions during training, which leads to a doubly robust model.

PM is evaluated on several standard semi-synthetic datasets (jobs, IHDP, TCGA) and PM shows state-of-the-art performance on some datasets, and overall looks quite promising. 

======= Comments =======

The paper is well-written, presents a novel method of some interest to the community, and shows quite good performance across a range of relevant benchmarks.

I have one major issue with this work: I don't see why propensity-score matching *within* a minibatch should provide a substantial improvement over propensity-score matching across the dataset (Ho et al 2011). I find the cursory explanation given (""it ensures that every gradient step is done in a way that is approximately unbiased"") unconvincing, since (a) proper SGD training should be robust to per-batch biases during training (the expected loss is identical for both methods, correct?), and (b) biases should go away in the limit of large batch sizes. If indeed SGD required unbiased *minibatches* then standard minibatch SGD wouldn't work at all.

Looking at the experimental details in the appendix, it appears that the MatchIt package was used to do PSM, rather than a careful comparison under the same conditions. Are the exact matching procedure, PS estimator model, choosing ""one of 6 closest  matches by propensity score"", batch size, etc. the same between your PM implementation and MatchIt? I'd be very curious to see the results of a controlled comparison between Alg S1 and S2 under the same conditions (i.e. run your PM implementation on the whole dataset), and perhaps even some more clever experiments illustrating why matching within a minibatch is important. 

Another hypothesis for why PM is better than PSM is that the matching distribution for PM changes at each epoch (at least due to the randomization among the 6 closest matches). Could it be that the advantage of PM is that it actually provides a randomized rather than constant distribution of matched points?

Can the authors provide more motivation for why PM should outperform PSM? Or some more careful comparison of these methods isolating the benefits of PM? I think a convincing justification and comparison here could change my opinion, as I like the paper otherwise. Thanks!

Detailed Comments:

- There is insufficient explanation of the PM method in the main text. The method is only mentioned in a single sentence buried in the middle of a long paragraph ""In PM, we match every sample within a minibatch..."". This should be made more clear, e.g. by moving Algorithm S1 to the main text.
- The discussion on Model Selection and the argument for nearest-neighbor PEHE is clever and well-supported by the experiments.
- In Table 3 and 4, it's not clear which numbers are reported by the original authors and which were replicated by the authors.",5
"Summary:
This paper proposed to extend TARNET (Shalit et al. 2017), a representation learning approach for counterfactual inference, in the following ways.

First, to extend TARNET to multiple treatment setting, k head networks (instead of 2) were constructed following the shared MLP layers, where each head network modeled the outcome of one treatment. This extension seemed quite straightforward.

Second, during training, for every sample in a minibatch, find its nearest neighbors from all other treatments and add them to the minibatch. The distance was measured by the propensity score, which was defined the probability of a sample being assigned to a treatment group and could be learned by a classification model (such as support vector machine used in this work). Therefore, 1) the augmented minibatch would contain the same number of samples for each treatment group; 2) different treatment group were balanced.

Third, a model selection strategy was proposed by estimating the PEHE using nearest neighbor search.

Comments:
This paper is well motivated. The key challenges in counterfactual inference is how to adjust for the bias in treatment assignment and the associated discrepancies in the distribution of different treatment groups. 

The main idea of this paper, i.e., augmenting the minibatch through propensity score matching for each sample, is well explained in Section 3. However, it could be better if the introduction of model architecture (in Appendix F) was presented in the method section.

Did the author need to train (k choose 2) SVMs to compute the propensity scores for samples from k treatment groups?

When comparing different approaches, as were shown in Table 3, 4 and Figure 3,4, did the author run any statistical test, such as t-test, to confirm the difference between those distributions were significant? The standard deviations of those errors seemed quite large so the difference could be non-significant.

Could the author provide more explanations on why the proposed approach, i.e., minibatch augmentation using propensity score matching, can outperform the TARNET? In TARNET, each sample it only used to update the head network corresponding to the sample's treatment assignment, why would balancing samples in the minibatch can improve the estimation of treatment effect?",5
"This paper proposes an augmentation of traditional neural network learning to allow for the inference of causal effects. Specifically, they modify the data sampling procedure of SGD during training to use matched samples that are paired via propensity score matching. Experimental results on a number of dataset show that the proposed methodology is comparable to alternative machine learning based causal inference methods. 

Overall, I think this is a nice idea. I have two main concerns: 
(1) The use of small batches for matching. Figure 2 does alleviate this concern to an extent, but there is a large literature in statistics and the social sciences on the effect that the quality of matches have on the final causal estimand. It is quite possible that this particular dataset is more amenable to PSM. It is also worth noting that while there is bias reduction shown in figure 2, it is not overwhelming. 

(2) The use of propensity scores for matching. One of the insights from the heterogeneous treatment effect literature is that it is not difficult to find cases where the propensity of treatment is identical for two sets of covariates that otherwise do not obey any real balance. This can lead to large biases in the final estimate. Given that PSM is still a relatively widely used practice, I don’t think that its use is a ground for rejection in itself, but given that neural networks are often used to estimate complex causal relations when they are used and this paper is interested in individual treatment effects it is worth noting. 

I found the experimental setup to do a very good job in covering large portions of the behavior of the algorithm. The final results are a little underwhelming–the proposed method does not appear to clearly define a new state of the art for the tasks it is applied to–but it is often competitive and the paper presents an interesting idea.
",6
"1. This papers leverages the concept of wavelet transform within a deep architecture to solve the classic problem (especially for wavelet analysis) of change point detection. The authors do a reasonably comprehensive job of demonstrating the efficacy of the proposed framework using various synthetic and real data sets with both gradual and abrupt changes

2. The concept of pyramid network idea is not really new, in the context of CNN it has been established quite well. The paper should highlight this fact by citing papers such as ""Lin, Tsung-Yi, et al. ""Feature Pyramid Networks for Object Detection."" CVPR. Vol. 1. No. 2. 2017."" 

3. Involving wavelet transforms in deep nets have been done before. This paper attempts to learn wavelet transform parameters by involving them as trainable layers. But even this kind of idea is also emerging in the community. Papers such as ""Fujieda, Shin, Kohei Takayama, and Toshiya Hachisuka. ""Wavelet Convolutional Neural Networks."" arXiv preprint arXiv:1805.08620 (2018)"" need to be discussed in this context. 

4. The biggest issue in my mind is that I feel ""Chung et al 2016"" is still a very similar framework as the proposed one. While authors argue that it uses more like CNN architecture and the proposed method may pick up the multi-scale features better, comparison with this seems to be most appropriate. This will also clearly identify the benefits of the wavelet structure to the filters and multi-resolution analysis approaches.

5. RCNN term has been used for CNN+RNN architecture. This may not be a good terminology to use since RCNN is a very popular term referring to Region based CNN for detection and localization purposes.

6. AUC metric, I believe is the - area under ROC curve, this needs to be spelled out, how it is computed? at least in the Appendix

xxxxxxxxxxxxxxxxxxx

Appreciate the authors' rebuttal, updated my score.",7
"The paper presents an interesting approach to change point detection. I agree we need more general model to capture the change. However, unfortunately, they did not place the contribution correctly with respect to existing literature. The comments for prior work seem to be highly biased. For instance,  in Section 2, ""these methods either have unrealistic assumptions, such as defining changes as a large difference in covariance matrix"". I would like to comment that, covariance change can capture a large number of changes in real applications and these are not unrealistic assumptions. 

The ""pyramid"" recurrent neural network seems to be a extension of RNN using the idea of multi-scale structure. Could be interesting.

The paper gives too much emphasis on the ""merit"" of the neural networks on capturing the change patterns. However, there is a very important aspect been ignored or hiding: in order to train neural networks to capture anomaly patterns, since neural networks are highly over-parameterized model, usually there won't be a large number of samples for anomalies. Therefore, in many situations, it is simply unpractical to train neural networks to capture post-change samples. 

There is a large body of literature on change point detection in statistics etc. (the author mentioned one, Chen and Zhang 2015, more over, the comment that ""they can only detect abrupt change"" is wrong, the method is quite general).

The paper fails to have any comparison with existing methods. For instance, how does the proposed method compare with hoteling T-square statistic, or CUSUM statistic, or generalize likelihood ratio statistic, or MMD statistic (non-parametric approach)? Without any comparison, it does not make sense to claim proposed method is superior. 
",4
"This paper proposes a pyramid based neural net which both decomposes a signal into several scales (learning the basis functions to do that) and processes the resulting bands in a scale invariant manner. The method is applied to 1D signals with underlying processes occurring at different time scales where the task is change point detection.

Pros:
* Nice model and model formulation - learning the basis functions both for the low and high frequency is a nice idea. I also liked the way weights are shared across scales. In particular that the information flow between consecutive scales is shared, as well as through time.

* Writing is very clear and method is well motivated

Cons:
* I found the experimental validation a bit limited - the presented results are nice and for the problem quite comprehensive but I would have wanted something a bit more complicated than change point detection. Specifically, since the natural world is full of scale free phenomena it would have been much more interesting with other tasks (generative models? natural images? many options). I feel this would have made the case for the paper much stronger.
* There's also very little analysis of what is learned from the data - how do the kernels look like and how do they correspond to known wavelets? to the data? would be nice to understand what's going on here.


Bottom line:
I like the proposed model and for what it is it's quite good but it would have been a much more convincing paper with more experiments demonstrating the power of the method and analyzing it.",6
"In this paper, the authors propose a method to generate predictions under fairness constraints by optimizing quadratic penalties over non-convex loss functions. The main idea is to replace the linear fairness constraints by the second-order penalty. Meanwhile, an efficient method is derived to compute the gradients associated with the second-order penalties in stochastic mini-batch settings. Finally, the experimental results show the effectiveness of their proposed method empirically. 

(1) The authors argued that their experimental results in a more practical training procedure in non-convex, large-data settings. However, in Section 6, the sample size of the data-sets they used are small and the loss functions of learning models are convex. I think they need to provide more experimental results to make their proposed method more convincing. 

(2) It is a little difficult to follow the motivation and contributions of this paper. I would recommend the authors to improve the presentation by providing more context for the use of double integral or sampling method and other mostly relevant works in this area. 

(3) From the optimization viewpoint, the second-order penalty in Eq. (3) is convex with respect to d. Why replacing the linear penalties with quadratic penalties to solve the shortcomings of using linear penalties or Lagrange multipliers. Isn't the square of an expectation always an expectation of pairs? In other word, Eq. (4) is always equivalient to Eq. (7) without additional conditions?

",5
"In this paper, the authors propose a method for optimizing quadratic penalties over non-convex loss functions. The authors motivate their approach by showing the complexity of training non-convex models with linear constraints and proposing a simpler method to introduce these constraints as regularization terms. Finally, they show how their proposed method compares to alternative solutions on a series of benchmarks.

Use of double sampling method for estimating the loss and second order penalty appears to be novel, however, there is no discussion of the implications of using this method to train non-convex models --- one would suspect that use of double sampling may make the gradient descent susceptible to high variance. Simulation results in the paper only demonstrate how the use of the loss changes the solution but there is no discussion or experiments on complexity of training models that use this approach. This is particularly important because authors are claiming their method does not suffer from complexity issues which other methods suffer, but this claim is not supported by any evidence. For example, how many iterations were necessary to train the model? How sensitive is the training to initial conditions and changes in hyper parameters?

Authors motivate their method by pointing out the limitations of using fairness constraints in non-convex models, however, they don’t provide-sufficient evidence for why non-convex models are actually useful in their experiments --- the datasets they used are small and models that are actually convex may perform just as good or nearly as good as highly non-linear, non-convex models which authors are trying to use.

Overall, I would recommend the authors to improve the presentation by providing more context for the use of double sampling method and other relevant works in this area (at least showing the impact of using double sampling on training). Moreover, given the datasets and scope of questions related to fairness they need to provide better experiments that motivate the use of their method (compared to simpler methods) or consider other problems where their approach could be more useful. Given these considerations I believe this paper does not meet the standards for publication.",5
"The authors propose a method to generate predictions under fairness constraints. The main idea is to take linear fairness constraints, and replace them with weak squared penalties plugged into the objectives, which supposed to help in cases where the loss function is not convex. The penalty coefficients are chosen by cross-validation, and the effectiveness of this approach is demonstrated empirically.

In Sec. 3.1, the authors point out several shortcomings of using linear penalties (using Lagrange multipliers) for non-convex losses. These seem valid. Sec. 3.2, however, is not clear on why exactly replacing the linear penalties with quadratic penalties solves these issues. I'm hoping the authors can clarify the following points:

1) The authors note that, for quadratic penalties, \lambda->0 means no constraints, and \lambda->\infty means hard constraints. Isn't this also true for linear constraints?

2a) Why do linear penalties have unique \lambda_k for each constraint k, but the quadratic objective has only a single \lambda for all constraints?

2b) Why can CV over \lambda be used for quadratic constraints - what is the justification? And, more importantly, why *can't* it be used with linear constraints? If it can, then this should be one of the baselines compared to in the experiments.

3) What is the criterion optimized for by CV - accuracy or the constraints? Different parts of the paper give different answers to this question. For example, ""... may be easily determined via standard hyperparameter optimization methods"" vs. ""tuning \lambda to directly satisfy the desired fairness metrics"". Or even more unclear - ""choose \lambda ... so that the final solution gives the desired fairness-accuracy trade-off"". How is the desired trade-off defined?

4) If there is a trade-off between fairness and accuracy, and no clear-cut criterion for evaluation is pre-defined, then the evaluation procedure should compare methods across this trade-off (similarly to precision-recall analysis).

5) The authors differentiate between cases where the loss is either convex or non-convex. This is confusing - most losses are convex, and non-linearity appears when they are composed with non-linear predictors. Is this the case here? If so, the fairness constraints are no longer linear, and they're quadratic counterpart is no longer quadratic. It would be helpful if the authors specify where the non-linearity comes from, and what they assume about the loss and predictors.

6) Why is it important to show that the quadratic constraints can be written as an expectation? Isn't the square of an expectation always an expectation of pairs? How does the double summation/integral effect runtime?

7) It would be helpful if the authors differentiate between loss/constraints over the entire distribution vs. over a given sample set.


",4
"This paper aims to address the problem of lacking sufficient demonstrations in inverse reinforcement learning (IRL) problems. They propose to take a meta learning approach, in which a set of i.i.d. IRL tasks are provided to the learner and the learner aims to learn a strategy to quickly recover a good reward function for a new task that is assumed to be sampled from the same task distribution. Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. The proposed algorithm is evaluated on a synthetic grid-world problem, SpriteWorld. The experimental results suggest the proposed algorithm can learn to mimic the optimal policy under the true reward function that is unknown to the learner. 

Strengths: 

1) The use of meta learning to improve sample efficiency of IRL is a good idea.
2) The combination of MAML and MaxEnt IRL is new to my knowledge. 
3) Providing the gradient expression is useful, which is the main technical contribution of this paper. (But it needs to be corrected; see below.)
4) The paper is well motivated and clearly written ""in a high level"" (see below). 

Weakness: 

1) The derivation of (5) assumes the problem is tabular, and the State-Visitations-Policy procedure assumes the dynamics/transition of the MDP is known. These two assumption are rather strong and therefore should be made explicitly in the problem definition in Section 3.

2)  Equation (8) is WRONG. The direction of the derivation takes is correct, but the final expression is incorrect. This is mostly because of the careless use of notation in derivation on p 15 in the appendix (the last equation), in which the subscript i is missed for the second term. The correct expression of (8) should have a rightmost term in the form  (\partial_\theta r_\theta) D  (\partial_\theta r_\theta)^T, where D is a diagonal matrix that contains \partial_{r_i} (\E_{\tau} [ \mu_\tau])_i and i is in 1,...,|S||A|. 

3) Comparison with imitation learning and missing details of the experiments. 
a) The paper assumes the expert is produced by the MaxEnt model. In the experiments, it is unclear whether this is true or not, as the information about the demonstration and the true reward is not provided. 
b) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. In imitation learning, it is known that the expert policy is often sub-optimal, and therefore the goal in imitation learning is mostly only to achieve expert-level performance. Given this, the way this paper evaluate the performance is misleading and improper to me, which leads to an overstatement of the benefits of the algorithm. 
c) It would be interesting to compare the current approach with, e.g., the policy-based supervised learning approach to imitation learning (i.e. behavior cloning). 

4) The rigorousness in technicality needs to be improved. While the paper is well structured, the writing at the mathematical level is careless, which leads to ambiguities and mistakes (though one might be able to work out the right formula after going through the details of the entire paper). Below I list a few points. 
    a) The meta-training set {T_i; i=1,...,N} and the meta-test set {T_j; i=1,...,M} seems to overload the notation. I suppose this is unintentional but it may appear that the two sets share the first T_1,.., T_M tasks, e.g., when N>=M, instead of being disjoint. 
    b) The set over which the summation is performed in (4) is unclear; alpha in (4) is not defined, though I guess it's a positive step size.
    c) On p4, ""we can view this problem as aiming to learn a prior over the intentions of human demonstrators"" is an overstatement to me. At best, this algorithm learns a prior over rewards for solving maximal entropy IRL, not intention. And the experiment results do not corroborate  the statement about ""human"" intention.
    d) On p4,  ""since the space of relevant reward functions is much smaller than the space of all possible rewards deﬁnable on the raw observations"" needs to be justified. This may not be true in general, e.g., learning the set of relevant functions may require a larger space than learning the reward functions.
    e) The authors call \mu_\tau the ""state"" visitation, but this is rather confusing, as it is the visiting frequency of state and action (which is only made clear late in the appendix). 
    f) On p5, it writes ""... taking a small number of gradient steps on a few demonstrations from given task leads"" But the proposed algorithm actually only takes ""one"" gradient step in training. 
    g) The convention of derivatives used in the appendix is the transpose of the one used in the main paper.

Minor points: 
1) typo in (2) 
2) p_\phi is not defined, L_{IRL} is not defined, though the definition of both can be guessed.
3) T^{tr} seems to be typo in (11)
4) A short derivation of (2) in the Appendix would be helpful.

",3
"This paper attempts to the solve  data-set coverage issue common with Inverse reinforcement learning based approaches - by introducing a meta-learning framework trained on a smaller number of basic tasks. The primary insight here is that there exists a smaller set of unique tasks, the knowledge from which is transferable to new tasks and using these to learn an initial parametrized reward function improves the coverage for IRL. With experiments on the SpriteWorld synthetic data-set, the authors confirm this hypothesis and demonstrate performance benefits - showcasing better correlation with far fewer  number of demonstrations.

Pros:
+ The solution proposed here in novel - combining meta-learning on tasks to alleviate a key problem with IRL based approaches.
The fact that this is motivated by the human-process of learning, which successfully leverages tranferability of knowledges across a group of basic tasks for any new (unseen) tasks, makes it quite interesting.
+ Unburdens the needs for extensive datasets for IRL based approach to be effective
+ To a large extent, circumvents the need of having to manually engineered features for learning IRL reward functions

Cons:
- Although the current formulation is novel, there is a close resemblance to other similar approaches  - mainly, imitation learning. It would be good if the authors could contrast the differences between the proposed approach and approach based on imitation learning (with similar modifications). Imitation learning is only briefly mentioned in the related work (section-2), it would be helpful to elaborate on this. For instance, with Alg-1 other than the specific metric used in #3 (MaxEntIRLGrad), the rest seems close similar to what would be done with imitation learning?
- One of main contributions is avoiding the need for hand-crafted features for the IRL reward function. However, even with the current approach, the sampling of the meta-learning training and testing tasks seem to be quite critical to the performance of the overall solution and It seems like this would require some degree of hand-tuning/picking. Can the authors comment on this and the sensitivity of the results to section of meta-learning tasks and rapid adaption?
- The results are limited, with experiments using only the synthetic (seemingly quite simple)  SpriteWorld data-set. Given the stated objective of this work to extend IRL to beyond simple  cases, one would expect more results and with larger problems/data-sets.
	- Furthermore, given that this work primarily attempts to improve performance with using meta-learned reward function instead of default initialization - it might make sense to also compare with method such as Finn 2017, Ravi & Larochelle 2016.

Minor questions/issues:
> section1: Images are referred to as high dimensional observation spaces, can this be further clarified?
>  section3: it is not immediately obvious how to arrive at eqn.2. Perhaps additional description would help.
> section4.1 (MandRIL) meta-training: What is the impact/sensitivity of computing the state visitation distribution with either using the average of expert demos  or the true reward? In the reported experiments, what is used and what is the impact on results, if any ?
> section4.2: provides an interesting insight with the concept of locality of the prior and establishes the connection with Bayesian approaches.
> With the results, it seems like that other approaches continue to improve on performance with increasing number of demonstrations (the far right part of the Fig.4, 5) whereas the proposed approach seems to stagnate - has this been experimented further ? does this have implications on the capacity of meta-learning ?
> Overall, given that there are several knobs in the current algorithm, a comprehensive sensitivity study on the relative impact would help provide a more complete picutre",4
"The paper defines a new machine learning problem setup by applying the meta-learning concept to inverse reinforcement learning (IRL). The motivation behind this setup is that expert demonstrations are scarce, yet the reward functions of related tasks are highly correlated. Hence, there is plenty of transferrable knowledge across tasks.

Strengths:
--
 * The proposed setup is indeed novel and very ecologically-valid, meaning meta-learning and IRL are natural counterparts for providing a remedy to an important problem.

 * The paper is well-written, technically sound, and provides a complete and to-the-point literature survey. The positioning of the novelty within literature is also accurate.


Weaknesses:
--

 * The major weakness of the paper is that its hypothesis is not tested exhaustively enough to draw sound conclusions. The paper reports results only on the SpriteWorld data set, which is both synthetic and grid-based. Having acknowledged that the results reported on this single data set are very promising, I do not find this evidence sufficient to buy the proposed hypothesis. After all, IRL is meant mainly for real-world tasks where rewards are not easy to model. A single result on a simplified computer game does not shed much light on where an allegedly state-of-the-art model stands toward such an ambitious goal. I would at least like to see results on some manipulation tasks, say on half-cheetah, ant etc.

 * Combination of MaxEnt IRL and MAML is novel. That said, the paper combines them in the most straightforward way, which does not incur any complications that call for technical solution that can be counted as a contribution to science. Overall, I find the novelty of this work overly incremental and its impact potential very limited.

 * A minor issue regarding clarity. Equation 3 is not readable at all. The parameter \phi and the loss \mathcal{L}_{IRL} have not been introduced.

This paper consists of a mixture of strong and weak aspects as detailed above. While the proposed idea is novel and the first results are very promising, I view this work to be at a too early stage to appear in ICLR proceedings as a full-scale paper. I would like to encourage the authors to submit it to a workshop, strengthen its empirical side and resubmit to a future conference.",4
"There is certainly additional novelty in that this paper focuses on models performing same/identical tasks (compared the results from the catastrophic forgetting paper), and because this model more clearly delineates the parameters that are shared across the models, vs those that are not. But both of those advances feel incremental.
 ",6
"
- This ""neural brainwashing"" is catastrophic forgetting. Technically speaking, this is catastrophic forgetting. 

- Also, some works targeting NAS (which I reckon should as well be cited due to being quite related) have targeted similar forgetting issues, e.g. Xu and Zhu, NIPS 2018 ""Reinforced continual learning"". It is nice to enrich the literature with new terms, when there is a need to. In my opinion, in this particular case, neural brainwashing is catastrophic forgetting. 

- Forgetting is not necessarily an ""individual problem"", sticking to the language used in the third paragraph of the first page. The same applies to ""single-model forgetting"". 

- page 1 ""Our work is the first of which we are aware to identify neural brainwashing and to propose a solution."": According to the authors' argument, this is the case. Again, mine is different.

- Novelty w.r.t. works tackling catastrophic forgetting, most notably EWC, is minimal. Also, comparing to other state-of-the-art algorithms targeting catastrophic forgetting can further enrich the experiments. 

- 3.1.1. On a technical level, there is no inherent difference, between EWC and the proposed algorithm. 

- Writing can improve, both in terms of the flow and the language. There are also a few typos, e.g. in the first line of the caption of Figure 2.

- Apart from the aforementioned issue (comparing to other state-of-the-art catastrophic forgetting algorithms), the experiments are rigorously prepared. ",5
"This paper discusses the phenomena of “neural brainwashing”, which refers to that the performance of one model is affected via another model sharing model parameters. To solve the issue, the authors derived a new loss out from maximizing the posterior of the parameters. With the new loss, the neural brainwashing is largely diminished. 
The derived new loss looks meaningful to me and I think this is a valuable work for handling the weights coadaptation between two neural models, which with no doubt will bring great interests within the community of neural architecture search.

Here are some comments on the aspects that this paper can be improved: 

1)	A very important related work [1] is missed in this paper. [1] discussed the properties of “one-shot model”, which means that several different architectures are unified into the same model by sharing model weights. Furthermore, [1] discussed “neural brainwashing” (although not with the same name) and how to handle it in a very simple way (by randomly dropping path). This definitely should be a baseline to compare with. In addition, a very recent work [2] also leverages model sharing to conduct neural architecture search.

2)	Although I understand that to improve accuracy of NAS is not the main goal of this paper, the baseline number to be improved over is too weak. For example, 4.87 of CIFAR10 in ENAS. Per my own hands on experience, it does not need too many hyperparameter tuning of ENAS to obtain < 4% error rate. Please provide more convincing baseline numbers and supporting evidences of the better performance of WPL in NAS.


[1] Bender, Gabriel, et al. ""Understanding and simplifying one-shot architecture search."" International Conference on Machine Learning. 2018.
[2] Luo, Renqian, et al. ""Neural architecture optimization."" NIPS (2018).
",6
"------------------------------------------
Summary
------------------------------------------
This paper performs unsupervised classification where the number of classes is unknown. The main idea is to use the CycleGAN framework such that one can reconstruct the original image by first moving to latent space that represents another class (via the connector network), then moving back to the original latent space and going back into image space using a generator. Experiments are conducted on MNIST and CIFAR.

------------------------------------------
Evaluation
------------------------------------------
The paper tackles an important problem: namely, unsupervised classification (i.e. clustering). I think the use cycle-consistency loss in an auxiliary latent space is quite clever. However, experimental results are lacking. Unsupervised clustering (even when number of classes is not known) is a very well studied problem in machine learning. The authors should compare against at least a few reasonable baselines.

------------------------------------------
Presentation
------------------------------------------
I found the presentation to be somewhat wanting. Section 3 is extremely confusing and in my opinion, not well-motivated. For example, why is self-expressivity important? Why can we assume propositions 3.1 and 3.2?

",4
"This paper develops an unsupervised classification algorithm using the idea of CycleGAN. Specifically, it constructs a piece-wise linear mapping (the Connector Network) between the discriminator network and the generator network. The learning objective is based on the cycle-consistency loss. Experiments show that it can achieve reasonable loss. This paper addresses an important problem, namely, unsupervised image classification, and may present interesting ideas. However, the paper is not in a good shape for publication in its current form.

First, the paper is not well written and many of the key ideas are not clear. It devotes more than half of the pages to the review of the preliminary materials in Sections 2-3 while only briefly explained the main algorithm in Section 4. Many of the details are missing. For example, why L1-loss is used in (5)-(7) in Algorithm 1? What is the “random-walk Laplacian matrix L_{sym}” (never defined)? More importantly, it seems that Section 3.4 is a key section to explain how to perform unsupervised classification. However, the ideas (regarding the footprints and footprint mask etc.) are totally unclear. It is assumed that all different classes have equal probabilities. In this setting, it is unclear (in its intuition) why it is possible to assign a cluster index to its true class labels. What is the key signal in the data that enables the algorithm to relate different clusters to their corresponding classes, especially when all classes have equal probability? Furthermore, it is not clear why the mapping from H to Z can be written as a sum of C_1,…,C_k in Proposition 3.2. If the final mapping is piece-wise linear, how can it be written as a sum of linear mappings? Similar question arises in the first paragraph of Section 4.1: if the connector network is constructed as a piecewise linear function (as stated earlier in the paper in abstract and introduction), then how can it be written as a matrix? (Only linear mapping can be expressed as a matrix.)

Second, the experiment is insufficient. None of the experiment details are presented in the paper. Only the final accuracy of 0.874 is given without any further details. What is the model architecture and size? More experimental analysis should be presented. For example, there are many different hyperparameters in the algorithms. How are the \lambda_D, \lambda_G chosen when there is no labeled validation set? How sensitive is the algorithm to different model architecture and model size? Furthermore, none of the baselines results are presented and compared against.

A lot of related works are missing. There have been a lot of emerging works related to unsupervised classification recently, which should be discussed and compared:
[1] G. Lample, L. Denoyer, and M. Ranzato.  Unsupervised machine translation using monolingual corpora only. ICLR, 2018.
[2] M. Artetxe, G. Labaka, E. Agirre, and K. Cho.  Unsupervised neural machine translation. ICLR, 2018.
[3] Y. Liu, J. Chen, and L. Deng.  Unsupervised sequence classification using sequential output statistics. NIPS, 2017
[4] A. Gupta, A. Vedaldi, A. Zisserman. Learning to Read by Spelling: Towards Unsupervised Text Recognition. arXiv:1809.08675, 2018.
[5] G. Lample, M. Ott, A. Conneau, L. Denoyer, M. Ranzato. Phrase-based & neural unsupervised machine translation. EMNLP 2018.

The presentation of the paper should be significantly improved as it is currently hard to read due to many grammar and English usage issues as well as other unclear statements. Just to name a few examples below:
-	(1st paragraph of Introduction): “…imagine the learned objectconstruct…”
-	The last paragraph in Section 1 is not in the right position and should be placed somewhere else in the introduction.
-	In the first paragraph of Section 2.2, “one of the clustering algorithm” should be “one of the clustering algorithms”.
-	In the first paragraph of Section 3, it is not clear what it means by “we can make the tuples (Z,X,H) for the whole dataset”.
-	At the end of the first paragraph of Section 3, there is a missing reference in “network in section()”.
-	In the third paragraph on page 4, there is a grammar issue in “H and Z have greater than convexity than X…” and in “it allows the linear combination on these two manifolds in the feature spaces H and Z are and”.
-	In the first paragraph of Section 3.2, it is not clear what it means by “two feature spaces will be trained by the cycle consistency loss to obtain the tuples (Z,X,H) with the correct permutation, where all of elements is in the same-class manifold and shares same learned features.”
-	In the first paragraph of page 5, “cycle-consistency loss z C(D(G(z))) and backward cycle consistency loss x G(C(D(x)))” does not read well. It sounds like z is the loss C(D(G(z))) and x is the loss G(C(D(x)))?
-	Typo in Figure 4: “a shows” should be “(a) shows”.
",4
"The manuscript proposes a method for unsupervised learning with unknown class number k. The problem is classical and important. The proposed method is interesting and novel, but the experiments are not convincing. In detail, it did not compare other methods in the experiments. 
Pros: clear description and novelty of the method
Cons: insufficient experiments. ",5
"This paper focuses on fast adaptation to new behaviour of the other agents of the environment, be it opponents or allies. To achieve this, a method based on MAML is proposed, with two main components:
1) Learn a model of some characteristics of the opponent, such as ""the final goal, next action, or any other character we wish to predict""
2) Learn a policy that takes as input the output of the model and the state, and that outputs the action of the agent.

The goal is that after a phase of meta learning, where the agents learns how to play against some new agents sampled from the distribution of opponents, it can quickly adapt to a new unseen agent. (""Experimental results show that the agent can adapt to the new opponent with a small number of interactions with the opponent"")

While the motivation of this work is clear and the goal important for the RL community, the experiments fail to support the claim above.

The first task they demonstrate their approach on is a chasing game, where the opponent has a private goal cell it tries to reach, and the agent has to chase it. At the end of the game, it gets a reward of 10 if it is on the same cell, 5 if in an adjacent cell, and 0 otherwise. The exact details of the dynamic are not really clear, for example what happens in the event of a collision is not mentionned, and the termination condition is not mentionned either. (the text reads ""One game lasts for at least 15 steps"", maybe it was meant to be ""at most 15 steps"" ?).
The first incoherent aspect of this experiment is that they use 800 iterations of meta-learning, and then, when testing, they fine-tune their networks against each test opponent during 4000 games. That is, they use 5 times more game when fine-tuning as opposed to when pre-training, which contradicts the claim ""the agent can adapt to the new opponent with a small number of interactions with the opponent"" (this is not really few-shot learning anymore).
Further more, they compare their approach with various ablations of it: they either remove the meta-learning for the model (MA), for the policy (MO), or both (NM). The description of the NM baseline is not very precise, but it seems that it simply boils down to a regular (dueling) DQN: In this setting, since the opponent appears to have a fixed goal, finetuning against a single opponent simply boils down to learning a policy that reaches a specific cell of the grid, which we can expect DQN to solve perfectly on a 8x8 grid with 4000 training games. And yet, the curves for NM in graph 2c is not only really noisy, but also falls far from the optimum, which the authors don't discuss. There might be a problem with the hyperparameters used or the training loop.

The second task is a blocking game: the opponent has to choose amongst 5 paths to get to the top, and the agent has to choose the same path in order to block it. The action space should be precisely described, as it stands it is difficult to understand the dynamic. There are at least two possible ways to parametrize the actions:
1) Similarly to the blocking game, the agents could move in the 8 directions. In that case, based on the picture 3a, it seems that the agent can just mirrors the move of the opponent: since the moves are simultaneous, that would mean that the agent is always one step late, but each path is long enough for the agent to reach the exit before its opponent (it explicitly stated that the agent needs to block the exit, and that the opponent will not change path during one game). That would imply that perfect play is possible without any meta-learning or oponent modeling, and once again the NM baseline (or any vanilly DQN/Policy gradient method) should perform much better.
2) One other alternative is to have an action space of 5 actions, which correspond to the 5 paths. In that case the game boils down to a bandit, since both agents only take one action. Note that under this assumption, the random policy would get the right path (and reward +10) with probability 1/5 and a wrong one (reward -10) with probability 4/5, which leads to an expectated reward of -10*4/5 + 10/5 = -6. This is not consistent with the graph 3c, since at the beginning of the training, the NM agent should have a random policy, and yet the graph reports an average reward of -10 (the -6 mark seems to be reached after ~1000 episodes)

The last task boils down to one opponent that reaches one cell on the right, and the agent must reach the matching cell on the left. In this setting, the same discussion on the action space as the second task can be made. We note that the episode for 16 steps, and the distance from the center to any cell is at most 4 steps: an optimal policy would be to wait for 4 steps in the middle, and as soon as the opponent has reached its goal, use the remaining 12 steps to get to the mirror one. Once again, this policy doesn't require any prediction on the opponent's goal, and it's hard to believe that DQN (possibly with an lstm) is not able to learn that near perfectly.


In a last test the authors compare the performance of their algorithms in a one shot transfer setting: they sample 100 opponents for each task and play only one game against it (no fine-tuning). It is not clear whether special care has been taken to ensure that none of the sampled opponents has already been seen during training.
We note that the rewards reported for MO and MA (resp 0.0 and -0.08) are not consistent with the description of the reward function: on the worst case, the opponent chooses a goal on one extreme (say y1 = 1) and the agent chooses an object on the other end (say y2 = 7). In that case, the reward obtained is sampled from a gaussian with mean \mu = 10 - 3/2 * |y1 - y2| (which in this case evalutes to 1), and variance 1. This is highly unlikely to give such a low average reward over 100 episodes (note that this is worst case, if the opponent's goal is not on the extreme, the expected reward is necessarily higher). One possibility is that the agent never reaches an object, but in that case it would imply the that the meta-learning phase was problematic.
We also note that it is explicited that the MOA, MO and MA methods are tested after meta-training, but nothing is precised for NM. Has it been trained at all? Against which opponents? Is it just a random policy? There are too many missing details for the results to be interpretable.


Apart from that, the paper contains a significant amount of typos and gramatical mistakes please proof-read carefully. Some of them are:
""To demonstrate that meta-learning can do take""
""player 1 is the red grid and player 1 is the green one""
""we further assume that there exist a distribution""
"" the goal’s location over the map is visualize in figure""
""Both players takes actions simultaneously""",4
"The paper presents an approach to multi-agent learning based on the framework of model-agnostic meta learning. The originality of the approach lies in the decomposition of the policy in two terms, with applications to opponent modeling: the first part of the policy tries to predict some important characteristic of the agent (the characteristic itself is prior knowledge, the value it takes for a particular opponent is learnt from observations). The second part of the policy takes the estimated characteristic of the opponent as input, the current state and produces the action. All networks are trained within the MAML framework. The overall approach is motivated by the task of opponent modeling for multi-agent RL.

The approach makes sense overall -- the ""value"" of the opponent is valuable prior knowledge. The originality is limited though. In this kind of paper, I would expect the experiments to make a strong case for the approach. Unfortunately, the experiments are extremely toyish and admittedly not really ""multi-agent"": the ""opponent"" has a fixed strategy that does not depend on what the current is doing (it is therefore not really an opponent). The experimental protocol is more akin to multitask RL than multi-agent RL, and it is unclear whether the approach could/should work for opponent modeling even on tasks of low complexity. In other words, the experimental section does not address the problem that is supposed to be addressed (opponent modeling).

other comments:
- ""The opponent in our game is considered as some player that won’t adapt its policy to our agent."" -> in the experiments it is worse than that: the opponents actions do not even depend on what the agent is doing... So admittedly the experiments are not really ""multi-agent"" (or ""multi-agent"" where the ""opponent"" is totally independent of what the agent is currently doing).

- ""Each method trains 800 iterations to get the meta learners and use them to initialize their networks. Then 10 new opponents are sampled as testing tasks. Four methods all train 4000 games for each testing task."" -> what does 800 iterations mean? Does it mean 800 episodes (it would seem strange for a ""fast adaptation task"" to have fewer episodes for training than for testing).

- ""Notice that the reward trend for MOA first drops and then raises as the testing process goes on. This shows the process that the meta-learner adapt to the current task."" -> the adaptation to the new opponent does not really explain the drop?

- Figure 3(c): the MA baseline has a reward of ~-10, which is worse than random (a uniform random placement at the 5 strategic positions would get 10*1/5-10*4/5 = -6). On the other hand, MOA achieves very high rewards, which indicates that the ""opponents"" strategies have low entropy. What is the best achievable reward on the blocking game?
",4
"This paper proposes to apply MAML to a multi-agent setting. In this formulation each opponent corresponds to a task and two separate parts of the policy are learned via meta-learning: 
1) the opponent modelling network that predicts the value function for a given opponent based on past actions and states. 
2) the policy network which takes in the state and the predicted value function of the opponent. 
The main concern with this paper is the lack of technical detail and an important missing baseline. The paper also suffers from lacking clarity due to a large number of grammatical mistakes. 

Technical detail and concerns: 
The paper mentions Duelling DQN as the RL algorithm in the inner loop. This is very unusual and it's a priori unclear whether MAML with DQN in the inner loop is a sensible algorithm. For example, DQN relies both on a target network and an argmax operator which seem to violate the differentiability requirements needed for MAML regarding higher order gradients. The authors entirely miss this and fail to address possible concerns. 

The authors also fail to provide any details regarding the exploration scheme used. In fact, a value function is never mentioned, instead the authors talk about a policy pi^a_i, leaving it unclear how this policy is derived from the value function. When the Q-function takes as input the true opponent, there is no need for meta-learning of the policy: Given a known opponent, the tuple (s_t, opponent) defines a Markov state. As far as I could gather from the paper, the authors are missing a baseline which simply learns a single Q-function across all opponents (rather than meta-learning it per opponent) that takes as input the predicted opponent. 
My expectation is that this is more or less what is happening in the paper. The authors also fail to compare and contrast their method to a number of recent multi-agent algorithms, eg. MADDPG, COMA and LOLA. 

Furthermore, the results are extremely toy and seem to be for single runs , rendering them insignificant. 

While the idea itself is interesting, the above concerns render the paper unsuitable for publication in it's current form.


",4
"[Post-rebuttal update] No author response was provided to address the reviewer comments. In particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern. I've left my overall score for the paper unchanged.

[Summary] The authors propose a protocol for training a model over private user data in a federated setting. In contrast with previous approaches which tried to ensure that a model would not reveal too much about any individual data point, this paper aims to prevent leakage of information about any individual client. (There may be many data points associated with a single client.)

[Key Comments] The submission generally seems polished and well-written. However, I have the impression that it's largely an incremental improvement over recent work by McMahan et al. (2018).
* If the main improvement of this paper over previous work is the dynamic adaptation of weight updates discussed in Section 3, the experimental results in Table 1 should compare the performance of the protocol with vs. without these changes. Otherwise, I think it would be helpful for the authors to update the submission to clarify their contributions.
* Updating Algorithm 1 / Line 9 (computation of the median weight update norm) to avoid leaking sensitive information to the clients would also strengthen the submission.
* It would also be helpful if the authors could explicitly list their assumptions about which parties are trusted and which are not (see below).

[Details]
[Pro 1] The submission is generally well-written and polished. I found the beginning of Section 3 especially helpful, since it breaks down a complex algorithm into simple/understandable parts.

[Pro 2] The proposed algorithm tackles the challenging/well-motivated problem of improving federated machine learning with strong theoretical privacy guarantees.

[Pro 3] Section 6 has an interesting analysis of how the weight updates produced by clients change over the course of training. This section does a good job of setting up the intuition for the training setup used in the paper, where the number of clients used in each round is gradually increased over the course of training.
 
[Con 1] I had trouble understanding the precise threat model used in the paper, and I think it would be helpful if the authors could update their submission to explicitly list their assumptions in one place. It seems like the server is trusted while the clients are not. However, I was unsure whether the goal was to protect against a single honest-but-curious client or to protect against multiple (possibly colluding) clients.

[Con 2] During each round of communication, the protocol computes the median of a set of values, each one originating from a different client, and the output of this computation is used to perform weight updates which are sent back to the clients. The authors note that ""we do not use a randomized mechanism for computing the median, which, strictly speaking, is a violation of privacy. However, the information leakage through the median is small (future work will contain such privacy measures)."" I appreciate the authors' honesty and thoroughness in pointing out this limitation. However, it does make the submission feel like a work in progress rather than a finished paper, and I think that the submission would be a bit stronger if this issue was addressed.

[Con 3] Given the experimental results reported in Section 4, it's difficult for me to understand how much of an improvement the authors' proposed dynamic weight updates provide in practice. This concern could be addressed with the inclusion of additional details and baselines:
* Few details are provided about the model training setup, and the reported accuracy of the non-differentially private model is quite low (3% reported error rate on MNIST; it's straightforward to get 1% error or below with a modern convolutional neural network). The authors say they use a setup similar to previous work by McMahan et al. (2017), but it seems like that paper uses a model with a much lower error rate (less than 1% based on a cursory inspection), which makes direct comparisons difficult.
* The introduction argues that ""dynamically adapting the dp-preserving mechanism during decentralized training"" is a significant difference from previous work. The claim could be strengthened if the authors extended Table 1 (experimental results for differentially private federated learning) in order to demonstrate the effect of dynamic adaptation on model quality.",4
"The paper revisits the federated learning framework from McMahan in the context of differential privacy.  The general concern with the vanilla federated learning framework is that it is susceptible to differencing attacks. To that end, the paper proposes to make the each of the interaction in the server-side component of the gradient descent to be differentially private w.r.t. the client contributions. This is simply done by adding noise (appropriately scaled) to the gradient updates.

My main concern is that the paper just described differentially private SGD, in the language of federated learning. I could not find any novelty in the approach. Furthermore, just using the vanilla moment's accountant to track privacy depletion in the federated setting is not totally correct. The moment's accountant framework in Abadi et al. uses the ""secrecy of the sample"" property to boost the privacy guarantee in a particular iteration. However, in the federated setting, the boost via secrecy of the sample does not hold immediately. One requirement of the secrecy of the sample theorem is that the sampled client has to be hidden. However, in the federated setting, even if one does not know what information a client sends to the servery, one can always observe if the client is sending *any* information. For a detailed discussion on this issue see https://arxiv.org/abs/1808.06651 .",4
"The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. 

Some of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:
1.	The main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record – why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork & Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?
2.	Another issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals. Therefore, are the sizes used in the experiments reasonable?
3.	In the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016
4.	The theoretical analysis of the algorithm is only implied and not stated clearly
5.	In reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?
6.	In describing the experiment it says that “For K\in\{1000,10000} data points are repeated.” This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?
7.	Since grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds
8.	The results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.

Additional comments
9.	The introduction is confusing since it uses the term “federated learning” as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.
10.	In the abstract the term “differential attacks” is used – what does it mean?
11.	“An independent study McMahan et al. (2018), published at the same time”- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.
12.	In the section “Choosing $\sigma$ and $m$” it is stated that the higher \sigma and the lower m, the higher the privacy loss. Isn’t the privacy loss reduced when \sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. 
13.	At the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.
14.	In the experiments (Table 1), what does it mean for \delta^\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?
15.	The methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?
16.	The citations are used incorrectly, for example “sometimes referred to as collaborative Shokri & Shmatikov (2015)” should be “sometimes referred to as collaborative (Shokri & Shmatikov, 2015)”. This can be achieved by using \citep in latex. This problem appears in many places in the paper, including, for example, “we make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).” Which should be “we make use of the moments accountant as proposed by Abadi et al. (2016).” In which case you should use only \cite and not quote the name in the .tex file.
17.	“We use the same deﬁnition for differential privacy in randomized mechanisms as Abadi et al. (2016):” – the definition of differential privacy is due to Dwork, McSherry, Nissim & Smith, 2006
18.	Notation is followed loosely which makes it harder to follow at parts. For example, you use “m_t” for the number of participants in time t but in some cases,  you use only m as in “Choosing $\sigma$ and $m$”.
19.	In algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. 
20.	Figure 2: I think it would be easier to see the results if you use log-log plot
21.	Discussion: “For K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.” – it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.
22.	In the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan


It is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. 

",4
"The paper addresses a means of boosting the accuracy of automatic translators (sentences) by training dual models (a.k.a. language A to B, B to A), multipath (e.g. A to B to C) and cyclical (e.g. A to B to C to A) while starting with well initialized models for translating simple pairs. The idea that additional errors are revealed and allow classifiers to adapt, thus boosting classifier performance, is appealing and intuitive. That a theoretical framework is presented to support this assumption is encouraging, but the assumptions behind this analysis (e.g. Assumption 1) are rather strong. Equation 2 assumes independence. Equations 3-5 can be presented more clearly. Not only are translation errors not uniform in multiclass settings, but they can be highly correlated - this being a possible pitfall of boosting approaches, seen as general classifier errors. The same strong assumptions permeate from the dual to the cyclical case. On the other hand, the (limited) empirical test presented, using a single dataset and baseline classifier method, does support the proposed improvement by boosting, albeit leading to improvements which are slight.",6
"The paper gives theorems concerning ""dual learning"" - that is, making
use of round-trip consistency in learning of translation and other
tasks.

There are some interesting ideas here. Unfortunately, I think there
are issues with clarity/choice of notation and correctness (errors
resulting from problems with the notation - or at least it's very
hard to figure out if things are correct under some intepretation).

More specifically, I'm uneasy about the use of x^i and x^j as defined
in section 2. In some cases x^j is a deterministic function of x^i, in
some cases it's a random variable, these cases are mixed. Section 2
becomes tangled up in this issue. It would be much better I think to
define a function f_ij(x) for each (i,j) pair that maps a sentence
x \in S^i to its correct translation f_ij(x) \in S^j.

A critical problem with the paper is that Eq. 2 is I think incorrect.
Clearly,

Pr(T_ij(x_i) = f_ij(x_i), T_ji(f_ij(x_i)) = x_i)      [1]
=
Pr(T_ij(x_i) = f_ij(x_i))                             [2]
*
Pr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i))    [3]

I think [1] is what is meant by the left-hand-side of Eq 2 in the paper -
though the use of x^j is ambiguous (this ambiguity is a real issue).

It can be verified that

Pr(T_ij(x_i) = f_ij(x_i)) = p_ij

however

Pr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i)) \neq p^r_ji

The definition of p^r_ji is that of a different quantity.

This problem unfortunately permeates the statement of theorem 1, and
the proof of theorem 1. It is probably fixable but without a
significantly revised version of the paper a reader/reviewer is
basically guessing what a corrected version of the paper would
be. Unfortunately I think publishing the paper with errors such as
this would be a problem.

Some other points:

[1] The theorem really does have to assume that there is a unique
correct translation f_ij(x^i) for each sentence x^i. Having multiple
possible translations breaks things. The authors say in section 2 ""In
practice, we may select a threshold BLEU (Papineni et al., 2002a)
score, above which the translation is considered correct"": this seems
to imply that the results apply when multiple translations (above
a certain BLEU score) are possible. But my understanding is that this
will completely break the results (or at least require a significant
modification of the theory).

[2] A further problem with ambiguity/notation is that T^d is never
explicitly defined. Presumably we always have T_ij^s(x^i) = T_ij(x^i) if
T_ji(T_ij(x^i)) = x^i? That needs to be explicitly stated.

[3] There may be something interesting in theorem 1 - putting aside point
[1] above - but I am just really uneasy with this theorem and its proof
given that it uses p^r_ji, and the issue with Eq. 2.

[4] Another issue with the definition of p^r_ij: the notation
P_{X^(j, r) ~ \mu}(...) where the expression ... does not refer
to X^{j, r} (instead it refers to x^j) is just really odd,
and confusing.
",2
"This paper provides a theoretical perspective of the dual learning tasks and proposes two generalizations (multipath/cycle dual learning) that utilize multiple language sets. Through experiments, the paper discusses the relationship between theoretical perspective and actual translation quality.

Overall, the paper is well written and discussed enough. My concern is about Theorem 1 that could be a critical problem.
In the proof of Theorem 1, it discussed that the dual learning can minimize Case 2. This assumption is reasonable if the vanilla translator is completely fixed (i.e., no longer updated) but this condition may not be assumed by the authors as far as I looked at Algorithm 2 and 3 that update the parameters of vanilla translators directly. The proof is constructed by only the effect against Case 2. However, if the vanilla translator is directly updated through dual training, there should exist some random changes in also Case 1 and this behavior should also be included in the theorem.

Correction and suggestions writing:
* It is better to introduce an additional $\alpha$, $\beta$ and $\gamma$ for the vanilla translation accuracy (e.g., $\alpha_0 := p_{ij}p_{ji}^r$) so that most formulations in Section 3 can be largely simplified.
* In Justification of Assumption1 ... ""the probability of each cluster is close to $p_max$"" ->  maybe ""greater than $p_max$"" to satisfy the next inequality.
* Eq. (3) ... $T_{ji}^d(T_{ij}^d(x^{(i)})$ -> $T_{ji}^d(x^{(j)})$ to adjust other equations.
* Section 3 Sentence 1: ""translatorby"" -> ""translator by""
* Section 4.2: ${\rm Pr}_{ X^{(3)} \sim T_{23}^d (X^{(1)}) }$ -> $... (X^{(2)}) }$",5
"This paper presents one end-to-end multi-task learning architecture for depth & segmentation map estimation and the driving prediction. The whole architecture is composed of two components, the first one is the  perception module (segmentation and depth map inference), the second one is the driving decision module. The training process is sequential, initially train the perception module, then train the driving decision task with freezing the weights of the perception module. The author evaluated the proposed approach on one simulated dataset, Experimental results demonstrated the advantage of multi-task compared to the single task. 

Advantages:
The pipeline is also easy to understand, it is simple and efficient based on the provided results.
The proposed framework aims to give better understanding of the application of deep learning in self-driving car project. Such as the analysis and illustration in Figure 3. 

Questions:
There are several typos needed to be addressed. E.g, the question mark in Fig index of section 5.1. There should be comma in the second sentence at the last paragraph of section 5.2.  
Multi-task, especially the segmentation part is not novel for self-driving car prediction, such as Xu et al. CVPR’ 17 paper from Berkeley. The experiment for generalization shows the potential advancement, however, it is less convincing with the limited size of the evaluation data, The authors discussed about how to analyze the failure causes, however, if the perception  learning model does not work well, then it would be hard to analyze the reason of incorrectly prediction.

In general, the paper has the merits and these investigations may be helpful for this problem, but it is not good enough for ICLR.

",4
"Major Contribution:
This paper details a method for a modified end-to-end architecture that has better generalization and explanation ability. The paper outlines a method for this, implemented using an autoencoder for an efficient feature extractor. By first training an autoencoder to ensure the encoder captures enough depth and segmentation information and then using the processed information as a more useful and compressed new input to train a regression model. The author claimed that this model is more robust to a different testing setting and by observing the output of the decoder, it can help us debug the model when it makes a wrong prediction.

Organization/Style:
The paper is well written, organized, and clear on most points. A few minor points:
1) On page 5, the last sentence, there is a missing table number.
2) I don't think the last part FINE-TUNE Test is necessary since there are no formal proofs and only speculations.

Technical Accuracy:
The problem that the paper is trying to address is the black-box problem in the end-to-end self-driving system.
The paper proposes a method by constructing a depth image and a segmentation mask autoencoder. Though it has been proved that it is effective in making the right prediction and demonstrated that it has the cause explanation ability for possible prediction failures. I have a few points:
The idea makes sense and the model will always perform better when the given input captures more relevant and saturated representations. The paper listed two important features: depth information and segmentation information. But there are other important features that are missing. In other words, when the decoder performs bad, it means the encoder doesn't capture the good depth and segmentation features, then it will be highly possible that the model performs badly as well. However, when the model performs bad, it does not necessarily mean the decoder will perform badly since there might be other information missing, for example, failure to detect the object, lines and traffic lights etc.

In conclusion, the question is really how to get a good representation of a self-driving scene. I don't think to design two simple autoencoders for depth image construction and image segmentation is enough. It works apparently but it is not good enough.

Adequacy of Citations: 
Good coverage of literature in self-driving.",4
"# Summary

This submission proposes a multi-task convolutional neural network architecture for end-to-end driving (going from an RGB image to controls) evaluated using the CARLA open source simulator. The architecture consists of an encoder and three decoders on top: two for perception (depth prediction and semantic segmentation), and one for driving controls prediction. The network is trained in a two-step supervised fashion: first training the encoder and perception decoders (using depth and semantic segmentation ground truth), second freezing the encoder and training the driving module (imitation learning on demonstrations). The network is evaluated on the standard CARLA benchmark showing better generalization performance in new driving conditions (town and weather) compared to the CARLA baselines (modular pipeline, imitation learning, RL). Qualitative results also show that failure modes are easier to interpret by looking at predicted depth maps and semantic segmentation results.


# Strengths

Simplicity of the approach: the overall architecture described above is simple (cf. Figure 1), combining the benefits of the modular and end-to-end approaches into a feed-forward CNN. The aforementioned two-stage learning algorithm is also explained clearly. Predicted depth maps and semantic segmentation results are indeed more interpretable than attention maps (as traditionally used in end-to-end driving).

Evaluation of the driving policy: the evaluation is done with actual navigation tasks using the CARLA (CoRL'18) benchmark, instead of just off-line behavior cloning accuracy (often used in end-to-end driving papers, easier to overfit to, not guaranteed to transfer to actual driving).

Simple ablative analysis: Table 2 quantifies the generalization performance benefits of pretraining and freezing the encoder on perception tasks (esp. going from 16% to 62% of completed episodes in the new town and weather dynamic navigation scenario).


# Weaknesses

## Writing

I have to start with the most obvious one. The paper is littered with typos and grammatical errors (way too many to list). For instance, the usage of ""the"" and ""a"" is almost non-existent. Overall, the paper is really hard to read and needs a thorough pass of proof-reading and editing. Also, please remove the acknowledgments section: I think it is borderline breaking the double-blind submission policy (I don't know these persons, but if I did that would be a breach of ICLR submission policy). Furthermore, I think its contents are not very professional for a submission at a top international academic venue, but that is just my opinion. 


## Novelty

This is the main weakness for me. The architecture is very close to at least the following works:
- Xu, H., Gao, Y., Yu, F. and Darrell, T., End-to-end learning of driving models from large-scale video datasets (CVPR'17): this reference is missing from the paper, whereas it is very closely related, as it also shows the benefit of a segmentation decoder on top of a shared encoder for end-to-end driving (calling it privileged training);
- Codevilla et al's Conditional Imitation Learning (ICRA'18): the only novelty in the current submission w.r.t. CIL is the addition of the depth and segmentation decoders;
- Müller, M., Dosovitskiy, A., Ghanem, B., & Koltun, V., Driving Policy Transfer via Modularity and Abstraction (CoRL'18): the architecture also uses a shared perception module and segmentation (although in a mediated way instead of auxiliary task) to show better generalization performance (including from sim to real).

Additional missing related works include:
- Kim, J. and Canny, J.F., Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention (ICCV'17): uses post-hoc attention interpretation of ""black box"" end-to-end networks;
- Sauer, A., Savinov, N. and Geiger, A., Conditional Affordance Learning for Driving in Urban Environments (CoRL'18): also uses a perception module in the middle of the CIL network showing better generalization performance in CARLA (although a bit lower than the results in the current submission).
- Pomerleau, D.A., Alvinn: An autonomous land vehicle in a neural network (NIPS'89): the landmark paper for end-to-end driving with neural networks!


## Insights / significance

In light of the aforementioned prior art, I believe the claims are correct but already reported in other publications in the community (cf. references above). In particular, the proposed approach uses a lot more strongly labeled data (depth and semantic segmentation supervision in a dataset of 40,000 images) than the competing approaches mentioned above. For instance, the modular pipeline in the original CARLA paper uses only 2,500 labeled images, and I am sure its performance would be vastly improved with 40,000 images, but this is not evaluated, hence the comparison in Table 1 being unfair in my opinion. This matters because the encoder in the proposed method is frozen after training on the perception tasks, and the main point of the experiments is to convince that it results in a great (fixed) intermediate representation, which is in line with the aforementioned works doing mediated perception for driving.

The fine-tuning experiments are also confirming what is know in the litterature, namely that simple fine-tuning can lead to catastrophic forgetting (Table 3).

Finally, the qualitative evaluation of failure cases (5.3) leads to a trivial conclusion: a modular approach is indeed more interpretable than an end-to-end one. This is actually by design and the main advocated benefit of modular approaches: failure in the downstream perception module yields failure in the upstream driving module that builds on top of it. As the perception module is, by design, outputting a human interpretable representation (e.g., a semantic segmentation map), then this leads to better interpretation overall.


## Reproducibility

There are not enough details in section 3.1 about the deep net architecture to enable re-implementation (""structure similar to SegNet"", no detailed description of the number of layers, non-linearities, number of channels, etc).

Will the authors release the perception training dataset collected in CARLA described in Section 4.2?



# Recommendation

Although the results of the proposed multi-task network on the CARLA driving benchmark are good, it is probably due to using almost two orders of magnitude more labeled data for semantic segmentation and depth prediction than prior works (which is only practical because the experiments are done in simulation). Prior work has confirmed that combining perception tasks like semantic segmentation with end-to-end driving networks yield better performance, including using a strongly related approach (Xu et al). In addition to the lack of novelty or new insights, the writing needs serious attention.

For these reasons, I believe this paper is not suitable for publication at ICLR.",3
"In this paper, the authors propose a novel method of Task-GAN of image coupling by coupling GAN and a task-specific network, which alleviates  to  avoid hallucination or mode collapse. In general, the paper is addressing an important problem but I still have several concerns as follows:
1. The technical contribution is rather incremental since there exist numerous works on introducing another discriminator to GAN, such as Triple-GAN. 

2. Actually, as the authors mentioned, GAN is not an appropriate model for image restoration when  accurate image completion is required. The authors are expected to make comparison with methods not based on GAN framework. 

3.  The authors should clarify the details on the Task network since it is non-trivial to model a task. 

 ",4
"Authors propose to augment GAN-based image restoration with another task-specific branch such as classification tasks for further improvement.

However, the novelty is limited and not well explained.
1. The idea of adding a task-specific branch has been proposed in Huang et al’s work.
Rui Huang, Shu Zhang, Tianyu Li, Ran He, Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis, ICCV 2017.

2. It is not clear why for task-specific loss authors use mse loss instead of cross-entropy loss.
3. It is not clear how much data is used to train the super-resolution model and whether there is overlap between training data for super-resolution task and test data for recognition task.
4. The proposed method is not compared with other super-resolution methods.
5. There are typos with citations. There should be parenthesis around citations.",5
"This paper proposed a new method for image restoration based a task-discriminator in addition to the GAN network. It shows superior performance than the baseline methods without such task-discriminator on medical image restoration and image super-resolution. While the results are better, the idea seems straightforward and has limited novelty. Please see the following comments:

1. Adding an task-discriminator in a GAN network seems straightforward to improve the specific task. And this idea has already used in existing papers, e.g. Cycada.  

Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A. and Darrell, T., 2017. Cycada: Cycle-consistent adversarial domain adaptation. ICML, 2018

2. On the application side, the results are not very convincing because the baseline methods were not selected properly. For medical image reconstruction and image super-resolution, the proposed method was not compared with any of the state-of-the-art methods, but only with the same method without a task-discriminator as a baseline. For those tasks, there are many traditional methods and deep nets with different losses. For example, a simple L1/L2 or perceptual loss probably leads to better PSNR than the GAN loss, which is not compared at all. See the attached references. 


Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z. and Shi, W., Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR 2017.

Johnson, J., Alahi, A. and Fei-Fei, L., Perceptual losses for real-time style transfer and super-resolution. In ECCV 2016.

Kim, J., Kwon Lee, J. and Mu Lee, K., Accurate image super-resolution using very deep convolutional networks. In CVPR 2016.

3. Some questions about medical image datasets. For the low-dose PET dataset, the input was randomly undersampled by a factor of 100. What is the random pattern? Is it uniform? In addition, why not acquire real low-dose data and show the quality results using the proposed model? For the multi-constast MRI data, how is the input generated and what is the ground-truth? 
",4
"This paper considers over-parametrized neural networks (n > d), where n is the number of parameters and d is the number of data, in the regression setting. It consists of three main results:

a) (Theorem 2.1) If the output of the network is a smooth function of its parameters and global minimum with zero squared loss is attainable, then the locus of global minima is a smooth $n - d$ dimensional submanifold of R^n.
b) (Lemma 3.3) A neural network with a hidden layer as wide as the number of data points can attain global minima with zero loss.
c) (Theorem 4.1) If a neural network has a hidden layer as wide as the number of data points and is a smooth function of its parameters, then the locus of global minima with loss zero is a smooth $n - d$ dimensional submanifold. This is just a combination of a) and b).

I think the only contribution of this paper is Theorem 2.1. It is already well-known (e.g., Zhang et al. 16’) that a network with a hidden layer as wide as the number of data can easily attain zero empirical error. The authors claim that it is an extension over ReLU, but the extension is almost trivial. Theorem 4.1 is just a corollary from Theorem 2.1 and Lemma 3.3.

Theorem 2.1 is an interesting observation, and its implication that the Hessian at minima has only d positive eigenvalues is interesting and explains empirical observations. 

From my understanding, however, the n - d dimensional submanifold argument can be applied to any regular values of H(w,b) (not necessarily global minima), so the theorem is basically saying there are a lot of equivalence classes in the parameter space that outputs exactly the same outcome. If my understanding is correct, this result does not give us any insight into why SGD can find global minima easily, because we can apply the same proof to spurious local minima and say, “look, the locus of this spurious local minimum is a high-dimensional submanifold, so it is easy to get stuck at poor local minima.”

Moreover, the notation used in this paper is not commonly used in deep learning community, thus might confuse the readers. $n$ is more commonly used for the number of data points and $d$ is used for some dimension. Also, there are overloading uses of $p$: 
* In Section 2.1 it is used for the dimension of the input space
* In the proof of Prop 2.3 it is now a vector in R^n
* In section 3 it is now a parameter vector of a neural network
* Back to the input dimension in Lemma 3.3 and Section 4

Also, the formulation of neural networks in Section 3 is very non-standard. If you are to end up showing some result on standard fully-connected feed-forward networks, why not just use, for example, $W_2 \sigma(W_1 x_i + b_1) + b_2$? To me, this graph theoretical formulation only adds confusion. Also at the end of page 4, there is a typo: the activation function (\sigma) must be applied after the summation.

Overall, I believe this paper does not meet the standards of ICLR at the moment. I would recommend rejection.",5
"This paper gave an interesting theoretical result that the global minima of an overparameterized neural network is a high-dimensional sub-manifold. This result is particularly meaningful as it connected several previous observations about neural networks and a indirect evidence for why overparameterization for deep learning has been so helpful empirically.

The proof in the paper was smart and the rough logic was quite easy to follow. The minor issue is that the proof in the paper was too sketchy to be strict. For example, in proof for Thm 2.1, the original statement about Sard’s theorem was about the critical values, but the usage of this theorem in the proof was a little indirect. I can roughly see how the logic can go through, but I still hope the author can give more detailed explaining about this part to make the proof more readable and strict.

Overall, I think the result in this paper should be enough to justify a publication. However, there’re still limitations of the result here. For example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here. It also didn’t explain why stochastic gradient descent can find these minima empirically. In particular, even though the minima manifold is n-d dimensional, it’s still a zero-measure set which will almost never get hit with a random initialization. Of course, these are harder questions to explore, but maybe worthy some discussion in the final revision.
",7
"The paper shows that the set of global minimums of an overparametrized network with smooth activation function is almost surely a high dimensional manifold. In particular, the dimension of this manifold is exactly n-d, where n is the number of parameters and d is the number of datas. To the best of my knowledge, this is the first proper proof of such non-surprising result.  

The theoretical analysis is a straightforward combination of neural network's expressiveness result and some classical theorems in the field of differential topology. However, the assumption on the overparametrized neural network is somehow unrealistic since it requires that at least one layer has no less than d neurons, which is as many as the number of datas. This is usually not the case. Moreover, the result is to some extent ""asymptotic"", in the sense that a small perturbation in terms of data may be required in order to make the statement holds.  

More importantly, the result does not provide any useful characterization distinguishing stationary point/local minimum versus global minimum. It might be possible that the set of stationary points is also a manifold with very high dimension, which is indeed supported by the argument listed in the bottom of page 7: ""the Hessian of the loss function tended to have many zero eigenvalues, some positive eigenvalues, and a small number of negative eigenvalues"". It is possibly the case that the set of stationary points has even higher dimension. This suggests that the dimensionality itself is not right indicator, but the difference in terms of dimension between different type of stationary points that matters. 

Overall, the paper is short and concise but I find the contribution a bit limited.",5
"Previous works have shown that DNNs are able to memorize random training data, even ignoring the enforcing of data-dependent geometric regularization constraints. In this work, authors show convincing results indicating that this is due to a lack of consistency between the main classification loss (typically soft-max cross entropy) and the selected geometric constraint. Consequently, they propose a simple approach where the softmax loss is replaced by a validation loss that is consistent with the enforced geometry. Specifically, for each training batch, instead of considering a join loss (soft-max cross entropy + geometric constraint), they apply a sequential process, where each training batch is split into two sub-batches: a first batch used to apply the geometric constraint, and a second batch (based on the proposed feature geometry) where a validation loss is used to generate a predicted label distribution. Authors test the proposed idea using an implementation that enforces that samples from each class belong to an independent low-rank sub-space (enforced geometric constraint). Results verify the main hypothesis. Specifically, the resulting model is able to fit real data but not data with random labels. The strength of this evaluation is enhanced including results from relevant baselines. In terms of generalization of real data, the proposed approach offers a small increase in accuracy.

Paper is well written, main hypothesis is relevant, and results are convincing. While not a complete answer to the main questions related to the abilities of DNNs to fit and generalize on real data, this paper offers relevant insights related to the role of finding/using a suitable loss function to train DNNs. These results are relevant to the community and they can illuminate future work, so this reviewer recommend to accept this paper.",7
"The paper proposes a framework for data-dependent DNN regularization which claimed to be capable of producing highly discriminative features residing in orthogonal-low-rank subspaces. The main claim is that the proposed regularization makes the neural network not memorizing from the training data and motivate learning the intrinsic patterns.  The experiments were done with three image dataset. 

The main problem with this paper is the low training accuracy, but the high testing accuracy (Table 1). This implies that the model has a high bias and low variance. Intuitively the model is consistently predicting a wrong target function (probably from the self-validation).

",4
"The paper proposes a data-dependent regularization method which is coupled with softmax loss to train deep neural networks for classification. The paper turns to Orthogonal Low-rank Embedding (OLE) loss for the geometric constraint that one class of data/feature are assumed to reside on a low-rank subspace that subspaces of different classes are orthogonal ideally. The probability in the softmax is then modeled as cosine similarity between data feature and the class-specific subspaces. In this way, geometric loss and softmax loss have the common goal for optimization. Moreover, during training, the geometry enforced on one batch of features is simultaneously validated on a separate batch using a validation loss. The experiments seem to suggest such a model helps avoid overfitting/memorizing noisy training data. The paper reads well and is easy to follow.

However, the paper is limited in technical novelty and practical significance. Here are some concerns -- 

1) The paper only studies one method based on OLE, though it cites the center loss [19]. How does the center loss behave in face of noisy training label? Would it also be able to refuse to fit the noisy training data?

2) As each class has its own (low-rank) subspace, and the rank is reduced by imposing the nuclear norm. It seems that the proposed method is hard to extend to many classes (class number is larger than the dimension)?

3) The datasets in the experiments are quite small in scale and class number. It is not persuasive unless tested on larger scale data or with large class number.

4) The proposed method seems to be limited in deal with discrete labels (e.g., classification), is it easy to extend to continuous target, say regression problems like depth estimation and surface normal estimation?

5) While the authors claim as a main contribution that the proposed GRSVNet is a general framework, it is hard to see how this framework can be used in other tasks other than classification.

6) The experiments are less persuasive. It's better to add the error bar to see the improvement by the proposed method is not due to random initialization. Running time should also be compared, as nuclear norm seems to be time consuming.",4
"==============Updated=====================
The authors addressed some of my concern, and I appreciated that they added more experiments to support their argument.
Although I still have the some consideration as R3, I will raise the rating to 6.

===========================================

This paper is easy to follow. Here are some questions:

1. The argument about ALI and ALICE in the second paragraph of the introduction, “… by introducing a reconstruction loss in the form of a discriminator which classifies pairs (x, x) and (x, G(E(x)))”, however, in ALI and ALICE, they use one discriminator to classify pairs (z, x) and (z, G(z)). Therefore, “… the discriminator tends to detect the fake pair (x, G(E(x))) just by checking the identity of x and G(E(x)) which leads to vanishing gradients” is problematic. Therefore, the motivation in the introduction may be some modification.

2. The authors failed to compare their model with SVAE [1] and MINE [2], which are improved versions of ALICE. And we also have other ways to match the distribution such as Triple-GAN [3] and Triangle-GAN [4], I think the authors need to run some comparison experiments.

3. The authors should discuss more about the augment mapping a(x), i.e., how to choose a(x). I think this is quite important for this paper. At least some empirical results and analysis, for example, how inception score / FID score changes when using different choices of a(x).

4. This paper claims that the proposed method can make the training more robust, but there is no such experiment results to support the argument.

[1] chen et al. Symmetric variational autoencoder and connections to adversarial learning, AISTATS 2018.
[2] Belghazi et al, Mutual Information Neural Estimation, ICML 2018.
[3] Li et al. Triple Generative Adversarial Nets, NIPS 2017.
[4] Gan et al. Triangle generative adversarial networks, NIPS 2017.",6
"The paper propose a adversary method to train a bidirectional GAN with both an encoder and decoder. Comparing to the existing works, the main contribution is the introducing of an augmented reconstruction loss by training a discriminator to distinguish the augmentation data from the reconstructed data. Experimental results are demonstrated to show the generating and reconstruction performance.

The problem studied in this paper is very important, and has drawn a lot of researchers' attentions in recent years.  However, the novelties of this paper is very limited. The techniques used to train a bidirectional GAN are very standard. The only new stuff may be is the proposed reconstruction loss defined on augmented samples and reconstructed ones. But this is also not a big contribution, seems just using a slightly different way to guarantee reconstruction. ",4
"Thank you for an interesting read.

The paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial ""reconstruction"" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. 

Experiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed ""reconstruction"" loss. Improvements has been shown on reconstructing input images with significant numbers.

Overall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.

There is one theoretical issue for the defined ""reconstruction"" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). 

This is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].

I think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). 

1. Indeed matching p_{\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.

2. The authors mentioned that picking r(y|x) = \delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section ""...we used a combination of reflecting 10% pad and the random crop to the same image size..."". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. 

In summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).

[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.
[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.",5
"The paper proposes a new evaluation metric for generative adversarial networks and shows that it is better aligned with human judgment than FID. The metric is based on a domain-specific encoder to extract features of the image rather than ImageNet inception network and a class-aware Frechet distance which makes a Gaussian mixture assumption for the extracted features rather than a simple Gaussian assumption for FID. The paper shows an advantage for the new metric vs the others by constructing examples where FID fails while the proposed metric doesn't. Although this is an interesting finding, it is not a breakthrough in the sense that a domain-specific representation is expected to be better behaved than the features of the inception classifier and using a Gaussian mixture would be an obvious step after FID. Moreover, other metrics don't even rely on any assumption on the features distributions [1,2], so I would expect them to behave at least as well as the proposed metric.  


[1] :M. Arjovsky, S. Chintala, L. Bottou, Wasserstein gan
[2] :M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs.
",5
"This paper proposes a variant of the popular FID score for evaluating GAN-type generative models. The paper makes two major complaints about the FID as it is currently used:

1. The standard Inception network features trained on ImageNet might not be a good representation for whatever different dataset is being modeled, e.g. CelebA or CIFAR-10.

2. The globally-Gaussian assumption made by the FID doesn't hold, which can cause some problems with the metric.

To address issue 1, the paper proposes choosing features based on a dataset-specific VAE, which can additionally incorporate labels when they're available. For 2, the authors propose to compute something like the FID between each component of a Gaussian mixture, based on soft assignments of points to a class with the VAE's inference network to estimate p(y|x), when labels y are available.

In terms of the definition of the CAFD: it is worth emphasizing that (9) is *not* the Frechet = Wasserstein-2 distance between Gaussian mixtures (which is fine). Rather, it's essentially the mean FID of the class-conditional distributions. This has previously been considered in the conditional GAN case, e.g. by Miyato and Koyama (ICLR 2018, https://openreview.net/forum?id=ByS1VpgRZ ). The difference here is that soft-assignments are supported, through the VAE's inference network, though using any classifier would be essentially equivalent. As long as you have a classifier, you can compute the CAFD, regardless of using a VAE representation or not; the VAE just conveniently gives you a classifier out too. Thus the two components of your proposal are essentially orthogonal.



On the choice of dataset-dependent features:

You say several times through the paper that ImageNet-based features are ""ineffective"" because the class labels do not match with the target, e.g. ""fine-grained features distinguishing 'African hunting dog' from 'Cape hunting dog' (which all belong to the category 'dog' in CIFAR-10) are not needed."" This is, I think, somewhat misguided: imagine I took ImageNet and assigned higher-level labels to it, such that each image is only assigned a label at the level ""dog,"" and then trained a GAN on it. Then a classifier wouldn't need to distinguish ""African hunting dog"" from ""Cape hunting dog."" But a GAN, which doesn't see the labels at all, is being given *exactly the same problem*, and so the GAN still needs to be able to produce both African hunting dogs and Cape hunting dogs (though it doesn't need to be able to tell the two apart).

Moreover, some people believe that CNNs trained on general-purpose approximate the human visual system reasonably well (for an overview of the arguments, see https://neurdiness.wordpress.com/2018/05/17/deep-convolutional-neural-networks-as-models-of-the-visual-system-qa/ ), and although the overall goals of GANs are somewhat fuzzy, ""the distribution appears the same to the human visual system"" seems pretty good as a goal.

- I think it's obvious that ImageNet-trained Inception features do not model the human visual system very well on, say, MNIST.

- They're probably also not amazing on CelebA, because it hasn't been fine-tuned for faces the way the human visual system has. (Incidentally, you say that ""the ImageNet models can hardly distinguish different faces"" -- this needs either a citation or some experimental support, in the appendix, because this is not a well-known fact and seems quite relevant to the common practice of applying ImageNet-trained features to CelebA evaluation.)

- But it's not clear to me that they don't model the human visual system reasonably well on CIFAR-10, or at least a theoretical higher-resolution version of it. It's true that ImageNet models will contain some features specific to distinguishing different types of guitars, and there are no guitars in CIFAR-10. But as long as those features aren't strongly activated by actual images from your model, they shouldn't mess up the distributions you're comparing too much.

So if you're going to argue that ImageNet representations are insufficient on vaguely ImageNet-like tasks such as CIFAR-10, I don't think the arguments you have here are quite convincing. Probably, you need some evidence that the scores are made noisier by the irrelevant features and thus harder to estimate, or else maybe strong empirical evidence that using comparable features specific to the dataset distribution performs better.

Anyway, for datasets that are not very much like ImageNet, using dataset-specific features is clearly sensible and perhaps necessary. But:

- You only provide pretty limited evidence that the VAE is better than a plain autoencoder, namely Table 2 which shows that the VAE puts less information in the top few principal components. But you only show that up to the top 5 components, and in any case it's not obvious that a more-spread distribution would be better.

- An important question that's not really considered here: how much does the FID/CAFD then just measure how well the generative model matches the VAE you get features from? Is it the case that this VAE would give a (nearly-)perfect score under the CAFD, or not?

- The results of Figure 1/Table 3 are very interesting. But I wonder how much of this difference in behavior is due to training on CelebA vs ImageNet and how much is due to the architecture or objective of the autoencoder. It might be interesting to compare to features from an ImageNet VAE and/or a CelebA classifier and see what those say. (The discriminator features are something like a CelebA classifier, but there's other things going on there too.)



On the CAFD versus FID:

Your main argument for the CAFD over the FID is that it is based on a richer model of the distribution, which you claim to be closer to true: the FID is based on a multivariate Gaussian assumption with a total of n + n (n-1)/2 parameters, while you use K times as many parameters. Your Table 9 also gives some slight evidence that the Gaussian mixture gives a better fit to the data than a single Gaussian.

I'm not entirely convinced by Table 9; comparing p-values is in general not necessarily very meaningful, and in particular it seems quite possible that the Anderson-Darling test simply prefers the mixture because the samples are more closely ""clumped together"" by the VAE than a random subset of inputs. Moreover, in either case the Gaussian assumption is clearly false a priori: in the Inception case, features are the output of a ReLU activation function and hence zero-inflated, and this or something like it may also be the case in your VAE. So comparing the p-value of tests for hypotheses known a priori to be false is probably a misguided endeavor.

But in any case the FID doesn't *really* assume Gaussianity. It coincides with the Frechet / Wasserstein-2 distance between Gaussians, but it's a perfectly plausible semimetric between any pair of distributions that have means and variances. The claim for superiority of CAFD over FID would then need to be something like ""the class-conditional means and variances are more representative of the distribution than the global means and variances.""

Re: your claim that ""As both FID and CAFD aim to model how well domain-specific images are generated, they are not designed to deal with mode dropping"" -- this is something of a strange claim, as dropping an entire mode will hopefully affect both the feature mean and especially the variance unless it is done extremely carefully. A related problem, though, is that the CAFD is essentially insensitive to drastically *reweighting* modes, e.g. producing twice as many 1s as 2s on MNIST: if each mode is modeled correctly, the CAFD will not be changed, while the FID would be strongly affected with reasonable features. The Mode Score KL(p(y*) || p(y)) would be sensitive to this, as you suggest, but it feels somewhat hacky.

The type of analysis in Table 1 is interesting, but one issue is that it is sensitive to the scale of each mode in feature space: if your encoder happens to place 1s close together and 2s relatively more spread apart, you'll see a higher conditional FID for 2s than for 1s even if the visual ""sample quality"" is the same.

One important piece of related work that's missing is Binkowski et al. (ICLR 2018, https://openreview.net/forum?id=r1lUOzWCW ), who demonstrate that the FID estimator is strongly biased in a misleading way. The same problems are inherited by the CAFD, which you should at least mention. Binkowski et al., and independently Xu et al. (https://arxiv.org/abs/1806.07755 ), also proposed using MMD variants on top of Inception features. This has better statistical properties as shown by Binkowski et al., and also explicitly does not make any parametric assumptions about the distribution of features. It would be worth thinking about the relationship of that approach to the FID/CAFD.

Another metric you could compare to is the ""Adversarial Divergence"" of Yang et al. (ICLR 2017, https://openreview.net/forum?id=HJ1kmv9xx ) which compares the distribution of classifier output, p(y|x), for x from the model to that from test data. It's a pretty different metric from CAFD with different properties, but since you both require a classifier, it would be good to know how the two compare.



Minor points:

In the related work, your discussion of the MMD is misleading: Dziugaite et al. and Li et al. proposed using the MMD for *training* generative models, not for evaluating. Evaluating with two-sample tests based on the MMD using simple kernels was done e.g. by Sutherland et al. (ICLR 2017, https://openreview.net/forum?id=HJWHIKqgl ) and Olmos et al. (https://openreview.net/forum?id=HJWHIKqgl ), and used on top of Inception-like representations e.g. by Lopez-Paz and Oquab (2017), as well as Xu et al. and Binkowski et al. mentioned above.

The derivation (6) of the CAFD is that the derivation (6) is somewhat sloppy about exactly what p() means -- in particular, it's somewhat confusing to use a lowercase p when every distribution you deal with here is actually discrete (for discrete y or for the empirical distribution S, since you're dealing with that and not actually the true distribution of the model, where p(x_i) would not be constant across samples x). It would probably be clearer to distinguish your notation for the true model distribution from the empirical distribution of the S samples.

I don't understand your claim on page 6 that ""Unlike Inception Score, because CAFD measures distance on the feature space as FID does, it is able to report overfitting."" CAFD, like FID, probably doesn't allow for distributions to appear better than the target in the way that Inception score does. But I don't see how this corresponds to ""reporting overfitting""; a model that simply reproduces exactly the empirical distribution of the training set would get an excellent CAFD/FID score, but that's the usual sense of ""overfitting.""



Overall thoughts:

Using dataset-specific features for evaluation metrics makes a lot of sense, but I don't feel totally satisfied by this paper's investigation of the specific proposal of a VAE, and am particularly worried about whether the metric just ends up preferring models similar to that VAE. I'd really like to see some theoretical and empirical investigation into that.

The CAFD as opposed to FID doesn't feel as nice to me; it's both something of an obvious extension of the previously-used ""intra-class FID,"" and I am also unconvinced by the paper's arguments for its preferability over the FID or other metrics based on image representations like those of Xu et al.",5
"The authors study the task of sample-based quantitative evaluation applied to GANs. The authors suggest multiple modifications to existing evaluation pipelines: (1) Instead of embedding the samples in the InceptionNet feature space, train a domain-specific encoder. If labeled data is available, add a cross-entropy loss to the encoder training objective so that the class can be predicted. (2) Instead of fitting a single Gaussian in the feature space, fit a GMM instead. This should allow for a more fine-grained “class-aware” distance between the (empirical) distributions. 

Pro: 
Attempt to attack a critical issue in generative modeling. Good overview of competing approaches.
Several ablation studies of evaluation measures and the behavior of FID with respect to the representation space.
The ideas make sense on a conceptual level, albeit suffering from major practical concerns.

Con:
- Clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (>3) times (i.e. deficiencies of FID and IS, etc.), Many statements which should be empirically tested are stated as folklore (last paragraph on page 3). In general the paper merits another polishing pass (mode != model, last paragraph in  section 3, “unmatch”, etc.).
- Why would a VAE capture a good feature space? It is known that the tradeoff between what is stored in the latent space versus the discriminator *completely* depends on the power of the discriminator -- if the discriminator is flexible enough it can just learn the marginal distribution and ignore the latent code. Hence, this subtle issue will likely undermine the entire model comparison.
- Using the predictive distribution as a soft label for CAFD. Interesting idea, but why would one have access to labels in the first place? Why wouldn't one use a conditional GAN if we already have labels? Secondly, why would the modes necessarily correspond to classes?
- Stated issues with FID: Why would you expect FID to be resistant to such drastic transformations as blocking out a significant proportion of pixels with “blocks”? This is a *major* change in the underlying distribution. The fact that humans can “fill in” this gap should have nothing to do with the quality of the underlying model. Arguably, you can also hide one eye, the nose and the mouth and still judge the sample as “good”.

The ideas presented in this paper are conceptually interesting. However, given the drawbacks discussed above I cannot recommend the acceptance of this work.
",3
"Summary: This paper introduces the Neural Rendering Model (NRM), a generative model in which the computations involved in inference correspond to those of a CNN forward pass. The NRM’s supervised learning objective is lower bounded by a variant of the cross-entropy objective. This objective is used to formulate a max-min network, which has a particular type of weight sharing between a standard branch with max pooling / ReLUs and a second branch with min pooling / NReLUs. The max-min objective and network show strong performance on semi-supervised learning tasks. 

Posing a CNN as inference in a generative model is an interesting direction, and could be very useful for probabilistic inference in the context of neural nets. However, the paper is rather difficult to follow and requires frequent reference to the appendix to understand the main body. Some important components (like those relating to rendering paths and RPNs) are given good intuitive explanations early on but remain a bit ambiguous throughout the paper. I would recommend improving the presentation before publication.

Question: “we can modify NRM to incorporate our knowledge of the tasks and datasets into the model and perform JMAP inference to achieve a new CNN architecture.“
I appreciate the CNN / NRM correspondence in Table 1, and see how the NRM may be modified to produce modified CNN architectures. That being said, I am not sure I understand what sorts of task-specific knowledge are being referred to here. Could you give an example of a type of knowledge that the NRM would allow you to bake into a CNN architecture, but would otherwise be difficult to incorporate?

Minor:
“As been shown later in Section 2.2…”

“…is part of the optimization in equation equation 6.”",5
"pros:
- Interesting probabilistic interpretation of the CNNs improving work of [Patel 2016].
- State-of-the-art results following from the proposed probabilistic model. 

cons:
- The regular 10 pages of the paper are not self-contained. 

The paper is written in overly condensed way. I found it impossible to clearly understand major claims of the paper without reading the accompanied 34 pages long appendix. Many concepts/notations used in the paper are introduced in the appendix. My assessment is done solely based on reading the 10 regular pages. 

- The probabilistic model NMR (equ (1) and (2)) defines distribution of inputs given latent variables and the outputs, $p(x|z,y)$, as well as it defines a distribution $p(z|y)$. Hence, in principle, one could maximize $p(x)=\sum_{i} E_{z} p(x_i|z,y)p(z|y)p(y)$ when learning from unsupervised data. Instead, the authors propose to learn by MINIMIZING the expectation (not clear w.r.t which variables) of $\log p(x,z|y)$ (equ (7)). Although it leads to empirically nice results, I do not see a clear motivation for such objective function. 

- The motivation for using the MIN-MAX entropy as a loss function (sec 3) is also not clear. Why it should be better than the standard cross-entropy in the statistical sense?

- The proposed probabilistic model NMR differs form the previous work of [Patel 2016] by introducing the prior (1) on the latent variable. Unfortunately, pros and cons of this modifications are not fully discussed. E.g. how using dependent latent variables impact complexity of the inference.  
",5
"The paper claims to propose a novel generative probabilistic neural network model such that its encoder (classifying an image) can be approximated by a convolutional neural network with ReLU activations and MaxPooling layers. Besides the standard parameters of the units (weights and biases), the model has two additional latent variables per unit, which decide whether and where to put the template (represented by the weights of the neuron) in the subsequent layer, when generating an image from the class. Furthermore, the authors claim to derive new learning criteria for semi-supervised learning of the model including a novel regulariser and claim to prove its consistency. 

Unfortunately, the paper is written in a way that is completely incomprehensible (for me). The accumulating ambiguities, together with its sheer length (44 pages with all supplementary appendices!), make it impossible for me to verify the model and the proofs of the claimed theorems. This begins already with definition of the model. The authors consider the latent variables as dependent and model them by a joint distribution. Its definition remains obscure, let alone the question how to marginalise over these variables when making inference. Without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.

In my view, the presented material exceeds the limits of a single conference paper. A clear and concise definition of the proposed model accompanied by a concise derivation of the basic inference and learning algorithm would already make a highly interesting paper.

Considering the present state of the paper, I can't, unfortunately, recommend to accept it for ICLR.

",3
"
Summary: 
The paper proposes to use Riemannian stochastic gradient algorithm for low-rank tensor train learning in deep networks. 

Comments:
The paper is easy to follow. 

C1.
The novelty of the paper is rather limited, both in terms of the convergence analysis and exploiting the low-rank structure in tensor trains. It misses the important reference [1], where low-rank tensor trains have been already discussed. Section 3 is also not novel to the paper. Consequently, Sections 2 and 3 have to be toned down. 

Section 4 is interesting but is not properly written. There is no discussion on how the paper comes about those modifications. It seems that the paper blindly tries to apply the low-rank constraint to the works of Chung et al. (2014) and Luong et al. (2015).  

[1] https://epubs.siam.org/doi/abs/10.1137/15M1010506
Steinlechner, Michael. ""Riemannian optimization for high-dimensional tensor completion."" SIAM Journal on Scientific Computing 38.5 (2016): S461-S484.

C2.
The constraint tt_rank(W) \leq r in (5) is not a manifold. The equality is needed for the constraint to be a manifold.

C3.
Use \langle and \rangle for inner products. 
",4
"In this paper, the authors proposed a new method to update the weights in RNN by SGD on Riemannian manifold.  Due to the properties of manifold learning, the updated weights in each iteration are contracted with a low-rank structure, such that the number of the parameters of TT can be automatically decreased during the training procedure. By using the new algorithm, the authors modified two types of sophisticated RNNs, i.e., bi-directional GRU/LSTM and Encoder-Decoder RNN. The experimental results validate effectiveness of the proposed method. How to determine the rank of the tensor networks in weight compression problem is indeed an important and urgent task, this paper does not clearly illustrate how RSGD can efficiently solve this problem.

1. Compared to the conventional SGD, not only the convergence rate of the proposed method seems slower (mentioned in the paper,), but also additional computational operations should be done in each iteration like exponential mapping (with multiple QR and SVD). I’m worried about the computational efficiency of this method, but  this paper neither discusss the  computational complexity nor illustrate the results in the experimental section.

2. In proof of proposition 1, I’m confused why the input tensor X should belong to M, and why the eq. (8) holds?

3. In the convergence analysis, I don’t know why the eq.  $Exp^{-1}(y)=-\eta….$ holds even though the authors claims the it is not hard to find. So that, I cannot find the relationship between Theorem 3 and the proposed method.  Furthermore, can Theorem 3 be used to prove the convergence of the proposed method?

4. Eq. (16) would make no sense because the denominator might be very small. 

5. In the experiment, please compare with other existing (tensor decomposition based) compression methods to demonstrate how the proposed method makes sense in this task.

Minior:
1. By the definition in Oseledets’ paper, the tensor decomposition model used in this paper should be called TT-matrix rather than TT.
2. 9 ->(9) in Definition 2, and 15->(15) in the proof of Theorem 3.",4
"This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.

I like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.

But, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and I’m not happy with lack of baselines in the experimental comparison (again, see below).

The math problems
1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesn’t depend on r and the right-hand side does.
2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).
3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesn’t form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.
4) The function f() minimized in (5) is not defined (it should be!), but if it doesn’t have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I don’t get how the automatic rank reduction can be done).
5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I don’t even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).
6) I don’t get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.
7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).
8) Also, I don’t get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).
9) Finally, I don’t get the proposition 1 nor it’s proof: how can it be that rounding to a fixed r_max won’t change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.
10) I didn’t get this line: “From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = ∇f(x) and Exp−1 x (y) = −η∇xf(x), and thus Theorem 3 can be derived.” What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?
11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?
12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?
13) What is E in Theorem 3? Expected value w.r.t. something? Since I don’t understand the statement, I was not able to check the proof.

The experimental problems:
1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).
2) The convergence plots are shown only in iteration (not in wall clock time) and it’s not-obvious how much overhead the Riemannian machinery impose.
3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.

And finally some typos / minor concerns:
1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesn’t allow for Riemannian optimization and can be harder to work with numerically).
2) It’s very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.
3) In Def 2 “different function” should probably be “differentiable function”.
4) How is W_c represented in eq (25), as TT or not? It doesn’t follow the notation of the rest of the paper. How is a_t used?
5) What is “score” in eq (27)?
6) Do you include bias parameters into the total number of parameters in figures?
7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).
8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).


Please, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns!
",3
"[Edit] I changed my rating from 4 to 5 based on the author responses.
=======
This paper proposed a GAN that learns a disentangled factors of variations in unsupervised (or weakly-supervised) manner. To this end, the proposed method incorporates a contrastive loss together with Siamese network, which encourages the generator to output smaller variations in samples if they are drawn by varying the same latent factors. The proposed idea is evaluated on simple datasets such as MNIST and centered faces, and show that it is able to learn disentangled latent codes by incorporating some heuristics. 

Although the paper presents an interesting and reasonable idea, I think the paper is incomplete and in the proof-of-concept stage. In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper. In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing. 

In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.
",5
"Summary

The paper presents a novel approach for learning a generative model where different factors of variations can be independently manipulated. The method is build upon  the GAN framework where the latent variables are divided into different subsets (chunks) which are expected to encode information about high-level factors of variation. To this end, a Siamese Network for each chunk is trained with a contrastive loss minimizing the distance between generated images sharing the same factor (the latent variables in the chunk are equal), and maximizing the distance between pairs where the latent variables differ. Given that the proposed model fails in this fully-unsupervised setting, the authors propose to add weak-supervision into the model by forcing the Siamese networks to  focus only on particular aspects of generated images (e.g, color, edges, etc..). This is achieved by applying  a basic transformation  over the input images in order to remove specific information. The evaluation of the  proposed model is carried out using the MS-Celeb dataset where the authors provide qualitative results.


Methodology

*Disentangling generative factors without explicit labels is a challenging and interesting problem. The idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3]. However, the use of Siamese networks in this context is novel and sound.

*As shown in the reported results, the proposed method fails to learn meaningful factors in the unsupervised setting. However, the authors do not provide an in-depth discussion of this phenomena. Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios. 

*The strategy proposed to introduce weak-supervision is too ad-hoc. I agree that using cues such as the average color of an image can be useful if we want to model basic factors of variation. However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.

*As far as I understand, the transformations applied to the input images (e.g, edge detection) must be differentiable (given that it is necessary to backpropagate the gradient of the contrastive loss through the generator network). If this is the case, this should be properly discussed in the paper. Moreover, given that the amount of differentiable transformations is reduced, this also limits the application of the proposed method for more interesting scenarios. 

*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior. How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?  Have the authors considered to use categorical or binary variables? The use of the contrastive loss sounds more appropriate in this case.


Experimental results

*The experimental section is too limited. First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion. For example, when the edge extractor is used, what kind of information is modeled by the latent variables? Is it consistent across different samples?

Moreover, it is not clear why the authors have limited the evaluation to the case where only two “chunks” are used. In principle, the method could be applied with many more subsets of latent variables and then manually inspect them to check it they are semantically meaningful (see [2]) 

*As previously mentioned, there are many recent works addressing the same problem from a fully-unsupervised perspective [1,2,3]. All these works provide quantitative results evaluating the learned representations by using them to predict real labels (e.g, attributes in the CelebA data-set). The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation. This could clarify the advantages of the weakly-supervised strategy compared to unsupervised approaches.

Review summary

+The addressed problem (learning disentangled representations without explicit labeling) is challenging and interesting.

+The idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.

- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.

-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications

-The experimental section do not clarify the benefits of the proposed approach. In particular, the qualitative results are too limited and no quantitative evaluations is provided.


[1] Variational Inference of Disentangled Latent Concepts from Unlabelled Observations (Kumar et al, ICLR 2018)

[2] Beta-vae: Learning basic visual concepts with a constrained variational framework. (Higgins et. al, ICLR 2017)

[3] Disentangling Factors of Variation by Mixing Them. (Hu et. al, CVPR  2018)
",6
"[EDIT]: I have updated my score after the author response and paper revision.
=============================

[I was asked to step in as a reviewer last minute. I did not look at the other reviews].

-------------------------------
Summary
-------------------------------
This paper proposes to learn disentangled latent states under the GAN framework. The core idea is to partition the latent states into N partitions, and correspondly have N Siamese networks that pull the generated images with the same latent partition towards each other, along with a contrastive loss which ensures generated images with different latent partitions to be different. The authors experiment with two setups: in the ""unguided setup"" training is completely unsupervised, while in the ""guided"" setup, there is some weak supervision to encourage different partitions to learn different factors.

-------------------------------
Evaluation
-------------------------------
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming. This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different. Results with weak supervision (their method for injecting weak supervision was very nice) are more impressive. However, there is no comparison against existing work. Learning disentangled representations with deep generative models is very much an active area. Here are some recent papers:

https://openreview.net/references/pdf?id=Sy2fzU9gl
https://arxiv.org/abs/1802.05822
https://arxiv.org/abs/1802.05983
https://arxiv.org/abs/1802.04942

Importantly, there are no quantitative metrics. I do not think this work is ready for publication.


",6
"The paper proposes a framework for learning interpretable latent representations for GANs. The key idea is to use siamese networks with contrastive loss. Specifically, it decomposes the latent code to a set of knobs (sub part of the latent code). Each time it renders different images with different configurations of the knobs. For example, 1) as changing one knob while keeping the others, it expects it would only result in change of one attribute in the image, and 2) as keeping one knob while changing all the others, it expects it would result in large change of image appearances. The relative magnitude of change for 1) and 2) justifies the use of a Siamese network in addition to the image discriminator in the standard GAN framework. The paper further talks about how to use inductive bias to design the Siamese network so that it can control the semantic meaning of a particular knob. 

While I do like the idea, I think the paper is still in the early stage. First of all, the paper does not include any numerical evaluation. It only shows a couple of examples. It is unclear how well the proposed method works in general. In addition, the InfoGAN work is designed  for the same functionality. The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty. ",5
"Summary--
The paper tries to address an issue existing in current image-to-image translation at the point that different regions of the image should be treated differently. In other word, background should not be transferred while only foreground of interest should be transferred. The paper propose to use co-segmentation to find the common areas to for image translation. It reports the proposed method works through experiments.

There are several major concerns to be addressed before considering to publish.

1) The paper says that ""For example, in a person’s facial image translation, if the exemplar image has two attributes, (1) a smiling expression and (2) a blonde hair, then both attributes have to be transferred with no other options"", but the model in the paper seems still incapable of transferring only one attribute. Perhaps an interactive transfer make more sense, while co-segmentation does not distinguish the part of interest to the user. Or training a semantic segmentation make more sense as the semantic segment can specify which region to transfer.

2) As co-segmentation is proposed to ""capture the regions of a common object existing in multiple input images"", why does the co-segmentation network only capture the eye and mouth part in Figure 2 and 3, why does it capture the mouth of different shape and style in the third macro column in Figure 4 instead of eyes? How to train the co-segmentation module, what is the objective function? Why not using a semantic segmentation model?

3) The ""domain-invariant content code"" and the ""style code"" seem rather subjective. Are there any principles to design content and style codes? In the experiments, it seems the paper considers five styles to transfer as shown in Table 1. Is the model easy to extend to novel styles for image translation?

4) What does the pink color mean in the very bottom-left or top-right heatmap images in Figure 2? There is no pink color reference in the colorbar.

5) Figure 5: Why there is similariy dark patterns on the mouth? Is it some manual manipulation for interactive transfer?

6) Though it is always good to see the authors are willing to release code and models, it appears uncomfortable that github page noted in the abstract reveals the author information. Moreover, in the github page,
even though it says ""an example is example.ipynb"", the only ipynb file contains nothing informative and this makes reviewers feel cheated.

Minor--
There are several typos, e.g., lightinig.",5
"The paper deals with image to image (of faces) translation solving two main typical issues: 1) the style information comes from the entire region of a given exemplar, collecting information from the background too, without properly isolating the face area; 2) the extracted style is applied to the entire region of the target image, even if some parts should be kept unchanged. The approach is called LOMIT, and is very elaborated, with source code which is available (possible infringement of the anonymity, Area Chair please check). In few words, LOMIT lies on a cosegmentation basis, which allows to find semantic correspondences between image regions of the exemplar and the source image. The correspondences are shown as a soft mask, where the user may decide to operate on some parts leaving unchanged the remaining (in the paper is shown for many alternatives: hair, eyes, mouth). Technically, the paper assembles other state of the art techniques,  (cosegmentation networks, adaptive instance normalization via highway networks) but it does it nicely. The major job in the paper lies in the regularization part, where the authors specify each of their adds in a proper way. Experiments are nice, since for one of the first times provide facial images which are pleasant to see. One thing I did not like were on the three set of final qualitative results, where gender change results in images which are obviously diverse wrt the source one, but after a while are not communicating any newer thing. Should have been better to explore other attributes combo.  ",6
"This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions.

Pros:
* This paper proposes to jointly learn the local mask to make the translation focus on the foreground instead of the whole image.
* The local mask-based highway adaptive instance normalization apply the style information to the local region correctly.

Cons:
* There seems a conflict in the introduction (page 1): the authors clarify that “previous methods [1,2,3] have a drawback of ....” and then clarify that “[1,2,3] have taken a user-selected exemplar image as additional input ...”. 
* As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. 
* It is mentioned in the introduction (page 2) that “This approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation”. However, the differences between the proposed method with these prior works are not compared or mentioned. Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. I wonder the advantages of the proposed method compared to these works.
* The experiment setting is not clear enough. If I understand correctly, the face images are divided into two groups based on their attributes (e.g. smile vs no smile). If so, what role does the exemplar image play here? Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? 
* The github link for code should not provide any author information.

[1] Multimodal Unsupervised Image-to-Image Translation
[2] Diverse Image-to-Image Translation via Disentangled Representations
[3] Exemplar Guided Unsupervised Image-to-Image Translation
[4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation

Overall, I think the proposed method is well-designed but the comparison and experiment setting are not explained well. My initial rating is weakly reject.",5
"Revision:

The authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.

I believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work.

Original Review below:

Summary: They propose a biologically motivated short term attentive working memory (STAWM) generative model for images. The architecture is based on Hebbian Learning (i.e. associative memories are represented in the weight matrices that are dynamically updated during inference by a modified version of Hebbian learning rule). These memories are sampled from glimpses on an input image (using attention on contextual states, similar to [1]), in addition to a latent, query state. This model learns a representation of images that can be used for sequential reconstruction (via a sequence of updates, like a sketchpad, like DRAW [1], trained in an unsupervised manner). These memories produced by drawing can also be used for semi-supervised classification (achieves very respectable and competitive results for MNIST and CIFAR-10).

This paper is beautifully written, and the biological inspiration, motivation behind this work, and links to neuroscience literature as well as relation to existing ML work (even recent papers) is well stated. The main strength of this paper is that the author went from a biologically inspired idea to a complete realization of the idea in algorithmic form. The semi-supervised classification results are competitive to SOTA, and although the CIFAR-10 reconstruction results are not great (especially compared to generative adversarial models or recent variation models [2]), I think the approach is coming from a very different angle that is different enough compared to the literature to warrant some attention, or at least a glimpse, so to speak, from the broader community. The method may offer new ways to interpret ML models that is current models lack, which in itself is an important contribution. That being said, the fact that most adversarial generative models achieved a far better performance raises concern on the generalization ability of these memory-inspired learned representations, and I look forward to seeing future work investigate this area in more detail.

The authors also took great care in writing details for important parts of the experiments in the Appendix section, and open sourced the implementation to reproduce all their experiments. Given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks. Even without the code, I felt I might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.

I highly recommend this paper for acceptance, with a score of 8 (edit: revised to 9 after rebuttal period). The paper might warrant a score of 9 if they had also achieved higher quality results for image generation, on Celeb-A or demonstrated results on ImageNet, and provided more detailed analysis about drawbacks of their approach vs conventional generative models.

[1] https://arxiv.org/abs/1502.04623
[2] https://arxiv.org/abs/1807.03039

",9
"Summary: this paper introduces a new  network architecture inspired by visual attentive working memory. The network consists of a recurrent components that generates glimpses of features from a CNN applied to the input (inspired by the DRAW network), and a working memory component that iterative stores memories using Hebbian mechanisms. The authors apply this network to classification tasks (MNIST) as well as using it as a generative model.

The ideas in the paper are somewhat  interesting, but I have major concerns with the motivation (it is unclear) and the experiments (not convincing):

Motivation: The authors motivate their inclusion of a Hebbian working memory from the perspective of trying to mimic the human visual system. The main problem here is that it is unclear what problem the authors are trying to solve by including this Hebbian mechanism. In the fast-weights paper (Ba et al), they had a clear example of a task that standard recurrent networks could not easily solve, which motivated the inclusion of a working memory mechanism. A similar motivation here is lacking, with the main justification seeming to be to ""move towards a biologically motivated model of vision"". Are the authors interested in more biologically motivated models because they think they will be useful for some task? Or are the authors interested in models of biological vision itself? If the former, it is unclear what new tasks would be solved by their model (all the results focus on tasks that can be solved without these mechanisms). If the latter, there should be some clear goals for what they hope their model to achieve. ""Moving towards biological vision"" is too vague and broad of a justification in order for us to judge progress. Section 2 discusses, at a high level, broad concepts from the visual neuroscience literature, but this also does not clearly motivate why the authors are interested in this particular instantiation of these ideas, indeed, their model is only weakly related to many of the neuroscience ideas discussed.

Results: The authors evaluate their network on two tasks: classification and image generation.
- For classification, I have a hard time understanding what these results tell us. Very simple models can achieve low test error on MNIST, so it is unclear what the attention or working memory buys you. One simple improvement would be if the authors ablated different parts of their network to show how critical they are to performance. Increasing the window size to 28 helps the model, suggesting that the network is hindered by the glimpses, so I do not feel like I have learned much from these results. In addition, the authors only mention Cifar10 results in a couple of lines, so it is hard to take anything away from those statements.
- For image generation, similarly, the authors do not compare their model to other standard generative models. Does their model perform better than simple baselines?
Finally, for all of these results, what is the working memory doing? Why is it necessary? Does it learn something interesting? It is hard to understand the significance of the work without answers to these questions.",4
"The paper proposes a novel Hebb-Rosenblatt working memory model to augment the recurrent attention model and achieves competitive results on the MNIST dataset. Some results on CIFAR and Celeb-A datasets are also shown. The code is released anonymously which substantiated the reproducibility of the results; however, I haven’t physically run the code to verify it.

Motivation: First, as much as I appreciate the research direction of combining recurrent attention models and working memory, the use of recurrent attention models is not well motivated throughout the article. It it of course biologically inspired, but the engineering benefit is not obvious. It has the benefit of model compression, using less parameters to process the attended region, yet pooling mechanisms can also achieve similar effect. It also has the benefit of model interpretability, but for vanilla feed-forward counterparts, it is also possible to visualize salient regions that impact the decisions. While feed-forward CNNs process all regions in parallel, sequential models will be much slower. The core question is the following, if the final task is just image classification, why is sequential processing necessary? The author should spend some text explaining why studying recurrent attention model + working memory is important. Ideally, if the author could consider tasks other than image classification, which could potentially highlight the need for sequential processing and working memory.

Notation clarity: The clarity of model notation could be significantly improved. I am confused what is `e`, and my guess it is the layer I input. Figure 1 does not help my understanding. Notations such as \mu, \nu, M are left unexplained.

Understanding of the model: Although Hebbian learning rule is a well established mechanism, the article doesn’t provide much insight into why the rule is applied here but rather just stipulates them in Section 3.1 as the overall formulation of the memory network. What would be the objective function for such an update rule (e.g. the delta decay term corresponds to a L2 weight decay term)? For the applications studied in the paper, i.e. image classification, why does the model need to ever forget?

CIFAR/Celeb-A experiments: There is no tabulated results for these experiments. I only see CIFAR gets 93.05% accuracy, without mention of any baselines. Since attention mechanisms are used here, it would be a much stronger argument to report results on higher resolution images than CIFAR (which is 32x32).

Model interpretability: Model interpretability is often one of the biggest selling point of attention-based models. However, by examining the glimpse locations on CIFAR datasets, the model learns to look at the whole image for all glimpses, which hurts the argument of using a recurrent attention model. Also for MNIST experiments, the best number is achieved by using S_g=28, which has the glimpse size of the whole image. The author claims that it can learn a good representation despite the not so good looking glimpse visualization, but so can regular feed-forward CNNs.

Comparison to VAEs: Since VAE formulations are used, it would be good if the authors can compare the model with vanilla VAE and convolutional VAEs, both in terms of classification accuracy and reconstructed sample quality.

Comparison to DRAW: It is also recommended to show more visualization comparison to DRAW, and pinpoint the differences of using a canvas based memory vs. the proposed Hebb-Rosenblatt working memory.

In conclusion, I think the paper opens a promising direction of combining recurrent attention models and working memory networks. I believe it is a huge amount of engineering effort to make this model to work, as we can see in the Appendix and the released code base. However, there are several issues as I pointed above, most notably the motivation, and understanding of the models. The experiments on CIFAR and Celeb-A could been done more thoroughly, and I also believe that it would make more impact if the authors can show experiments other than image classification that highlight the need for a recurrent attention model equipped with working memory. Based on the above reasons, I think the paper could be better polished for future publication.",5
"The authors propose an interesting idea of generating synthetic data sets for ride sharing. In particular, they split the space/time into small spatial/temporal cells (50mx50m and 5min) each containing number of requests (or a scaled version of it), and train a conditional GAN to output these cell values given an input 5-min time label. They validate the results using metrics from graph and fractals theory.
While the idea is interesting, the execution of the paper is lacking. Some details are missing and especially key things such as metrics should be explained better.
- How is the data represented? It says that pixel represents the number of ride requests, how exactly? Then, in the next paragraph it is said that pixel represents presence/absence of ride requests, so which one is it. This is a critical part of the proposal and is not well explained.
- y-axis in Figure 2 is not explained.
- Metrics should be better explained. How are edges defined, when you only model requests, not destinations? This is far from being clear.
- In addition, how is D2 defined? Do we compute one for each time, or how? What exactly is ""side e"", what is a ""side"" here? Basically both metrics are not well defined.
- ""We can claim strong similarity ..."", what is this justified by?
- Second paragraph in Section 3.1 is not clear, reads very strangely.
-  Labels are being mentioned before being defined, adding to confusion.
- It is clear from Figure 2 that workdays and weekends are very different, yet the authors chose to ignore that fact during modeling. They do mention that we can choose any labeling we want, but still strange that for the experiments this was not taken into account.
- The authors mention that cells as 1.2km x 1.2km, but Figure 3 shows much different resolution. Seems that the figure is just given as an example, but reading the text one gets an impression that the figure was actually used in the paper. This needs to be clarified.
- For the classifier, it says that ""time sequence of the data"" is a label, what does this mean? You mean the actual label, or some time sequence? This is confusing, although it seems that simply the 5-min label was used.
- Could we add the metrics to the loss, to enforce them as the authors say that that would result in strong similarity?
- One of the major flaws of the paper is missing baseline. It is very difficult to appreciate the results without any reference result.
- Again, I am not sure how results in Section 5.2 are computed when only requests are modeled.
- Footnote 2 in the conclusion mentions baselines, yet there are none mentioned in the paper.",4
"The paper works on a very interesting problem: generating ride hailing demand map using deep learning technique. The idea is novel and interesting. The paper adopts two metric to evaluate the performance of the algorithm and shows that the performance is good. However, the problem is a little far from real world cases, which limits the contribution of the paper.

The title of the paper is very attractive. Before reading the paper, I was very excited and wanted to see the algorithm could generate the driving trajectory by using GAN. However, it can only generate the pickup location, which makes me a little disappointed. In real world applications, in riding hailing industry, the demand/supply estimation have been wide investigated. And it can be very accurate. It is not clear why we need to generate it. On the other hand, the paper only adopts conventional GAN to this application. Technically the contribution is not significant. The paper only considers time slot for generating the new data. In this area, much more information has been used. The authors are suggested to survey the smart transportation or riding sharing research area. The training solution is not satisfied. The model is only trained for each small area in the city. The training set is very limited, which may make the model overfit. Thus the experiments and the results are not convincing. In current GIS or transportation area, usually we would use a unique model for the whole city. At least the authors should discuss their algorithm for the scale issue.

Overall, the paper works on a very interesting problem. However, the current solution should be improved.
",5
"The paper produces a heat-map of ride-share requests in four cities in the USA. For each city 'block' they produce a time-sequence of 2016 images representing a week-long run from combining each 5-minute interval. This is used with a GAN to produce new data. The techniques applied, although not commonly used in the context of ride sharing / hailing, have been used extensively in other literature.

Some major points on the paper:
1) A GAN approach is normally used to generate more data when enough real data is not obtainable. However, here you only use one week of data from a much larger set. Surely, it would be better to make use of all the weeks available?

2) It is not clear how the heat-maps once produced could be used in the future. There is a hint in the results section about how they can be converted back to ride requests, but this is not clearly defined.

3) There are a number of cases where you state that some approach has been found to be better. However, no evidence is presented for how you determined this to be true.

4) The conversion of data to heat-maps has been used extensively in prior research. Although I'm not directly aware of the use in machine learning I am aware of the use in transport - ""Interactive, graphical processing unit- based evaluation of evacuation scenarios at the state scale"". The novelty here seems to be the application to this specific problem.

More specific points:
- ""Our real ride request data sets consist of all the ride requests for an entire week for the four cities."" - it's not clear - are all four cities used to train one model?

- ""Hence the week-long data should be quite representative."" - This fails to take into account such things as national holidays or other major events such as sports. Did your chosen week contain one of these?

- ""Hence we believe the ride request data sets also reflect the overall urban mobility patterns for these cities."" - This is a huge assumption, which would seem to need evidence to back it up.

- ""and lump together all the ride requests within each interval."" - Presumably you mean that all time values are to the granularity of 5 minutes? 

- ""We arbitrarily sized each block to represent an image of 2424 pixels"" - this seems particularly small.

- ""Each image of that block is labeled with a time interval (for our experiments, the hour in a day)."" - Can the variability within an hour not make this more difficult? 

- ""We find that small networks are appropriate for the training data"" - evidence to support this.

- ""This network is pre-trained on the training data"" - which training data are you referring to?

- ""This is found to increase the efficiency of the training process"" - evidence?

- ""In this work we set the block size for each of the four cities to be
1200  1200 meters"" - how was this value arrived at?

- You state that GPUs were no more efficient, it would be good to see more analysis of this.

- ""To help enhancing the scalability"" -> ""To help enhance the scalability""

- ""and other useful functions"" - such as?

- Figure 4 would probably work better as a speedup graph.

- ""Running times for sampling ride requests from the trained models and stitching the images of all the blocks together are significantly less than the training times, and are not included in these results."" - at least some figures to give an idea of scale should be provided.",5
"Different from an existing variational dropout method which used variational inference to explain Dropout, this paper proposes to interpret Dropout from the MAP perspective. More specifically, the authors utilize the Jensen inequality to develop a lower bound for log-posterior, which is used as training objective for dropout. They then exploit the power mean to develop the conditional power mean model family, which provide additional flexibility for evaluation during validation.
Even though the way how the proposed method is analyzed/generalized is interesting, the proposed method is not convincing, but I am not absolutely sure. Besides the paper is hard to follow, some other concerns are listed below.
(1) “…the original/usual dropout objective” and “the dropout rate” are not defined in the paper, even though they appear many times in the paper.
(2) In the last paragraph of Sec. 2, the authors argue that utilizing their MAP objective “sidestep any questions about whether variational inference makes sense.” However, the presented MAP lower bound has its own problem, since it is derived using the Jensen inequality.  For example, as shown in Appendix C, the equality becomes true only when p(w|\Theta) is a delta function.
(3) How to tune the hyperparameters (alpha, lambda) of the extended dropout family in practice?
(4) The current experiments might be weak. Additional experiments on popular image datasets are recommended.

Minors:
(1) In Eq. (3), is p(w_r|\Theta) of the second formula identical to p(w|\Theta) of the third formula?
(2) In the second row below Eq. (6), E_w p(w|\Theta) p(y|x,w) is a typo.
",5
"The paper point that the dropout in training is equivalent to MAP estimate of hierarchical models when the prior distribution of weights, \Theta, is a zero mean Gaussian. Based on that observation the authors propose several different evaluation methods for dropout. The experimental results show that the proposed evaluation methods improved the performance of language models.

Are there any experimental results of the proposed evaluation methods for another type of data beyond language modeling?

Do the term ""deterministic dropout"" in the last sentence of the first paragraph on page 1 and the one in Sec 3 (the first bullet) refer to the same thing? 

Minor: gaussian -> Gaussian",5
"This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. 

I find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. 

Here I detail my main concerns:


1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, $p(y|x) = \int p(y|x,w)p(w|\Theta)dw$. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). 

In the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound
$\ln p(y*|x*) >= E_q[\ln p(y*|x*,w)] - KL(q|p)$
In this case, q(w) = p(w|\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the $\Theta$ are learnt by maximum log-likelihood and do not have a $q$ associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time.   

My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. 

3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. 

So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. 

Minor comments:

1. The generative model for Variational dropout is the same than the generative model for the ""conditional model"", eq. (3). 

2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.

3. Section 3.3. I find some parts are difficult to understand. ""suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation."" Later, I can understand authors are referring to the possibility of reducing the dropout rate. ",4
"The paper used the family of $L^p$-nested distributions as the prior for the code vector of VAE and demonstrate a higher MIG. The idea is adopted from independent component analysis that uses rotationally asymmetric distributions. The approach is a sort of general framework that can be combined with existing VAE models by replacing the prior. However, I think the paper can be much improved in terms of clarity and completeness.

1. The authors used MIG throughout section 4. But I have no idea what it is. Does a better MIG necessarily imply a good reconstruction? I am not sure if we can quantify the model performance by the mere MIG, and suggest the authors provide results of image generations as other GAN or VAE papers do. 
2. Is the ""interpretation"" important for high dimensional code $z$? If yes, can the authors show an example of interpretable $z$?
3. I had difficulty reading Section 4, since the authors didn't give many technical details; I don't know what the encoder, the decoder, and the specific prior are. 
4. The authors should have provided a detailed explanation of what the figures are doing and explain what the figures show. I was unable to understand the contribution without explanations.
5. Can the authors compare the proposed prior with VampPrior [1]?

The paper should have been written more clearly before submission.
[1] Tomczak, Jakub M., and Max Welling. ""VAE with a VampPrior."" arXiv preprint arXiv:1705.07120 (2017).",4
"The authors point out several issues in current VAE approaches, including the rotational symmetric Gaussian prior commonly used. A new perspective on the tradeoff between reconstruction and orthogonalization is provided for VAE, beta-VAE, and beta-TCVAE. By introducing several non rotational-invariant priors, the latent variables' dimensions are more interpretable and disentangled. Competitive quantitative experiment results and promising qualitative results are provided. Overall, I think this paper has proposed some new ideas for the VAE models, which is quite important and should be considered for publication.

Here I have some suggestions and I think the authors should be able to resolve these issues in a revision before the final submission:
1) The authors should describe how the new priors proposed work with the ""reparameterization trick"". 
2) The authors should at least provide the necessary implementation details in the appendix, the current manuscript doesn't seem to contain enough information on the models' details.
3) The description on the experiments and results should be more clear, currently some aspects of the figures may not be easily understood and need some imagination. 
4) There are some minor mistakes in both the text and the equations, and there are also some inconsistency in the notations.
",7
"This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results.

I understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described.
P1) What is the shape of the posterior distribution?
P2) How does the reparametrization trick work in your case?
P3) How can one choose the layout of the subspaces, or this is also learned?

Moreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. 

When discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect.

In addition, there are other (secondary) questions that require an answer.
S1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces?
S2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right?
S3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript?
S4) Which parameters of this family are learned, and which of them are set in advance?
S5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this?
S6) How is the Lp layout chosen?
S7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE?
S8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed.

Finally, there are a number of minor corrections to be made.
Abstract: latenT
Equation (3) missig a sum over j
Figure 1 has no caption
In (8), should be f(z) and not x.
Before (10), I understand you mean Lp-nested
I did not find any reference to Figure 3
In 4.1, the standard prior and the proposed prior should be referred to with different notations.

For all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.",4
"The paper presents theoretical analysis for recovering one-hidden-layer neural networks using logistic loss function. I have the following major concerns:

(1.a) The paper does not mention identifiability at all. As has been known, neural networks with even only one hidden layer are not identifiable. The authors need to either prove the identifiability or cite existing references on the identifiability. Otherwise,  the parameter recovery does not make sense.

Example: The linear network takes f(x) = 1'Wx/k, where 1 is a vector with every entry equal to one. Then two models with parameters W and V are identical as long 1'W = 1'V.

(1.b) If the equivalent parameters are not isolated, the local strong convexity is impossible to hold. The authors need to carefully justify their claim.

(2) When using Sigmoid or Tanh activation functions, the output is bounded between [0,1] or [-1,+1]. This is unrealistic for logistic regression: The output of [0,1] means that the posterior probability has to be bounded between 1/2 and e/(1+e); The output of [-1,1] means that the posterior probability has to be bounded between 1/(1+e) and e/(1+e).

(3) The most challenging part of the logistic loss is the lack of curvature, when neural networks have large magnitude outputs. Since this paper assumes that the neural networks takes very small magnitude outputs, the extension from Zhong et al. 2017b to the logistic loss is very straightforward. 

(4) Spectral initialization is very impractical. Nobody is using it in practice. The spectral initialization avoids the challenging global convergence analysis.

(5) Theorem 3 needs clarification. Please explicitly write the RHS of (7). The result would become meaningless, if under the scaling of Theorem 2, is the RHS of (7) smaller than RHS of (5).

I also have the following minor concerns on some unrealistic assumptions, but these concerns do not affect my rating. These assumptions have been widely used in many other papers, due to the lack of theoretical understanding of neural networks in the machine learning community.

(6)	The neural networks take independent Gaussian input.
(7)	The model is assumed to be correct.
(8)	Only gradient descent is considered.",3
"This paper studies the problem of learning the parameter of one hidden layer neural network with sigmoid activation function based on the negative log likelihood loss. The authors consider the teacher network setting with Gaussian input, and show that gradient descent can recover the teacher network’s parameter up to certain statistical accuracy when the initialization is sufficiently close to the true parameter. The main contribution of this paper is that the authors consider the classification problem with negative log likelihood loss, and provide the local convergence result for gradient descent. However, based on the previous results in Mei et al., 2016 and Zhong et al., 2017, this work is incremental, and current results in this paper is not strong enough. To be more specific, the paper has the following weaknesses:

1.	The authors show the uniformly strongly convex and smooth property of the objective loss function which can get rid of the sample splitting procedure used in Zhong et al., 2017. However, the method for proving this uniform result has been previously used in Mei et al., 2016. And the extension to the negative log likelihood objective function is straightforward since the derivate and Hessian of the log likelihood function can be easily bounded given the sigmoid activation function. 
2.	The authors employ a tensor initialization algorithm proposed by Zhong et al, 2017 to satisfy their initialization requirement. However, it seems like that the tensor initialization almost enables the recovery as it already lands on a point close to the ground truth, the role of GD is somehow not 
that crucial. If the authors can prove the convergence of GD with random initialization, the results of this paper will be much stronger.
3.	The presentation of the current paper needs to be improved. The authors should distinguish \cite and \citep. There are some incomplete sentences in the current paper, such as in page 3, “Moreover, (Zhong et al., 2017b) shows…the ground truth From a technical perspective, our…”.
",4
"Paper Summary:
This paper studies the problem of recovering a true underlying neural network (assuming there exists one) with cross-entropy loss. This paper shows if the input is standard Gaussian, within a small ball around the ground truth, the objective function is strongly convex and smooth if there is a sufficiently large number of samples. Furthermore, the global minimizer is actually the true neural network. This geometric analysis implies applying gradient descent within this neighborhood, one can recover the underlying neural network. This paper also proposed a provable method based on spectral learning to find a good initialization point. Lastly, this paper also provides some simulation studies.

Comments:
This paper closely follows a recent line of work on recovering a neural network under Gaussian input assumption. While studying cross-entropy loss is interesting, the analysis techniques in this paper are very similar to Zhong et al. 2017, so this paper is incremental. I believe studying the global convergence of the gradient descent or relaxing the Gaussian input assumption is more interesting.",5
"UPDATE:
Thanks for your response. As you mentioned, methods like [1] and [2] do perform open-ended recombination. Note that these methods perform not only texture transfer but also color transfer, while the proposed method seems to perform mostly only color transfer. As shown in Figure 6, essentially what the method does is transfer the color of the style image to the content image, sometimes with a little tweak, making the image distorted. One could say that in terms of image style transfer, the proposed method actually underperforms [1] and [2]. 

Hence I agree with R2 that comparison is still necessary for the submission to be more convincing and complete.

------------------------------

This paper proposed to use a mechanism of leakage filtering to separate styles and content in the VAE encoding, and consequently enable open-ended content-style recombination. Essentially the model tries to maximize the similarity between images in S^+ and minimize the similarity between those in S^-.

I have several questions:

One concern that I have is the relationship/difference between this work and previous work on style transfer, especially universal/zero-shot style transfer as in [1,2]. In the introduction and related work sections, the authors argue that most previous work assumes that content classes in testing are the same as those in training, and that they are not general purpose. Note that various works on style transfer already address this issue, for example in [1, 2]. For those models, content is represented by high-level feature maps in neural networks, and style is represented by the Gram matrix of the feature maps. The trained model is actually universal (invariant to content and styles). Actually these methods use even less supervision than STOC since they do not require labels (e.g., digit labels in MNIST).

This brings me to my second concern on proper baselines. Given the fact that previous universal/zero-shot style transfer models focus on similar tasks, it seems necessary to compare STOC to them and see what the advantages of STOC is. Similar experiments can be conducted for the data augmentation tasks.

In Sec. 4, the authors mentioned that U-Net skip connection is used. Does it affect the effectiveness of the content/style separation, since the LF objective function is mostly based on the encoding z, which is supposed ‘skipped’ in STOC. Will this lead to additional information leakage?

It is not clear how the last term of L_{LF} is computed. Could you provide more details?

The organization and layout of figures could be improved. The title/number for the first section is missing.

Missing references:

[1] Universal style transfer via feature transforms, 2017
[2] ZM-Net: Real-time zero-shot image manipulation network, 2017
[3] Structured GAN, 2017",5
"In this paper, the authors study an interesting problem called open-ended content style recombination, i.e., recombining the style of one image with the content of another image. In particular, the authors propose a VAE (variational autoencoder) based method (i.e., Style Transfer onto Open-Ended Content, STOC), which is optimized over a VAE reconstruction loss and/or a leakage filtering (LF) loss. More specifically, there are four variants of STOC, including CC (content classifier), CE (content encoding), PM (predictability minimization, Section 2.1) and LF (leakage filtering, Section 2.2). The main advantage of STOC is its ability to handle novel content from open domains. Experimental results on image synthesis and data set augmentation show the effectiveness of the proposed method in comparison with the state-of-the-art methods. The authors also study the comparative performance of four variants, i.e., CCF, CE, PM and LF.

Overall, the paper is well presented.

Some comments/suggestions:

(i) The authors are suggested to include an analysis of the time complexity of the proposed method (including the four variants).

(ii) The authors are suggested to include more results with different configurations such as that in Table 1 in order to make the results more convincing.
",7
"SUMMARY
The paper considers several methods for building generative models that disentangle image content (category label) and style (within-category variation). Experiments on MNIST, Omniglot, and VGG-Faces demonstrate that the proposed methods can learn to generate images combining the style of one image and the content of another. The proposed method is also used as a form of learned data augmentation, where it improves one-shot and low-shot learning on Omniglot.

Pros:
- The paper is well-written and easy to follow
- The proposed methods CC, CE, PM, and LF are all simple and intuitive
- Improving low-shot learning via generative models is an interesting and important direction

Cons:
- No comparison to prior work on generation results
- Limited discussion comparing the proposed methods to other published alternatives
- No ablations on Omniglot or VGG-Faces generation
- Low-shot results are not very convincing

COMPARISON WITH PRIOR WORK
There have been many methods that propose various forms of conditional image generation in generative models, such as conditional VAEs in [Sohn et al, 2015]; there have also been previous methods such as [Siddharth et al, 2017] which disentangle style and content using the same sort of supervision as in this paper. Given the extensive prior work on generative models I was a bit surprised to see no comparisons of images generated with the proposed method against those generated by previously proposed methods. Without such comparisons it is difficult to judge the significance of the qualitative results in Figures 3, 5, and 6. In Figure 3 I also find it difficult to tell whether there are any substantial differences between the four proposed methods.

The proposed predictiability minimization is very related to some recent approaches for domain transfer such as [Tzeng et al, 2017]; I would have liked to see a more detailed discussion of how the proposed methods relate to others.

OMNIGLOT / VGG-FACES ABLATIONS
The final model includes several components - the KL divergence term from the VAE, two terms from LF, and a WGAN-GP adversarial loss. How much do each of these terms contribute the quality of the generated results?

LOW-SHOT RESULTS
I appreciate low-shot learning as a testbed for this sort of disentangled image generation, but unfortunately the experimental results are not very convincing. For one-shot performance on Omniglot, the baseline Histogram Embedding methods achieves 0.974 accuracy which improves to 0.975 using STOC. Is such a small improvement significant, or can it be explained due to variance in other factors (random initializations, hyperparameters, etc)?

For low-shot learning on Omniglot, the proposed method is outperformed by [Antoniou et al, 2017] at all values of k. More importantly, I’m concerned that the comparison between the two methods is unfair due to the use of different dataset splits, as demonstrated by the drastically different baseline accuracies. Although it’s true that the proposed method achieves a proportionally larger improvement over the baseline compared with [Antoniou et al, 2017], the differences in experimental setup may be too large to draw a conclusion one way or the other about which method is better.

OVERALL
Although the paper is well-written and presents several intuitive methods for content/style decomposition with generative models, it’s hard to tell whether the results are significant due to incomplete comparison with prior work. On the generation side I would like to see a comparison especially with [Siddharth et al, 2017]. For low shot learning I think that the proposed method shows some promise, but it is difficult to draw hard conclusions from the experiments. For these reasons I lean slightly toward rejection.

MISSING REFERENCES
Siddharth et al, “Learning Disentangled Representations with Semi-Supervised Deep Generative Models”, NIPS 2017

Sohn, Lee, and Yan, “Learning structured output representation using deep conditional generative models”, NIPS 2015

Tzeng, Hoffman, Darrell, and Saenko, “Adversarial Discriminative Domain Adaptation”, CVPR 2017
",5
"The paper describes a sampling distribution construction over examples from which to draw mini-batches to train multi-classification models. A distance function on examples is described wherein an example's current (softmax) label probabilities and correctness are taken into account. The bounded distance function supports quantization of example distances and then subsequent sampling from an exponentially decaying probability mass function defined over the binned examples. Results from experiments implementing the proposed method and some baselines on three image classification datasets are provided.

Clearly, any generic improvement to training DNN's has the potential for far-reaching impact. I thought the exposition was fairly clear and appreciated how the introductory sections provided an intuitive understanding of e.g., the differences between the proposed method and the method of Loshchilov and Hutter (2016). The relative conceptual simplicity of the proposed method is a clear positive. The experimental methodology and results are my biggest issues with the paper. The experimental evaluation suggests the proposed method was run 3 times, one for each value of the selection pressure parameter. Then, the best run was selected for comparison. This suggests the proposal is not practical. For results, the benefit of the proposed method is only clearly apparent in one of the three experiments (Fashion MNIST). In the MNIST case, the proposed method does not seem to improve upon the online batch method. For CIFAR-10, where a good case for the proposed method could have been made since the architecture is more complex and potentially more difficult to train, the improvement seems slight. Moreover, it isn't clear whether a relevant baseline was included (see second question below). Also, at least some discussion of computational cost incurred by the method should have been provided. Even better would be to include results wrt/ wall clock training time.

Questions/Comments:

Why not set \gamma = 1? Having a larger value seems to run counter to making training faster. Technically, to use the proposed method, all of the examples need to be processed only once before the distance-based sampling distribution can be utilized. 

Does the random method of the paper denote uniformly at random from the entire dataset per batch or sequential batches from a pre-shuffled dataset per epoch? The distinction is important as Loshchilov and Hutter (2016) report that the latter method performs better than their online batch method on CIFAR-10.

ADA-easy seems to be an irrelevant baseline given the context of the paper.

The phrase ""learner's level"" is used multiple times, but not defined.

The average is reported in the convergence curves, but shouldn't the variance be reported as well?

Perhaps the selection pressure parameter can be annealed as performed in Loshchilov and Hutter (2016)?",5
"The paper introduces an adaptive importance sampling strategy, as opposed to uniform sampling, for batch normalization. The key idea is to assign higher importance to those correctly classified training samples with relatively smaller soft-max prediction variance, hopefully to push the deep nets to learn faster from uncertain samples near the decision boundary. Experimental results on several benchmark datasets (MNIST, CIFAR-10) and commonly used deep nets (LeNet, ResNet) are reported to show the power of boundary batch selection in improving the overall training efficiency.

The paper is clearly presented and the numerical results are mostly easy to access. My main concern is about the novelty of technical contribution which is mainly composed by two: 1) a prediction variance based importance sampling strategy for batch selection and 2) an empirical study the show the merits of approach. Concerning the first contribution, the idea of defining boundary samples according to prediction variance looks fairly common, if not superficial, in modern machine learning. The way of defining the sampling probability (see Eq. 4 & 5) follows largely the rank-based method (Loshchilov and Hutter 2016) with slight modifications. The numerical study shows some promise of the proposal on several relatively easy data sets. However, as a practical paper, the numerical results could be much more supportive if more challenging data sets (e.g., ImageNet) are included for evaluation. 

Pros: 

-The method is well motivated and clearly presented. 
- The paper is easy to follow. 


Cons:

-  The overall contribution is incremental with limited novelty.  
- As a practical paper, the numerical study falls short in evaluation on large-scale data. 
",5
"This paper attempts to speed up convergence of deep neural networks by intelligently selecting batches. The experiments show this method works moderately well.

This paper appears quite similar to the recent work ""Active Bias"" [1].
The motivation for the technique and setting appear very similar, while the details of the techniques are different. Unfortunately, this is not mentioned in the related work, or even cited.

When introducing a new method, it is important that design choices are principled, have theoretical guidance, or are experimentally verified against similar design choices. Without one of these, the methods become arbitrary and it is unclear what causes better performance. Unfortunately, this paper makes several choices, about an uncertainty function, the probability distribution, the discretization, and the algorithm (when to update) that appear rather arbitrary. For instance, the uncertainty function is a signed standard deviation of the softmax output. While there are a variety of uncertainty functions, such as entropy and margin, a new seemingly arbitrary uncertainty function is introduced.

The experiments are good but could be designed a bit better. For instance, it is unclear if the gains are because of lower asymptotic error or because of faster convergence. The learning curves are stopped too early, while the test error is still dropping quickly.

In summary, it is not clear if this paper adds any insight beyond ""Active Bias"".

[1] Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. 2017. Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum.",5
"Neural Network Cost Landscapes as Quantum States

The authors describe a method where a deep learning framework can be quantised, this is done by considering the two state form of a Bloch sphere/qubit and mapping binary neural network. onto the quantum object creating a quantum binary neural network

I have to say I liked the paper, it is indeed novel and I haven’t seen this anywhere else. More then that, it addresses the quantum aspects of deep learning which has only recently started getting so much attention rather the regular machine learning algorithms. And such I think it’s a good fit for ICLR.

With that I have a few concerns/ nitpicking issues I would like the authors to address if possible
1. While authors show basically a discreet network (much like Soudry’s work) there has also been recently a show of continues variable networks (https://arxiv.org/pdf/1806.06871.pdf), how does your scaling compare to that ? Could one think of this is the continuum limit (albeit you showed a *very* small system) of your model ?
2. As a general style remark there is a lot of introduction on quantum information and I wondering if authors could just reference classical text such as Isaac Chuang, Michael Nielsen or for the CS flavour Classical and Quantum Computation  ? I would have liked to see all 3.1,3.2 maybe in an appendix and a lot more details on the experiments and setup for example 
3. At the end of section 3.2 authors mention the lack of correspondence principle in some quantum systems, I would be happy for a refinement in the aspect of quantum computing that is true and also in general quantum mechanics but there is a huge body of work and an entire field dedicated to just that, settling the difference in correspondence principle and it’s meaning (for example Chaos and the semiclassical limit of quantum mechanics (is the moon there when somebody looks?) by Michael Berry. While this is defiantly far from a deal breaker I would be happy for a bit more clarification on subtle difference. My guess is that authors are referring to the fact that an essential part of quantum computing is the lack of correspondence that can also be taken advantage of for quantum parallelism/ quantum speed ups ? Same for the last paragraph of 3.1.
4. In the method section  all of this procedure is described for a fault tolerant machine, can you say something about coupling to error correction codes for near term quantum devices ?
5. On the same note, there is a feeling that non linearities are swept under the rug, do you have a hunch how can one use non linear activation functions ? (General question, you don’t really have to answer and can take the fifth )
6. In the problem statement (4.2)  x is in fact the binary representation of the qubit state ?
7. In your method Fig. 2, should there be some sort of measurements ?
8. Can you elaborate what exactly goes into the U gates and how are they constructed in Fig.2 ? Or is it some  oracle model that can do all the actions 
9. You report a quadratic speedup, is there some relationship to random walk  is this in fact a case of a RW search over the parameter space ?
",5
"Review of ""Neural Network Cost Landscapes as Quantum States""

Paper summary:

The paper proposes a new algorithm ""quantum amplitude amplification""
for training and model selection in binary neural networks. (in which
both weights and activations are restricted to the set -1, 1)

Section 2 references related work and gives some motivation, that some
quantum algorithms scale better (in terms of big-O notation) than
classical algorithms.

Section 3 explains the basics of quantum computing (qubits and quantum
gates).

Section 4 explains the proposed method. There are two toy
problems. The binary neural network has 8 weight parameters. There are
helpful Figures 1-2 which explain the network structure and the
quantum circuit.

Section 5 explains the results of using the proposed method in a
quantum computer simulator (not an actual quantum computer). On the
two toy problems the paper observes quadratic speedups with respect to
a brute force search.

Comments:

A strong point is that the paper is very well-written and easy to
understand. 

However there are several weak points which should be addressed before
publication. Major weak points are (1) only (noiseless?) toy data sets
are used, (2) some terms in the paper are unclear/undefined, and (3)
results are unconvincing.

It is not clear that this article should be published in the machine
learning literature. One of the hallmarks of machine learning is a
focus on algorithms for real data sets / problems. In contrast the
focus of this paper is quantum computations on toy data /
problems. Maybe this paper would be better suited for publication in
the quantum computation literature?

The toy problems are explained in section 4.2. Is there any noise or
are these noiseless simulations? How does your model/algo perform as a
function of the noise level? How many data points did you simulate
from the model? (e.g. what is the number of observations in the training set?)

The paper uses the terms ""cost landscape"" and ""meta-cost landscape""
without explicitly defining them. Equations should be added to clarify
these terms.

Results could be made more convincing by
1. using a real quantum computer.
2. using real data rather than toy data.
3. adding error bars or confidence intervals to Figures 4-5.
4. using a more appropriate baseline -- why not try the algorithms mentioned in section 2.1?

Figure 3 could be clarified by providing ticks and labels on the x
axes.

",3
" This paper proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN). And then the landscape state is utilized to training the neural network by using the standard quantum amplitude amplification method.   

Although this idea is interesting, I trend to reject this submission as I think its presentation is unclear and the technical detail is a little difficult to follow. So, the correctness and soundness of this work is difficult to verify. I urge the authors to revise their draft to provide more and clearer technical details.

Detailed comments and questions:

Could the authors further point out that what the scope of the binary features are, {0, 1} or {-1, +1}? To my understanding, it should be {-1, +1}, or the corresponding variables are always +1. In addition, the construction of the “multiplying values by binary weights” module implies that the value should take -1 or +1, rather than 0 or 1. However, at the bottom of page 5, the authors claim that the binary values take +1 and 0. Could the authors clearly explain the term “parameter” and “value”?
 
Could the author further explain how to construct the majority activation function?
In the part of “calculating accuracy”, the authors mention that “running the QBNN with the weights in superposition for each point in the training set separately” and “there are N qubits containing the prediction of the QBNN”. To my understanding, there are 3 qubits representing the 8 weights, several qubits representing the input values, and N qubits representing the predictions. But how to construct the final landscape state to be optimized with these qubits?
 
Could the author explain intuitively the main idea of the amplitude amplification method? Specifically, what is the relation between the qubits representing parameters and the qubits presenting prediction results?
 
During the amplitude amplification process, the probabilities change periodically. How to select the best number of steps k in advanced if we do not known the best parameter? Or how to judge if the training is success?
 
Overall speaking, I think this paper is interesting. However, the presentation is unclear and I suggest the authors to revise their draft by providing more technical details.",4
"Edit: changed ""Clarity""

[Relevance] Is this paper relevant to the ICLR audience? yes

[Significance] Are the results significant? no

[Novelty] Are the problems or approaches novel? no

[Soundness] Is the paper technically sound? okay

[Evaluation] Are claims well-supported by theoretical analysis or experimental results? marginal

[Clarity] Is the paper well-organized and clearly written? no

Confidence: 3/5

Seen submission posted elsewhere: No

Detailed comments:

In this work, the authors compare several state-of-the-art approaches for high-resolution microscopy analysis to predicting coarse labels for the outcomes of pharmacological assays. They also propose a new convolutional architecture for the same problem. An empirical comparison on a large dataset suggests that end-to-end systems outperform those which first perform a cell segmentation step; the predictive performance (AUC) of almost all the end-to-end systems is statistically indistinguishable.

=== Major comments

The paper is primarily written as though its main contribution is as an empirical evaluation of different microscopy analysis approaches. Recently, there have been a large number of proposed approaches, and I believe a neutral evaluation of these approaches on datasets other than those used by the respective authors would be a meaningful contribution. However, the current paper has two major shortcomings that prevent it from fulfilling such a place.

First, the authors propose a novel approach and include it in the evaluation. This undercuts claims of neutrality. (Minor comments about the proposed approach are given below.) 

Second, the discussion of the results of the empirical evaluation is restricted almost solely to repeating in text the what the tables already show. Further, the discussion focuses only on the “top line” numbers, with the exception of a deep look at the Gametocytocidal compounds screen. It would be helpful to instead (or additionally) identify meaningful trends, supported by the data acquired during the experiments. For example: (1) Do the end-to-end systems perform well on the same assays? (2) Would a simple ensemble approach improve things? if they perform well on different assays, then that suggests it might. (3) What are the characteristics of the assays on which the CNN-based approaches perform well or poorly (i.e., how representative is Figure 5)? (4) What happens when the FNN-based approach outperforms the CNN-based ones? in particular, what happens in A13? (5) How sensitive are the approaches to the number of labeled examples of each assay type? (6) Are there particular compounds which seem particularly informative for different assays?

A second major concern is whether the binarized version of this problem (i.e., assay result prediction) is of interest to practitioners. In many contexts, quantitative information is also important (“how much of a response do we see?”). While one could imagine the rough qualitative predictions (“do we see a response?”) shown here as an initial filtering step, it is hard to believe that the approach proposed here would replace other more informative analysis approaches.  

=== Minor comments

Are individual images from the same sample image always in only the training, validation, or testing set? that is, are there cases where some of the individual images from a particular sample image are in the training set, while others from that sample image are in the testing set?

I did not find the dataset construction description very clear. Does each row in the final, 10 574 x 209 matrix correspond to a single image? Does each image correspond to a single row? For example, it seems as though multiple rows may correspond to the same image (up to four? the three pChEMBL thresholds as well as the activity comment). What is the order in which the filtering and augmenting happens? It would be very helpful to provide a coherent, pipeline description of this (say, in an appendix).

Do all the images in the dataset come from the same microscope (and cell line) at the same resolution, zoom, etc.? If so, it is unclear how well this approach may work for images which are more heterogeneous. There are not very many datasets of the size described (I believe, at least) available. This may significantly limit the practical impact of this work.

How many epochs are required for convergence of the different architectures? For example, MIL-net has significantly fewer parameters than the others; does it converge on the validation set faster?

=== Typos, etc.

The references are not consistently formatted.

“not loosing” -> “not losing”
“doesn’t” -> “does not”
",3
"The paper is well written, deals with a valid and crucial end-to-end imaging problem. 

Comments
1) Section 2: It is not clear how 10574 compounds increase to 11585 (2nd paragraph page 3). Also how does one arrive at 11171 compounds (para 3). 
2) How do you arrive at 209 assays from 10818?
Do consider enumerating this Section: data dimensions you started with and then how the dimensions were reduced per step. I gather you have mentioned this but it is confusing to grasp, at this point. 

3) In page 2, you mention the images have 5 channels but towards the end of the section on page 3, it says 1) views have ‘6’ such images per sample image and 2) 4 channels for stains. How many stains are there per channel and how are 5 channels related to the ‘6’ and 4 channels? 

4) In Section 4 and Appendix 6, it does not seem that Gapnet outperforms, rather it is at par to, other architectures. Is the only gain with Gapnet the runtime across epochs?",6
"The authors explore the possibility of using an end-to-end approach for predicting pharmacological assay outcome using fluorescence microscopy images from the public Cell Painting dataset. In my view, the primary contributions are the following: an interesting and relatively new application (predicting assay outcomes), enriching the CellPainting dataset with drug activity data, and a comparison of several relevant methods and architectures. The technical novelty is weak, and although the authors demonstrate that end-to-end holistic approaches outperform previous segmentation-and-feature-extraction approaches, this result is not surprising and has been previously reported in closely related contexts.


OVERVIEW

The authors evaluate the possibility of using and end-to-end deep learning approach to predict drug activity using only image data as input. The authors repurpose the CellPainting dataset for activity prediction by adding activity data from online ChEMBL databases. If made available as promised, the dataset will be a valuable resource to the community. The authors compare a number of previous approaches and state-of-the-art image classification network architectures to evaluate the use of CNNs instead of more classical image analysis pipelines. The comparison is a strong point of the paper, although some details are lacking. For example, the authors claim that GapNet is the quickest method to train, and while they report the number of hyperparameters and time per epoch, the number of epochs trained is never mentioned. 

The authors propose an architecture (GapNet) for the assay prediction task. While the way Global Average Pooling is used to extract features at different stages in the network might be new, it is a straightforward combination of GAP and skip connections. Little insight into why this approach is more efficient or evidence for its effectiveness is provided. Similarly, more explanation for why dilated convolutions and SELU activations would be appreciated. A comparison between GapNet and the same network without the GAP connections could possibly provide a more interesting comparison and might also provide a more pervasive argument as to why GapNet’s should be used. Ultimately, the benefit of using GapNet over the other architectures is not strongly motivated, as training time is less of a concern in this application than predictive power.


RELATED WORK

The authors present previous work in a clear and comprehensive manner. However, the reported finding that “CNNs operating on full images containing hundreds of cells can perform significantly better at assay prediction than networks operating on a single-cell level” is not surprising, and partial evidence of this can be found in the literature. In [1], it was shown that penultimate feature activations from pre-trained CNNs applied to whole-image fluorescence microscopy data (MOA prediction) outperform the baseline segmentation-then-feature extraction method (FNN). Similarly, in [2] (the paper proposing MIL-Net), it is shown that end-to-end whole-image CNN learning for protein localization outperforms the baseline (FNN). In [3] whole image end-to-end learning outperforms whole image extracted features for a phenotyping task. All of these references use fluorescence microscopy data similar to the dataset in this work.

[1] Pawlowski, Nick, et al. ""Automating morphological profiling with generic deep convolutional networks."" bioRxiv (2016): 085118.
[2] Kraus, Oren Z., Jimmy Lei Ba, and Brendan J. Frey. ""Classifying and segmenting microscopy images with deep multiple instance learning."" Bioinformatics 32.12 (2016): i52-i59
[3] Godinez, William J., et al. ""A multi-scale convolutional neural network for phenotyping high-content cellular images."" Bioinformatics 33.13 (2017): 2010-2019.


APPROACH

The authors compile enrich the CellPaining dataset with activity data from various drug discovery assays. In my view, the creation of this dataset is the strongest and most valuable contribution of the paper. The method used to collect the data is described clearly and the choices made when compiling the dataset, including the thresholds and combinations of activity measures seems like a well founded approach.

The authors then identify a number of approaches that are relevant for the problem at hand, binary prediction of drug activity based on image data. These include previous approaches used for cell images and modern image classification networks.


EXPERIMENTS

The different approaches/networks mentioned above were evaluated on a testset. The results indicate that end-to-end CNN approaches outperform all non-end-to-end with no significant difference between the individual end-to-end CNNs. The results are stated clearly and the presentation of different metrics is a nice addition to properly compare the results. It would however contribute valuable information if the authors stated how the confidence intervals of the F1 score are calculated (are the experiments based on several runs of each network or how is it done).


NOVELTY/IMPACT

+ Creation of a new dataset on a new and interesting problem 
+ Useful comparison of modern networks on the task
- GapNet - lacking technical novelty, insight, and performance is unconvincing
- Demonstrates that end-to-end learning outperforms cell centric approach - was this really surprising or even new information?


OTHER NOTES:
* Figure 3 is never mentioned in the main text
* Figure 3 (*’s) are confusing. Do they represent outliers? Statistical significance tests?
* Figure 5 which panel is which?
* Be clear what you mean when you refer to “upper layers” of a network
* An important point not mentioned: in practice, many assays use stains that are closely tied to the readout, unlike the dataset here which provides only landmark stains. The results found here do not necessarily apply in other cases.
",5
"The paper tries to bring together the replay buffer and on-policy method. However, the reviewer found major flaws in such a method.

- Such replay buffers are used for storing simulations from several policies at the same time, which are then utilised in the method, built upon generalised value and advantage functions, accommodating data from these policies.

If the experience the policy is learning from is not generated by the same policy, that is off-policy learning. 

In the experiment part, the replay buffer size is often very tiny, e.g., 3 or 5. The reviewer believes there may be something wrong in the experiment setting. Or if the reviewer understood it incorrectly, please clarify the reason behind such a tiny replay buffer.",7
"The authors introduce a off-policy method for TRPO by suggesting to use replay buffers to store trajectories and sample from them during training. To do this they extend the definition of the Q function to multiple policies where the Q_pi bar is then the expectation over the several policies. They propose the same for the value function and consequently the advantage function. 
In my opinion this is some interesting work, but there are some details that are not clear to me, so i have several questions.

1. Why is it necessary to define these generalized notions of the Q, Value and Advantage functions? You motivate this by the fact the samples stored in the replay buffer will be generated by different policies, i.e. by differently parametrized policies at a certain time step. But this also holds almost all algorithms using replay buffers. Could you plese explain this part further?

2. In eq. (26) you introduce the parameter alpha as a sort of Lagrange multiplier to turn the unconstrained optimization problem defined by TRPO into a constrained one. This is was also proposed early by Schulman et al. in Proximal Policy Optimization. Yet, it is not cited or referenced. In the discussion of the experimental results go further into this. Please explain this part in more detail.

3. Another point of your work is the learnable diagonal covariance matrix. How can you be sure that the improvements you show are due to the replay buffers and not due to learning these? Or learning covariance in combination with the penalty term alpha?

4. Can you provide comparative results for PPO? PPO outperforms DDP and TRPO on most tasks so it would be interessting to see

5. How many trajectory samples do you store in the replay buffers? Can you provide results where you use your method but without any replay buffers, i.e. by using the last batch of data points?

Minor Suggestions:
- The references for the figures in the Experiments part are off. In fig. 1 you cite Todorov et al. for Mujoco but not TRPO and ACKTR, the same in fig. 2. Then in fig. 3 you cite DDPG also with Todorov et al.
- Some parts of the text is a bit unorganized. In section 2.1 you introduce AC algorithms and on the next page you give the definitions for all components but you don't say anything about how the interact. Also, the definition of the expected return was not ""invented"" by Schulman et al, and neither were Advantages, Q-, and Value functions. Maybe add a second or third reference.  
",6
"In this paper, the authors present how to integrate replay buffer and on-policy trust region policy optimization (TRPO) by generalizing Q/V/advantage function and then empirically show the proposed method outperforms TRPO/DDPG.

The generalization of advantage function is quite interesting and is well written. One minor issue is that d^{\pi_n} (s) is confusing since it appears after ds. 

The theory in Section 3.1 makes sense. However, due to the limitation in Theorem 1 that $\theta$ is the joint parameters, applying Theorem 1 can be difficult. In Eq (25), what is the $\theta$ here? And what does $\nabla_\theta \pi_n$ mean? Does $\pi_n$ uses $\theta$ for computation? One of the problems of using replay buffers in on-policy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue. Further explanation on Eq (25) might help. If the distributions of states are assumed to match, then the joint distribution of states and actions may mismatch so additional importance sampling might help, as suggested in [1] Eq (3). 

Another problem is on the barrier function. In Eq (26), if we only evaluate $\rho_b(\theta)$ (or its gradient w.r.t. $\theta$) at the point $\theta_old$, it doesn't differ with or without the barrier function. So in order to show the barrier function helps, we must evaluate $\rho_b(\theta)$ (or its gradient) at a point $\theta \neq \theta_old$. As far as I know, the underlying optimizer, K-FAC, just evaluates the objective's (i.e., $\rho_b$) gradients at $\theta_old$. Both Conjugate Gradient (CG), which TRPO uses, and K-FAC are trying to solve $F^{-1} g$ where $g$ is the gradient of the objective at the current point. 

The experiments show significant improvement over TRPO/DDPG. However, some experiments are also expected.
1. How is the proposed algorithm compared to PPO or Trust PCL? 
2. How does the barrier function help? More importantly, what's the comparison of the barrier function to [1] Eq (5)? 

The proposed algorithm seems more like a variant of ACKTR instead of TRPO since line search is missing in the proposed algorithm and the underlying optimizer is K-FAC instead of CG.

Ref:
[1]: Proximal Policy Optimization Algorithms, by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 
",5
"The authors prove a PAC-Bayes bound on a perturbed deterministic classifier in terms of the Lipschitz constant of the Hessian. They claim their bound suggests how insensitive the classifier is to perturbations in certain directions. 

The authors also “extract” from the bound a complexity measure for a particular classifier, that depends on the local properties of the empirical risk surface: the diagonal entries of the Hessian, the smoothness parameter of the Hessian, and the radius of the ball being considered.  The authors call this “metric” “PAC-Bayes Generalization metric”, or pacGen.

Overall, this seems like a trivial extension of Neyshabur et al. PAC-Bayes bounds. 

The experiments demonstrating that pacGen more or less tracks the generalization error of networks trained on MNIST dataset is not really surprising. Many quantities track the generalization error (see some of Bartlett’s, Srebro’s, Arora’s work). In fact, these other quantities track it more accurately. Based on Figure 2, it seems that pacGen only roughly follows the right “order” of networks generalizing better than others. If pacGen is somehow superior to other quantities, why not to evaluate the actual bound? Or why not to show that it at least tracks the generalization error better than other quantities?

The introduction is not only poorly written, but many of the statements are questionable. Par 2: What complexity are you talking about? What exactly is being contradicted by the empirical evidence that over-parametrized models generalize? 

Regarding the comment in the introduction: “ Dinh et al later points out that most of the Hessian-based sharpness measures are problematic and cannot be applied directly to explain generalization.”, and regarding the whole Section 5, where the authors argue that their bound would not grow much due to reparametrization:
If one obtains a bound that depends on the “flatness” of the minima, the bound might still be useful for the networks obtained by SGD (or other algorithms used in practice). The fact that Dinh et al. paper demonstrates that one can artificially reparametrize and change the landscape of a specific classifier does not contradict any generalization bounds that rely on SGD finding flat minima. Dinh et al. did not show that SGD finds classifiers in a sharp(er) minima that generalize (better).

In the experiment section, the authors compare train and test errors of perturbed (where the perturbation is based on the Hessian) and unperturbed classifiers. However, they don't compare their results to other type of perturbations, e.g. dropout. It’s been shown in previous work that certain perturbations improve generalization and test error.

There are numerous typos throughout the paper.


****************

[UPDATE]

I would like to thank the authors for implementing the changes and adding a plot comparing their algorithm with dropout. While the quality of the paper has improved, I think that the connection between the perturbation level and the Hessian is quite obvious. While it is a contribution to make this connection rigorous, I believe that it is not enough for a publication. Therefore, I recommend a rejection and I hope that the authors will either extend their theoretical or empirical analysis before resubmitting to other venues.",5
"The authors study generalization capabilities of neural networks local minimums thanks to a PAC-Bayesian analysis that grasps the local smoothness properties. Even if some assumptions are made along the way, their analysis provides a metric that gives insight on the accuracy of a solution, as well as an optimization algorithm. Both of these result show good empirical behavior.

However, despite my favorable opinion, I consider that the paper presentation lacks rigor at many levels. I hope that the criticism below will be addressed in an eventual manuscript.

It is confusing that Equations (4) and (9) defines slightly differently \sigma*_i(w*,\eta,\gamma). In particular, the former is not a function of \eta. 

The toy experiment of Figure 1 is said to be self-explainable, which is only partly true. It is particularly disappointing because these results appear to be really insightful. The authors should state the complete model (in supplementary material if necessary). Also, I do not understand Figures (b)-(c)-(d): Why the samples do not seem to be at the same coordinates from one figure to the other? Why (d) shows predicted green labels, while the sample distribution of (b) has no green labels?

It is said to justify the perturbed optimization algorithm that Theorem 1 (based on Neyshabur et al. 2017) suggests minimizing a perturbed empirical loss. I think this is a weak argument for two reasons:
(1) This PAC-Bayes bounds is an upper bound on the perturbed generalization loss, not on the deterministic loss.
(2) The proposed optimization algorithm is based on Theorem 2 and Lemma 3, where the perturbed empirical loss does not appear directly.
That being said, this does not invalidate the method, but the algorithm justification deserves a better justification

There is a serious lack of rigor in the bibliography:
- Many peer-reviewed publications are cited just as arXiv preprints
- When present, there is no consistency in publication names. NIPS conference appears as ""Advances in Neural ...,"", 'NIPS'02"", ""Advances in Neural Information Processing Systems 29"", ""(Nips)"". The same applies to other venues.
- Both first name initials and complete names are used 
- McAllester 2003: In In COLT
- Seldin 2012: Incomplete reference

 Also, the citation style is inconsistent. For instance, the first page contains both ""Din et al, (2007) later points out..."" and ""Dziugaite & Roy (2017) tries to optimize..."" 

Typos:
- Page 3: ...but KL(w*+u | \pi) => KL(w*+u || \pi)
- In this/our draft: Think to use another word if the paper is accepted
- Line below Equation (5): \nabla^2 L => \nabla L (linear term)
- it is straight-forward -> straightforward
",6
"This paper gives various PAC-Bayesian generalization guarantees and some
empirical results on parameter perturbation in training using an algorithm
motivated by the theory.

The fundamental issue addressed in this paper is whether parameter
perturbation during training improves generalization and, if so, what
theoretical basis exists for this phenomenon.  For continuously
parameterized models, PAC-Bayesian bounds are fundamentally based on
parameter perturbation (non-singular posteriors).  So PAC-Bayesian
theory is naturally tied to parameter perturbation issues.  A more
refined question is whether the size of the perturbation should be
done on a per-parameter bases and whether per-parameter noise levels
should be adaptive --- should the appropriate noise level for each
parameter be adjusted on the basis of statistics in the training data.
Adam and RMS-prop both adapt per-parameter learning rate eta_i to be
proportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running
estimate of the expectation over a draw of a training point of the
square of the gradient of the loss with respect to parameter i.  At
the end of the day, this paper, based on PAC-Bayesian analysis,
proposes that a very similar adaptation be made to per-parameter noise
during training but where E g_i^2 is replaced by the RMS value \sqrt{E
g_i^2}.  It seems that all theoretical analyses require the square
root --- the units need to work.  A fundamental theoretical question,
perhaps unrelated to this paper, is why in learning rate adaptation the
square root hurts the performance.

This paper can be evaluated on both theoretical and empirical grounds.
At a theoretical level I have several complaints.  First, the
theoretical analysis seem fairly mechanical and without theoretical
innovation. Second, the analysis obscures the prior being used (the
learning bias). The paper first states an assumption that each
parameter is a-priori taken to be uniform over |w_i| <= \tau_i and the
KL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where
sigma_i is the width of a uniform posterior over a smaller interval.
But later they say that they approximate tau_i by |w_i| + kappa_i with
kappa_i = \gamma |w_i| + epsilon.  I believe this works out to be
essentially a log-uniform prior on |w_i| (over some finite range of
log |w_i|).  This seems quite reasonable but should be made explicit.

The paper ignores the possibility that the prior should be centered at
the random initialization of the parameters.  This was found to be
essential in Dziugaite and Roy and completely changes the dependence
of k_i on w_i.

Another complaint is that the Hoefding bound is very loose in cases
where the emperical loss is small compared to its upper bound.  The
analysis can be more intuitively related to practice by avoiding the
rescaling of the loss into the interval [0,1] and writing expressions
in terms of a maximum bound on the loss L_max.  When hat{L} << L_max
(almost always the case in practice) the relative Chernoff bound is
much tighter and significantly alters the analysis.  See McAllester's
PAC-Bayesian tutorial.

The theoretical discussion on re-parameterization misses an important
point, in my opinoin, relative to the need to impose a learning bias
(the no-free-lunch theorem).  All L_2 generalization bounds can be
interpreted in terms of a Gaussian prior on the parameters.  In all
such cases the prior (the learning bias) is not invariant to
re-parameterization.  All L_2 generalization bounds are subject to the
same re-parameterization criticism.  A prior tied to a particular
parameterization is standard practice in machine learning for in all
L_2 generalization bounds, including SVMs.  I do think that a
log-uniform prior (rather than a Gaussian prior) is superior and
greatly reduces sensitivity to re-parameterization as noted by the
authors (extremely indirectly).

I did not find the empirical results to very useful.  The value of
parameter perturbation in training remains an open question. Although
it is rarely done in practice today, it is an important fundamental
question. A much more thorough investigation is needed before any
conclusions can be drawn with confidence. Experimentation with
perturbation methods would seem more informative than theory given the
current state of the art in relating theory to practice.
",6
"A Stackelberg competition is a nonzero-sum game where 1) each player has their own objective, which do not sum up to a constant, and 2) there is an order at which the players interact. The proposed formulation only assumes that parameters of one player (data generator) partition in I tuples \gamma_i of parameters, where each tuple parameterizes a different data generator component (e.g., a separate neural network). Further, each of those components is assumed to contribute a term to the game's objective that only depends on the corresponding parameter tuple \gamma_i, and the other player's parameters \theta (e.g., weights of the discriminator). From a game theoretic perspective, this still yields a 2-player zero-sum game where the action space of the data generator is the product space of the I tuple spaces. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.

The gap between the a maximin and minimax solution is determined by the shape of the objective \phi(\gamma,\theta) and is zero, for example, if \phi is (quasi) convex in \gamma=[\gamma_1, ..., \gamma_I], and (quasi) concave in \theta. The authors bound the violation of this property w.r.t. the data generator components' parameters \gamma_i, and argue that this degree of violation is the same for the whole data generator parametrized by \gamma=[\gamma_1, ..., \gamma_I] if the data generator components are from the same family of mappings (e.g., having the same network architecture). While this conclusion is true under worst cast assumption, e.g., the globally maximal possible gap, this would also imply that all data generator components find the same global best solution, that is, yield the same mapping, in which case the gap would be identical to just using one of those components.

Intuitively, the only reason to have multiple data generator components is to learn different mappings such that the joint data generator -- mixing the outputs of the different components -- is more expressiv than just a single mapping. If the different mappings only result from the inability of finding the global best solution, a worst case argument is not very insightful; in this case, one should study the duality gap in the neighborhood of the starting solutions. On the other hand, if we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.

So why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments? Solving a maximin/minimax problem for highly non-convex-concave functions is challenging; there is an infinity of saddle point solutions which yield different ""performances"". The multi-branch GAN can be seen as a model averaging approach giving more stable results, whereas the proposed GAN seems more of an ensemble approach to stabilize the result. Though, this is speculative and I would encourage the authors to study this in-depth; the reasoning in Remark 1 is not convincing to me.

UPDATE:

I read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong.",4
"This paper proposes the Stackelberg GAN framework of multiple generators in the GAN architecture. The architecture is similar with previous multiple-generator GANs (MAD-GAN and MGAN). In fact, it's even simpler in the sense that Stackelberg GAN has simpler loss function for the discriminator compared with the previous two. The authors prove that the minimax duality gap shrinks as the number of generators increases. And this proof has no assumption on the expressive power of generators and discriminator. With this proof, the authors argues that because the duality gap shrinks as the number of generators increases, the training of GANs gets more stable.

From the algorithm part, I think the algorithm is very similar (and even simpler) than MAD-GAN and MGAN. The MAD-GAN and MGAN even proposed some specific loss for the discriminator so that it will encourage different generator to generate different modes in the target distribution. The Stackelberg GAN does not do this, but ""partially"" achieved the same goal. However, from Figure 9, we see that the simpler the generator is, the easier different generator will capture different modes. I think that this is due to the simplicity of discriminator loss. Therefore, on the algorithm part, the author may want to address the difference between Stackelberg GAN and MAD-GAN and MGAN. On the experiment part, we need to see more comparison between these three methods. In the current experiment, MGAN result is very similar to the proposed method, and MAD-GAN result is missing. Personally, I think that on cifar dataset (or larger datasets), these three methods should have very similar behavior. 

From the theoretical part, the authors derived a bound of the minimax duality gap for the Stackelberg GAN, without the assumption on the expressive power of generators and discriminator. Although the bound may not be practical, these are nice efforts. There are many typos in the paper (and appendix), which make me difficult to follow the proofs. For example, ""Let clf (bclf) be the convex(concave) closure of f, which is defined as the function whose epigraph (subgraph) is the convex
(concave) closed hull of that of function f."" Do we have concave closed hull of subgraph of function f? What is the concave closed hull of a set? The usage of sub(sup)-script is also very confusing, like in the definition of h_i(u_i). The authors may want to correct typos and improve the presentation. In the conclusion, the authors conclude ""We show that the minimax gap shrinks to \eps as the number of generators increases with rate e O(1/\eps)."" This is an over-claim, because the authors only proved this under the assumption of concavity of the maximization w.r.t. discriminators. 

Finally, the authors may want to provide some simple results of the Stackelberg GAN from the perspective of density approximation, even assuming infinite capacity of the discriminator set, as other GANs does. Whether the distance defined by the maximization problem a distance or divergence. If we exactly minimizing that objective function, do we get the target distribution? 
",5
"This paper proposes a way of training multi-generator in the GAN setting.
While a proposed approach is simply to put N generators and form a sum of GAN losses to train a model, the paper carefully presents a theoretical analysis on the duality gap, and shows as N goes infinity, the duality gap can shrink to zero.
One can think of this as a usual ensemble approach to increase model's capacity and performance, but the main difference to the usual ensemble approach is to form a sum of losses (ensemble losses) instead of a loss on output of ensemble.
The paper shows this can be more effective approach to train a multi-generator architecture and I believe that this can be an effective approach to capture multi-modal sample distributions.
Finally, a paper is well-written and well-organized. ",7
"In this paper the authors analyse the role of residuals in the performance of residual networks (res-nets) with shared layer weights and propose a method to adapt the depth of such networks during training. They exploit  the fact that res-nets identical blocks / shared layer weights are discrete time dynamical systems to conduct their analysis. Their main contribution seems to be an empirical evaluation of the role of transient dynamics in the performance of such res-nets. Experiments are done on a toy dataset on concentric circles and MNIST data.

I find the scope of the paper very narrow (res-nets with shared weights) even though there seem to be quite some other papers addressing this problem.

Clarity and quality of writing. It seems to me that the paper could  be much better written, the authors present long trains of thoughts and sometimes unsubstantiated arguments and at times there seems to be no clear storyline. In particular I would strongly  suggest a rewrite of  Sections 1, 2.1, and 4.  The storyline often very sketchy and is littered with hypothetical claims or less relevant information that can distracts and confuse the reader. I find that the hypotheses are not well formulated,   the claimed contributions of the paper don't seem to be significant enough and they should also be better connected to the experimental sections. Sometimes there are some odd choices of concepts/words such as ""softmax algorithm"" and ""classification behaviour.""

Technical quality and contribution. The main technical contribution of the paper seems to be the observation that  that the solution of an  ODE if the integral of the RHS an that one can derive an ODE for the residuals only. I don't find  these results significant/relevant enough to justify the publication of this paper.  I expected a better written and more informative Section 2.1: some approximations of rates of convergence, a bit more about basins of attraction. I am also a bit skeptical about the claim that the analysed network us a prey-predator model. 

Experimental section. I find the description of experiments in Sec 4 very hard to read. The metrics are not clearly defined (not sure visualisations serve the purpose either), and the performed experiments are not well motivated and explained, for example in Section 4/P2 while I think I understand what the authors want to show (path of relevant neurons), I find the purpose of the whole experiment not very relevant.  I better analysis of the number of fixed points for classification tasks should be added, comparison of resulting features to other methods such as simple MLPs with same last layer could help . More relevant datasets should be be added e.g. CIFARxxx., ImageNet.

Overall: I find that this paper needs to be improved both in terms of readability and technical contribution to ready for publication.",4
"This paper aims to view the computations performed by residual networks through the lens of transient dynamical systems. A rough intuition is provided, followed by experiments in a toy concentric circle example and on MNIST. Finally, a method to determine the appropriate depth of ResNets is proposed, though experiments are only performed in the toy concentric circle experiment. 

The approach of attempting to interpret feedforward networks through a dynamical systems perspective is an interesting and worthwhile one. However, this paper suffers from a number of flaws, and is clearly not ready for publication. 

The clarity of this paper can be significantly improved. In general, the text is confusing, and as currently written, it is difficult to understand the central narrative of the manuscript. The review of literature in the introduction is relatively complete, though again, the presentation makes this section difficult to understand. 

Scientifically, it is clear that further experiments on less toy datasets and settings will be required. While MNIST is useful for prototyping, experiments on datasets such as CIFAR (or ideally ImageNet) will be necessary to evaluate whether the observations made hold in more realistic settings. Moreover, the primary claim: that ResNets sum the residuals across layers is by definition true and by design. The scientific contribution of this statement is therefore questionable.

In addition, the case analyzed in the majority of the paper -- weight sharing across all layers -- is an unusual one, and as Figure 3 shows, clearly changes the network dynamics. The use of sigmoid activation functions is also an unusual one given that ResNets are generally trained with ReLU activations. 

Finally, the proposed method for determining the optimal depth for ResNets is an interesting idea, and worth further examination. However, the paper currently evaluates this only on an an extremely toy dataset of concentric circles. Evaluation on more realistic datasets (comparatively) with appropriate baselines will be required to determine whether this method is in fact helpful. 
",2
"This paper presents the following main insight (quoting the authors): Resnets classify input patterns based on the sum of the transient dynamics of the residuals in different layers.
The authors formulate this insight mathematically, and experimentally demonstrate its validity on a toy binary classification problem. They also show that the behaviour of a Residual network trained on MNIST is in line with the claims. Finally, a method to adapt the depth of a Residual network during training is proposed and applied to the toy classification problem.

The paper is generally of good quality and easy to understand. My only complaint: the introduction (including related work) is too long and I think it will be unclear to a general reader before reading section 2, where the terms used in the paper are explained and clarified. I think it will be an improvement to leave out detailed discussion of related work for a separate section, and focus on clarifying what the paper is about.

Overall, while the paper is related to an interesting and potentially fruitful perspective of neural networks (as dynamical systems), in my view the contributions are not significant at this stage. That the sum of transients determines network outputs is almost by design, and can be shown without a dynamical systems perspective. Using the paper’s notation, one can sum over the equations for all the layers to obtain this.

x(1) = x(0) + y(1)
…
x(T) = x(T-1) + y(T)
————————————————————————
x(T) = x(0) + sum(y(t))

Since the classification is performed using x(T), it is clear that the sum of transients is what causes the change in representation and that y(T) can be the same or not for different class inputs.

Based on my understanding, I don’t find the findings to be significantly novel or surprising. In particular, I don’t see any concrete benefits of employing a dynamical systems perspective here. Nevertheless, the experimental analysis is interesting, and I’m hopeful that this direction will lead to more insights in the future.

The final contribution is a new method to learn the depth of Residual networks during training, but this is insufficiently explored (only tested on the toy dataset) so its practical significance can not be evaluated at this stage.


Minor notes:
- Note that many citations are textual when they should be parenthetical.
- The reference “No & Liao (2016)” has incorrect author name.",5
"Thanks for submitting your paper. It takes a lot of effort and courage to put your ideas out into the world. Sometimes the hardest work for researchers is conveying their thoughts to others in a manner in which those ideas can be understood.

With that in mind, I had an extremely difficult time following your arguments. 

I noticed several things:
 - There are numerous places in the text that lack proper citation, or are cited improperly.

 - Why was there not a related methods section? I find it hard to believe that all of your ideas have no precursor.

 - When there are citations, there is usually only one text and it is quite old. For example, all of your neuroscience citations reference a work that is almost 40 years old. There have been quite a few improvements in our biological understanding as well as theoretical understanding since then. ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology. )

- There is a claim regarding how this can be used in fintech. This statement doesn't belong in this work.

 - There are many different equations given throughout the text. Some of these equations come from areas like physics or information theory, and others seem to be of your own design. Regarding the latter, there is no justification or explanation for the origin of the equations. Regarding the former, if you are using equations from lots of different fields, or even field you think part of your audience might not be familiar with, you should, at the very least, include a some description of the algorithm or intuition as to why it is being leveraged.

 - It wasn't clear from your diagrams or your descriptions what the difference between a synapse and a neuron was in your architecture. It seemed like the name was used interchangeably in some areas, but then had a strict definition in others.

 - I was also not able to understand how the excitatory and the inhibitory connections that were to enter each neuron were connected to the previous layer of the network. Is a link between neurons in Figure 2 actually two links? If this is the case, then it is a direct violation of Dale's law. Again, I only mention this because most arguments seem to be of the form ""this is correct because it is how it is done biologically"".

 - There were a few claims made in the paper that were completely unsubstantiated. A good example of this was in the conclusion section part ii) where it was stated that ""using a large number of synapses and neurons SynaNN can solve the complex problems in the real world.""

- Also, the last sentence of the conclusion was not discussed anywhere in the rest of the paper. Nor was the statement itself supported except with a single citation and no description.

Regarding the empirical testing of your algorithm, I was very dissapointed to see that the only dataset it was tested against was MNIST. Furthermore there was absolutely no benchmarking against other comparative algorithms. At the very least I would have expected a comparison to the perceptron algorithm that you use as inspiration, but that would also still not have been enough.

This paper needs heavy amounts of work to make it understandable. Once it is understandable an attempt to evaluate the merit of the scientific contribution would then be possible.",2
"The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST.

I am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And it’s not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful.

Moreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. ",3
"The authors present a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells.  Unfortunately, the paper is structured incoherently, making it nearly impossible to appreciate the authors' contribution.  The introduction references neuroscience alongside ResNets, FinTech, surprisal spaces, Bose-Einstein statistics, and topological conjugacy without adequately motivating or defining any of the above.  The fundamental definition of the model synapse as a conditional probability (Eq. 1) is not guaranteed to be non-negative, casting serious doubt on any of the subsequent conclusions.   Figure 1 conveys no further information about the proposed model.  There is no explicit related work or background section.  The single experiment offers no comparison to alternative methods.   I suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologically-inspired learning models.  This is indeed a subfield of machine learning worthy of more investigation. ",2
"Quality - poor
The highly complicated work is evaluated only on the simplest of benchmarks with no significant results. 

Clarity - poor
The paper seems to amount to gobbledygook, many disparate terminology strung together. 

Originality
No idea. 

Significance 
None. 

cons: the paper to me seems a hashing of citations to the main works in neuroscience and deep learning for which only the simplest network is demonstrated (single hidden layer MLP on MNIST) with results that do not exceed that of a standard MLP.
pros: the only pro I can think of for this work is that synaptic computing imo deserves more consideration, as real synapses are very complicated beasts, the functioning of which relatively little is known about. ",2
"Summary: This paper modifies an existing technique designed for image classification to make it applicable to outlier detection. 


Strengths: The outlined problem is of significant practical importance.

Weaknesses: 
- The improvement over the existing method is incremental; 
- The regularization on routing decision may not really be necessary as, in DNDF, the soft splits start as uniform and gradually converge to something close to hard splits; this is discussed in the supplementary material of the DNDF paper;
- the datasets tested are standard image datasets, not even captured from vehicles or video surveillance. The SVHN (street view numbers) dataset is the closest the experiments get to the motivating application. 
Overall assessment: reject

Recommendations for the authors: Test on a surveillance or street view benchmark. Even then, it's questionable whether the paper is suitable for ICLR due to lack of methodological novelty.


Note: I'd like to apologize to the authors for the delay in submitting this review. It was due to a technical error on my part (I thought the reviews had posted, but they had not). In the spirit of independent evaluation, this review was not influenced by the other comments on this paper. I will follow-up with a response which will take into account the existing dialogue.",4
"The paper proposes a decision forest based method for outlier detection and claims that it is better than current methods. 

A few questions:
What is the precise definition of maximum weighted sum? Why not using maximum probability instead in Figure 1? Are they equivalent? What does this 8.1701 threshold refer to? What architectures you use for the experiment in Section 2?

Comments: 
The observation that simple methods for outlier detection are not good enough is interesting, and deserves deeper understanding. 
However, directly calculating max. prob. may be a weak baseline. A stronger method to compare with would be using dropout during testing, see [1], which is easy to calculate and very practical (can easily be deployed to other tasks such as sequence tagging). 
The extensibility of the proposed method is not clear to me. 

Also, the reason that the observed failure of detection happens may due to the optimization procedure, i.e., how you train the model matters. The authors should provide the details of the training methods and architectures, along with the observation. 

The baseline compared in the experiments are methods that do not use the classification feature. It would be necessary to compare with stronger baselines, such as using dropout. 

Typo:
'a sample x $\in$ based on its features'

Reference:
[1] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, by Yarin Gal, Zoubin Ghahramani ",5
"Pros
----

[Originality/Clarity]
The manuscript presents a novel technique for outlier detection in a supervised learning setting where something is considered an outlier if it is not a member of any of the ""known"" classes in the supervised learning problem at hand. The proposed solution builds upon an existing technique (deep neural forests). The authors clearly explain the enhancements proposed and the manuscript is quite easy to follow.

[Clarity/Significance]
The enhancements proposed are empirically evaluated in a manner that clearly shows the impact of the proposed schemes over the existing technique. For the data sets considered, the proposed schemes have demonstrated significant improvements for this scoped version of outlier detection.

[Significance]
The proposed scheme for improving the performance of the ensemble of the neural decision trees could be of independent interest in the supervised learning setting.

Limitations
-----------

[Significance]
Based on my familiarity with the traditional literature on outlier detection in an unsupervised setting, it would be helpful for me to have some motivation for this problem of outlier detection in a supervised setting. For example, the authors mention that this outlier detection problem might allow us to identify images which are incorrectly labelled as one of the ""known"" classes even though the image is not a true member of any of the known classes, and might subsequently require (manual) inspection. However, if this technique would actually be used in such a scenario, the parameters of the empirical evaluation, such as a threshold for outliers that considers 5000 images as outliers, seem unreasonable. Usually number of outliers (intended for manual inspection) are fairly low. Empirical evaluations with a smaller number of outliers is more meaningful and representative of a real application in my opinion.

[Significance]
Another somewhat related question I have is the applicability of this proposed outlier detection scheme in the unsupervised scheme where there are no labels and no classification task in the first place. Is the proposed scheme narrowly scoped to the supervised setting?

[Comments on empirical evaluations]
- While the proposed schemes of novel inlier-ness score (weighted sum vs. max route), novel regularization scheme and ensemble of less correlated neural decision trees are extremely interesting and do show great improvements over the considered existing schemes, it is not clear to me why the use of something like Isolation Forest (or other more traditional unsupervised outlier detection schemes such as nearest/farthest neighbour based) on the learned representations just before the softmax is not sufficient. This way, the classification performance of the network remains the same and the outlier detection is performed on the learned features (since the learned features are assumed to be a better representation of the images than the raw image features). The current results do not completely convince me that the proposed involved scheme is absolutely necessary for the considered task of outlier detection in a supervised setting.
- [minor] Along these lines, considering existing simple baselines such as auto-encoder based outlier detection should be considered to demonstrate the true utility of the proposed scheme. Reconstruction error is a fairly useful notion of outlier-ness. I acknowledge that I have considered the authors' argument that auto-encoders were formulated for dimensionality reduction.

[Minor questions]
- In Equation 10, it is not clear to me why (x,y) \in \mathcal{T}. I thought \mathcal{T} is the set of trees and (x,y) was the sample-label pair. 
- It would be good understand if this proposed scheme is limited to the multiclass classification problem or is it also applicable to the multilabel classification problem (where each sample can have multiple labels).
",5
"The authors challenge the CNNs robustness to label noise, but when the label noise is class dependent, more realistic scenario than class independent noise. 
To analyse the CNNs behavior in such a scenario, they consider the ImageNet 1k dataset, and change some labels to labels that are close according to the ImageNet 1k tree of WordNet. 
The authors conduct multiple experiment to compare the effect of class dependent and class independent noise on:
* the model accuracy
* the robustness to adversarial perturbation 
* the learned representation

The paper is generally well written and well structured. The analysis is sound and addresses interesting points, giving insightful results. Nevertheless, the overall conclusion is not very surprising. This work  confirms the commonly admitted fact that CNNs learn features that are visually meaningful.   Moreover, there is no significant novelty in the paper. The paper only analyses the CNNs behavior, without suggesting any new algorithm based on the observations. One specific point that seems under-investigated in my sense is the observation about the robustness to adversarial perturbations. The model with the class dependent noisy labels is in average less sensitive to the perturbations, even if this is not significant for the tested noise level. Did the authors test with different noise levels? This calls for a further analysis. It has the potential to give more insights, and probably inspire new methods to improve training robustness.    ",5
"This paper attempted to analyse the performance of CNN models when data is mislabelled in different manners, i.e. class dependent labels and class independent labels. It carried out several good experiments as a good start, but several points are not comprehensively studied and analysed. 
1. Try to provide more direct and solid proofs on the relationship between conceptual and visual distances between class dependent labels. 
2. In table 1, model trained with noise on class dependent label has lower fooling rate than model trained with clean data. Is it worth exploiting in a deeper manner?
3. In figure 5, why the curve appears so different after block 4 only? Would visualising feature maps from different layers help understand this observation?
4. In figure 6, it seems that the difference between those experiments is marginal, which contribute little to the argument of this paper.
5. About the discussion on 'cluster', it would be better if sufficient experiments and analysis can be provided.

",4
"This paper demonstrates that CNNs are more robust to class-relevant label noise. They argue that real-world noise should be class-relevant.

Pros:

1. The authors find a new angle to exploit robust learning with noisy labels.

2. The authors perform numerical experiments to demonstrate the effectiveness of their proposal. And their experimental result support their previous claims.

Cons:

We have two questions in the following.

1. Basic definition: in learning with noisy labels, there are two basic models. First, most research focuses on class-conditional noise (CCN) model [1]. Second, recent research explore a bit on instance-dependent noise (IDN) model [2, 3]. As far as I know, there is no class-irrelevant label noise and class-relevevant label noise. In CCN mode, people would like to use symmetric noise and asymmetric noise as a basic benchmark to conduct experiments.

2. Motivation: The authors want to claim CNNs are more robust to such realistic label noise than class-irrelevant label noise. However, they make one mistake. They do not have a clear definition about realistic label noise. In my mind, I believe Clothing1M [4] should be realistic label noise dataset.

By the way, in learning with noisy labels, there are two kinds of research. First, people propose new robust methods for CCN model. Second, people propose new robust methods for IDN models. Proposing new setting should be encouraged. However, the setting and conclusion should be reasonable.

References:

[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 1988.

[2] A. Menon, B. Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. Machine Learning, 2018.

[3] J. Cheng, T. Liu, K. Ramamohanarao, D. Tao. Learning with bounded instance-and label-dependent label noise. arxiv 1709.03768, 2017.

[4] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In CVPR, 2015.",5
"In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF,  it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian.

Authors pointed out, one of the weak points in competing models such as  Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. 

 In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. 

To summarize the contribution of this paper, following three points can be named as main contribution of this paper:
- proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques  to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables.  

- used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters.

-discussed how to overcome  Stick-breaking VAE “component collapse” issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE.


Quality and Novelty:
claims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points:

- Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. ""Variational inference with normalizing flows."" arXiv preprint arXiv:1505.05770 (2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper.

- second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA  etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. ""Autoencoding variational inference for topic models."" arXiv preprint arXiv:1703.01488 (2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point

- Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk


Clarity: 
The paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear.



Significance of experiments:  
As discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.",6
"This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented.

The motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement ""to our knowledge, combining the two statistical results is the first finding in the machine learning field"". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. 

Minor question
- What is the difference between negative LL's and reconstruction losses in experiments?
- The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1?


References
[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.
[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.",5
"Review:

This paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out.

Quality: 

	I think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section.

Clarity: 

	The paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice.

Originality: 
	
	The paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. 

Significance:

	The results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high.

pros:

	- Good results.

	- Simple method proposed.

	- Extensive experiments.

	- Well written paper.

cons:
	
	- The idea is a combination of already known techniques put in practice for the VAE.

	- A better motivation that the Dirichlet VAE gives good results should be given at the introduction.
",7
"This paper is clearly written and identifies an important point that exploration is dangerous in the autonomous driving domain. My key objection to this paper is that, even though the method is intended to deal with problems where exploration is dangerous and therefore should not be done, the method relies on negative examples, which are presumably dangerous. If simulations are used to generate negative examples and those are used, then the benefit of the presented method over standard reinforcement learning goes away.

I have several questions/comments/suggestions about the paper:

1. Can one perhaps present only mildly bad examples (e.g., mild swerving) to reinforcement learning in a way where the algorithm can understand that significant swerving, like what is shown in figure 2, is even worse?
2. The backseat driver feedback described seems to granular. I think that, to be realistic, the algorithm should allow for feedback that is less precise (e.g., turn further, turn the other way), without requiring information on proportions.
3. Please add an architecture diagram.
4. In figure 4, what is the difference between the first and fourth items? They have exactly the same description in the legend.
5. The experiments are not convincing. They lead one to conclude that negative examples are beneficial, which is good, but not surprising. Because negative examples are generated, a comparison with regular reinforcement learning should be done.",3
"Despite many high profile successes in research, DeepRL is still not widely used in applications. One reason for this is that current methods typically assume the agent can learn by exploring many states and actions, however, in many real world tasks, such as driving used here, poor actions can be dangerous. Therefore, methods that can provides the flexible benefits of RL while avoiding this are of significant interest, one promising general ideas pursued for this has been to use human demonstrations.

A number of approaches to Inverse RL have been studied, but many make the assumption that the demonstrations are all examples of optimal behavior. This can be challenging if, for example, some examples contain suboptimal behavior, and it also means that the agent does not get to observe non-optimal trajectories and how to correct for them; the resulting policy often performs poorly due to the distributional shift between the demonstration trajectories and the trajectories induced by the learned policy.

This work attempts to correct for these problems by labeling the demonstration actions between $[-1, 1]$ indicating how good or bad the demonstration actions are. This introduces a challenge for learning, since good actions can be copied, but a bad action is more ambiguous: it does not necessarily imply the action are far away from the bad action is a good action.

One view of this work is that they introducing 3 losses for behavior cloning with weighted labels: A weighted (based on the label) L2 loss, an exponential loss and directly fitting the loss and searching over a discrete set of actions to find the highest estimate weighting. Note the current equation for $Loss_{FNET}$ doesn't make sense because it simply minimizing the output of the network, from the text it should be something like $(f - \hat{\theta})^2$?

The text discusses why rescaling the negative examples may be beneficial, but as far as I can tell, figure 4 you only consider $\alpha=\{0, 1\}$? Based on the text, why weren't intermediate values of $\alpha$ considered?

It could benefit from copy-editing, checking the equations and in some cases describing concepts more concisely using clear mathematical notation instead of wordy descriptions that are difficult to follow.

``Thus the assumption
that our training data is independent and identically distributed (i.i.d.) from the agent’s encountered
distribution goes out the window'' This is a misleading statement regarding the challenge of distributional shift in off-policy RL. The challenge is that state distribution between the behavior policy and the learned policy may be quite different, not just not iid. Even in on-policy RL the state visitation is certainly not usually iid.

``In the off-policy policy gradient RL framework, this issue is typically circumvented by changing the
objective function from an expectation of the learned policy’s value function over the learned policy
state-visitation distribution to an expectation of the learned policy’s value function over the behavior
(exploratory) state-visitation distribution (Degris et al., 2012). In the RL framework, this could be
dealt with by an approximation off-policy stochastic policy gradient that scales the stochastic policy
gradient by an importance sampling ratio (Silver et al., 2014) (Degris et al., 2012). ''. The importance sampling in Degris is not to correct for the objective being under the behavior state policy and DPG (Silver et al., 2014) specifically does not require importance sampling so it shouldn't be referenced here. This paragraph seems to be conflating two issues: the distributional shift between the behavior state distribution and the policy state distribution that can make off-policy learning unstable, and importance sampling to estimate outcome likelihoods using behavior experience.

This work is on a very important topic. However, in its current form it is not well-communicated. Additionally, the best performing method is not novel (as the author's state $\alpha=1$, the best performing setting, is essentially the same as COACH but with scalar labels). For these reasons reason, I think this work may be of limited interested.",4
"Summary: 

This paper proposes a method to get feedback from humans in an autonomous vehicle (AV). Labels are collected such that the human actually moves a steering wheel and depending on the steering wheel angle disagreement with the direction the vehicle is actually moving a feedback value is collected which is used to weight the scalar loss function used to learn from these demonstrations. 

Experiments on a simple driving simulator is presented. 

Comments: 

I think this paper is attempting to address an important problem in imitation learning that is encountered quite often in DAgger, AggreVate and variants where the expert feedback is provided on the state distribution induced by the learnt policy via a mixture policy. In DAgger (where the corrections are one-step as opposed to AggreVate where the expert takes over and shows the full demonstration to get Q(s,a)) it is difficult to actually provide good feedback especially when the expert demonstrations are not getting executed on the vehicle and hence hard for humans to ascertain what would be the actual effect of the actions they are recommending. In fact there is always a tendency to overcorrect which leads to instability in DAgger iterations. 

The paper proposes using a modified feedback algorithm on page 6 whose magnitude and sign is based on how much the correction signal is in agreement or disagreement with the current policy being executed on the vehicle. 

Unfortunately this paper is very confusingly written at the moment. I had to take multiple passes and still can't figure out many claims and discussions: 

- ""To the best of our knowledge, no research so far has focused on using any kind of human feedback in the context of AV control with learning from demonstration"" - This is not true. See: 

""Learning Monocular Reactive UAV Control in Cluttered Natural Environments, Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert"" who used DAgger for autonomous driving of a drone with human pilot feedback. 

- Lots of terms are introduced without definition or forward references. Example: \theta and \hat{\theta} are provided early-on are refered to on page 3 in the middle of the page but only defined at the end of the page in 3.1. 

- Lots of confusing statements have been made without clear discussion like ""...we could also view our problem as a contextual bandit, since the feedback for every action falls in the same range..."" This was a baffling statement since contextual bandit is a one-step RL problem where there is no credit assignment problem unlike sequential decision-making settings as being dealt with in this paper. Perhaps something deeper was meant but it was not clear at all from text. 

- The paper is strewn with typos, is really verbose and seems to be written in a rush. For example, ""Since we are off-policy the neural network cannot not influence the probability of seeing an example again, and this leads can lead to problems.""

- The experiments are very simple and it is not clear whether the images in figure 2 are the actual camera images used (which would be weird since they are from an overhead view which is not what human safety drivers would actually see) or hand-drawn illustrations.

",2
"General:
In general, this looks like a technical report rather than a research paper to me. Most parts of the paper are about the empirical analysis of adaptive algorithms and hyper-gradient methods. The contribution of the paper itself is not sufficient to be accepted.

Possible Improvements:
1. The study of such optimization problem should consider incorporating mathematics analysis with necessary proof. e.g. show the convergence rate under specific constraints. Even the paper is based on others' work, the author(s) could have extended their work by giving stronger theory analysis or experiment results.
2. Since this is an experimental-based paper, besides CIFAR10 and MNIST data sets, the result would be more convincing if the experiments were also done on ImageNet(probably should also try deeper neural networks).
3. The sensitivity study is interesting but the experiment results are not very meaningful to me. It would be better if the author(s) gave a more detailed analysis.
4. The paper could be more consistent. i.e. emphasize the contribution of your own work and be more logical. I might miss something, but I feel quite confused about what is the main idea after reading the paper. 

Conclusion:
I believe the paper has not reached the standard of ICLR. Although we need such paper to provide analysis towards existing methods, the paper itself is not strong enough.",3
"Clarity: Below average
- The introduction would be easier to follow if you named Baydin's approach and your own approach, because in the 2-4 bullet points you say ""this online scheme"", and ""the learning rate schedule"", without being perfectly clear what you are talking about
- The last sentence of the introduction is meant to clearly state your hypothesis, so I was expecting ""emphasize the value of *"", i.e. either adaptive or non-adaptive methods, rather than just general 'tuning', which is self-apparently important.

Quality: Below average
This is a purely empirical study that does not go too deep. It is not quite a review paper, but only compares previous methods.

Pros:
I especially appreciate the sensitivity analysis, ie Fig 6. If only all ML papers had something like this to suggest the difficulty of setting hyperparameters for their proposed methods.

Cons:
- You should use mathematics to describe what you are talking about with adaptive stepsize in Sec 2.1. ""these methods multiply the gradient with a matrix"". Just giving one equation would be extremely helpful.
- If I understand correctly, you are interpreting the inverse-Hessian as used in Newton's method and other non-diagonal 'gradient conditioners' as types of stepsize. This is definitely interesting, but again it would be very simple to see what you are saying with an equation instead of starting with the phrase ""stepsize"" which is generally understood to be a scalar multiple on the gradient.
- I'm surprised you jump right into experiements after your background settings. It's apparent that this paper fundamentally relies on the Wilson (2017) hypergradient paper. Your paper should be more self-contained: 'hypergradient' is not even defined in this paper, is it?...

Especially:
How do you know that if you change the model architecture, data, and loss, that a similar result will occur? I imagine that it heavily relies on the data and model-- in other words, that the sensitivity is dependent on ""how an algorithm reacts to a certain data/loss/model landscape"". I'm trying to say that I'm not convinced these results generalize to any other situation than the one presented here (so does it really say anything about the different stepsize selection rules?)

Random side note:
Since your appendix is only a few lines, you could consider succinctly listing learning rates with set notation, for example {1e-n,5e-n : -5<n<1}.",3
"The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). 

Though it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. 
  
On page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?

The URL in References looks out of bound. 


",4
"The paper proposes using structured matrices, specifically circulant and diagonal matrices, to speed up computation and reduce memory requirements in NNs. The idea has been previously explored by a number of papers, as described in the introduction and related work.  The main contribution of the paper is to do some theoretical analysis, which is interesting but of uncertain impact.

The experiments compare performance against DeepBagOf`Fframes (DBOF) and MixturesOfExperts (MOE). However, there are other algorithms that are both more competitive and more closely related. I would like to see head-to-head comparisons with tensor-based algorithms such as Novikov et al: https://papers.nips.cc/paper/5787-tensorizing-neural-networks, which achieves huge compression ratios (~200 000x), and other linear-algebra based approaches. 

AFTER READING REBUTTAL
I've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. 

I strongly agree with authors when they state: ""We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings"".  I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.",6
"The experiments in the paper are similar to those explored in previous work! The main contribution claimed in the paper is the theoretical formulation for compact design of neural networks using circulant matrices instead of fully connected matrices. 

I do not think the claim is sufficiently justified by the theoretical results provided. 

Earlier result already shows how any matrix fully connected matrices can be approximated by 2n-1 circulant matrices. As the authors themselves point out, this theoretical result does not necessarily imply reduction in number of parameters since the for a depth l network, the equivalent diagonal-circulant-ReLU network will now require (2n-1)l depth, or 2n(2n-1)l parameters. 

The main results (Proposition 3, 4) show that if the fully connected networks of depth l network are parameterized by (approximately) rank k matrices, then the resultant depth of diagonal-circulant network required to approximate the original network is (4k+1)l, which results in a total of 8n(4k+1)l parameters. Similar to the case of full rank fully connected networks (proposition 2), this result does not necessarily indicate a compression of number of parameters either. In particular, if fully connected networks are indeed rank k, then we only need nkl parameters parameters to represent the matrix, which is lower than the number of parameters required by the diagonal-circulant network. 

So I do not see how the result can be seen as a justification for using diagonal-circulant networks as compact representations. 

Writing:
Theorem 1: The statement about approximability with B_1B_2…B_{2n-1} is independent of p and S. 
Proposition 3: The expression for depth should be \sum_{i=1}^l (4k_i+1)  — sum should go from i=1 to l and there should be no multiplicative factor l 

Other non-critical comments: Multiplication by circulant matrices amounts to circular convolution with full dimensional kernel. In this sense, replacing a fully connected layers by circulant matrices is similar to replacing it with convolutional layers.  May be this connection can be explicitly stated in the paper.

",4
"In this paper, the authors prove that bounded width diagonal-circulant ReLU networks (I will call them DC-ReLU henceforth) are universal approximators (this was shown previously without the bounded width condition). They also show that bounded width and small depth DC-ReLUs can approximate deep ReLU nets with row rank parameters matrices. This explains the observed success of such networks. The authors also provide experiments to demonstrate the compression one can achieve without sacrificing accuracy.

Pros: The authors provide strong approximation results that explain the observed success of DC-ReLUs.

Cons: Too many grammatical errors (mainly improper pluralization of verbs and punctuation errors), typos, stylistic inconsistencies seriously affect the readability of the paper. The authors should pay more attention to these.",7
"This paper starts with the bold aim of extracting Montague's universal grammar from multiple languages. In order to do so, the authors train multiple language models where each LM is explicitly factorized into language-specific and language-independent representations. The authors then apply the GAN framework to the language-independent parts to enforce all languages to share the same latent space. The claim here is that the language independent parameters capture the essence of universal grammar. The authors show that their framework enables effective zero-shot learning of tasks over new languages (for example sentiment classifier learned on top of English data generalize to Chinese when trained on the universal grammar embeddings). 

The paper is overall well written and the experimental results are convincing.

The gripe, however, I have is that this paper makes the claims that go too far without evaluating them. It is entirely sufficient to claim that you're trying to learn language agnostic parameters/embeddings --  I'd be happy with that. But the paper goes further and claims to be learning a form of universal grammar. To justify this claims, it is not sufficient to show in the experiments that the new representations do better at sentiment and NLI. The authors must show that this captures the ""innate"" language learning abilities akin to human babies. While the paper aims to do some analysis in the discussion section, it is not unsatisfactory. As the paper says in the discussion section ""From a machine learning perspective, we’re interested in extracting informative features and not necessarily a completely grammatical language model. That being said it is of interest to what extent language models capture grammar and furthermore the extent to which models trained toward the universal grammar objective learn grammar.""

The problem is that simply comparing LM perplexities is not a solid test of whether this model has learned some form of universal grammar. First, this paper does not define a clear falsifiable hypothesis on the proof of learning universal grammar. One example of testing for learning grammar can be: does this model learn basic syntactic rules of a new language (e.g. as the authors suggested -- head-first or head-final syntax, or rules of conjugation)  with a small amount of data after being training a universal representation with n languages? There have been a series of recent papers on checking if Language Models have appropriately learned syntax. See e.g. Tal Linzen's work https://arxiv.org/pdf/1809.04179.pdf. Just to be clear I am not suggesting citing works in unpublished places but potentially using some of the tests suggested in these papers.

In conclusion, I think this work is useful but I also think it makes really grandiloquent claims without verifying them. That to me is a dangerous precedent.",5
"This paper proposes the idea of language agnostic representation which could potentially provide zero shot solution if the downstream task is trained using another language. The solution uses linguistic features from every sentence, trains language model for multiple languages simultaneously, and matches distribution by using Wasserstein distance measure. 

pros:
The motivation of this paper is clear. 
The method proposed looks reasonable. 
The experimental results also make sense. 

cons:

Key technical parts are not clear. The description of the training method is vague, e.g., the author(s) mentioned 'we utilized dropout and locked dropout where appropriate'. What does 'appropriate' mean? The training procedure was described in only few sentences. For example, it is not clear to me if a batch is fully random, or a batch consists of same number of sentences from each language, or a batch consists of same number of sentences from two languages, and how you train the WGAN. It is a bit surprising to me that different lambda gives similar performance. 

The writing of the paper is not clear. Here are some of the reasons:
1. The last paragraph in Section 2 does not fit into 'related work' section at all, instead, it is almost a repetition of the last paragraph in Section 1. 
2. The notations in Section 3 are very inconsistent. Just to name a few: the input dimension of function $e_j$ defined in the last paragraph in page 2 is not consistent with (1); the '$\circle$' operation in (1) is not explained (although I can guess what it means); the $j_alpha, j_beta$ are not consistent with the $j^{th}$ language; in the last equation in page 3, the summation should be from 1 to m (instead of 0 to m) if there are m languages, and the superscript in $w$ is not defined. 
3. Key references missing, for example: there is no reference when deriving (4) using the 'Kantarovich-Rubenstein' duality. 
4. The organization for section 4 is not clear. The first sentence is quite confusing, and the content is a mixture of architecture design, training details, and experimental settings. Instead, one should separate these contents and address each of them. 
5. At the beginning of section 5.1, the hypothesis in the sentence 'to test this hypothesis' actually refers to the last paragraph in section 4. Figure 4 should be referred to in the last paragraph in section 5. 'english', 'german', 'chinese' should be 'English', 'German', 'Chinese'. 
",4
"This paper introduced a GAN-based method to learn language universal representations without parallel data. The model architecture is analogous to an autoencoder. The encoder is a compound of language-universal mapper plus a language-specific LSTM. For decoding, another language-universal module first map language-universal representation back to language-specific embedding space, then another LSTM decoder generates the original sentence. The authors used GAN to encourage intermediate representation to be language-universal. The authors tested the proposed method on zero-shot semantic analysis and NLI tasks and showed nice results.

Overall the proposed method is novel and nice, and experiment results are good. On both tasks the proposed method performs better than NMT methods on target languages while still achieving competitive performance on source languages. The paper is also clearly written and could be useful for future research on multilingual transfer.

My main complaint is around Figure 5, Table 3, and the corresponding analysis.
1. In Figure 5, does it make more sense to show the perplexity of a standard LM. That is, train 7 independent LMs and report averaged perplexity. My concern is that, even with \lambda=0.0, the model still have modules u and h that are shared across languages, and therefore I'm not sure if it implies ""representative power of UG-WGAN grows as we increase the number of languages"". It could be that the language-universal impose more constraints to model all languages, so the two variation (\lambda=0.0 or 0.1) come closer to each other.

2. In Figure 3, the perplexity difference is huge when number of languages is 2. In Table 3, however, the authors show no fundamental differences between the English and Spanish language models. I feel the two arguments contradict to each other. Is it because of the language pairs are different? The authors should provide more explanation on that.

Minor:
1. Equation 1 and 2 in page 2. Are they both compound functions? Why the first one use \circ and the second one use parenthesis?",6
"Summary: 
The paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). The paper suggests that the standard practice of training the student to imitate the behavior of the teacher *on the same training data* that the teacher was trained on is problematic and can lead to overfitting. Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression.
Experimental results show that this idea seems to improve the performance of convnet student models on CIFAR-10 classification and random forest student models on tabular data from UCI and Kaggle.
Another contribution of the paper is to propose an evaluation metric for generative model, called the compression score. This score evaluates the quality of generated data by using it in model compression: “good” synthetic data results in a smaller gap in performance between student and teacher models.

Strengths:
-	The paper sheds a light on an interesting aspect in model compression. The idea of teaching a student model to imitate behavior of the teacher model on *new* data is interesting. In fact, it emphasizes the fact that we are mostly interested in imitating the teacher model’s capability of generalizing to new examples rather than overfitting to training examples.
-	Experiments show that for several settings (model class, architecture and datasets), using synthetic data by a cGAN can be useful in reducing the gap between student and teacher models. 
-	The paper is clearly written and easy to follow.

Weaknesses:
-	The claim that reusing the same training data used for training the teacher model in model compression can lead to overfitting of student model is not very obvious and needs more experimental evidence in my opinion. One way to test this is to use some unseen real data (e.g. validation or a held-out part of training data) for model compression, and showing that it can indeed help in improving student performance.
-	The claim that cGAN can generate “infinite” amount of realistic data is too strong. In light of some well-known problems of GANs such as mode collapse [2] and low-support learned distributions [1], this assumption seems unrealistic. In fact, it is not too obvious how synthetic data by a generative model learned on *same training data as the teacher* can provide any additional information to real data.
-	While the idea of the proposed evaluation metric seems interesting, I believe it is not very practical, because: 
1.	It is computationally intensive (requires training a model from scratch on fake data)
2.	It relies on performance of the compression mechanism, which might also have some idiosyncrasies that prefer some features in synthetic data which do not necessarily correspond to quality of generated data.

Questions/Suggestions:
-	In addition to using held-out real data for model compression as suggested above, a useful baseline could be using standard data-augmentation techniques in model compression.
-	What would happen if a student model is very small and cannot possibly overfit training data? Would using synthetic data be still useful there?
-	I am actually confused about a claim made when presenting compression score in Section 5. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data. I would appreciate if authors can clarify this point.

Overall recommendation: 
While the paper presents an interesting problem in model compression, I’m leaning towards rejecting the paper because of the weaknesses mentioned above. That being said, I am happy to reconsider my decision if there is any misunderstanding on my part.

References:
[1] Arora, Sanjeev, and Yi Zhang. ""Do GANs actually learn the distribution? an empirical study."" arXiv preprint arXiv:1706.08224 (2017).
[2] Goodfellow, Ian. ""NIPS 2016 tutorial: Generative adversarial networks."" arXiv preprint arXiv:1701.00160 (2016).


-----

Updated score and posted a comment to author response.
",6
"This paper focused on training a small network with a pre-trained large network in a student-teacher strategy, which also known as knowledge distillation. The authors proposed to use a separately trained GAN network to generate synthetic data for the student-teacher training. 

The proposed method is rather straightforward. The experimental results look good, GAN generated data help train a better performed student in knowledge distillation. However, I have concerns about both motivations and experiments. 

1. The benefits of GAN for generating synthetic data to assist supervised training are still mysterious, especially when GAN is separately trained on the same dataset without more information introduced. I would love the authors to clarify why GAN generated data are particularly effective for knowledge distillation. Does GAN generated data also help standard supervised training? I would expect following experiments: use mixture of training and GAN data to train teacher and student network by standard supervised loss without knowledge distillation, and compare with values in table 1. 

2. The performance of the proposed method depends on the quality of GAN. To help me further understand the quality of GAN, I hope to see the following experiments to compare with scores in table 1.
i) The accuracy of supervised trained teach and student on GAN generated image. 
ii) The classification accuracy on test data by the classifier trained in AC-GAN. 

3. I would like the authors to clarify their experiments to convince me the comparison in table 1 is fair.
i) How many data and iterations in total are used for standard training and knowledge distillation with/without GAN data? Does the better performance come from synthetic data, or come from exploiting more data and training for longer time?
ii) Related to i), In figure 1 (a), how many data and iteration for each epoch? It would help if the standard supervised training curve for student can be provided. 
iii) The experiments have a lot of hyperparameters, for example, the weight \alpha, the temperature T,  optimizer, the learning rate, learning rate decay, the probability p_fake. These hyperparameters are different for each experimental setting. How are they chosen?

4. Please explain conceptually why the proposed compression score is better than inception score. 

5. The paper is missing a conclusion section. The following papers introduce adversarial training for knowledge distillation. Though it is not necessary to compare with them in experiments as they are complicated method and the usage of GAN is different from this paper, I think it is still worth to mention them in related work. 
Wang et al. 2018 Adversarial Learning of Portable Student Networks
Xu et al. 2018 Training Student Networks for Acceleration with Conditional Adversarial Networks 

================ after rebuttal ====================
I appreciate the authors' response and slightly raise the score. It is a good rebuttal and it has clarified several things. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. The explanation reminds me of the mixup data augmentation paper from last year. I also like the additional experiments which clearly show the benefits of GAN data augmentation. 

However, I still think it is borderline for several reasons.
1. As the other reviewer has pointed out, CIFAR-10 is a bit too toy and some models (like LeNet for Figure 2) cannot really show the advantage of the method. I would suggest try ImageNet, and use more recent networks for ablation study.  
2. As the other reviewer has pointed out, the compression ratio can be impractical. The compression ratio  depends on student-teacher training, which can take a relatively long time. 
3. I would suggest the following experiments that may strengthen the paper. I would consider these as a plus, not necessarily related to my current evaluation. 
i) Try not use GAN, but use mixup (linear interpolation of samples) as data augmentation, and go through the student-teacher training.
ii) Try evaluate the effect of generator structure for data augmentation. Does the generator have to be very strong? The GAN generated results did not improve supervised learning may suggest the generator is not necessarily to be strong.  
",5
"I like this paper. What the authors have done is of high quality. It is well written and clear. However, quite a lot of experiments are necessary to make this paper publishable in my opinion.

Strenghts:
- The idea to use a GAN for model compression is something that many must have considered. It is good to see that someone has actually tried it and it works well.
- I think the compression score is definitely an interesting idea on how to compare GANs that can be of practical use in the future.
- The experimental results, which are currently in the paper, largely support what the authors are saying.

Weaknesses:
- The authors don't compare how good this technique is in comparison to simple data augmentation. My suspicion is that the difference will be small. I realise, however, that the advantage of this method over data augmentation is that it is harder to do it for tabular data, for which the proposed method works well. Having said that, models for tabular data are usually quite simple in comparison to convnets, so compressing them would have less impact.
- The experiments on image data are done with CIFAR-10, which as of 2018 is kind of a toy data set. Moreover, I think the authors should try to push both the baselines and their technique much harder with hyperparameter tuning to understand what is the real benefit of what they are proposing. I suspect there is a lot of slack there. For comparison, Urban et al. [1] trained a two-layer fully connected network to 74% accuracy on CIFAR-10 using model compression.

[1] Urban et al. Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? 2016.",5
"This paper targets at developing new DFA method to replace BP for neural network model optimization, in order to speed up the training process. The paper is generally written clearly and relatively easy to follow. 

My main concern is about significance of the contribution of this paper.

1. the novelty is limited. This paper only simply combines two well-known approach BP and DFA together. 

2. performance contribution seems not significant from the proposed approach. In the implementation, the authors only apply their approach to optimize a few top layers. A majority of the layers in the NN model are still optimized via BP. 

3. the authors should provide more evaluations on different NN backbones and datasets, to make the experiments stronger and more convincing.",4
"The paper propses to use a combination of Direct Feedback Allignment (DFA) and BackPropagation (BP) to improve upon standard back propagation.
To understand what is done, consider the following: Feedback Alignment is +- equal to back propagation when using random but fixed weights in the backwards pass. Direct feedback alignment uses random backprojections directly to the layer of interest. 
The advantage of DFA is that It bypasses the normal computational graph. The advantage of this is that if compute is infinite, all of these updates can be computed in parallel instead of pipelining them as is done in standard BP.

In the current paper, the use of DFA for dense layers and BP for conv layers which is named CDFA is proposed.
In addition the paper also proposes a binarized version of BDFA to limit memory consumption and communication. It is claimed that the proposed techniques improve upon standard back propagation.

Overall, the paper is easy to understand, but I lean towards rejecting this paper because I am not convinced by the experimental evidence. As outlined below, the key issue is that the baseline appears to be weak. Additionally, the main limitation of the proposed approach can only benefit a very limited set of architectures. 

Positive points:
---------------------
The authors did an excellent job of introducing BP, FA and DFA in the paper. This makes the core concepts and ideas accessable without having to delve through prior work.


The own contributions and the key idea is easy to understand. 

Limitations and possible improvements
-------------------------------------------------------
A core limitation is that recent networks do not have a combination of dense layers and convolutional layers. In many cases the networks are fully convolutional, this limits the applicability of the proposed combination of DFA and BP. The use of additional networks would benefit the paper. Currently only VGG 16 on Cifar 10 is used. Also, the data augmentation strategy is not discussed. Of course, it would be nice if additional datasets could be included as well, but this of course depends on the computational resources the authors have available. 


The key issue to me is that performance improvements for CIFAR are reported, but I fear that the baseline accuracy for VGG16 might be a bit low. If I memory serves me well, it should be able to achieve around 90% at least on CIFAR 10 using VGG style networks. I did a quick search and found http://torch.ch/blog/2015/07/30/cifar.html corroborating this but I did not verify this directly. 


Related to the previous point, since this is an empirical paper, describing the hyper-parameter optimizations and final settings in detail  can convince the reader that the study is exectued correctly. Much of the information is missing now.


Similarly, I have trouble understanding section 4.1 and section 4.2 since I do not know the exact details of the experiments. This can be fixed easily however.


Provide complexity estimates of the potential speedup or provide actual timing information. (Although this might not be that meaningful without much additional work given that gpu kernels are often heavily optimized).


Last year there was a submission to ICLR about fixing the final output layer and only learning the convolutional layers. If we consider that random projections work remarkably well and can be considered approximations of kernels, it could be interesting to add a baseline where the fully connected layers are fixed and only the convolutional layers are trained. The error signal can be propagated using standard BP, FA or DFA methods but it would shed light on whether learning in the higher layers is actually needed or BP in the conv layers is sufficient.

Minor possible improvements
------------------------------------------
Finally, I would strongly suggest that the authors perform some additional proofreading. There are quite a few strange formulations and spelling mistakes. That being said, it did not prevent me from understanding the manuscript so this remark DID NOT factor into my judgement.

In addition to remark above, I would suggest removing the second paragraph from the introduction. It feels out of place to me, and the vanishing gradient effects are not discussed in the remainder of the manuscript.


The list of possible optimizers before the selection for SGD+Momentum is not needed. Simply stating that SGD with momentum is used should be sufficient. 


“Training from scratch” instead of “Training from the scratch”
",4
"This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.

Originality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. 

The experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
Therefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.

Minor typos:
In Equation 1, the letter i is overloaded.",5
"This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE).  By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy.

Pros:
- The paper carefully constructs a method to estimate the mutual information between high dimensional variables and address the infinite mutual information issue by adding noise to the output. This is novel and theoretical sounding. 
- The paper connects the IB theory of DNN with weight decay, which is a novel founding.

Cons:
- The paper claims no literature has been doing IB functional on a layer-by-layer objective, however, see [1, 2] for the total correlation explanation work which is closely related to IB functional and they have also verified the effectiveness of layer-by-layer objective. 
- The scope of the paper is unclear. It seems that the paper is trying to convince two things to the readers: 1) The compression phase in DNN does exist 2) Layer-wise training helps to improve the accuracy. Although these two things are close related to each other (because they all requires to estimate the IB functional), it seems that neither these two conclusions are convincing. First, the compression phase is achieved only through weight decay; without weight decay, as shown in the paper, the compression phase is gone. Does that verify the incorrectness of IB theory of deep nets? Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. But if we really want to show the `effectiveness` of `layer-wise` training, one should compare the `layer-wise` training with `end-to-end` training while keeping the objective itself fixed. Otherwise, it is really difficult to draw conclusions about why the accuracy is improving, it is because of the objective changes or because of the `layer-wise` training.
- How does the beta (in IB objective) selected in the experiments for comparison? Do you use a validation dataset, and what is the final beta? If the paper fine-tune beta on the validation dateset, then the comparison of ""IB functional, only the first term"" and ""IB functional"" is unfair. 

[1]  Ver Steeg et al. Maximally Informative Hierarchical Representations of High-Dimensional Data. AISTATS 2015
[2]  Gao et al. Auto-Encoding Total Correlation Explanation. Arxiv 1802.05822.

[update] After carefully reading the response (also from other reviewers), I decide not to change my rating.",5
"
This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent ""compression"" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). As a consequence, no p.d.f. exists (with respect to the Lebesgue measure of R^{p+q}). Thus, defining mutual information is cumbersome. The paper attempts to overcome this problem by using a noisy version of the latent compression, i.e. L' = L + \epsilon, which can be seen as an ""ad hoc"" fix of this problem. Not too surprising, this additive noise works as a ridge-type (or weight-decay) regularizer, just as a Gaussian prior in regression.

On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Such results are certainly interesting, both from a theoretical and from a practical point of view. On the other hand, I honestly think that on the conceptual side, this work does not make that many really interesting contributions. The observation that additive noise works as a weight-decay regularizer is in my opinion almost trivial, and any claims about experimental results ""validating(!) the IB theory"" seem to contain some degree of over-selling. In summary, I think that this is a paper that certainly contains some interesting ideas, but on the other hand I am not fully convinced about the significance and relevance of the findings.       ",5
"While overall the writing quality of the paper is high, the paper itself is a strong rejection.  I believe the analysis of the paper is at points flawed, and the experiments are minimal.  

This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017.  Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone.  To combat this they use the noise-regularized mutual information estimator (I(X; L+eps)).  To actually estimate the mutual information the authors use the MINE estimator of Belghazi (2018).  Here they suggest using the neural network itself as a structural element in the form of the discriminator to take advantage of the specific circumstances in this case.  Doing this ensured that their estimator diverged in the zero noise limit as expected.  From here they show some experimental results of the effect of their objective on an MNIST / CIFAR10 classification task.

This paper fits into what is an increasingly large discussion in the literature, surrounding Information Bottleneck.  The paper itself does a very good job of citing recent relevant work.  Technically however I take issue with the framing of previous work in the last paragraph of the ""Deep neural nets"" subsection of Section 2.  Technically Achille & Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a ""single bottleneck layer"" as stated in the paper.  More generally at the end of that paragraph it is implied that the single bottleneck layer scheme ""deviates from the original theory"".  This is a misleading characterization of the original information bottleneck (Tishby et al 1999) in which there was a single random variable, a representation of the data (Z) satisfying the Markov conditions Z <- X -> Y.   I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017).  In general the paper does a poor job of distinguishing between the Shwartz-Ziv & Tishby paper and the rest, but this is a distinction that should be maintained.  The original information bottleneck may and has demonstrated utility regardless of whether the information bottleneck generally can help explain why ordinary deterministic feed forward networks trained with cross entropy and sgd generalize well.  

This also raises one of the main problems with the current work. The title, abstract and especially the conclusion (""This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning"") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks.  Here the authors modify the ordinary cross entropy objective, and so their networks are necessarily not ordinary and so they cannot claim they have helped clarify our understanding of the vast majority of neural networks currently being trained.  Again, this is distinct and should be kept distinct from the utility of their proposed objective, itself inspired by the information bottleneck.  Here too the paper falls flat.  If instead of attempting to comment on networks as they are designed today they aim to proposed a new information bottleneck inspired objective they really ought to directly compare other attempts along those lines (such as the ones they themselves cite  Alemi et al. 2018, Kolchinsky et al. 2017, Chalk et al. 2016, Achile & Soatto 2018, Belghazi et al. 2018) but there are no comparative studies.

The experiments are extremely lacking, not only are any of their cited alternatives compared, they don't compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.  Their reported numbers are not very impressive with their top MNIST number at 98.09 and their baseline at 97.73. These numbers are worse than many of the papers they themselves cite.  Only a single comparative results for both a limited training set run and the full one are shown, as well as only a single choice of beta.  The CIFAR10 numbers are not very good either.  There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.

Technically a deterministic function need not have infinite mutual information, if it is non-invertible, i.e. the sign function, or just floating point discretization. 

Their own results in Figure 2 and the main body of the text highlight that the authors believe the true mutual information between the activations of the intermediate layers and the input is infinite.  If the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?

Just plugging in the Discriminator for the objective (equation (7)) is flawed.  The discriminator, if optimal would learn to approximate the density ratio 1 + log p(x,y)/(p(x) p(y)) .   ( see f-GAN, Norowin et al. 2016).  How does this justify using the individual elements of the discriminator in the functional form of the IB objective?  

At the bottom of page 6 they rightfully say that mutual information is invariant to reparameterizations, but their noise regularized mutual information estimator is not (by their own reference (Saxe et al 2017).

The discussion at the center of page 8 is confusing.   They claim that Figure 5 (a) is more 'quantized' than (b) and ""has reduced entropy"".  I think it should be the other way.  More clusters should translate to a higher KL divergence, or higher entropy.  If you need only identify which cluster an activation is in, that should require log K nats where K is the number of clusters.  (a) shows more clusters and so seems like it should cost more and have a higher entropy not a lower one.

Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well.",2
"This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. The regularisation enforces smoothness on a graph built on the different features at different layers of the NN system. The proposed ideas are quite interesting, and integrates nicely into NN architectures. 

A few paths for improvements:

- the 'optimal' choice of the power of the Laplacian, in 3.5, is eluded
- the figures are not presented ideally, nor in a very readable form - for example, their are 90-degree rotated compared to classical presentations, and the plots are hardly readable
- the might exist a tradeoff between robustness, and performance (accuracy), that seem to be explaining the proposed results (see Fawzi - Machine Learning 2018, for example)
- in 4.2, what is a mean case of adversarial noise? Also, it would be good to see the effect of the regularizer of both the 'original' network, and on the network trained with data augmentation. It is not clear which one is considered here, but it would be interesting to study both, actually. 
- the second paragraph of the conclusion (transfer of perturbations) opens interesting perspective, but the problem might not be as trivial as the authors seem to hint in the text. 

Overall, very interesting and nice work, which might be better positioned (especially in terms of experiments) wrt to other recent methods that propose to improve robustness in NNs.",9
"The paper proposes to use a regularization which preserves nearest-neighbor smoothness from layer to layer. The approach is based on controlling the extent to which examples from different classes are separated from one layer to the next, in deep neural networks. The criterion computes the smoothness of the label vectors (one-hot encodings of class labels) along the nearest-neighbor graph constructed from the euclidian distances on a given layer's activations. From an algorithmic perspective, the regularization is applied by considering distances graphs on minibatches. Experiments on CIFAR-10 show that the method improves the robustness of the neural networks to different types of perturbations (perturbations of the input, aka adversarial examples, and quantization of the network weights/dropout0.

The main contribution of the article is to apply concepts of graph regularization to the robustness of neural networks. The experimental evaluation is solid but the significance is unclear (error bars have rather large intersections), and there is a single dataset.

While the overall concept of graph regularization is appealing, the exact relationship between the proposed regularization and robustness to adversarial examples is unclear. There does not seem to be any proof that adersarial examples are supposed to be classified better by keeping the smoothness of class indicators similar from layer to layer. Section 3.4 seem to motivate the use of the smoothness from the perspective of preventing overfitting. However, I'm not sure how adversarial examples and the other forms of perturbations considered in the experiments (e.g., weight quantization) are related to overfitting.

strengths:
- practical proposal to use graph regularization for neural network regularization
- the proposal to construct graphs based on the current batch makes sense from an algorithmic point of view


cons: experimental results are a bit weak -- the most significant results seem to be obtained for ""implementation robustness"", but it is unclear why the proposed approach should be particularly good for this setting since the theoretical motivation is to prevent overfitting. The results vs Parseval regularization and the indications that the metohd works well with Parseval regularization is a plus, but the differences on adversarial examples are tiny.

other questions/comments:
- how much is lost by constructing subgraphs on minibatches only?
- are there experiments (e.g., on smaller datasets) that would show that the proposed method indeed regularizes and prevents overfitting as motivated in Section 3.4?


",5
"To improve the robustness of neural networks under various conditions, this paper proposes a new regularizer defined on the graph of the training examples, which penalizes the large similarities between representations belonging to different classes, thus increase the stability of the transformations defined by each layer of the network.

The paper is overall well written, and the idea involving the Laplacian of the similarity graph is interesting. I have reviewed this paper before. Compared to the previous version, this paper made a good improvement in its experimental results, by adding two different robustness settings in section 4.1 and section 4.3, and also include DeepFool as a strong attack method for testing adversarial robustness.

However, my main concern about the paper is still about its significance. 
1. It is still not clear why would this regularization help robustness especially when considering adversarial examples. Example 1 seems not obvious to me why maintaining the boundary margin (rather than expanding or shrinking) is preferred. As stated in the second paragraph in section 3.4, “lower value of \sigma^\ell(s) are indicative of better separation between classes”, what is the reason of not directly penalizing this value, rather than requesting a “stability” property on this value? How is this stability related to the robustness? This would request a deeper analysis and more empirical proofs in the paper.
2. Experimental results still seem not convincing to me. On one hand, based on the reported result, I am not very convincing that the proposed method outperforms Parseval, especially when considering the inconsistent behaviour of “Proposed + Parseval”. On the other hand, for adversarial robustness, the authors should have compared to the method of adversarial training as well. Beyond that, the authors should also be careful of the gradient masking effect of the proposed method. I am not sure if there is some other obvious benchmarks should be included for the other two robustness settings.

Other comments:
1. Descriptions in the last 3 paragraphs in section 3.2 are not very clear. It always took me a while to figure it out every time I read the paper. It would be very helpful if the computation process and the discussions can be separated here, maybe with a pseudo-code for computing the regularizer. 
2. On the other hand, while the proposed regularizer can be interpreted in a perspective of the Laplacian of the similarity graph, the third part in Equation (4), that expresses the smoothness as the sum of similarities between different classes, seems more intuitive to me. Emphasizing in this interpretation may also help convey the message.",5
"COMMENTS RELATED TO REVISION:
The new analysis that has been added takes a step towards getting at the relationship to invariance. This is a positive. In general comments, the authors state as important contributions: 

""1) to the debate on the harmfulness/importance of selectively activated units in DNNs [1-3] by presenting concrete examples where selectivity is important for generalization, and 2) to the neuroscience community, where, although orientation selectivity has been extensively analyzed for these 60 years since [4], its functional importance in a natural environment has remained unanswered.""

On point 2, while this provides an example of an artificial network where orientation plays an important role, it's a stretch to generalize this to conclusions concerning functional importance in neuroscience.

One point 1, I agree that this paper takes steps in the right direction, but ultimately the overall conclusions still feel as though they are a natural and implied consequence of limiting orientation selectivity. It is noted that  orientation selectivity is ""not just a superficial byproduct of object recognition, but is causally indispensable for object recognition"". The idea that selectivity for oriented edges is indispensible for object recognition again is a conclusion that feels as though it would be shocking if this were not true. The notion of it being a superficial byproduct of object recognition presupposes that the purpose of the system is to recognize objects. Again, it would be surprising if there were vestigial features in the network that are learned, but play no important role - especially among early layers. 

I think the paper might be strengthened by re-working this second point to more strongly establish the causality the authors claim. 

ORIGINAL COMMENTS:
This paper presents interesting analysis and an ablation study on orientation selectivity in neural networks. This is analyzed with respect to generalization performance in decisions made. Overall, the paper is well written and interesting. However, I have a number of comments / concerns as described below:

Positives:
- The paper presents an in depth analysis of the role of orientation selectivity in neural hierarchies. This style of analysis is sorely lacking and fits the theme of learning representations
- The paper itself is well written and quite polished
- The authors have taken great care to rule out any possible confounds through experiments that are not identical to the main claims or objective (specifically the study of section 3.3)

To address:
- The only concern I have (and it is somewhat significant), is the notion of ""generalization"". This is a rather loaded term, and it is not clear on cursory inspection where this generalization comes from. Is it a function of invariance among higher layers to scene geometry? What are the fundamental underpinnings of these observations outside of correlation to orientation selectivity.
- It seems almost a tautology that removing orientation selectivity would impair performance. I would be more satisfied, and render a higher rating if I felt there was a ""smoking gun"" with respect to evidence. The conclusion is convincing, but the reasoning comes across as somewhat vague. With that said, it is also understood that this is a non-trivial matter to address and perhaps this paper is an important first step.",5
"*Update after discussion period*
I remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks.


Summary:
The authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network's ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network's generalization performance. 

Strengths:
+ Very straightforward and easy to follow 
+ Technically sound

Weaknesses:
- Feels trivial
- The claims seem to be too general

Conclusion:
I'm torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I'm not really sure what we learn from the paper.


Specific comments:

- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?

- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.

- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (""orientation selectivity [plays] a causally important role in object recognition"" – abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.

- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?
",4
"The paper presents a study on orientation selectivity in DNNs for image classification, arguing that this type of selectivity in the lower layers is crucial for generalization. This hypothesis is tested through an ablation study, which the authors interpret as a suggestion of the existence of a causal relation.

The authors tackle a very interesting problem that seems to have not received yet enough attention. The paper is quite well-written and clear, making it understandable also to non-experts. The only concern I would have is about the causal claims in Section 3.4. I’m not completely sure the ablation experiments are the correct way to “prove” causality, as opposed to somehow trying to intervene on the orientation selectivity directly. On the other hand, this seems complicated to prove and the approach proposed in the paper seems a pragmatic solution. I would possibly hedge slightly the causal claims.

From an outsider point of view, I think the paper provides an interesting contribution to the discussion on orientation selectivity. I particularly appreciated its clarity and reproducibility.
",7
"This paper proposes to use 8/4-bit approximation of activations to save the memory cost during gradient computation.  The proposed technique is simple and straightforward. On the other hand, the proposed method only saves up to a constant cost of the storage. With the constant factor (4x, 8x) depending on whether fp16 or fp32 is used during computation. Notably, there is a small but noticeable accuracy drop in the final trained model using this mechanism.

The alternative method, gradient checkpointing, can bring sublinear memory improvement, with at most 25%  compute overhead, with no loss of accuracy drop.

As a result, the proposed method has a limited use case. The author did mention, during the response that the method could be combined further with the sublinear checkpointing. However, since sublinear checkpointing already brings in significant savings, it is unclear whether low bit compression is necessary.

Given the limited technical novelty(can be described as oneliner ""store forward pass in 4/8 bit fixed point""),  limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), I think this is a boarder-line paper

On the positive side, the empirical result could still be interesting to some readers in the ICLR community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum.
",5
"In this paper the authors describe a quantization approach for activations of the neural network computation to improve the memory efficiency of neural network training and thus training efficiency of a single worker.

Prior work
-----------------
They compare the proposed method with other approaches involving the quantization of gradients or recomputation of activations in a sub-graph during back-propagation. However the literature survey lacks survey of more relevant quantization techniques e.g. [1]. 
[1] : Hubara, Itay, et al. ""Quantized neural networks: Training neural networks with low precision weights and activations."" The Journal of Machine Learning Research 18.1 (2017): 6869-6898.

experimental setup
-----------------------------
A more formal description of experimental setup assuming a general reader not familiar with the specific toolkits is advised. Any toolkit specific details like how the layer-wise forward & backward propagation is done via separate sess.run calls can be delegated to an appendix or footnote. Further given that the authors have chosen not to utilize the auto-diff functionality or other computation graph optimization features provided by Tensorflow; and given that they are even manually managing the memory allocation it is not clear why they are relying on this toolkit.  Irrespective of this choice, this section could be re-written to make the implementation description more accessible to a general reader and toolkit specific details could specified separately.

Reg. manual memory management - The authors specify how common buffers are being used for storing activations and gradients across layers. Given that typical neural network models need not be composed of homogenous layer types which can actually share the buffers it would be useful to add a detail on how much efficiency is achieved by reducing the memory allocation calls for the architectures being used in this paper.


results
-----------
Comparisons with prior work using other quantization methods to achieve memory efficiency is lacking.",5
"The authors detail a procedure to reduce the memory footprint of deep networks by quantization of the activations only on back propagation. While this scheme does not benefit from computational speedups of activation quantization on both passes (and indeed has a slight computational overhead), the authors demonstrate that for common convolutional architectures it nicely preserves the accuracy of computation by computing the forward pass at full accuracy and limiting propagation of errors in the backward pass. This is possible because the majority of errors are introduced in gradient calculation of the weights and not the inputs each layer. The authors also wisely perform quantization after batch normalization and use the known mean and variance of the activations to scale the quantization and reduce errors. They demonstrate very slight drops in performance accuracy for ResNets on Cifar10, Cifar100, and ImageNet with memory compression factors up to 8. They also point to natural future directions such as using vector quantization to better leverage the activation statistics. The paper is also very clearly written with appropriate references to the relevant literature. 

An area of improvement I could see for the paper would be to demonstrate the utility of the reduced memory footprint. Their motivation clearly outlines that reducing memory can allow for larger batch sizes and larger networks that can improve the performance of training, but the authors do not demonstrate an example of this principle. They do mention that they are able to train with a larger batch size on ImageNet without combining batches, but more quantitative evidence of improvements in wall clock time (for different batch sizes) or improvement in performance (for larger networks) would help support the arguments of the paper. Given that the authors are focusing on single device training, they don't have to necessarily improve the state of the art, but a relative comparison would be illustrative. Also, specific measurements of the change in memory footprint for real networks would be helpful. ",7
"In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.

Strength:

1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. 

2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.

Weakness:

1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.

2, Authors motive their work by saying in the abstract that “other graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters”. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.

3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.

4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?

5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.

Typos:

1, There are a few references missing (question mark) in the first and second paragraphs of section 2.

2, Methods in the experiment section are given without explicit reference, like GCPN.

3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. 

Overall, I do not think this paper is ready for publishing and it could be improved significantly.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Update:

Thanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. 

However, I still found some claims made by authors problematic. 
For example, it reads in the abstract that ""...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters..."". 
Clearly, Li et al. 2018b has a differentiable formulation which falls under your description.

Besides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. 
Also, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.
Directly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.

Therefore, I would like to keep my original rating. 
",4
"The paper proposes a conditional graph generation that directly optimizes the properties of the graph. The paper is very weak.
1. I think almost all probabilistic graph generative models are differentiable. If the  objective is differentiable function of real   
    variables, it is usually differentiable.

2.  The authors claim that existing works Simonovsky and Komodakis (2018) and Cao & Kipf (2018) are restricted to use small graphs with predefined maximum size. This work does not overcome the limitation of small graphs issue too.

3. The authors do not show any measure on validity, novelty or uniqueness which are now standard in literature.
   Also I do not find any comparison with molGAN paper which tackles a similar objective.

4. Could the authors show if the decoding process is permutation invariant? I am not really sure of that. I was trying to prove that thing formally, but I failed.


 ",3
"This paper proposed a variant of the graph variational autoencoder [1] to do generative modeling of graphs. The author introduced an additional conditional variable (e.g., property value) into the decoder. By backpropagating through the discriminator, the model is able to find the graph with desired property value. 

Overall the paper reads well and is easy to follow. The conditional generation of graphs seems also helpful regarding the empirical performance. However, there are several concerns regarding the paper:

1) The edge factorization-based modeling is not new. In fact [1] already uses the node embeddings to factorize the adjacency matrix. This paper models extra information including node tags and edge types, but these are not fundamental differences compared to [1].

2) The paper claims the method is ‘cheaper’ and ‘scalable’. Since essentially the computation cost is similar to [1] which requires at least O(n^2) to generate a graph with n nodes, I’m not super confident about the author’s claim. Though this can be parallelized, but the memory cost is still in this order of magnitude, which might be too much for a sparse graph. Also there’s no large graph generative modeling experiments available.

3) Continue with 2), the adjacency matrix of a large graph (e.g., graph with more than 1k nodes) doesn’t have to be low rank. So modeling with factorization (with typically ~256 embedding size) may not be suitable in this case. 

Some minor comments:
4) Regarding Eq (2), why the lstm is used, instead of some simple order invariant aggregation?

5) the paper needs more refinement. E.g., in the middle of page 2 there is a missing citation. 

[1]  Kipf & Welling, Variational Graph Auto-Encoders, https://arxiv.org/pdf/1611.07308.pdf
",5
"This paper is clearly written and in an interesting domain. The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks. However, there are some key issues with the paper that are not clear.

The first is regarding the structure of the paper. The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks. The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs. It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.  Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).

In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:
see paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf 

It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.

It is also unclear how the calculation of relative entropy ""D"" was performed in figure 3. Obtaining the normalized marginal density in a BM is very challenging due to the partition function.

The second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM. This is a very good point, however the paper do not compare or contrast with existing methods. For example, it is curious to see how denoising Auto encoders would perform. In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf

- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box. Defending against black box attacks is considerably easier than defending against white-box attacks.

In summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage  of the proposed MF BMs in increasing robustness against adversarial attacks.",4
"Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks. They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor. Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it. The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser. 

One question which is not addressed is the reason for only one RBM layer. In ""Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning"" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST. Attacking CRBMs is highly relevant and should be included as a baseline.

The only set of experiments are comparisons on first 500 MNIST test images. If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits. Authors should clarify the justification behind experimenting only on 'first 500 test images'. 

Furthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input. Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here. The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers. Although their network was resilient toward white box attacks they suffered from black box attacks. The boundary method on MNIST could be  weaker than a black box attack.  ",4
"The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks. The probabilities are approximated with variational autoencoders. During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.

This paper focuses on the second part, with a different model. Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification. This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.  This pre-trained model is then incorporated into the neural net for MNIST classification.  The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model. This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)

The authors compare to the work of Schott for one type of attack. It would be nice to see more detailed experiments as done in Schott.

Questions:
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.
4- Could you please add the found J_h's to the appendix. This architecture reminds me of the good old MRFs for image denoising. Could it be that what we are seeing is the attack being denoised?

I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott. 

Thanks in advance. I will re-adjust the review rating following your reply.



",6
"This paper presents a pretty cool idea for enabling ""adaptive"" kernels for CNNs which allow dramatic reduction in the size of models with moderate to large performance drops.  In at least one case, the training time is also significantly reduced (2x).

The best part about this paper is that the size of the models are much smaller; but the paper does offer any explanation of the value of this.  For example, even a 1% drop in accuracy can be unacceptable; but in some applications (like cell phones and IOT devices) model size is critical.  The authors' should add some wording to explain this value.

The ""adaptive""kernels the the authors talk about are really a new class of nonlinear kernels.  It would be very interesting to see a discussion of the class of functions these nonlinear kernels represent.  This kind of discussion would give the reader  motivation for the choice of function, ideas for how to improve in this class of functions, and insight into why it works.

The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.  It would be nice if the authors pointed to a git repository with their code an experiments.  More importantly, the results presented are quite meager.  If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.  And the analysis of the ""dynamic range"" of the algorithim is missing.  How do performance and model size trade off?  How were the number of layers and kernels chosen?  Was the 5x10x20x10 topology used for MNIST the only topology tried?  That would be very surprising.  What is the performance on all of the other topologies tried for the proposed algorithm?  Was crossvalidation used to select the topology?  If so, what was the methodology. 

Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g., how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear); and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word ""energy"" and the phrase ""degradation problem"".  All of these issues should be addressed in a future version of the paper.

Not sure why Eqns. 2 and 9 need any parentheses.  They should be removed.",5
"The paper develops a new 'convolution' operation. 
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.

p2-3, Section 3.1 - I found the equations impossible to read. What are the subscripts over?
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?

Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?

Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost. It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.

It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.",4
"The paper introduces adaptive kernels (that adapts its weights as a function of image content) to the framework of CNN. The benefit of adaptive kernels is the reduction of memory usage (at training and at the inference time) as well as training speedups (up to 2x). The kernels are evaluated on two datasets MNIST and CIFAR10

I like the idea of building models that are memory efficient at training and at evaluation time. However, the evaluation of the proposed adaptive kernels is rather limited. In order to improve the paper, the authors could take into consideration the following points:

1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?
3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance. How big is the generalization gap for the tested models when adaptive kernel is used?
4. How sensitive are the results to the number of adaptive kernels in the layers.
5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?
6. On CIFAR10 the results seem to be worse that other methods. However, it is important to note that the Adaptive Kernels CNN has way less parameters. It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016). It might be beneficial to include comparison to this approach in the experimental section. Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.
9. The ideas presented in the paper seems related to general concept of hypernetworks, where one network learns (or helps to learn) paramenters of the other network. It would be nice to position the ideas from the paper w.r.t. this line of research too.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).

I like the drawings, however, the font on the drawings is too small - making it hard to read.

Some typos:
1. the difficult to train the network
2. table 2: Dynamic -> Adaptive?

Overall, the paper presents interesting ideas with some degree of originality. I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.",4
"This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values.

weakness:
1. the paper is lack of novelty and the motivation is weak. I even can't find any contribution to OCR or object detection.

2. the paper is written badly so that I can't follow easily. In addition, the figures and tables are not always explained in the main body, which makes the experimental results confusing.

3. There are no titles in the figures and tables in this paper

4. the authors don't confirm the superiority of the proposed method to others.

minor comments
1. what's the meaning of Target mAP in the table?
2. It seems that Some figures are cropped from TensorBoard, with some extra shadows.",2
"This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.

Moreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. 

The authors claim “This work is the first to apply modern object detection deep learning approaches to document data” but there are previously published works. For example:

Tuggener, Lukas, et al. ""DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects."" ICPR 2018.
Pacha, Alexander, et al. ""Handwritten music object detection: Open issues and baseline results."" DAS 2018.

Actually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. 

Finally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand.",1
"Unfortunately, the work does not introduce new contributions, with the point of the paper provided in the introduction:
In our experiments, we show that best performing approaches currently available for object detection
on natural images can be used with success at OCR tasks.

The work is applying established object detection algorithms to OCR. While the work provides a thorough experimental section exploring trade offs in network hyper-parameters, the application of object detection to the OCR domain does not provide enough novelty to warrant publication.
",2
"The authors experiment with building an SSD-like object detection network for text detection in documents, by replacing the usual VGG or ResNet base architecture with a light weight model inspired by the original digits classification CNN from [LeCun et al 1999].

This paper is a pure technical report with no novel contribution: all the authors do is replace the ""body"" network in the well-known SSD architecture with a simpler model (taken from existing literature) and evaluate it on two synthetic benchmarks of their creation.
The idea of employing object detection CNNs for OCR is not novel either, as pointed out in the related works section.

Beside the absence of novelty, the paper also suffers from several other serious flaws:

1) One of the main motivations provided by the authors for this work is that existing ""classification [...] detection [...] or segmentation networks, cannot be applied directly, even with finetuning"".
However, no experimental results are reported to justify this claim.
In fact, in the experimental section the proposed network is not compared against any existing baseline.

2) The text has serious clarity and formatting issues, in particular:
- most tables and figures have no caption, and the few that have one are not numbered
- the text exceed both the 8 pages limit and the extended 10 pages limit allowed in the case of big figures
- the experimental section is very confusing, in particular the way the authors refer to the various network variants using long code names makes it really hard for the reader to follow the ablation studies
- given the absence of proper captions and numbering, it is quite hard to understand which table refers to which experiment
- most of the graphs seem to be in the form of low-resolution bitmaps, which are quite hard to read even on screen
- many entries in the References section are either missing the venue, or point to an arXiv link even when a proper conference / journal reference would be available

3) Some important details about the network are missing, in particular the authors do not mention how labels are assigned to the network outputs, and only give a vague indication about the losses being used.
Similarly, there's no mention about the use of NMS, which is also an important component of the two architectures (SSD and YOLO) that inspire this work.
Assigning labels and performing NMS are actually some of the most crucial components in the training of object proposal / object detection networks, often requiring numerous meta-parameters to be properly configured and tuned, as testified by the meticulous descriptions given in previous works (e.g. YOLO and Fast / Faster / Mask r-CNN).

4) The experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.
A lot of space is devoted to presenting a detailed ablation study which, in my opinion, doesn't contribute much to the overall paper and actually reads more like a report on meta-parameter tuning.
Finally, starting from Section 5.3.1 the text seems to be copy-pasted without a second read from some differently formatted document, as entire phrases or possibly tables / figures seems to be missing.

In conclusion, in my opinion this paper does not meet the conference's minimum quality standards and should definitely be rejected.",1
"Summary
The authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms.
The method consists of a) mining documents that refer to the same topic, b) extracting from these documents parallel sentences, c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level.

Novelty: the approach is novel.

Clarity: the paper is clearly written.

Empirical validation: The empirical validation is solid but limited. The authors could further strengthen it by testing on low-resource language pairs (En-Ro, En-Ur).
It would also be useful to report more stats about the retrieved sentences in tab. 1 (average length compared to ground truth, BLEU using as reference the translation of a SoA supervised MT method, etc.)

Questions
1) Sec. 3.2 is the least clear of the paper. The notation of eq. 7 is quite unclear because of the overloading (e.g., P refers to both the model and the empirical distribution).
I am also unclear about this constraint about matching the topic distribution: as far as I understood, the model gets only one gradient signal for the whole document. I find then surprising that the authors managed to get any significant improvement by adding this term.
Related to this term, how is it computed? Are documents translated on the fly as training proceeds? Could the authors provide more details?

2) Have the authors considered matching sentences to any other sentence in the monolingual corpus as opposed to sentences in the comparable document?
 ",7
"This paper proposes a method to train a machine translation system using weakly paired bilingual documents from Wikipedia. A pair of sentences from a weak document pair are used as training data if their cosine similarity exceeds c1, and the similarity between this sentence pair is c2 greater than any other pair in the documents, under sentence representations formed from word embeddings trained with MUSE. The neural translation model learns to translate from language X to Y, and from Y to X using the same encoder and decoder parameters, but the decoder is aware of the intended target language given an embedding of the intended language. The model is also trained to minimise the KL divergence between the distribution of terms in the target language document and the distribution of terms in the current model output. The model also uses the denoising autoencoding and reconstruction objectives of Lample et al. (2017). The results show improvements over the Lample et al. (2017) and that performance is heavily dependent on the number of sentences extracted from the weakly aligned documents.

Positives
- Large improvement over previous attempts at unsupervised MT for the En-De language pair.
- Informative ablation study in Section 4.4 of the relative contribution of each part of the overall objective function (Eq 9).

Negatives
- The introduction gave the impression that this method would be applied to low-resource language pairs but it was applied to two high-resource language pairs. Because you have not evaluated on a low-resource language pair, it's not clear how your proposed method would generalise to a low-resource setting.

Questions
- Can you give some intuition for why you remove the first principal component from the word embeddings in Equations 1 - 3?
- Are the Supervised results in Table 2 actually a fair reflection of a reasonable NMT model trained with sub-word representations and back translated data?
- What is the total number of sentences in the weakly paired documents in Table 1? It would be useful to know the proportion of sentences you managed to extract to train your models.

Comments
- Koehn et al. (2003) is not an example of any kind of neural network architecture.",6
"The major issue in this paper is that the ""new direction"" in this paper has been explored before [1]. Therefore the introduction needs to be rewritten with arguing the difference between existing methods. 

The proposed method highly relies on the percentage of implicitly aligned data. I suggest the author do more experiments on different data set with a significant difference in this ""percentage"". Otherwise, we have no idea about the performance's sensitivity to the different datasets. 

More detailed explanations are needed. For example, what do you mean by ""p(w)  as the estimated frequency""? Why do we need to remove the first principal components?

Section 3.2 title is "" aligning topic distribution"" but actually it is doing word distribution alignment.

Do you do normalization for P(w^Y;d_i^X,\theta) in eq.6 which is defined on the entire vocab's distribution?

I think the measurement of the alignment accuracy and more experiments with different settings of \alpha and \beta are needed.

Citation needed for ""Second, many previous works suggest  that the word distribution ...""

[1] Munteanu et al, ""Improving Machine Translation Performance by Exploiting Non-Parallel Corpora"", 2006",5
"Pros:
- The authors consider an interesting problem of learning from complementary labels
- They propose an approach that, assuming that the complementary label is selected uniformly at random, provides an unbiased estimate for any loss function, which is an improvement over the previous work. 
- Experiments show promising results for modifications of the proposed estimate

Cons:
- Having an unbiased estimate doesn't imply that its minimisation is a successful learning strategy. Indeed, the authors show that minimising their original estimate for the cross-entropy loss leads to overfitting. While the authors attribute this behaviour to the fact that the estimate can be negative, I believe the loss being negative is not problem per se (for example, substituting 0/1 loss with -100/-99 loss would not change the learning; similarly, this is not a problem for the losses considered in [Ishida'17]). I would rather attribute the problem to the fact that the proposed estimate is unbounded from below and there are no generalisation guarantees for it. Indeed, assuming there exists a training example that appears in the training set only once, with one complementary label, estimate (8) can be made arbitrary small by just training to predict probability 0 for the provided complementary label on that example ( and any non-zero probability for other classes). 
- to cope with the above mentioned problem, the authors propose two heuristic-based modifications of the estimate, which are potentially biased. This weakens the initial motivation for finding an unbiased estimate and shifts the focus towards the experimental evaluation
- one of the mentioned motivations for unbiased estimates - being able to perform model selection on complementary labeled validation set - is not illustrated in the experiments

Questions:
- I believe 1/(K-1) normalisation factor in (5) is not needed
- there seems to be a mistake in (9) (and its modifications later on) - I would expect either the subscript $j$ of the probability distribution in the last summand to be exchanged with $k$ in the loss, or a factor $\pi_j/\pi_k$ added
- also, I think there are some mistakes in subscripts in (11)
- what loss is the method from [Ishida'17] optimising in the experiments?",5
"This paper proposes an improved approach to the ""complementary-label"" form of weak supervision, in which a label that is *not* the true label is marked. Specifically, this paper proposes an unbiased estimator that accepts arbitrary loss functions and models. Noting that this proposed estimator can suffer from overfitting due to unbounded negative loss, a lower-bounded estimator is proposed. Experiments are then performed on several image classification datasets.

Pros:
- This paper addresses a creative form of weak supervision, proposed by prior work, in which labels that are *not* the true label are labeled, in a clear fashion.

- The first proposed estimator is unbiased, as shown by a proof, and accepts arbitrary losses, an improvement over prior approaches

- The overall presentation is clear and clean

Cons:
- One of the main claims of the paper is the proposal of an unbiased estimator. However, this estimator then does not seem to work well enough due to degenerate negative loss.  So then a modified version is proposed- which does not appear to be unbiased?  Either way, no assertion or proof of it being unbiased is given.  So then presumably this also reverses the claim of being able to cross-validate?  This seems like a major weakening of the paper's contributions

- Since the unbiased estimator does not appear to work well, two implementations of a corrected one are proposed, using heuristic approaches without explicit theoretical guarantees.  This shifts the burden to the experimental studies.  These are somewhat thorough, but not extremely so: for example, one set of hyperparameters were used for all of the methods?  This seems like it could implicitly handicap / favor some over others?

- The proposed estimator is based on the assumption that the probability of classes in the complement set (the set of labels other than the one marked as incorrect) is uniformly distributed (e.g. see beginning of Proof of thm 1).  However, this seems like a potentially naive assumption. Indeed, in the related work section, it is mentioned that work in 2018 already considered the case where this uniformity assumption does not hold.

- More broadly, but following from the above: The paper does not provide any real world examples, real or hypothetical, to give the reader an idea of whether the above uniformity assumption---or really any of these assumptions---are well-motivated or empirically justified.  At the bottom of page 3 in the related work, a concrete application used in prior work is mentioned---where crowd workers are shown single labels and vote Y/N, leading to a mix of standard (if Y) and complement-labeled (if N) data---however this mixed setting is not considered explicitly in this paper.  So, how is the reader supposed to get any idea of whether the assumed setup is motivated or justified?  The experiments do not provide this, because the complementary labels are synthetically generated according to the model assumed in the paper.  Additionally, it is briefly mentioned that collecting complementary labeled data is faster, but again no concrete examples are given to support this.",5
"pros:

- Clearly written and sound paper.
- Addresses interesting problem. 
- Improves existing methods used for this learning scenario.  

cons:

- The core contribution is a special case of previously published more general framework which is not cited in the paper.

It is clearly written paper with a good motivation. The major problem is that the core contribution, namely, the risk reformulation in Theorem 1 and the derived loss (6), are special cases of more general framework published in 
   Jesus Cid-Sueiro et al. Consistency of Losses for Learning from Weak Labels. ECML 2014.

The work of [Cid-Sueiro2014] proposes a general way how to construct losses for learning from weak labels. They require that the distribution of weak labels is a linear transformation of the true label distribution, i.e. the assumption (3) of the paper under review. According to [Cid-Sueiro2014], the loss on weak labels is constructed by $weak_loss = L*original_loss$, where $L$ is the left inversion of the ""mixing matrix"" $T$ in (3). [Cid-Sueiro2014] also shows that such weak loss is classification calibrated which implies statistical consistency of the method. 

Learning from complementary labels is a special case when the mixing matrix is $T=(E-I)/(K-1)$ (E is unitary matrix, I is matrix of ones, K is number of labels). In this case, the left inversion of $T$ is simply $L=- E*(K-1) + I$ and so the weak loss is $weak_loss=L*loss$ which corresponds to the loss (5) proposed in the paper under review (in fact, the loss (5) also adds a constant term (Y-2)/(Y-1) which however has no effect on the minimizer). 

The novel part of the paper is the non-negative risk estimator proposed in sec 3.3 and the online optimization methods addressed in sec 3.4. These extensions, although relatively straightforward, are empirically shown to significantly improve the results.",6
"This paper proposes a continual learning approach which transforms intermediate representations of new data obtained by a previously trained model into new intermediate representations that are suitable for a task of interest.
When a new task and/or data following a different distribution arrives, the proposed method creates a new transformation layer, which means that the model’s capacity grows proportional to the number of tasks or data sets being addressed over time. Intermediate data representations are stored in memory and its size also grows.
The authors have demonstrated that the proposed method is robust to catastrophic forgetting and it is attributed to the feature transformation component. However, I’m not convinced by the experimental results because the proposed method accesses all data in the past stored in memory that keeps increasing infinitely. The authors discuss very briefly in Section 5.2 on the performance degradation when the memory size is restricted. In my opinion, the authors should discuss this limitation more clearly on experimental results with various memory sizes.

The proposed approach would make sense and benefit from storing lower dimensional representations of image data even though it learns from the entire data over and over again.
But it is unsure the authors are able to claim the same argument on a different type of data such as text and graph.",4
"Summary:
 a method is presented for on-going adaptation to changes in data, task or domain distribution. The method is based on adding, at each timed step, an additional network module transforming the features from the previous to the new representation. Training for the new task/data at time t relies on re-training with all previous data, stored as intermediate features. The method is shown to provide better accuracy than naïve fine tuning, and slightly inferior to plain re-training with all the data.
While the method is presented as a solution for life long learning, I think it severely violates at least two demands from a feasible solution: using finite memory and using finite computational capacity (i.e. a life-long learning cannot let memory or computation demands to rise linearly with time). Contrary to this, the method presented induces networks which grow linearly in time (in number of layers, hence computation requirements and inference time), and which use a training set growing indefinitely, keeping (representations of) all the examples ever seen so far. If no restrictions on memory and computation time are given, full retraining can be employed, and indeed it provides better results that the suggested method. In the bottom line, I hence do not see the justification for using this method, either as a life-long learner or in another setting.

Pros:
+ the method shows that for continuous adaptation certain representations can be kept instead of the original examples 
Cons:
- The method claims to present a life long learning strategy, yet it is not scalable to long time horizon (memory and inference costs rise linearly with time)
- Some experiments are not presented well enough to be understood.

More detailed comments:
Page 3:
-	Eq. 2 is not clear. It contains a term ‘classification loss’ and ‘feature_loss’ which are not defined or explained. While the former is fairly standard, leaving the latter without definition makes this equation incomprehensible. 
o	I later see that eq. 7 includes the details. Therefore eq.2 is redundant. 
Page 4:
-	Eq. 5 seems to be flawed, though I think I can understand what it wants to say. Specifically, it states two sets: one of examples (represented by the previous feature extractor) and one of labels (of all the examples seen so far). The two sets are stated without correspondence between examples and labels – which is useless for learning (which requires example-label correspondence). I think the intention was for a set of (example, label) pairs, where the examples are represented using feature extractor of time t-1.
-	Algorithm 1 seems to be a brute force approach in which the features of all examples from all problems encountered so far are kept (with their corresponding labels). This means keeping an ever growing set of examples, and training repeatedly at each iteration on this set. These are not realistic assumptions for a life-long learner with finite capacity of memory and computation.
o	For example, for the experiment reported at page 6, including 25 episodes on MNist, each feature transformer is adding 2 additional FC layers to the network. This leads to a network with >50 FC layers at time step 25 – not a reasonable and scalable network for life ling learning
Page 6:
-	The results show that the feature transformer method achieve accuracy close to cumulative re-training, but this is not too surprising, since feature transformer indeed does cumulative re-training: at each time step, it re-trains the classifier (a 2 stage MLP) using all the data at all times steps (i.e. cumulative retraining). The difference from pure cumulative re-training, if I understand correctly, is that the cumulative re-training is done not with the original image representations, but with the intermediate features of time t-1. What do we earn and what do we loose from this? If I understand correctly, we earn that the re-training is faster since only a 2-layer MLP is re-trained instead of the full network. We loose in the respect that the model gorws larger with time, and hence inference becomes prohibitively costly (as the network grows deeper by two layers each time step). Again, I do not think this is a practical or conceptual solution for life long learning.
-	The experiment reported in figure 3 is not understandable without reading Lopez-Paz et al., 2017 (which I didn’t). the experiment setting, the task, the performance measurements – all these are not explained, leaving this result meaningless for a stand-alone read of this paper.
-	Page 8: it is stated that “we only store low dimensional features”. However, it is not reported in all experiment exactly what is the dimension of the features stored and if they are of considerably lower dimension than the original images. Specifically for the MNIst experiments it seems that feature stored are of dimension 256, while the original image is of dimension 784 – this is lower, but no by an order of magnitude (X10).
-	The paper is longer than 8 pages.
",3
"The authors provided a training scheme that ensures network retains old performance as new data sets are encountered (e.g. (a) same class no drift, (b) same class with drift, (c) new class added). They do this by incrementally adding FC layers to the network, memory component that stores previous precomputed features, and the objective is a coupling between classification loss on lower level features and a feature-loss on retaining properties of older distributions. The results aren't very compelling and the approach looks like a good engineering solution without strong theoretical support or grounding. ",4
"The authors propose an approximate MCMC method for sampling a posterior distribution of weights in a Bayesian neural network.  They claim that existing MCMC methods are limited by poor scaling with dimensionality of the weights, and they propose a method inspired by HMC on finite-dimensional approximations of measures on an infinite-dimensional Hilbert space (Beskos et al, 2011).  In short, the idea is to use a low dimensional approximation to the parameters (i.e. weights) of the neural network, representing them instead as a weighted combination of basis functions in neural network parameter space.  Then the authors propose to use HMC on this lower dimensional representation.  While the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance in the current form.

The authors define a functional, f: \theta -> [0, 1], that maps neural network parameters \theta to the unit interval.  They claim that this function defines a probability distribution on \theta, but this not warranted.  First, \theta is a continuous random variable and its probability density need not be bounded above by one; second, the authors have made no constraints on f actually being normalized.  

The second flaw is that the authors equate a posterior on f given the data with a posterior on the parameters \theta themselves.  Cf. Eq 4 and paragraph above.  There is a big difference between a posterior on parameters and a posterior on distributions over parameters.   Moreover, Eq. 5 doesn't make sense: there is only one posterior f; there are no samples of the posterior. 

The third problem appears in the start of Section 3, where the authors now call the posterior U(theta) instead of f.  They make a finite approximation of posterior U(\theta) = \sum_i \lambda_i u_i, which is inconsistent with Beskos et al.  I believe the authors intend to use a low dimensional approximation to \theta rather than its posterior U(\theta).  For example, if \theta = \sum_i \lambda_i u_i for fixed basis functions u_i, then you can approximate a posterior on \theta with a posterior on \lambda.

The fourth, and most important problem, is that the basis functions u_i are never defined.  How are these chosen? Beskos et al use the eigenfunctions of the Gaussian base measure \pi_0, but no such measure exists here.  Moreover, this choice will have a substantial impact on the approximation quality. 

There are more inconsistencies and notational problems throughout the paper.  Section 4.1 begins with a mean field approximation that seems out of place.  Section 3 clearly states that the posterior on theta is approximated with a posterior on lambda, and this cannot factorize over the dimensions of theta.  Finally, the authors again confuse the posterior on weights with a posterior on distributions of weights in Eq 11.   \tilde{U} is introduced as a function of lambda in Eq 14 and then called with f in line 4 of Alg. 1.  These two types are not interchangeable. 

These inconsistencies cast doubt on the subsequent experiments.  Assuming the algorithm is correct, a fundamental experiment is still missing. 
To justify this approach, the authors should show how the posterior approximation quality varies as a function of the size of the low dimensional approximation, D.

I reiterate that the idea of approximating the posterior distribution over neural network weights with a posterior distribution over a lower dimensional representation of weights is interesting.  Unfortunately, the abundance of errors in presentation cloud the positive contributions of this paper.",3
"This paper considers a new learning paradigm for Bayesian Neuron Networks (BNN): learning distribution in the functional space, instead of weight space. A new SG-MCMC variant is proposed in Algorithm 1, and applied to sampling in a 
""functional space"". The approach is demonstrated on various tasks.

Quality: Low, due to the low clarity detailed below.


Clarity: I do not fully follow the core algorithm:  The posterior is U_D(\theta) = \sum_{i=1}^D  \lambda_i * u_i, where  \lambda_i is represented as MCMC samples,  what is u_i then? I guess u_i is defined in (2), which is approximated in (3) if weight sample is used. However, how is u_i represented in the functional approach? I guess it is similar to the weight-based approach. If this is true, how could we distinguish between a functional approach and weight-based approach?

The proposed SGFuncRLD is essentially Adam plus Gaussian noise, but performed in a so-called ""functional space""? It is therefore not surprise to me that SGFuncRLD performs better than pSGLD (RMSprop plus Gaussian noise), just as Adam performs better than RMSprop. If we only focus on the new SG-MCMC approach itself, the authors need to justify: (1) the smoothed gradient is an unbiased gradient estimator, how does it effect convergence? Does it guarantee to  true posterior? this should be done in theory. (2)  The SGFuncRLD  algorithm itself is the same with pSGLD except the smoothed gradient part.  This makes  the clear comparison even important. Does SGFuncRLD  perform better just because the proposed smoothed gradient, or because the sampling is done in the functional space?

My suggestions: Please disentangle the contributions clearly. There are two things: (1) smooth gradient, (2) sampling in a functional space. Which one really contributes the performance improvement?

To demonstrate (1),  the authors could at least conduct on a toy distribution, to demonstrate the difference with pSGLD, regardless it is to the functional space or the weight space. 
To demonstrate (2), the authors  could apply the same SG-MCMC variant to the functional space and to the weight space, and see the difference. 

Originality: To me, the idea of learning uncertainty of BNN in the functional space appeared in Prof.  Yee Whye Teh's NIPS 2017 presentation. The motivation in his presentation is very clear. However, how to implement this abstract idea in practice is unclear yet. This submission is the first attempt. However, I am concerned about the real contribution.

Significance: It is a very interesting research direction. The paper could have been significant if every part is clearly motivated and demonstrate. At this point, I am not fully convinced. ",4
"The idea of extending  Riemannian Langevin dynamics to functional spaces is elegant, however it is extremely hard to follow the proposed method as details are kept to a minimum. The finite approximation of the posterior distribution is a function of the parameters theta, however it displays parameters lambda. The couple of sentences: ""Then by sampling λ, we sample a functional f equivalently. The Riemannian Langevin dynamics on the functional space can thus be written as: (6)"" come without a single explanation.

Minor comments
* Max and Whye is the casual version for reference Welling and Teh.
* proper nouns in References should be capitalized",5
"This paper investigates how the SARAH stochastic recursive gradient algorithm can be applied to Trust Region Policy Optimization. The authors analyze the SARAH algorithm using its approximating ordinary and stochastic differential equations. The empirical performance of SARAPO is then compared with SVRPO and TRPO on several benchmark problems.

Although the idea of applying SARAH to reduce the variance of gradient estimates in policy gradient algorithms is interesting and potentially quite significant (variance of gradient estimates is a major problem in policy gradient algorithms), I recommend rejecting this paper at the present time due to issues with clarity and quality, particularly of the experiments.

Not enough of the possible values for experimental settings were tested to say anything conclusive about the performance of the algorithms being compared. For the values that were tested, no measures of the variability of performance or statistical significance of the results were given. This is important because the performance of the algorithms is similar on many of the environments, and it is important to know if the improved performance of SARAPO observed on some of the environments is statistically significant or simply due to the small sample size.

The paper also needs improvements in clarity. Grammatical errors and sentence fragments make it challenging to understand at times. Section 2.3 seemed very brief, and did not include enough discussion of design decisions made in the algorithm. For example, the authors say ``""the Fisher Information Matrix can be approximated by Hessian matrix of the KL divergence when the current distribution exactly matches that of the base distribution"" but then suggest using the Hessian of the KL of the old parameters and the new parameters which are not the same. What are the consequences of this approximation? Are there alternative approaches?

The analysis in section 3 is interesting, but the technique has been applied to SGD before and the results only seem to confirm findings from the original SARAH paper.

To improve the paper, I would suggest moving section 3 to an appendix and using the extra space to further explain details and conduct additional simpler experiments. Additional experiments on simpler environments and policy gradient algorithms (REINFORCE, REINFORCE with baseline) would allow the authors to try more possible values for experimental settings and do enough runs to obtain more conclusive results about performance. Then the authors can present their results applying SARAH to TRPO with some measure of statistical significance.",5
"The paper extends Sarah to policy optimization with theoretical analysis and experimental study. 

1) The theoretical analysis under certain assumption seems novel. But the significance is unknown compared to similar analysis. 

2) The analysis demonstrates the advantage of Sarah over SVRG, as noted in Remark 1. It would be better to give explicit equations for SVRG in order for comparison.

3) Experimental results seem to show empirically that the SARAH is only comparable to SVRG.

4) Presentation needs to be improved. ",6
"This paper proposes a new policy gradient method for reinforcement learning.
The method essentially combines SARAH and trust region method using Fisher information matrix.
The effectiveness of the proposed method is verified in experiments.

SARAH is a variance reduction method developed in stochastic optimization literature, which significantly accelerates convergence speed of stochastic gradient descent.
Since the policy gradient often suffers from high variance during the training, a combination with variance reduction methods is quite reasonable.
However, this work seems to be rather incremental compared to a previous method adopting another variance reduction method (SVRG) [Xu+2017, Papini+2018].
Moreover, the advantage of the proposed method over SVRPG (SVRG + policy gradient) is unclear both theoretically and experimentally.
[Papini+2018] provided a convergence guarantee with its convergence rate, while this paper does not give such a result.
It would be nice if the authors could clarify theoretical advantages over SVRPG.

Minor comment:
- The description of SVRG updates in page 2 is wrong.
- The notation of H in Section 3.1 (""ODE analysis"") is not defined at this time.
",5
"Summary:
This paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as “ultrafine-grained” class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of “class embedding” for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.

Comments:
Arguably, the primary contribution of this paper is the introduction of a new “ultrafine-grained” classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it’s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It’s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.

The model introduced is interesting, but its novelty is limited. It’s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an “embedding” is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.

Other Comments:
-Since you ran a cross-validation, you should add confidence intervals to your reported numbers
-One additional dataset detail I was hoping to see that you didn’t provide is the mean/standard deviation of the number of instances per class,
-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well
-The title of your paper is somewhat misleading - it’s hard to argue that the form of class embedding you use is a “deep” class embedding since it’s just a matrix of parameters that are learned during training.

Overall, I’m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.

[1] Ma, Xuezhe, and Eduard Hovy. ""End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.""


REVISION:
The other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.
",4
"This paper proposed to tackle a large-scale fine-grained object classification problem by approximated CRF. The main motivation is to exploit the spatial conference of object labels to reduce noises in the instance-wise prediction. To this end, the task is formulated by sequential inference problem using CRF. To speed up training, several techniques are applied such as factorized pairwise-potential and approximation of CRF objective.  

Although the paper presented a reasonable idea for their particular problem (i.e. classification of products in the store display), the significance of the work is quite limited as the same idea is not generally applicable to other settings (e.g. there is no strong spatial correlation of labels in general images). Also, the performance improvement over the instance object classification is not significant as shown in Figure 5 (Unary vs. Approximate factorized). Due to the limited significance and impact of the work, this reviewer suggests a rejection of this paper. 
",3
"This paper tackles the problem of estimating pairwise potentials when the number of labels is large. Two modifications are proposed: one is to factorize the matrix for pairwise potentials, and the other is to approximate the log likelihood objective with the MEMM objective.

The problem and the proposed approach are well motivated. It is particularly useful to draw the connections between MEMM and piecewise-pseudolikelihood.

The major weakness of the paper is whether the approximations are necessary. It is hard to see why approximating the log likelihood with MEMM is necessary, because inference and computing the gradients of the log likelihood have the same computational complexity. So the authors could have trained the model with the log likelihood.

Regardless, it is still valuable to compare MEMM and log likelihood for training CRFs. However, the authors fail to show how well MEMM approximates the log likelihood. For example, the authors can compare the solutions when optimizing with the gradients of log likelihood and the with the gradients of MEMM. It is especially important to compute the training log likelihood for the two solutions, as it tells us how well MEMM approximates the log likelihood. This is also true for the low-rank approximation of the pairwise potentials. The authors fail to compare the case with low-rank approximation and the case without. It is important to evaluate the training error first with both methods as they share the same objective. This type of comparison should be apply to batch normalization as well.

Approximating the pairwise potentials with matrix factorization is also not novel.  See the list below. (The list is by no means exhaustive. Please see the citations therein.)

Dense and low-rank Gaussian CRFs using deep embeddings
Chandra et al., ICCV 2017

Efficient SDP inference for fully-connected CRFs based on low-rank decomposition
Wang et al., CVPR 2015

Neural CRF parsing
Durrett and Klein, ACL 2015

Finally, some of the claims made in the paper (listed below) should be more careful.

p.4

the likelihood function, therefore, is log-linear and concave.
--> concave in what?

the scoring function is still concave, ...
--> concave in what?

the objective function is no longer linear or concave with respect to the network parameters, ...
--> what are the network parameters?

but deep learning training techniques have been shown to yield good results ...
--> this argument is weak. the key is point out that SGD is used, plus SGD has been shown to work well on many matrix factorization problems. see the paper below.

Online learning for matrix factorization and sparse coding
Mairal et al., JMLR 2010

p.5

the test time inference uses a global normalization ... avoids the label bias problem.
--> the partition function is not even computed when using Viterbi. I'm also not sure how this avoids the label bias problem.

whitening the inputs to each layer may also prevent converging into poor local optima.
--> this is a hand-wavy claim. it would be best if the authors can provide citations to the claim.
",3
"The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning. They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation. The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem. 

The paper is clearly written and easy to follow. It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning. 
 
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore. Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not. Therefore, it is a little misleading to still call it Bayesian active learning. Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective. 

The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout. So it seems not a reasonable solution for the mode collapse problem of MC-Dropout. It is not clear to me why we need to add MC-Dropout to the ensemble. What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?

In terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout. While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.

The labels of figures are hard to read. 
",4
"The paper shows that Bayesian neural networks, trained with Dropout MC (Gal et al.) struggle to fully capture the posterior distribution of the weights.
This leads to over-confident predictions which is problematic particularly in an active learning scenario.
To prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.
The proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.


The paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce. 

However, the paper contains only little novelty and does not provide sufficiently new scientific insights.
It is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.
For instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores. Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.
It would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].


Another weakness of the paper is that the empirical evaluation is not sufficiently rigorous: 

1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.

 2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M. 
 
 3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures. The same holds for the type of data, since the paper only shows results for image classification benchmarks.
 
 4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.
 



[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundel
NIPS 2017

[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger
    Snapshot Ensembles: Train 1, get {M} for free}
    ICLR 2017

[3] Bayesian Optimization with Robust Bayesian Neural Networks
    J. Springenberg and A. Klein and S.Falkner and F. Hutter
    NIPS 2016
 
[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams
    Scalable Bayesian Optimization Using Deep Neural Networks
    ICML 2015

[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling
    Carlos Riquelme, George Tucker, Jasper Snoek
    ICLR 2018",4
"This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.

In active learning, there is generally a trade-off between data efficiency and computational cost. This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both. The improvements over basic ensembling are rather minimal, at the cost of extra computation. More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so. On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments). As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout? At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?

The novelty of this method is minimal. The technique basically fills out the fourth entry in a Punnett square.

The paper is well-written, has good experiments, and has a comprehensive related work section.

Overall, this paper is good, but is not novel or important enough for acceptance.",5
"This paper suggests a source of slowness when training a two-layer neural networks: improperly trained output layer (classifier) may hamper learning of the hidden layer (feature). The authors call this “inverse” internal covariate shift (as opposed to the usual one where the feature distribution shifts and trips the classifier). They identify “hard” samples, those with large loss, as being the impediment. They then propose a curriculum, where such hard samples are identified at early epochs, their loss attenuated and replaced with a requirement that their features be close to neighboring (in feature space) samples that are similarly classified, but with a more comfortable margin (thus “easy”.) The authors claim that this allows those samples to contribute through their features at first, without slowing the training down, then in later epochs fully contribute. Some experiments are offered as evidence that this indeed helps speedup.

The paper is extremely unclear and was hard to read. The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said. Based on this only, I do not deem this work ready for sharing. Furthermore, there are some general issues with the concepts. Here are some specific remarks.

-	The intuition of the inverse internal covariate shift is perhaps the main merit of the paper, but I’m not sure if this was not mostly appreciated already.

-	The paper offers some experimental poking and probing to find the source of the issue. But that part of the paper (section 3) is disconnected from what follows, mainly because hardness there is not a single point’s notion, but rather that of regions of space with a heterogeneous presence of classes. This is quite intuitive in fact. Later, in section 4, hard simply means high loss. This isn’t quite the same, since the former notion means rather being near the decision boundary, which is not captured by just having high loss. (Also, the loss is not specified.)

-	Some issues with Section 3: the notions of “task” needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. it’s all too vague. The term “non-computable” has very specific meaning, best to avoid. Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own). The whole paragraph preceding Figure 3 is hard to follow. I sort of can make up what is going, especially with the hindsight of Section 4, since it’s basically a variant of the proposed schedule (easy to hard making sure all clusters, as proxy to classes, are represented) without the feature loss, but it needs a rewriting.

-	It is important to emphasize that the notion of “easy” and “hard” can change along the training, because they are relative to what the weights are at the hidden layer. Features of some samples may be not very separable at some stage, but they may become very separable later. The suggested algorithm does this reevaluation, but this is not made clear early on.

-	In Section 4, the sentence where S_t(x) is mentioned is unclear. I assume “surpass” means achieving a better loss. Also later M_t (a margin) is used, when I think what is meant is S_t (a set). The whole notation (e.g. “topk”, indexing that is not subscripted, non-math mode math) is bad.

-	If L_t is indeed a loss (and not a “performance” like it’s sometimes referred to, as in minus loss), then I assume larger losses means that the weight on the feature loss in equation (3) should be larger. So I think a minus sign is missing in the exponent of equation (2), and also in the algorithm.

-	I’m not sure if the experiments actually show a speedup, in the sense of what the authors started out motivating. A speedup, for me, would look like the training progress curves are basically compressed: everything happens sooner, in terms of epochs. Instead, what we have is basically the same shape curve but with a slight boost in performance (Figure 4.) It’s totally disingenuous to say “this is a great boost in speed” (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically.

-	So the real conclusion here is that this curriculum may not have sped up the training in the way we expect it at all. However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results.
",2
"This paper describes an approach for automated curriculum learning in a deep learning classification setup. The main idea is to weigh data points according to the current value of the loss on these data points. A naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples). This last part is implemented by caching hidden representations and classification loss values during training and fetching nearest neighbours in the feature space whenever a hard data point is encountered. The final loss takes the form of a linear combination of the classification loss and the representation loss.

The idea is interesting in the sense that it tries to use information about how difficult classification of a given data point is to improve learning. The proposed representation loss can lead to forming tight cluster of similar data point in the feature space and can make classification easier. It is related to student-teacher networks, where a student is trained to imitate the teacher in generated similar feature representations.

The authors justify the method by introducing the notion of “inverse internal covariate shift”. However, it is not defined formally, nor is it supported empirically, and is based on the (often criticized [1]) notion of “internal covariate shift”. For this reason, it is hard to accept the presented argumentation in its current state.

Moreover, there seems to be a mistake in equation (2) in §4.2. The equation defines the method of computing loss weighting for a given datapoint. The authors note that it converges to the value of one with increasing training iterations, but for correctness it should be \in [0, 1]. If it is > 1, one of the losses in equation (3) is negated and is therefore maximised (instead of being minimised), which can lead to unexpected behaviour. Current parameterization allows it to be \in [0, + infinity].

Experimental evaluation consists of quantitative evaluation of random sampling (usual SGD) and the proposed approach in training a classification model on MNSIT, CIFAR-10 and CIFAR-100. The proposed approach outperforms random sampling. This is encouraging, but the method should be compared to state of the art in curriculum learning in order to gauge how useful this approach is.

The paper is poorly written, with many grammatical (lack of “s” at the end of verbs used in singular 3rd person, many places in the paper) and spelling mistakes (e.g. §3.2¶6 “tough” instead of “through”, I think). Some descriptions are unclear (e.g. §4.2¶2), while some parts of the paper seem to be irrelevant to the problem at hand (§3.1 describes training on a single minibatch for multiple iterations as if it were a separate task and motivates random sampling, which is just SGD).

To summarize, the paper presents a very interesting idea. In its current state it is hard to read, however. It also contains a number of unsupported claims and can be misleading. It could also benefit from a more extensive evaluation. With this in mind, I suggest rejecting this paper.

[1] Rahimi, A (2017). Test of Time Award Talk, NIPS.",3
"This paper proposes a curriculum that encourages training on easy examples first and postpones training on hard examples. However, contrary to common ideas, they propose to keep hard examples contribute to the loss and only forcing them to have internal representations similar to a nearby easy example. The proposed objective is hence biased at the beginning but they dampen it over time to converge to the true objective at the end.

Positives:
- There is not much work considering each example as an individual subtask.
- The observation that an under-fitted classifier can destroy a good feature extractor is good.

Negatives:
- In the intro it says “[update rule of gradient descent] assumes the top layer, F2, to be the right classifier.”. This seems like a fundamental misunderstanding of gradient descent and the chain rule. The term d output/d F1 takes into account the error in F2.
- The caption of figure 2 says the “... they cannot separate from its neighbors…”. If the loss of all examples in a cluster is high, all are being misclassified. A classifier then might have an easy job fixing them if all their labels are the same or have a difficult job if their labels are random. The second scenario is unlikely if based on the claim of this figure, the entropy has decreased during training. In short, the conclusion made in fig 2 does not necessarily hold given that figure.
- This method is supposed to speed up training, not necessarily improve the final generalization performance of the model. The figures show the opposite outcomes. It’s not clear why. The improvement might be due to not tuning the hyperparameters of the baselines.
- Figure 3 does not necessarily support the conclusion. The fluctuations might be caused by any curriculum that forces a fixed ordering across training epochs. Often on MNIST, the ordering of data according to the loss does not change significantly throughout training.",4
"This paper introduces the use of sequential information (state-action pairs) for enhancing imitation learning, and using recurrent networks (LSTM) in that process.  The authors motivate this by pointing out that while the state information, if Markovian, should contain all information necessary for decision making, with incomplete learners redundant information in the sequential state-action information leading to the current state can be helpful, citing some concrete examples. 
After describing a number of variants of this idea, in the context of IRL, BC, etc., the authors conduct a systematic empirical evaluation to assess the effectiveness of the proposal, over the baselines, using a number of RL benchmark problems. 
The results are favorable and convincingly show that the proposed sequential enhancement can bring significant improvement in terms of attained rewards, convergence speed and stability in many of the tested cases. 
One suggestion I have is that it would be interesting to investigate into the question of how the addition of sequential information adds value is related to the validity of Markovian assumption in each of the problem being considered. 
It is a good empirical paper demonstrating the practical use of an idea that is simple but reasonable, and in a way that is substantiated using proper cutting edge framework and baselines. 
",6
"The paper puts forward the idea of using a recurrent neural network in algorithms for learning from demonstration in order to take into account sequential information. The authors test it in the inverse reinforcement learning setting and the behavioral cloning setting on different control problems.

I feel the basic idea is really straightforward. Although some promising results are obtained in the experimental setting, I believe the contribution may not be sufficient for a publication at ICLR. Moreover, there are some issues in the writing, e.g., 

- classically, as far as I know, RL is not considered to be a metaheuristic, although I understand that someone could make the case for it.

- although there’s not really a consensus on terminology, I think using imitation learning to define the whole class of problems encompassing IRL and behavioral cloning is not the best. Generally, imitation learning is equated to behavioral cloning. I think a better term for this general class is learning from demonstration. For instance, there are some IRL approaches that don’t try to mimic a demonstrated policy, but aim at learning an even better policy.

- the issue described in the paper about the missing sequential information is due to the fact the authors consider POMDPs and not MDPs. This should be made clearer. I think the authors should also cite the following paper:

@article{ChoiKim11,
	Author = {Jaedeug Choi and Kee-Eung Kim},
	Journal = {JMLR},
	Pages = {691--730},
	Title = {Inverse Reinforcement Learning in Partially Observable Environments},
	Volume = {12},
	Year = {2011}}

- the related work has to be reworked. Kuderer et al. (2013) is not about urban route planning, but deals with learning driving style; Mnih et al. (2015) is not about training multi-agent systems, but introduces DQN; Silver et al. (2016) is about go, not chess. Are TRPO or PPO really off-policy or asynchronous?

- the last section of Sec.3.4 sounds strange. It’s not MC that assumes that the impact of an action decays with time. The discount factor comes from the choice of the total discounted reward criterion.

Other comments:

- in abstract: BL -> BC
- notations issues in (2-5)
- l.6-7, Algo 1: t = T_m?
- The text should be checked for typos.",4
"The paper proposes to integrate sequential information into imitation learning techniques.  The assumption is that mostly all the IL techniques are learning a policy which depends on state at time t, while the information contained in this state may be not sufficient to choose the right action (actually, this is the POMDP setting, the notion of POMDP not appearing in the paper....). The authors thus propose to use a recurrent neural network to encode the state by aggregating past information, instead of just using the features of the state at time t. They thus instantiate this idea on different methods and show that, on some problems, this approach can increase the quality of the final policy.

Actually, the contribution of the paper is a simple extension of existing methods: using a RNN instead of a simple NN in imitation learning models. First of all, when dealing with classical environments such as Atari, many papers propose to use the last N frames as a state encoding (instead of the last frame), following the same intuition. The studied setting thus corresponds to the PO-MDP case and using a RNN in POMDP is for example what is done in  [Merel etal. 2017]. Moreover, the problem of imitation learning (and particularly inverse RL) in POMDP has been of the interest of many papers like [Choi et al. 2008] for instance and many more, and it is unclear what is the positioning of this paper w.r.t existing works. Since the paper proposes just to encode history with a RNN, the proposed solution lacks of originality, and the contribution of the paper in term of model is quite low.  But the authors explain how this can be instantiated in three different settings (IRL, GAIL and BC) -- note that the section concerning the use of Adaboost is not clear and could be better described -- which can be of the interest of the community. 
Concerning the experiments, I don't understand what is the split between training and testing data. Is it pairs of state-action coming from the experts ? or trajectories ? Moreover, I don't understand why these environments correspond to POMDP cases and the authors have to give details on that. For instance, mountain-car is clearly not a POMDP problem in its classical shape, nor Acrobot. As if, it makes the experiments very difficult to reproduce. The interest of using the RNN to encode history does not seem clear for each of the cases since it often degrades the final performance, so I don't know exactly what insights I can extract from the paper.

Pro:
* The approach is proposed for IRL, GAIL and BC

Cons:
* Lack of positionning w.r.t POMDP litterature
* Lack of details in the experiments, and lack of good experimental results
* Low contribution in term of model


[Merel et al. 2017]  Learning human behaviors from motion capture
by adversarial imitation
[Choi et al.] Inverse Reinforcement Learning in Partially Observable
Environments",4
"This paper proposes the Structure-Aware Program Synthesis (SAPS) system, which is an end-to-end neural approach to generate snippets of executable code from the corresponding natural language descriptions. Compared to a previous approach that used search in combination of a neural encoder-decoder architecture, SAPS relies exclusively on neural components. The architecture uses a pretrained GloVe embedding to embed tokens in the natural language description, which are then embedded into a vector representation using a bidirectional-LSTM. The decoder uses a doubly-recurrent neural network for generating tree structured output. One of the key ideas of the approach is to use a single vector point in the latent space to represent the program tree, where it uses a tree2tree autoencoder to pre-train the tree decoder. The results on the NAPS dataset show an impressive increase in accuracy of about 20% compared to neural-only baselines of the previous approach.

While overall the SAPS architecture achieves impressive practical results, some of the key contributions to the design of the architecture are not evaluated and therefore it makes it difficult to attribute the usefulness and impact of the key contributions. For example, what happens if one were to use pre-trained GloVe embeddings for embedding NL specifications in Polosukhin & Skidanov (2018). Such and experiment would lead to a better understanding of how much gain in accuracy one can obtain just by using pre-trained embeddings.

One of the key ideas of the approach is to use a tree2tree autoencoder to train the latent space of program trees. The decoder weights are then initialized with the learnt weights and fine-tuned during the end-to-end training. What happens if one keeps the decoder weights fixed while training the architecture from NL descriptions to target ASTs? Alternatively, if one were to not perform auto-encoding based decoder pre-training and learn the decoder weights from scratch, how would the results look?

Another key point of the paper is to use a soft attention mechanism based on only the h^latent. What happens if the attention is also perform on the NL description embeddings? Presumably, it might be difficult to encode all of the information in a single h^latent vector and the decoder might benefit from attending over the NL tokens.

It was also not clear if it might be possible to perform some form of a beam search over the decoded trees to possibly improve the results even more? 

There are also other datasets such as WikiSQL and Spider for learning programs from natural language descriptions. It might be interesting to evaluate the SAPS architecture on those datasets as well to showcase the generality of the architecture.
",4
"# Summary

This paper introduces a model called SAPS for the task of mapping natural language descriptions of programs to the AST tree of the corresponding program. The model consists of a variation of a double recurrent neural network (DRNN) which is pre-trained using an autoencoder. The natural language description is turned into a latent vector using pretrained word embeddings and a bidirectional stacked LSTM. The final model consists of training this sentence embedding model jointly with the decoder of the autoencoder.

# Quality

The authors introduce a reasonable model which achieves good performance on a relevant task. The results section contains a fair amount of numbers which give some insight into the performance of the model. However, the results make it hard to compare the proposed model with other models, or with other training procedures.

For example, the Seq2Tree model that is shown in table 2 was not necessarily intended to be used without a search algorithm. It is also not mentioned how many parameters both models have which makes it hard to judge how fair of a comparison it is. (I couldn't find the dimensionality of the encoder in the text, and the decoder dimensionality is only shown in figure 2.)

The model proposed in this work uses decoder pretrained in an autoencoder setting. No results are shown for how the model performs without pretraining. Pretraining using autoencoders is a technique that fell out of favor years ago, so it seems worthwhile to investigate whether or not this pretraining is necessary, and if so, why and how it aids the final performance.

It is unclear to me what type of robustness the authors are trying to show in table 5. The use of robustness here is not clear (robustness is often used to refer to a network's susceptability to adversarial attacks or perturbations of the weights). It also seems that the type of ""simple replacements"" mentioned are very similar to the way the examples were generated in the first place (section 4 of Polosukhin). If the goal is to measure generalization, why do the authors believe that performance on the test set alone is not a sufficient measure of generalization?

Some smaller comments and questions:

* In section 4 you mention training the sentence-to-tree and the sentence-to-vector mappings. Isn't the sentence-to-vector model a subset of the sentence-to-tree model? Should I interpret this as saying that, given the pretrained decoder and the glove embeddings, you now train the entire model jointly? Or do you first learn the mapping from sentences to the latent space, and only then finetune the entire model?
* The attention mechanism is not actually an attention mechanism: Attention mechanisms are used to reduce a variable number of elements to a single element by learning a weighting function and taking a weighted sum. My understanding is that in this case, the input (the latent representation) is of fixed size. The term ""gating function"" would be more appropriate.
* You specify that the hidden states of the decoder are initialized to zero, but don't specify what the cell states are initialized to.

# Clarity

The writing in the paper is passable. It lacks a bit in structure (e.g., I would introduce the problem and dataset before introducing the model) and sometimes fails to explain what insights the authors draw from certain results, or why certain results are reported. Take table 3 as an example: As a reader, I was confused at first why I should care about the reconstruction performance of the autoencoder alone, considering its only purpose is pretraining. Then, when looking at the numbers, I am even more confused since it is counterintuitive that it is harder to copy a program than it is to infer it. At the end of the paragraph the authors propose an explanation (the encoder isn't as powerful as the decoder) but leave it unclear as to why these numbers were being reported in the first place.

In general, the paper would do well to restructure the text so that the reader is explained what the goal of the different experiments is, and what insights should be drawn from them.

A variety of smaller concerns and comments:

* Please reduce and harmonize the terminology in the paper: the terms latent-to-AST, NLP2Tree, NLP2Vec, tree2tree/tree-to-tree, sentence-to-tree, sentence-to-vector, NL-to-latent, and spec-to-latent all appear in the paper and several of them are redundant, making it significantly harder to follow along with the text.
* Avoid citing the same work multiple times within a paragraph; cite only the first use and use prose to make clear that future references are to the same work.
* Formula 14 has h_i^{(pred)} on both sides of the quation, and is used in the definition of A as well. I am assuming these two terms are actually the h_i^{(pred)} from equation 8, but this should be made clear in the notation.
* Why does figure 1 have boxes for ""NL specification"", ""NL spec."", and ""NL query""? In general, the boxes inconsistently seem to represent both values and operations.
* It is never explicitly stated that Seq2Tree is the model from the Polosukhin et al. paper, which is a bit confusing.
* Parameters is missing an -s in the first paragraph of section 4.
* It is said that regularization is applied to layer normalization, which I assume means that the regularization is applied to the gain parameters of the layer normalization.
* It says ""like a in the above example"" when table 1 is rendered at the bottom of the page by Latex.

# Originality and significance

The paper introduces model variations that the authors claim improve performance on this particular program synthesis problem. In particular, in the DRNN decoder the hidden state is never reset for the ""horizontal"" (breadth-first order) decoder, and each node is only allowed to attend over the latent representation of the program. The authors claim this means their model ""significantly diverges from [other] works"", which seems hyperbolical.

The main contribution of this work is then the performance on the program synthesis task of Polosukhin and Skidanov. However, the model fails to improve on the Seq2Tree-guided search approach, so its main claimed benefit is that it is trained end-to-end. Although there is a strong trend in ML research to prefer end-to-end systems, it is worthwhile to ask when and why end-to-end systems are preferred. It is often clear that they are better than having separately learned components that are combined later. However, this does not apply to the model from Polosukhin et al., which consists of a single learned model being used in a search, which is a perfectly acceptable technique. In comparison, translations in neural machine translation are also produced by performing a beam search, guided by the probabilities under the model.

The paper would be significantly stronger if it could show that some alternative/existing method (e.g., a standard DRNN, or a Seq2Tree model with the same number of parameters, or a non-pretrained network) would fail to solve the problem where the authors' proposed method does not. However, the single comparison with the Seq2Tree model does not show this.

# Summary

Pros:

* Reasonable model, extensive results reported
* Decently written

Cons:

* Unclear how the performance compares to other models
* Not well justified why end-to-end methods would be better than guided-search based methods
* Model architectural differences seem relatively minor compared to original DRNN
* The pretraining using an autoencoder and the use of pretrained word embeddings seems arbitrary and is not critically evaluated
* Lack of coherent story to several results (the autoencoder performance, robustness analysis)",4
"The submission proposes to combine a tree2tree autoencoder with a sequence encoder for natural language. It uses the autoencoding objective to appropriately shape the latent space and train the decoder, and then uses a second training step to align the output of a sequence encoder with the input for the tree decoder. Experiments on a recent dataset for the natural language-to-code task show that the proposed model is able to beat simple baselines.

There's much to like about this paper, but also many aspects that are confusing and make it hard to tease out the core contribution. I'm trying to reflect my understanding here, but the authors could improve their paper by providing an explicit contribution list. Overall, there seem to be three novel things presented in the paper:
(1) (Pre)training the (program) tree decoder using an autoencoder objective
(2) The doubly-recurrent tree decoder, which follows a different signal propagation strategy from most other approaches.
(3) An ""attention"" mechanism over the point in latent space (that essentially rescales parts of the decoder input)

However, the experiments do not evaluate these contributions separately; and so their relative merits remain unclear. Primarily, I have the following questions (for the rebuttal, and to improve the paper):

Re (1):
 (a) Does the pre-training procedure help? Did you evaluate joint end-to-end training of the NL spec encoder and the tree decoder? 
 (b) The auto-encoder objective would allow you to train on a larger corpus of programs without natural language specifications. Arguably, the size of the dataset is insufficient for most high-capacity deep learning models, and as you use word embeddings trained on a much larger corpus...), you could imagine training the autoencoder on an additional corpus of programs without NL specs. Did you attempt this?

Re (2): 
 (a) The tree decoder is unusual in that (one) part of the recurrence essentially enforces a breadth-first expansion order, whereas almost all other approaches use a depth-first technique (with the only exception of R3NN, as far as I remember). You cite the works of Yin & Neubig and Rabinovich et al.; did you evaluate how your decoder compares to their techniques? (or alternatively, you could compare to the absurdly complex graph approach of Brockschmidt et al. (arxiv 1805.08490)))
 (b) Ablations on this model would be nice: How does the model perform if you set the horizontal (resp. the vertical) input to 0 at each step? (i.e., ablations to standard tree decoder / to pure BFS)

Re (3): This is an unusual interpretation of the attention mechanism, and somewhat enforced by your choice (1). If you run an experiment on end-to-training (without the autoencoder objective), you could use a standard attention mechanism that attends over the memories of the NL encoder. I would be interested to see how this would change performance.

As the experimental evaluation seems to be insufficient for other researchers to judge the individual value of the paper's contribution, I feel that the paper is currently not in a state that should be accepted for publication at ICLR. However, I would be happy to raise my score if (some) of the questions above are answered; primarily, I just want to know if all of the contributions are equally important, or if some boost results more than others.


Minor notes:
- There are many spelling mistakes (""snipped"" for ""snippet"", ""isomorhpic"", ...) -- running a spell checker and doing a calm read-through would help with these details.
- page1par2: Writing specifications for programs is never harder than writing the program -- a program is a specification, after all. What you mean is the hardness of writing a /correct/ and exact spec, which can be substantially harder. However, it remains unclear how natural language would improve things here. Verification engineers will laugh at you if you propose to ""ease"" their life by using of non-formal language...
- page1par3: This (and the rest of the paper) is completely ignoring the old and active field of semantic parsing. Extending the related work section to compare to some of these works, and maybe even the experiments, would be very helpful.
- page2par3 / page6par4 contradict each other. First, you claim that mostly normal english vocabulary is used, with only occasional programming-specific terms; later you state that ""NL vocabulary used in specifications is strongly related to programming"". The fact that there are only 281 (!!!) unique tokens makes it very doubtful that you gain anything from using the 1.9million element vocab of GLoVe instead of direct end-to-end training...
- page4par3: You state ""a reference to a previously used variable may require 'climbing up' the tree and then descending"" - something that your model, unlike e.g. the work of Yin & Neubig, does not support. How important is this really? Can you support your statement by data?
- page5, (14) (and (13), probably): To avoid infinite recursion, the $h_i^{(pred)}$ on the right-hand-side should probably be $h_{i-1}^{(pred)}$

",4
"This paper focuses on dealing with a scenario where there are ""unseen"" intents or slots, which is very important in terms of the application perspective.

The proposed approach, TSSM, tries to form the embeddings for such unseen intents or slots with little training data in order to detect a new intent or slot in the current input.
The basic idea in the model is to learn the representations of utterances and intents/slots such that utterances with the same intents/slots are close to each other in the learned semantic space.
The experiments demonstrate the effectiveness of TSSM in the few-shot learning scenarios.
The idea about intent embeddings for zero-shot learning is not fully original (Chen, et al., 2016), but this paper extends to both intent classification and slot filling. 

The paper tests the performance in different experimental settings, but the baselines used in the experiments are concerned.
This paper only compares with simple baselines (MaxEntropy, CRF, and basic DNN), but there should be more prior work or similar work that can be used for comparison in order to better justify the contributions of the model.
In addition, this paper only shows the curves and numbers in the experiments, but it is better to discuss some cases in the qualitative analysis, which may highlight the contributions of the paper.
Also, in some figures of Fig. 2, the proposed TSSM is not better than DNN, so adding explanation and discussion may be better.
",6
"In this paper, an efficient SLU model, called as TSSM, is proposed to tackle the problem of insufficient training data for the task of spoken language understanding. TSSM considers the intent and slot detection as a unified multi-objective optimization problem which is addressed by a meta-learning scheme. The model is pre-trained on a large dataset and then fine-tuned on a small target dataset. Thus, the proposed TSSM can improve the model performance on a small datatset in new domains.

Pros:
1)	The transfer learning of spoken language understanding is very interesting.
2)	The proposed TSSM can integrate the task of intents and slots and take the relationship between intents and slots into consideration.
3)	Five datasets are used to evaluate the performance of the method.

Cons:
Overall, the novelty of this paper is incremental and some points are not clear. My main concerns are listed as follows.
1)	The authors state that the knowledge transfer is the main contribution of this paper. However, as introduced in 3.5, the transfer scheme in which the model is first pre-trained on a large dataset and then fine-tuned on a small target dataset is very straightforward. For example, currently, almost all methods in the area of object recognition are pre-trained on ImageNet and then fine-tuned on a small dataset for particular tasks.
2)	Authors also state that improvements for transferring from Restaurant, Laptop, TV, Atis to Hotel is not obvious. I think the results also need to be reported and the reasons why the improvement is not obvious should be provided and discussed.
3)	The paper needs more proofreading and is not ready to be published, such as “A survey fnor transfer” and “a structured multi-objective optimization problems”.
",5
"Summary: The authors present a network which facilitates cross-domain
learning for SLU tasks where the the goal is to resolve intents and
slots given input utterances. At a high level, the authors argue that
by fine-tuning a pre-trained version of the network on a small set of
examples from a target-domain they can more effectively learn the
target domain than without transfer learning.

Feedback:

* An overall difficulty with the paper is that it is hard to
distinguish the authors' contributions from previous works. For
example, in Section 3.1, the authors take the model of Goyal et al. as
a starting point but explain only briefly one difference
(contatenating hidden layers). In Section 3.2 the contributions
becomes even harder to disentangle. For example, how does this section
relate to other word-embeddings papers cited in this section? Is the
proposed method a combination of previous works, and if not, what are
the core new ideas?

* Some sections are ad-hoc and should be justified/explained
better. For example, the objective, which ultimately determines the
trained model behaviour uses a product of experts formulation, yet the
authors do not discuss this. Similarly, the overarching message, that
by fine-tuning a suitable model initialisation using small amounts of
data from the target domain is fairly weak as the authors do not
detail exactly how the model is fine-tuned. Presumably, given only a
small number of examples, this fine-tuning runs the risk of
overfitting, unless some form of regularisation is applied, but this
is not discussed.

* Lastly, there are some curious dips in the plots (e.g., Figure 2 bottom left, Figure 3 top left, bottom left), which deserve more explanation. Additionally, the evaluation section could be improved if the scores were to show error-bars. 

Minor: All plots should be modified so they are readable in grey-scale.",4
"This paper proposes an unsupervised method to disentangle the latent code of VAE. Overall, it is novel and well written. The experiment has shown good performance of the proposed method.
 
I have some concerns as follows:
1.  In Eq.(1), y is assumed to follow a Gaussian distribution. Is it possible that y follows a multinomial distribution? Then, this model can be used for clustering.

2. In section 2.3, the concatenation between z and y is used to learn a complementary of y.  Why does the concatenation encourage  to learn the complementary of y? More explanations are needed.  Additionally, some experiments are needed to verify this claim.


",6
"In this paper,  a conditional deep generative model is proposed for disentangling structure (more precisely shape)  and appearance.  The architecture of the proposed system is very similar to that in [A], however, in this paper different applications are considered. The paper is relatively well-written and a number of experiments are presented. However, they are that convincing.

I have two main concerns regarding this paper.

1)	The authors have not taken into account recently proposed deep generative models for disentangling shape and appearance along other modes of visual variations. A non-exhaustive list is as follows:

*GAGAN: Geometry-Aware Generative Adversarial Networks
*Geometry-Contrastive GAN for Facial Expression Transfer
*Cross-View Image Synthesis using Conditional GANs
*Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance
*Neural Face Editing with Intrinsic Image Disentangling


The authors should discuss how the proposed method is different from the above-mentioned ones and compare the performance of the proposed model against that obtained by GAGAN and Deforming Autoencoders, which are very relevant to the proposed one models.

2)	Some of the experimental results are not convincing. For example, in Fig. 6 it seems to me that all the chairs produced by the proposed method are identical. In the same figure, Jakab’s method seems to produce more meaningful results than the proposed method which appears to implement texture style transfer, rather than shape transfer.

Considering all the above, I believe the paper needs substantial improvement prior to being considered for publication.


Reference

[Α] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Conditional image generation for
learning the structure of visual objects. NIPS, 2018. 
",5
"Summary: The authors present an autoencoding strategy for disentangling structure and appearance in image data. They achieve this using a learned, spatial prior in the VAE framework.

Writing: The paper contains grammatical errors and I found Section 2 (2.1 and 2.2, specifically) to be a bit confusing as many ideas were described with words when they could have been outlined more precisely mathematically. 

Major comments:
The paper takes ideas from Zhang et al and Jakab et al and put them in a VAE context. The paper, however, constructs the ELBO in such a way that distance it from many key ideas of the VAE. Particularly, the paper decomposes the ELBO into three terms and proceed to define these terms as they wish. In this way, the novel contributions of this paper are left unclear and many decisions left unjustified.

- Incorporating landmark/spatial information into autoencoders is not a new idea. Zhang et al and Jakab et al both train autoencoders with disentangled structure, and Finn, 2016 [1] uses a similar spatial landmark strategy when learning representations. Incorporating structural information as a prior distribution (as in this paper) is an interesting idea. However, it is not clear that the defined prior log p(y) is a proper density (integrates to 1). Given the importance of a prior distribution in VAEs, this choice should be precisely justified (maybe relate it to beta-VAE or just use a properly normalizable distribution)

- The paper chooses variational distributions in such a way that removes the entropy term from the KL divergences. Specifically, both q(z | x, y) and p(z | y) have fixed, identity covariance, resulting in the KL divergences equating to L_2 distance. An important part of the VAE is the explicit incorporation of uncertainty by means of learned variances. Although this can sometimes be problematic (see beta-VAE), neglecting to include these variances at all removes an important aspect of VAE.

- The paper includes a likelihood model which also throws away key ideas from VAE. Although some neural likelihood models as in the VAE have issues with blurriness, the reasons are still not completely understood. However, there are well explored alternatives to what the authors propose. For example, many image-based VAEs use Bernoulli likelihoods [2, 3] or autoregressive likelihoods [4]. Autoregressive models, especially, can produce sharp images. The authors introduce a unnormalizable likelihood which combines L1 loss with a function that incorporates L1 distance in VGG space. Using VGG in the likelihood model is unjustified and seems unnecessarily complicated, given the existence of powerful decoders that already exist in VAE literature.

- The authors incorporate various connections and concatenations between neural networks and distributions that further complicate the variational lower bound. For example, p(z | y) is concatenated to q(z | x, y) in addition to acting as a prior on q(z | x, y). This introduces a dependency between the likelihood model and the variational posterior which normally does not exist. Furthermore skip connections are introduced between E_\theta and D_\theta, which complicate the ELBO further. The authors should explicitly write out the loss function they are optimizing at this point or describe how they are modifying ELBO to justify these chocise.

Overall, I find it difficult to call this a variational autoencoder given the liberal modifications to the evidence lower-bound. However, even if I was to interpret this work as an autoencoder with a custom loss function, this model ends up very similar to that in Zhang et al with the main differences being the inclusion of an equivariance constraint in Zhang that is not present in this paper and that Zhang et al use a feature map that’s multiplied by landmarks to incorporate appearance information whereas this paper uses the z representation as appearance information. 

The qualitative results of this paper, although good looking, are very similar to qualitative results in Zhang et al and Jakab et al. A quick comment: in Figure 6, you should use the same celebrity faces when comparing Jakab against your own work. The quantitative results only compare to the same model trained without the KL loss term; this, in my mind, is more of a sanity check than a fair baseline. The authors should be comparing against alternate strategies that incorporate spatial information, such as Zhang et al and Jakab et al.

[1] Finn, Chelsea, et al. ""Deep spatial autoencoders for visuomotor learning."" 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.
[2] Chen, Xi, et al. ""Variational lossy autoencoder."" arXiv preprint arXiv:1611.02731 (2016).
[3] http://ruishu.io/2018/03/19/bernoulli-vae/
[4] van den Oord, Aaron, et al. ""Conditional image generation with pixelcnn decoders."" Advances in Neural Information Processing Systems. 2016.",3
"This paper presents a multilingual NLP model which performs very well on a target language with any leveraging labeled data. The authors evaluated their framework on there different tasks: slot filling, named entity recognition and text classification. Overall, the results look very promising.
- Strengthens:
+ The proposed idea is novel.
+ The results are very good for all three tasks.
- Weaknesses:
+ The authors claimed that their model knows what to share. However, they did not provide any evidence proving this hypothesis. Only the experimental results are not enough.
+ The paper also lacks an analysis to show to some extent what the model learned, e.g. the attention weights or the value of the gate. Is there any correlation between the similarity among languages (source and target) and the attention weights.
- What are not clear:
+ It is not clear to me what exactly has been done with the CharCNN embeddings in Section 4.2? How did the authors train the embeddings (only with the source languages or also with the target language)? It seems to me that the proposed model did not work well in this case. ",6
"My main reservation with this paper is the limited novelty. The approach seems to be a rather direct application of a subset of the sluice network architecture in [0] - which has been available on ArXiV since 2017 - with MUSE pre-trained embeddings. In particular, I don’t think the claim that the authors “propose the first zero-resource multilingual transfer learning model” is necessary - and I think it is way too strong a claim. Training an LSTM on English data with MUSE/vecmap embeddings is pretty standard by now, and this does not require any target language training data or cross-lingual supervision either. See zero-shot scenarios in [1-2], for example.

Apart from that, I think the write-up is nice, the approach makes a lot of sense, and results are impressive. I would have liked to see a bit more analysis. In particular, the fact that you learn gate values, makes it easy to analyze/visualize what and how your networks learn to share. 

I think there’s a few baselines in between BWE and MAN, e.g., simple adversarial training and adversarial training with GradNorm [3], that would put your results in perspective. Finally, I would like to encourage the authors to run experiments with actual low-resource languages: A literature on cross-lingual transfer experimenting with German, Spanish, and Japanese, could end up being very heavily biased. For tasks with data in more languages, consider, for example, POS tagging [4], morphological analysis [5], or machine translation [6]. 

[0] https://arxiv.org/abs/1705.08142
[1] http://aclweb.org/anthology/P18-1074	
[2] http://aclweb.org/anthology/P18-2063
[3] https://arxiv.org/abs/1711.02257
[4] http://universaldependencies.org/
[5] http://unimorph.org/
[6] http://christos-c.com/bible/",5
"This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:

1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.

2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).

3. It demonstrates the value of the proposed approach through an extensive experimental setup.

At the same time, I see two major limitations to the paper:

1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.

2. The authors claim that: ""To our best knowledge, this work is the first to propose
an unsupervised CLTL framework without depending on any cross-lingual resource""

This is, unfortunately, not true. I refer the authors to the paper:

Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.

In their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.",6
"The paper describes a new loss function for training, that can be
used as an alternative to maximum likelihood (cross entropy), or
as a metric that is used to fine-tune a model that is initially
trained using ML.

Experiments are reported on the WMT 2014 English-German and
English-French test sets.

I think this is an idea worth exploring but overall I would not
recommend acceptance. I have the following reservations:

* I found much of the motivation/justification for the approach
unconvincing - too heuristic and informal. What does it mean
to ""overgeneralize"" or ""plunge into local optima""? Can we say
anything semi-formal about this alternative objective? 

* The improvements over ML are marginal, and there are a lot of moving
parts/experimental settings in these models, i.e., a lot of
tweaking. The results in tables 2 and 3 show a 0.36/0.34 improvement
over ML using DSD. (btw, what is meant by ""DSD-deep"" or ""ML-deep""? I'm
not sure these terms are explained?)

* The comparison to related work is really lacking. The ""Attention is
all you need"" paper (Vaswani et al.) reports 28.4/41.0 BLEU for these
test sets, respectively 3.4/5.96 BLEU points better than the results
in this paper. That's a huge gap. It's not clear that the improvements
(again, less than 0.5 BLEU points) will remain with a state-of-the-art
system. And I think the paper is misleading in how it cites previous
results on these data sets - there is no indication in the paper that
these better results are in the literature.

Some small things:

* unplausible -> implausible

* ""Husz (2015) showed that D(P || Q) is not identical to its inverse form
D(Q || P)"" this is well known, predating 2015 for sure.
",3
"This paper presents a new loss objective for NMT. The main idea is to optimize an interpolation of KL(P|Q) and KL(Q|P), which is Kulback-Liebler Divergence computed at the word-level for model distribution Q and true distribution P. The motivation is that KL(P|Q) finds a Q that covers all modes of the data whereas KL(Q|P) finds a Q that concentrates on a single mode. So optimizing on the interpolation gets the best of both worlds. In my opinion, this is a relatively simple and known idea in ML (but perhaps not in MT? I'm not sure.) On the other hand, the NMT experiments are well-implemented and convincingly shows that it improves BLEU on a WMT dataset. 

In general, the experiments look solid. I applaud the multiple baseline implementations, in particular even including the SMT baseline. The lack of transformer/CNN models is not a demerit in my opinion, since the focus is on loss objectives and the LSTM models are just as reasonable. 

The paper is clearly written, with a few exceptions. It is not clear why you have to first train with ML before switching to the proposed DSD objective. As such, Section 4.5 should be prefaced with a motivation. Also, Figure 3 is hard to read with the two kinds of plots -- maybe split into two figures? 

An open question is: does your model capture the issues of mode covering as mentioned in the motivation? It would be helpful to include analyses of the word-level distributions to quantify the differences (e.g. word entropy) between ML and various KL/DSD solutions. Also I would recommend showing train/test set perplexity scores of the various proposed and baseline methods. 

As a minor point for argumentation: it is not clear that your proposal addresses the sequence-level loss vs word-level loss issue. It is conceivable, but it seems indirect and there is no quantifiable connection between the word-level loss (such as DSD) and a sequence-level loss. Or is there? 
",6
"This paper describes an alternative training objective to cross-entropy loss for sequence-to-sequence models. The key observation is that cross-entropy is minimizing KL(P|Q) for a data distribution P and a model distribution Q; they add another loss that minimizes the inverse KL(Q|P) to create their dual-skew divergence. The idea is tested in the context of neural MT, using a model similar to that proposed by Bahdanau et al. (2015) with results on English-to-French and English-to-German WNT 2014. In the context of beam search, improvements are small (<=0.5 BLEU) but statistically significant.

This is an interesting idea, and one I certainly wouldn’t have thought of on my own, but I think it is currently lacking sufficient experimental support to warrant publication. The paper feels strangely dated, with most experiments on two-layer models, and only two citations from 2017. The experiments compare against an in-house maximum likelihood baseline (varying greedy-vs-beam search and model depth), and against a number of alternative training methods (minimum risk, scheduled sampling, RL) with numbers lifted from various papers. These latter results are not useful, as the authors (helpfully) point out that the baseline results in this paper are universally higher than the baselines from these other papers. Furthermore, it feels like methods designed to address exposure bias and/or BLEU-perplexity mismatch are not the right comparison points for this work, as it does not attempt to address either of these issues. I would instead be much more interested to see a comparison to label smoothing (Szegedy et al., 2015), which perhaps addresses some of the same issues, and which produces roughly the same magnitude of improvements. Also, the literature review should likely be updated to include Edunov et al., 2017. In general, the improvements are small (though technically statistically significant), the baseline models are somewhat shallow and the deltas seem to be decreasing as model depth grows, so it is hard to get too excited.

Smaller concerns:

For Table 1, it would be helpful to explain why Baseline is not equal to \Beta=1. With some effort, I figured out that this was due to the alpha term modifying the cross-entropy objective when \Beta=1.

It would also be useful to tell us what “switching point” was used for Table 1 and Figure 2.

Christian Szegedy, Vincent Vanhoucke, SergeyIoffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. http://arxiv.org/abs/1512.00567.

Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to sequence learning. In Proceedings of NAACL-HLT 2018.",5
"
[Summary]
This paper proposed an algorithm for zero-shot translation by using both dual learning (He et al, 2016) and multi-lingual neural machine translation (Johnson et al 2016). Specially, a multilingual model is first trained following (Johnson et al 2016) and then the dual learning (He et al 2016) is applied to the pre-trained model using monolingual data only. Experiments on MultiUN and WMT are carried out to verify the proposed algorithm. 

[Details]
1.	The idea is incremental and the novelty is limited. It is a simple combination of dual learning and multilingual NMT. 

2.	Many important multilingual baselines are missing. [ref1, ref2]. At least one of the related methods should be implemented for comparison.

3.	The Pseudo NMT in Table 3 should also be implemented as a baseline for MultiUN experiments for in-domain verification.

4.	A recent paper [ref3] proves that using more monolingual data will be helpful for NMT training. What if using more monolingual data in your system? I think using $1M$ monolingual data is far from enough.

5.	What if using more bilingual sentence pairs? Will the results be boosted? What if we use more language pairs?

6.	Transformer (Vaswani et al. 2017) is the state-of-the-art NMT system. At least one of the tasks should be implemented using the strong baseline.

[Pros] (+) A first attempt of dual learning and multiple languages; (+) Easy to follow.
[Cons] (-) Limited novelty; (-) Experiments are not enough.

References
[ref1] Firat, Orhan, et al. ""Zero-resource translation with multi-lingual neural machine translation."" EMNLP (2016).
[ref2] Ren, Shuo, et al. ""Triangular Architecture for Rare Language Translation."" ACL (2018).
[ref3] Edunov, Sergey, et al. ""Understanding back-translation at scale.""EMNLP (2018). 

I am open to be convinced.

==== Post Rebuttal ===
Thanks the authors for the response. I still have concerns about this work. Please refer to my comments ""Reply to the rebuttal"". Therefore, I keep my score as 5.

",5
"This paper can be considered as a direct application of dual learning (He et al. (2016)) to the multilingual GNMT model. The first step is to pre-train the GNMT model with parallel corpora (X, Z) and (Y, Z). The second step is to fine-tune the model with dual learning.

1. I originally thought that the paper can formulate the multilingual translation and zero dual learning together as a joint training algorithm. However, the two steps are totally separated, thus the contribution of this paper is incremental. 

2. The paper actually used two parallel corpora. In this setting, I suggest that the author should also compare with other NMT algorithm using pivot language to bridge two zero-source languages, such as ``A Teacher-Student Framework for Zero-Resource Neural Machine Translation``. It is actually unfair to compare with the completely unsupervised NMT, because the existence of the pivot language can enrich the information between two zero-resource languages. The general unsupervised NMT is often considered as ill-posed problem. However, with parallel corpus, the uncertainty of two language alignment is greatly reduced, making it less ill-posed. The pivot language also plays the role to reduce the uncertainty.",4
"Pros:
- The paper address the problem of zero-shot translation. The proposed method is essentially to bootstrap a Dual Learning process using a multilingual translation model that already has some degree of zero-shot translation capabilities. The idea is simple, but the approach improves the zero-shot translation performance of the baseline model, and seems to be better than either pivoting or training on direct but out-of-domain parallel data.
- The paper is mostly well written and easy to follow. There are some missing details that I've listed below.

Cons:
- There is very little comparison to related work. For example, related work by Chen et al. [1], Gu et al. [2] and Lu et al. [3] are not cited nor compared against.

Misc questions/comments:
- In a few places you call your approach unsupervised (e.g., in Section 3: ""Our method for unsupervised machine translation works as follows: (...)""; Section 5.2 is named ""Unsupervised Performance""). But your method is not unsupervised in the traditional sense, since you require lots of parallel data for the target languages, just not necessarily directly between the pair. This may be unrealistic in low-resource settings if there is not an existing suitable pivot language. It'd be more accurate to simply say ""zero-shot"" (or maybe ""semi-supervised"") in Section 3 and Section 5.2.
- In Section 3.1 you say that your process implements the three principles outlined in Lample et al. (2018b). However, the Initialization principle in that work refers to initializing the embeddings -- do you pretrain the word embeddings as well?
- In Section 4 you say that the ""UN corpus is of sufficient size"". Please mention what the size is.
- In Section 4.2, you mention that you set dropout to p=0.65 when training your language model -- this is very high! Did you tune this? Does your language model overfit very badly with lower dropout values?
- In Section 5.2, what is the BLEU of an NMT system trained on the es->fr data (i.e., what is the upper bound)? What is the performance of a pivoting model?
- In Section 5.3, you say you use ""WMT News Crawl, all years."" Please indicate which years explicitly.
- In Table 3, what is the performance of a supervised NMT system trained on 1M en-fr sentences of the NC data? Knowing that would help clarify the impact of the domain mismatch.
- minor comment: in Section 4.3 you say that you trained on Tesla-P100, but do you mean Pascal P100 or Tesla V100?

[1] Chen et al.: http://aclweb.org/anthology/P17-1176
[2] Gu et al.: http://aclweb.org/anthology/N18-1032
[3] Lu et al.: http://www.statmt.org/wmt18/pdf/WMT009.pdf",6
"Overall Score: 7/10.
Confidence Score: 7/10.

Detailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations.

Related to: RNN models and Sparse Nystrom approximation.

Strengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.

Weaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists.

Does this submission add value to the ICLR community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies.

Quality:
Is this submission technically sound?: Yes it is.
Are claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way.
Is this a complete piece of work or work in progress?: Complete piece of work.
Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest.

Clarity:
Is the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before.
Is it well organized?: Yes it is.
Does it adequately inform the reader?: Yes it is.

Originality:
Are the tasks or methods new?: Yes, they are sound.
Is the work a novel combination of well-known techniques?: Yes it is.
Is it clear how this work differs from previous contributions?: Yes.
Is related work adequately cited?: Yes, being a strength of the paper.

Significance:
Are the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems.
Are others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes.
Does the submission address a difficult task in a better way than previous work?: Yes I think.
Does it advance the state of the art in a demonstrable way?: Yes, empirically.

Arguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done

Arguments against acceptance: Clarity of the paper.

Minor issues and typos:
-> (V)SS not defined before being used.
-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions.
-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.
-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.
-> Q is not defined in section 3.1 paragraph 1.
-> A valid covariance function must produce a PSD matrix, put that in section 3.1. 
-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).
-> Section 3.4 statistics should be explained.

Reading thread and authors response rebuttal decision:
=================================================

I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.",7
"This paper proposes deep recurrent GP models based on the existing DRGP framework, two works on sparse spectrum approximation as well as that of inducing points. In these models, uncertainty is propagated by marginalizing out the hidden inputs at every layer.

The authors have combined a series of known ideas in the proposed work. There is a serious lack of discussion or technical insights from the authors for their technical formulations: in particular, what are the non-trivial technical challenges addressed in the proposed work? Furthermore, the authors are quite sloppy in referencing equations and inconsistent in the use of their defined notations and acronyms. I also find it hard to read and understand the main text due to awkward sentence structures.

Have the authors revealed their identity on page 2 of the paper? I quote: ""We refer to the report Foll et al. (2017) for a detailed but preliminary formulation of our models and experiments."" and ""DRGP-(V)SS code available from http://github.com/RomanFoell/DRGP-VSS.""



Detailed comments are provided below:

For the first contribution stated by the authors, what are the theoretical and practical implications of the different regularization terms/properties between the lower bounds in equations 10 vs. 8? These are not described in the paper.

Can the authors provide a detailed derivation of DVI for equation 13 as well as for the predictive distributions in Sectio 6.3.5?

Can the authors provide a time complexity analysis of all the tested deep recurrent GPs?


Would the authors' proposed approach be able to extend the framework of Hoang et al. (2017) (see below) that has generalized the SS approximation of Lazaro-Gredilla et al. (2010) and the improved VSS approximation of Gal & Turner (2015)?

Hoang, Q. M.; Hoang, T. N.; and Low, K. H. 2017. A generalized stochastic variational Bayesian hyperparameter learning framework for sparse spectrum Gaussian process regression. In Proc. AAAI, 2007–2014.



Minor issues:
Just below equation 6, equation 9, and throughout the entire paper, the authors need to decide whether to italicize their notations in bold or not.

Equations are not properly referenced in a number of instances.

The authors have used their commas too sparingly, which makes some sentences very hard to parse.

What is the difference between REVARB-(V)SS(-IP), DRGP-(V)SS(-IP), and DRGP-VSS-IP?

Equation 7: LHS should be conditioned on U.
Page 4:  (V)SSGP does not have the same...
Equation 8: q_a and q_Z should be placed next to the expectation.
Page 4: choosen?
Page 5: will makes it possible?
Page 5: DRGP-SSGP, -VSSGP, -SSGP-IP, -VSSG-IP?
Page 5: to simplify notation, we write h^{L+1}_{Hx+1:} = y_{Hx+1:}? Such a notation does not look simplified.

Equation after equation 12: On LHS, should U^(l) be a random variable?

Page 17: Should the expressions begin with >=?
",5
"This paper addresses the problem of modeling sequential data based on one of the deep recurrent Gaussian process (DRGP) structures proposed by Mattos et al (2016). This structure acts like a recurrent neural net where every layer is defined as a GP. One of the main limitations of the original method proposed by Mattos et al (2016) is that it is limited to a small set of covariance functions, as the variational expectations over these have to be analytically tractable.

The main contributions of this paper are the use of previously proposed inference, namely (i) the sparse spectrum (SS) of Lazaro-Gredilla et al (2010); its variational improvement by Gal and Turnner (2015) (VSS);  and the inducing-point (IP) framework of Titsias and Lawrence (2010) into the recurrent setting of Mattos et al (2016). Most (if not all) of the technical developments in the paper are straightforward applications of the results in the papers above. Therefore, the technical contribution of the paper is largely incremental. Furthermore, while it is sensible to use random-feature approximation approaches (such as SS and VSS) in GP models, it is very unclear why combining the IP framework with SS approaches makes any sense at all. Indeed, the original IP framework was motivated as a way to deal with the scalability issue in GP models, and the corresponding variational formulation yielded a nice property of an additional regularization term in the variational bound. However, making the prior over a (Equation 9) conditioned on the inducing variables U is rather artificial and lacks any theoretical justification. To elaborate on this, in the IP framework both the latent functions (f in the original paper) and the inducing inputs come from the same GP prior, hence having a joint distribution over these comes naturally. However, in the approach proposed in this paper, a is a simple prior over the weights in a linear-in-the-parameters model, and from my perspective, having a prior conditioned on the inducing variables lacks any theoretical motivation. 

The empirical results are a bit of a mixed bag, as the methods proposed beat (by a small margin) the corresponding benchmarks on 6 out of 10 problems. While one would not expect a proposed method to win on all possible problems (no free lunch), it will be good to have some insights into when the proposed methods are expected to be better than their competitors. 

While the proposed method is motivated from an uncertainty propagation perspective, only point-error metrics (RMSE) are reported. The paper needs to do a proper evaluation of the full predictive posterior distributions. What is the point of using GPs otherwise?

Other comments:
I recommend the authors use the notation p(v) = … and q(v) = … everywhere rather than v ~ … as the latter may lead to confusion on how the priors and the variational distributions are defined. 
It is unnecessary to cite Bishop to explain how one obtains a marginal distribution
Would it be possible to use the work of Cutajar et al (2017), who use random feature expansions for deep GPs,  in the sequential setting? If so, why aren’t the authors comparing to this?
The analysis of Figure 1 needs expanding 
What are the performance values obtained with a standard recurrent neural net / LSTM?
",5
"I really liked this paper and believe it could be useful to many practitioners of NLP, conversational ML and sequential learning who may find themselves somewhat lost in the ever-expanding field of dynamic neural networks.

Although the format of the paper is seemingly unusual (it may feel like reading a survey at first), the authors propose a concise and pedagogical presentation of Jordan Networks, LSTM, Neural Stacks and Neural RAMs while drawing connections between these different model families.

The cornerstone of the analysis of the paper resides in the taxonomy presented in Figure 5 which, I believe, should be presented on the front page of the paper. The taxonomy is justified by a thorough theoretical analysis which may be found in appendix.

The authors put the taxonomy to use on synthetic and real data sets. Although the data set taxonomy is less novel it is indeed insightful to go back to a classification of grammatical complexity and structure so as to enable a clearer thinking about sequential learning tasks. 

An analysis of sentiment analysis and question answering task is conducted which relates the properties of sequences in those datasets to the neural network taxonomy the authors devised. In each experiment, the choice of NN recommended by the taxonomy gives the best performance among the other elements presented in the taxonomy.

Strength:
o) The paper is thorough and the appendix presents all experiments in detail. 
o) The taxonomy is clearly a novel valuable contribution. 
o) The survey aspect of the paper is also a strength as it consolidates the reader's understanding of the families of dynamic NNs under consideration.

Weaknesses:
o) The taxonomy presented in the paper relies on an analysis of what the architectures can do, not what they can learn. I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks (in particular RNNs) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning. I believe that mentioning this issue along with older (http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf) and more recent (e.g. http://proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https://arxiv.org/pdf/1803.00144.pdf) papers on the topic is necessary for the paper to present a holistic view of the matter at hand.
o) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition, in particular for the sentiment analysis task. It is not clear enough in my view that it is true that ""since the goal is to classify the emotional tone as either 1 or 0, the specific contents of the text are not very important here"". One could argue that a single word in a sentence can change its meaning and sentiment.
o) The written could be more polished.

As a practitioner using RNNs daily I find this paper exciting as an attempt to conceptualize both data set properties and dynamic neural network families. I believe that the authors should address the shortcomings I think hinder the paper's arguments and exposition of pre-existing work on the analysis of dynamic neural networks.",7
"Summary
=========
The paper analyses the taxonomy over memory-based neural networks, in the decreasing order of capacity: Neural RAM to Neural Stack, Neural Stack to LSTM and LSTM to vanilla RNN. The experiments with synthetic and NLP datasets demonstrate the benefits of using models that fit with task types.  

Comment
========
Overall, the paper is well written and presents interesting analysis of different memory architectures. However, the contribution is rather limited. The proposed taxonomy is not new. It is a little bit obvious and mentioned before in [1] (Unfortunately, this was not cited in the manuscript). The theorems on inclusion relationship are also obvious and the main contribution of the paper is to formally show that in mathematical forms.  The experiments on synthetic tasks give some insights into the models’ operations, yet similar analyses can be found in [2, 3]. To verify the models really learn the task, the authors should include tests on unseen sequence lengths.  There remains questions unexplained in NLP tasks such as why multi-slot memory did not show more advantages in Movie Review and why Neural Stack performed worse than LSTM in bAbI data.  

Minor potential errors: 

In Eq. (6), r_{t-1} should be r_t   

The LSTM presented in Section 3.2 is not the common one. Normally, there should be x_t term in Eq. (3) and h_t=g_{o,t}*\tanh(r_t) in Eq. (6). The author should follow the common LSTM formulas (which may lead to different proofs) or include reference to their LSTM version.  

[1] Yogatama et al. Memory Architectures in Recurrent Neural Network Language Models. ICLR’18 

[2] Joulin et al. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS’15 

[3] Graves et al. Neural Turing Machines. arXiv preprint arXiv:1410.5401 (2014). ",5
"The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks.

Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g.

""LSTM: state memory and memory of a single external event""

to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs & DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. 

Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). ",3
"The authors applied the external memory module proposed by Graves et al. (2016) to the image segmentation task. SHAMANN is an extension to allow memory sharing between directions. 

Authors claimed that one of the contributions is a reformulation of the semantic segmentation problem as a sequence learning task.
There are many previous works done in this direction,
- ""Multi-Dimensional Recurrent Neural Networks"", 2007
- ""Scene Labeling with LSTM Recurrent Neural Networks"", 2015
- ""ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation"", 2016
- ""Robust, Simple Page Segmentation Using Hybrid Convolutional MDLSTM Networks"", 2017 
and many more.
Authors should compare with those LSTM-based image segmentation approaches as well.

Their second contribution is a network with a shared external memory module between directions. However, the experiments are not enough to show the benefits of it. See the details below.

Handling long-range dependencies:
- In Section 3.3.2, authors mentioned that ""One limitation of Bi-LSTM is that the number of network parameters grows proportionally to the memorization capacity, making it unsuitable for sequences with long-range dependencies."". 
However, the experiments are not with long range sequences: 169 sequence length for X-ray dataset and 49 length for MNIST. A classic LSTM (not bi-directional) is known to handle up to 200 timesteps. Some comparison/analysis of handling long-range dependencies of Bi-LSTM, Bi-MANN, and SHAMANN are needed (ideally on high-resolution real images).

Dataset:
-  Authors compared 3 models only on MNIST. The structure on MNIST is simple, and the resolution of images is small to show the benefit of using (shared) external memory module instead of individual memory cells. It is not surprising that the reported performance difference is small. Authors could have reported such a comparison on X-ray dataset too but they did not. I would recommend authors pick another high-resolution real-image dataset and compare the performance of these 3 models.  

Additional comparisons:
- Various patch size
- Longer sequence length
- Especially a trade-off between the patch size and the sequence length on the high resolution images (larger patch size with a shorter sequence length or shorter patch size with a longer sequence length)

- A comparison of Bi-LSTM with sharing weights will also be a good baseline. 
",4
"The authors present a model for semantic segmentation. The proposed method casts the full image segmentation as a sequence of local segmentation predictions. The image is split in multiple patches and processed sequentially in some order. A shared memory allows the local patch predictions to propagate information to improve other patch predictions which is necessary for resolving ambiguities.  They show a set of results on an XRay segmentation dataset with a reasonable ablation and baseline study. As well as a somewhat unclear result on image completion. The paper is well written, mostly clear and novel to the best of my knowledge.

pros:
- semantic segmentation is clearly very important problem with many applications
- the method seems clean and promising
cons:
- the segmentation community is much more familiar with MS-COCO and VOC. I think results on those datasets will make the paper much more impactful and clear any doubts about the method.
- it is not clear what processing order the patches are processed in. Does that matter ? This should be clearer in the paper.
- there is a brief mention of multiple actors but it seems to me its just one Bi-MANN actor is that true ? 
- sec. 4.2 is very surprising to me. From what is written I understand that an MNIST classifier is trained on the original MNIST dataset and that it still works to 56% on the test set with the bottom blanked out. Is that correct ? What architecture is this ? Also I find it very surprising that you can recover accuracy to 96% without seeing the trained classifier at all. Anything that can help me understand how that is possible would be appreciated. Are you aware of anyone else matching these results in the literature ?",5
"Summary:
The paper proposes a system of semantic segmentation based on sequential processing of the image in a patch-wise manner with multiple ""actors"", sharing a common external memory. This approach stands in contrast to the more usual approach of single-shot prediction for the whole image, where encoder-decoder architectures or dilated convolutions are used to capture the global context. The authors then discuss three-variants of this method, out of which two use external memory (Bi-MANN, SHAMANN), and one uses memory shared between actors (SHAMANN). Results are presented on segmentation of lung X-ray data and on MNIST digit completion.

Comments:
The paper is easy to read. The authors cite the relevant literature on the baseline semantic segmentation methods, as well as neural networks with external memories. However, similar patch-wise and sequential methods have been presented in the literature (e.g. https://arxiv.org/abs/1506.07452), including ones with external storage (e.g. https://www.nature.com/articles/s41592-018-0049-4), but these are not discussed as prior work.

Overall, the proposed approach is interesting, but significantly more complex than both the baselines and prior work. As is, the experimental results are not compelling enough to justify this (lack of clear quantitative improvement over state of the art). My recommendation would be to conduct additional experiments on semantic segmentation benchmark datasets. The proposed method seems promising for volumetric data as the authors note, but this also needs to be demonstrated experimentally.

Some more specific & technical questions follow:
- In Table 1, how is the confidence interval for the Dice score computed?
- Have any experiments been done with more than 2 actors?
- How exactly is the patch sequence formed, i.e. what is the spatial order of the patches? How much to the results depend on this order, if at all?
- In the discussion on page 6, it seems to be implied that the reduced parameter count should allow more efficient application to volumetric data. This is a bit surprising, since with modern networks it is usually the input size that is limiting, not the number of network parameters.
- Have experiments with Bi-MANN and Bi-LSTM been done on the X-ray segmentation data? How do the results compare to SHAMANN?
- How does the inference and training time compare to the baseline methods?",4
"The overall contribution makes sense. Consider solving a linear system i.e., learning an unknown matrix. Splitting it into two components (like in NMF or MMF) and learning each separately gives more control on the conditioning of the matrices. This is the basis of residual networks (at least the theory for linear resnets). Within this, the technical/theoretical results presented in the paper are sensible. Couple of issues: 
1) Where are we breaking the slow/fast learners in terms of the depth of the network? I.e., How many of the layers are slow? Does this break point influence the overall convergence? 
2) It is unclear what the aim of simulations is? The reported figures are not conveying useful information. It makes sense to do a repeatability experiment here with multiple sets of simulated datasets. 
3) Put confidence intervals on the results (table/figure). 
4) What is the nature and choice of g()? The evaluations uses LSTM but will the structure of g() influence the rate of learning? 
5) The authors should choose a better reference than miracle for the ",5
"[Summary:]
This paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce “miracle representations” and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.

[Strengths:]
The paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. 

[Weaknesses:]
- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].
- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. 
- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].

[Specific comments:]
- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. 
-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.
-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.
-The use of the term “miracle” keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one “miracle X” was defined and used as these are all essentially saying the same thing.

References
[1]https://arxiv.org/abs/1803.02999
[2]https://arxiv.org/abs/1801.05558

",5
"[Summary]
The paper presents a novel learning framework for meta-learning that is motivated by neural learning process of human over long periods. Specifically, the process of meta-learning is divided into a slow and a fast learning modules, where the slowly-learnt component accounts for low-level representation that is progressively optimized over all data seen so far to achieve generalization power, and the fastly-learnt component is supposed to pick up the target in a new task for quick adaptation. It is proposed that meta-learning should focus on capturing the meta-information for the fast learning module, and leave the slow module being updated steadily without task-specific adaptation. Theoretical analysis is presented on a linear MLP examples to shed some light on the properties of the proposed algorithm. Results on both synthetic dataset and benchmarks justify the theoretical observation and advantages.               

Pros
Novel treatment and formulation of meta-learning from the perspective of fast and slow  learning process
Cons
Some interesting cases not tested
Presentation could be improved 

[Originality]
The paper approaches the recently popular meta-learning from a novel perspective by decomposing the learning process into slow and fast ones. 

[Quality]
Overall,  the paper is well motivated and implemented with both theoretical study and empirical justification. There are a few questions / areas for further improvements, though:
- It seems that to initialize the slow module, another set of data is needed to pretrain it before the actual meta-learning takes place to learn to optimize the fast learner (as opposed to other meta-learning methods where all parameters in a base model were meta-learnt over the meta training set). How does this affect the performance? E.g., what if the slow module is only updated over the meta-training set (still without reinitialization across different batches) without pre-training?
- In the current formulation, the base model is decomposed into two distinct (slow and fast) modules. What is the rule to decide which layers should belong to slow or fast modules? How does different choice affect the performance? Can we decompose the base model into finer granularities for different learning behaviors? E.g., a third module module in-between the fast and slow ones that follows medium learning pace.          
- The theoretical study can be better organized. The proofs can be left in appendix to make room for more discussion on conclusions, non-linear and / or non-Gaussian cases.     
- The write-up can be improved too at some places: proper reference at line 4 of section 1 is missing; \phi in (1) is not well defined, as well as “SOA” in section 2;

[Clarity]
The paper is generally clearly written, with a few places to improve (see comments above).

[Significance]
The paper brings in an interesting perspective to meta-learning. It can also inspire more follow-up work to better understand the problem.   
",6
"In this paper, the author(s) propose a method, invariant feature learning under optimal classifier constrains (IFLOC), which maintains accuracy while improving domain-invariance. Here is a list of suggestions that will help the author(s) to improve this paper.
1.The paper explains the necessity and effectiveness of the method from the theoretical and experimental aspects, but the paper does not support the innovation point enough, and the explanation is too simple.
2.In this paper, Figure3-(b) shows that the classification accuracy of IFLOC-abl method decreases a lot when γ is taken to 0. Figure3-(c) shows that the domain invariance of IFLOC-abl method becomes significantly worse when γ is 10. The author(s) should explain the reasons in detail.
3. The lack of analysis on domain-class dependency of each dataset makes the analysis of experimental results weak.
",4
"This paper proposed to address domain generalization under inter-dependence of domains and classes. It motivates a new regularization term by analyzing an existing work, DAN. It shows that this term can improve the generalization performance when the classes and domains are not independent. Experiments are extensive and supportive. 

I do not have many comments about this paper. It was a joy to read. The proposed idea is well motivated. It is simple and seems like effective. Experiments are extensive. 

While the regularization term is motivated by analyzing DAN, it would be nice to discuss its application to other domain adaptation/generalization methods. What is even better is to show its effectiveness on another method in the experiments.",7
"The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \= p(y).   Figure 1 is used to illustrate this issue. 

If the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \= p(y|x_T)  ), the optimal invariance can lead the same generalization problem.   Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability.  It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. 

It is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. 

Reference:
[1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014.
[2]Conditional Adversarial Domain Adaptation, NIPS 2018",5
"The paper is addressing the problem of a specific multi-task learning setup such that there are two tasks namely main task and auxiliary task. Auxiliary task is used for the sole purpose of helping the main one. In other words, auxiliary task performance is not of interest. The simple and sensible approach proposed in the paper is using cosine similarity between the gradients of two loss functions and incorporating the auxiliary one if it is positively aligned with the main gradient. Authors suggest to further scale loss functions using the cosine similarity but it only experiments with the simpler case of binary decision of using both gradients or only the main one. Authors provide a convergence guarantee (without any convergence rate) by simply extending the convergence of gradient method.

The paper is definitely addressing an important problem as the authors cite many previous work which uses the setup of set of auxiliary tasks helping a main one. The method is simple and easy to implement. Hence, it has a potential to be useful for the community.

One major issue for me is the experimental setup. The authors cite many interesting, realistic and practical setups (Zhang et al., 2016; Jaderberg et al., 2017; Mirowski et al., 2017; Papoudakis et al., 2018), but do not use any of these setups in their experiments. Instead, paper uses set of toy experiments. This is very puzzling to me as all these papers set existing baselines for interesting problems which authors can easily compare. I think the paper needs to be experimented and compared with these established methods.

Another major issue is the weak multi-task learning baseline used in the paper. There have been many interesting developments in adaptive scaling of multiple loss functions in the literature. However, paper does not compare with them. Example of these methods are: [GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks, ICML 2018] and [Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics, CVPR 2018]. Although these methods addresses the case of all tasks being important, it is a valid baseline and need to be compared. Similar to my first points, these papers also use very realistic and interesting experiments which would fit better than the toy experiments in the paper.

Final major issue is the fact that experimental results are suggesting the method is not effective. In ImageNet experiment, auxiliary tasks actually hurt the final performance as the single task is better than all methods including the proposed one. Proposed method does not guarantee that auxiliary tasks will have no harm. The GridWorld experiment is sort of a sanity check to me as it is very hand-crafted. For Breakout experiment, single task actually outperforms all baselines and this means the proposed method results in a harm similar to ImageNet case. For Breakout+MSPacMan experiment, multi task and the proposed method performs almost exactly same. I do not get why the performance on Breakout is relevant for this case since it is not a main task. The paper clearly states that only performance of an interest is the main one which is MSPacMan in this case. Also, in this experiment clearly all methods are still learning as the curve did not plateau yet. I am curious, why the learning is stop there. I do not think we need the method to be effective to be published; but, the negative result should be explained properly.

MINOR NITPICKS
- Algorithm 1&2 are crucial to understand the paper, they should be in main text
- ImageNet class IDs change between years. So, actual wordnet IDs or class names is a better thing to state
- What happens if there are multiple auxiliary tasks?
- Does the theory still hold for loss functions which are not Lipschitz as the Cauchy's gradient method requires that for convergence
In summary, the paper is proposing a sensible method for an important problem. However, it is only tested for toy problems although there are interesting existing setups which would be ideal for the method to be tested. Moreover, it is only compared with the most-naive multi task learning baselines. Even this limited experimental setup does not confirm what the paper is claiming (using auxiliary tasks only when they help). And the paper fails to explain this failure cases. The method needs to be experimented with a more realistic setup with more realistic baselines.

------
After rebuttal:

I gave detailed responses to each part of the rebuttal below. Here is the summary:

Although the response addresses some of my concerns. There are still major issues with the experimental study. 1) there are existing, relevant and well-studied multi-task setups with negative interference. Method should be experimented with some of those setups. 2) Multi-task baseline in the paper is naive and far from state-of-the-art. Paper need strong baselines as discussed. Hence, I am keeping my score. Paper needs to be improved with a stronger experimental study and need to be re-submitted.",4
"The paper studies the problem of how to measure the similarity between an auxiliary task and the target tasks, and further decide when to use the auxiliary loss in the training epoches. The proposed cosine simiarity based soft gradient update scheme seems reasonable. The author(s) also experiment the proposed method on three tasks, one supervised learning image classification task, two reinforcement learning tasks, and show improved results respectively.

The paper is in generally well-written. However it would be great if the concerns below could be addressed or discussed in the paper.

1) The proposed method is based on the intuition: if the gradients of the target and auxiliary loss are in the same direction, the auxiliary loss will help the main/target task. Some examples are showed in the paper to support this argument, however it would be helpful if there is some theoritical gurantee on this. So a more general question would be: rather than define the similarity measure to measure the gradient similarity of the target and auxiliary loss, it would be more useful to try to learn or define whether the auxiliary task is good for the target task beforehand.

2) In proposition 1, if the concerns in 1) are reasonable, the equation would be doubtful. For example, one can simply try (g(target task)-g(auxiliary task)) in the equation. Besides, more similarity metrics are expected to be compared here to show why cosine is the optimal choice. For example, L2.

3) Too much content is embedded in appendix, for example, it would be helpful to move the two algorithms or at least discussed the two variants of the gradient updates in the experimental section. Since it is not clear to me whether hard cosine mixing or soft cosine mixing is used to produce the results in the image classification task.

4) In the image classification task, a quantitative analysis would be more convincing since the semantics of the near and far is really hard to define. Even the authors can show a vague definition, it will be helpful. In figure 2b), why the cosine method performs worse compared the other methods before 5000 in x-axis? Is this because of the noise of the gradient? Plus, what is the optimizer used in this experiment?

5) In the first reinforcement learning task, since cosine similarity is the only method used to measure the similarity between auxiliary task and the target task, it would be useful to show the comparison among other task relatedness method in reinforcement learning. For 'This is expected as the noise in the gradients make it hard to measure if the two tasks are a good fit or not',  why is this? Since cosine similarity would be zero if the two tasks are not good fit.",6
"The paper proposes a method for using auxiliary tasks to support the optimization with respect to a main task. In particular, the method assumes the existence of a loss function for the main task that we are interested in, and a loss function for an auxiliary task that shares at least some of the parameters with the main loss function. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. The method is demonstrated on image classification and a few reinforcement learning settings.

The idea of the paper is simple, and the method has a nice property of (if ignoring some caveats) guaranteeing steps that are directionally correct with respect to the main task. In that sense it is useful in practice, as it limits the potential damage the auxiliary task does to the optimization of the main task.

As the authors also note, the method suffers from some drawbacks. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. In that sense, the method is no silver bullet. In addition, the method seems fairly computationally expensive (it would be interesting to understand how much it slows down an update, I would assume the added complexity is roughly a constant multiplier). However, as an alternative to naively adding an auxiliary task, the proposed method is a welcome addition to the tool box of practitioners.

Although the experiments presented in the paper are quite different from each other, I would have wished for even more experiments. The reason is that as the method does not guarantee faster convergence, its applicability is mainly an empirical question. Especially experiments where auxiliary tasks have been used before would be interesting to test with the only addition being introducing the method proposed.

The paper is generally well written and the results are fairly clearly presented. As a minor comment, the authors might want to check that articles (such as ""the"") are not missing in the text.

All in all, the main merit of the proposed method is its conceptual simplicity and easy to understand value in practical applications where an auxiliary loss function is available. The method also seems to work well enough in the experiments presented.",6
"This paper presents a variation of dropout, where the proposed method drops with higher probability those neurons which contribute more to decision making at training time. This idea is evaluated on several standard datasets for image classification and action recognition.

Pros:
1. This paper has interesting idea related to dropout, and shows some benefit.
2. Paper is well-written and easy to understand.

Cons:
1. There are many variations in dropouts and they all claim superiority to others. Unfortunately, most of them are not justified properly. Excitation dropout looks interesting and has potential, but its validation is not strong enough. Use of Cifar10/100, Caltech256, and UCF 101 may be okay for concept proofing, but not be sufficient for thorough validation. Also, the reported results are far from the state-of-the-art performance of each dataset. I would recommended to add the idea to the network 
 to achieve the state-of-the-art performance because it will show real extra benefit of ""excitation"" dropout. 

2. There are many variations of dropouts including variational dropout, L0-regularization, and adaptive dropout, and the paper needs to report their accuracy in addition to curriculum dropout.

3. Dropout does not exist in many modern deep neural networks and its usability is a bit weak. It would be better to generalize this idea and make it applicable to ResNet-style networks.

4. There is no clear (theoretical) justification and intuition why excitation dropout improves performance. More ablation study with internal analysis would be helpful.

Overall, this paper has interesting idea but needs more efforts to make the idea convincing.",5
"This is an interesting idea that seems to do better than regular dropout.
However, the experiment seem a bit artificial, starting with less modern network designs (VGG) that can benefit from adding dropout. State of the art computer vision networks don't seem to need dropout so much, so the impact of the paper is unclear.

Section 4.4: How does this compare to state-of-the-art network compression techniques? (Deep compression, etc)
",5
"The authors propose a data-dependent dropout variant that produces dropout candidates based on their predictive saliency / relevance. Results are reported for 4 datasets (Cifar10, Cifar100, Caltech256 and UCF101) and 4 different models (CNN-2, AlexNet, VGG16 and VGG19), and suggest an increase in generalization performance over other dropout approaches (curriculum dropout, standard dropout, no dropout), as well as increase in the network's plasticity as measured by some existing metrics from the literature. The authors conclude that Excitation Dropout results in better network utilization and offers advantages for network compression (in the sense of neuron pruning).

Overall I find the idea to be interesting and fairly novel, and commend the authors for the fluid writing style. However, I find key issues with the testing and experiments. Specifically, the lack of confidence bounds for individual results makes it impossible to determine whether the reported incremental improvements are actually significant over those of existing approaches. Likewise, I criticize the choice of methods the authors have chosen to compare against, as several other data-dependent dropout approaches (e.g. Information Dropout) exist that may be conceptually closer (and therefore more comparable) to the proposed approach. I also question the choice of tested network architectures and the placement of the dropout layer.

The paper could be of high significance if all claims in the paper could be backed up by experiments that show the advantage of Excitation Dropout to be not a random effect. I will therefore give a lower score for the paper in its current form, but am willing to revise my rating the major points below are properly addressed.

Pros:
+ novel mechanism to improve dropout, results seemingly superior over other methods
+ achieves better utilization of network resources and achieves robustness to dropping out neurons at test time

Cons:
- results without error bars, unclear if advantage is significant
- did not compare against most relevant competing methods


MAJOR POINTS
Section 2 - The comparison to Moreiro et al. is not entirely clear. A fairer comparison would be with some of the other methods listed which also focus on answering the question of which neurons to dropout, or approaches which determine the dropout policy based on information gained from the data, such as Information Dropout (Achille & Soatto). The authors state that Morerio et al are the state-of-the-art in dropout techniques, however based on the results presented here (Figure 3) it seems to perform just as well as standard dropout. Perhaps there are architecture-specific or data-specific issues? In any case this example undermines the confidence of the claims.

Section 3.2, equation 3 - is there some theoretical underpinning as to how this equation was modelled, or was it chosen simply because it covers the expected corner cases described in paragraph 4 of this section? Also, given the intuition in this paragraph (e.g. p_EB = 1 / N), it is correct to assume this equation models the dropout probability but only for fully connected layers? What about dropout in convolutional layers? Though some previous statements do point to the usage of dropout predominantly for fully connected layers, I feel that this context is missing here and should be explicitly addressed. The caption to e.g. Table 1 seems to imply the authors add a single dropout layer in one of the fully connected layers, however this begs the question as to why this positioning was chosen - why only one dropout layer, and why precisely at that location? The scope of the claims should be adapted accordingly.

Section 4.2 - ""After convergence, ED demonstrates a significant improvement in performance compared to other methods"". If five trained models were used, then some sense of measure of uncertainty should be given throughout. For example, in the Cifar10 results for Figure 3, it is difficult to say whether the marginal improvement from about 80% (standard dropout and curriculum dropout) to about 82% (excitation dropout) is significant or not. Perhaps this would be less of an issue if the authors had worked with e.g. ImageNet, but for these smaller datasets it would definitely be worth to be on the safe side. I highly suspect that statistically speaking (perhaps with the exception of the results on Caltech256), the effects of all of these dropout variants are indistinguishable from each other. I urge the authors to include a measure of the standard deviation / 95% confidence interval across the models that were tested.

The results presented sub-section 4.3 do not justify the claim that the models trained with Excitation Dropout tend to be more informative. Perhaps the definition of ""informative"" should be expanded upon in length. Can the authors show that the alternative paths learned by the models augmented with Excitation Dropout indeed carry complimentary information and not just redundant information?

Figure 5 shows interesting results, but once again begs the question of whether there is any significant difference between standard dropout and curriculum dropout. I encourage the authors to include confidence bounds for each trace. Likewise, there is an inherent bias in the results, in that the leftmost figure compares EB and CD in the context in which EB was trained, i.e. dropping of ""most salient"" neurons. The comparison is one-sided, however, as no results are reported from the context in which CD was trained, i.e. dropping neurons more frequently as training progresses. Comparing these results would bring to light whether the performance boost see in Figure 5 is a function of ""overfitting"" to the training manner or not.
Also, I believe the results for the second column (dropping out least relevant neurons) are misleading. To the best of my understanding, as p_c increases, at some point neurons start to be dropped that actually have high relevance. This could explain why all curves start out similarly and EB slowly begins to stick out - at this point the EB models once again start to be used in the context within which they were trained, in contrast to the other approaches. The authors should perhaps also explicity clarify why this second column gives any more information than the first.



MINOR POINTS

The authors propose Excitation Dropout as a guided regularization technique. Batch normalization is another standard regularization technique which is often compared with dropout. In particular, for deep CNNs, batch normalization is known to work very well, often better than the standard dropout. In the experiments here, to what extent was batch normalization and / or any other widely utilized network regularizers used? Is it possible that the regularizing effect found here actually comes from one of these? I.e. were the models that were not trained from scratch trained with batch normalization? It would be good if more data could be provided for EB vs. other regularizing techniques, if the claim is that EB is a novel regularizer.

Section 3.1 - ""We choose to use EB since it produces a valid probability distribution for each network layer"". Though this is a nice property, were there any other considerations for choosing the saliency method? Recent work (Adebayo et al, ""Sanity Checks for Saliency Maps"") has shown that even some well established saliency techniques are actually independent from both the model and data. As this approach relies heavily on the correctness of EB, I feel that a further justification should be given to validate its use for this scenario other than just based on the type of output it produces.

Section 3.1, equation 2 - more detail and reasoning should be given as to why connections with negative weights are excluded from the computation of the conditional probability, if possible without referring the reader to the EB paper. Why is this justified? Is this probability modelled for a specific activation function? 

The authors do not provide the details of the CNN-2 architecture (even in the appendix) and simply refer to another article. If the majority of the results presented in the paper are based on this network (including a reference made to a specific layer of the network in subsection 4.2) – which is not commonly known – why not to detail the network architecture and save additional effort for the reader?

How are the class-wise training and test images chosen for Caltech256 dataset?

The authors test the CNN-2 architecture on Cifar10 and Cifar100, and AlexNet, VGG16, and VGG19 on UCF101. I feel that at least a couple architectures should be validated with more than a single dataset, or the authors should justify the current matching between architectures and datasets. Table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that VGG16 was also used for Cifar and Caltech, however other statements seem to say otherwise).

""To prove that the actual boost in accuracy with ED is not provided by the choice of specific masks,..."" I suggest that the authors rephrase or explain this sentence in more detail. To the best of my understanding, it is precisely the fact that different masks are used, each reflective of the particular input used to generate the forward activations, that gives boost in performance over ""standard"" dropout methods by identifying salient paths in the network.

Although it is a very important experimental detail, only in the end of sub-section 4.2, it becomes clear in which layers Excitation Dropout was applied. 

Y-axis labels are missing for the left panels in Figure 3.

The authors randomly choose to abbreviate Excitation Dropout as ED in some paragraphs, while write the full form in others.  

Table 2 - It is not clear that the ""Neurons ON"" metric refers to the ""average percentage of zero activations"" explained below. 

Table 2 - How is peak p_EB measured? Is this an average over a set of test images after convergence? If so, I similarly suggest for confidence bounds to be introduced. It would be interesting to compare this to intermediate values (e.g. after every epoch) during training. Same question for entropy of activations and entropy of pEB. This information would be useful for reproducibility.

Table 2 - Where do the delta values in Table 2 come from? If empirically determined, it should be stated explicitly.

Table 2 - In general, because the metrics provided in Table 2 are averages (second paragraph of this Section 4.3), both (to the best of my understanding) across input subsets (e.g. averging results over many test inputs) and models (caption to Table 1), I feel Table 2 in its current form raises confusion given the lack of confidence bounds. I recommend the authors to clarify what type of averaging was done and to introduce e.g. standard deviations across all reported scores. The authors should refrain from using the term ""significantly"" while describing results if no statistical testing was done, or explicitly clarify their usage of this term.

Table 2 - In general, Table 2 reports results on selected metrics which, if the authors' hypothesis is correct, should have a clear trend as training progresses. An interesting idea to explore would be to include an analysis (in the appendix) of how these factors change over the course of the training procedure. Intuitively, it seems plasticity is something that should be learned slowly over time, so if these plots were to reveal something different, it would be indicative that something else is going on.

Figure 4 - Judging heatmaps is difficult as it depends on the visual perception of the reader. Thus, it is difficult to judge whether, as the authors claim, ED is indeed less ""peaky"" than the other alternatives. I suggest that the authors use a perceptually uniform heatmap, and to acompany these figures with e.g. the histogram of the heatmap values. Likewise, it is unclear how the multi-model aspect of the testing plays a role in generating these results. From the 5 originally trained models, how was the model selected that generated these results? Was there averaging of any kind?

Figure 5: the text is too small to be readble

Is ""re-wiring"" the most appropriate term to use to describe what is happening at inference time? Although different paths may be used, the network connections themselves are fixed and thus this is a potential source for confusion.

What do numbers in Table-4 in the appendix represent? Test accuracy? ",5
"This paper studies adversarial training of robust classification models. It is based on PGD training in [madry17]. It proposes two points: 1) add attention schemes, 2) add a feature regularization loss. The results on MNIST and CIFAR10 demonstrate the effectiveness. At last, it did some diagnostic study and visualization on the attention maps and gradient maps.

1. Can you provide detailed explanations/intuitions why attention will help train a more robust models?

2. Two related adversarial training papers are missing ""Ensemble Adversarial Training"" (ICLR2018) and ""Adversarial Logit Pairing"" (ICML2018). Also, feature (logit) regularization has been studied in ALP paper on ImageNet.

3. For Table 2 on CIFAR10, I would like to see PGD20 (iterations) + 2 (step size in pixels), PGD100 + 2 and PGD200 + 2. Also, I am interested in seeing CW loss which is based on logit margin. 

4. I would like to see results using the ""wide"" model in [madry17] paper for ALP and LRM. I think results from large-capacity models are more convincing.

5. I would like to see results on CIFAR100, which is a harder dataset, 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for justification nowadays (maybe enough one year ago). Since ImageNet is,  to some extent, computationally impossible for schools, I want to see the justification results on CIFAR100.

##### Post-rebuttal

I appreciate the additional results in the rebuttal. I raise the score but it is still slightly below the acceptance. The reasons are 1) incremental novelty; 2) insufficient experiments. Also, I found in table 3 that, the larger-capacity model is less robust than the smaller-capacity model against white-box iterative attacks? This is strange.
",5
"This paper proposes a new architecture for adversarial training that is able to improve both accuracy and robustness performances using an attention-based model for feature prioritization and L2 regularization as implicit denoising. The paper is very clear and well written and the contribution is relevant to ICLR. 

Pros:

- The background, model and experiments are clearly explained. The paper provides fair comparisons with a strong baseline on standard datasets.
- Using attention mechanisms to improve the model robustness in an adversarial training setting is a strong and novel contribution 
- Both quantitative and qualitative results are interesting. 

",5
"Summary: This paper argues that improved resistance to adversarial
attacks can be achieved by an implicit denoising method in which model
weights learned during adversarial training are encouraged to stay
close to a set of reference weights using the ell_2
penalty. Additionally, the authors claim that by introducing an
attention model which focuses the model training on more robust
features they can further improve performance. Some experiments are
provided.

Feedback: My main concerns with the paper are:

* The experimental section is fairly thin. There are at this point a
  large number of defense methods, of which Madry et al. is only one. In
  light of these, the experimental section should be expanded. The
  results should ideally be reported with error bars, which would help
  in gauging significance of the results.

* The differential impact of the two contributions is not entirely
  clear. The results in Table 1 suggest that implicit denoising can
  help, yet at the same time, Table 2 suggests that Black-box
  performance is better if we just use the attention model. Overall,
  this conflates the contributions unnecessarily and makes it hard to
  distingish their individual impact.

* The section on gradient maps is not clear. The authors argue that if
  the gradient map aligns with the image the model depends solely on
  the robust features. While this may be (somewhat more) intuitive in
  the context of simple GLMs, it's not clear why it should carry over
  to DNNs. I think it would help to make these intuitions much more
  precise. Secondly, even if this were the case, the methodology of
  using a neural net to classify gradient maps and from this derive a
  robustness metric raises precisely the kinds of robustness questions
  that the paper tries to answer. I.e.: how robust is the neural net
  classifying the gradient images, and how meaningful are it's
  predictions when gradient maps deviate from ""clean"" images.

Overall, I feel this paper has some potentially interesting ideas, but
needs additional work before it is ready for publication.",4
"The authors propose a dropout method that uses the beta-Bernoulli process to learn the sparsity rate for each node. 

The model itself make sense to me, though I don't have an understanding of why learning a node-specific sparsity rate should improve dropout -- i.e., what is there to learn? From what I understand about dropout, it's a stochastic method that has the same marginal as the original model, but because of the randomness induced it avoids bad local optimal solutions. Thus it's a learning trick, not a modeling technique. This treats dropout as something to directly model.

My confusion is mainly about inference. While there are many approximations introduced to make it work, if the sparsity z is something to be learned then why is it only being sampled from the beta prior in (15)? There is a likelihood term that incorporates z as well and it seems like this should be included as well to be strictly correct from a modeling standpoint. I didn't see any explanation in the discussion.",5
"This work proposes Variational Beta-Bernoulli Dropout, a Bayesian way to sparsify neural networks by adopting Spike and Slab priors over the parameters of the network. Motivated by the Indian Buffet Process the authors further adopt Beta hyperpriors for the parameters of the Bernoulli distribution and also propose a way to set up the model such that it allows for input specific priors over the Bernoulli distributions. They then provide the necessary details for their variational approximations to the posterior distributions of both such models and experimentally validate their performance on the tasks of MNIST and CIFAR 10/100 classification.

This work is in general well written and conveys the main ideas in an clear manner. Furthermore, parametrising conditional group sparsity in a Bayesian way is also an interesting venue for research that can further facilitate for computational speedups for neural networks. The overall method seems simple to implement and doesn’t introduce too many extra learnable parameters.

Nevertheless, I believe that this paper needs more work in order to be published. More specifically:

- I believe that the authors need to further elaborate and compare with “Generalized Dropout”; the prior imposed on the weights for the non-dependent case is essentially the same with only small differences in the approximate posterior. Both methods seem to optimise, rather than integrate over, the weights of the network and the main difference is in how to handle the approximate distributions over the gates. Why would one prefer one parametrisation rather than the other? Furthermore, the authors of this work argue that they employ asymptotically unbiased gradients for the binary random variables, which is incorrect as the continuous relaxation provides a biased gradient estimator for the underlying discrete model.

- At section 3.2 the authors argue about the inherent sparsity inducing nature of the IBP model. In the finite K scenario this is not entirely the case as sparsity is only encouraged for alpha < K.

- At Eq. 11 the index “n” doesn’t make sense as the Bernoulli probability for each point depends only on the global pi_k. Similarly for Eq. 12.

- Since you tie q(z_nk|pi_k) = p(z_nk|pi_k) then it makes sense to phrase Eq.16 as just D_KL(q(pi) || p(pi)). Furthermore, I believe that you should properly motivate on why tying these two is a sensible thing to do.

- Figure 1 is misleading; you start from a unimodal distribution and then you simply apply a scalar scale and shift to the elements of that distribution. The output of that will always be a unimodal distribution but somehow you end up with a multimodal distribution on the third part of the figure. As a result, I believe that in this case you will not have two clear modes (one at 0 and one at 1) when you apply the hard-sigmoid rectification.

- The motivation for 21 seems a bit confusing to me; what do you mean with insignificant dimensions? What overflow does the epsilon prevent? If the input to the hard sigmoid is a N(0, 1) distribution then you will approximately have 1/3 of the activations having probability close to 1. Furthermore, it seems that you want beta to be small / negative to get sparse outcomes but the text implies that you want it to be large.

- It would be better to rewrite eq. 22 to include also the fact that you have a separate z per layer as currently it seems that the there is only one z. Furthermore, you have written that the variational posterior distribution depends on x_n on the RHS but not on the LHS.

- Above eq. 23 seems that it should be q(z_nk| pi_k, xn) = p(z_nk| pi_k, xn) rather than q(z_nk| pi_k) = p(z_nk| pi_k, xn)


Regarding the experiments; the MNIST results are not particularly convincing as the numbers are, in general, similar to other methods. Furthermore, Figure 2 is a bit small and confusing to read. Should FLOPS be on the y-axis or something else? Almost zero flops for the original model doesn’t seem right. Finally, at the CIFAR 10/100 experiment it seems that both BB and DBB achieve the best performance. However, it seems that the accuracy /sparsity obtained for the baselines is inferior to the results obtained on each of the respective papers. For example, SBP managed to get a 2.71x speedup with the VGG on CIFAR 10 and an error of 7.5%, whereas here the error was 8.68% with just 1.34x speedup. The extra visualisations provided at Figure 3 do look interesting though as it shows what the sparsity patterns learn.",5
"Summary
------------------

The authors propose a new method to sparsify DNNs based on a dropout induced by a Beta-Bernoulli prior. They further propose a data-dependent dropout by linking the Beta-Bernoulli prevalence to the inputs, achieving a higher sparsification rate. In the experimental section they show that the proposed method achieves better compression rates than other methods in the literature. However, experiments against some recent methods are missing. Also, some additional experiments using data-dependent dropouts not based on the Beta-Bernoulli prior would help to better disentangle the effects of the two contributions of the paper. Overall, the paper is well-written but the mentioning of the IBP is confusing. The authors devote quite a bit of space to the IBP when it is actually not used at all.

 Detailed comments
-------------------------

1)	Introduction

The paper is well motivated and the introduction of the paper clearly states the two main contributions of the paper: a Beta-Bernoulli dropout prior and a dependent Beta Bernoulli dropout prior. 

2)	Background

Section 3.1 is a nice summary of variational inference for BNNs. On the other hand, Section 3.2 is misleading. The authors use this section to introduce the IBP process (a generative sequential process to generate samples from a random measure called the Beta-Bernoulli process). However, this is not used in the paper at all. Then they introduce the Beta-Bernoulli prior as a finite Beta-Bernoulli process. I find this quite convoluted. I would suggest to introduce the Beta-Bernoulli distribution as a prior directly, and state that for alpha/K this is a sparse-inducing prior (where the average number of features is given by \frac{\alpha}{1 + \frac{\alpha}{K} ). No need to mention the IBP or the Beta Bernoulli process. 

3)	Main Contribution

I think the design of a link function that allows to implement a data-dependent Beta-Bernoulli dropout is one of the keys of the paper and I would suggest that the author clearly state this contribution at the beginning of the paper. I would also like to see the application of this link-function to other sparsity inducing priors different than the Beta-Bernoulli. This would allow to further understand the data-dependent contribution to the final performance and how transferable this is to other settings. Also, Have the authors try to train the data-dependent Beta-Bernoulli from scratch, i.e. without the two steps approach? I am assuming the performance is worse, but I would publish the results for completeness.

4)	Experiments

The main issues with the experimental section are:
a)	I am missing some recent methods (some of them even cited in the related work section): e.g. Louizos et al. (2017). I would be interested in comparisons against the horshoe-prior and a data-dependent version of it. Also, a recent paper based on the variational information bottleneck have been recently published outperforming the state of the art in the field (http://proceedings.mlr.press/v80/dai18d.html).
b)	Table 1 should report the variance or other uncertainty measure: Given that they run the experiments 5 times, I do not understand why they only report the median. I would encourage the authors to publish the mean and the variance (at least).
In addition, one of my main question about the method is, once the network has been sparsified, how does this translate into a real performance improvement (in terms of memory and speed). In term of memory, you can always apply a standard compression algorithm. If the sparsity is about a certain threshold, you can resort to sparse-matrix implementations. However, regarding the speed only when you reach a certain sparsity level you would get a tangible improvement if your DL framework support sparse matrices. However, if you get an sparsity level below this threshold, e.g. 20%, you cannot resort to sparse matrices and therefore you would not get a speed improvement, unless you enforce structure sparsity or you optimize to low-level matrix multiplication routines. Are the Speedup/Memory results reported in Table 1 real or theoretical?

",7
"The paper proposed a RNN with skip-connection (external memory) to past hidden states, this is a slightly different version of the TARDIS network. The authors experimented on PTB and a temporal action detection method.

Novelty:

I dont see a lot of novelty to the method. The authors proposed a method very similar to TARDIS, the difference seems to be that MMARNN does not use extra usage vectors for reading from previous memory, but this is not a fundamental difference between MMARNN and Tardis.

Shortcomings of the paper:

1. The experiments seem rather weak. The authors experimented on PTB and temporal action detection method. It is not clear why authors experimented with PTB, this is not a task with long-term dependencies, I do not see how this task (compared to many other tasks) can benefit from using external memory (especially when only 1 past hidden state is used

2. The model uses a single past hidden state, it is not clear to me why this is better than using  a weighted sum of a few past hidden states, as many tasks requires long-term dependencies from multiple steps in the past. The authors should cite ""Sparse attentive backtracking"" (https://arxiv.org/abs/1809.03702) at NIPS 2018. SAB is very related in that it also propagate gradients to a few hidden states in the memory. The difference is that SAB used a few hidden states from the past/ memory instead of one; another difference is that it propagates gradients locally to the selected hidden states/ memory slots.

3. The paper only demonstrated experimental results on PTB and temporal action prediction. I think it would make the paper a lot stronger if the authors experimented with a variety of  different tasks. Tasks that requires long term dependencies can really demonstrate the strength of the model (copy and adding tasks).

4. If the authors could run the model on copy and adding tasks, I would be curious to see if the model is picking the ""correct"" timestep in the memory / past.

post rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes.
",5
"
Summary:

This paper introduces a new RNN architecture with external memory for sequence modeling. The proposed architecture (MARNN) is a simplification of TARDIS (Gulcehre et al., 2017). It uses the similar reader-writer tying mechanism, gates to control information flow from previous hidden state and memory. However, it has a simpler addressing mechanism. Authors show results in Character level PTB and a temporal action detection/proposal task.

Major comments:

MARNN looks like a simplification of TARDIS architecture with Gumbel softmax. The major difference between the two architectures is the addressing mechanism.

1.	Can the authors clearly differentiate MARNN vs TARDIS?

2.	Authors compare against TARDIS only in character level PTB which is actually a task which does not require very long term dependencies. It would be better if authors consider more tasks and directly compare against the TARDIS addressing mechanism to prove that the proposed addressing mechanism is indeed better.

3.	Authors should consider more tasks, to show the efficiency of the proposed architecture.

4.	The name of the model seems to be too generic. NTM, TARDIS, DNC with recurrent controller can be considered as memory augmented RNN. Please change the name.

",4
"This paper introduces a memory-augmented RNN (MARNN) which aims at being lightweight and   differentiable. In a nutshell, authors propose to augment a LSTM-type architecture with several memory cells. At each time-step, MARNN retrieves one memory cell, updates his state, and updates the memory cell content. To learn the retrieval operation that requires discrete addressing,  authors rely on the Gumbel-Softmax. Authors evaluate their approach on PennTreeBank character level modelling where they demonstrate competitive performances. They also report state-of-art performance on the Thumos dataset. The paper is overall clear and pleasant to read. 

Authors highlight that MARNN is more lightweight compared to existing memory networks. MARNN can indeed retrieves only memory cells at inference. However,  since MARNN uses a Gumbel-Softmax to train the discrete addressing scheme, it is it not clear if there is any advantage in term of memory and computation of MARNN relatively to other network during training? It would be nice to compare the computation time/memory usage of MARNN with other memory augmented network such as TARDIS, NTM or Memory Network during training and inference. 

Another claim is that MARNN can possibly boost training speed by reducing the lengths of TBTT.  But MARNN also haves a training time overhead as showed in Figure 2.  How does the overall training time/performances of MARNN with TBPTT of 50 compared to a LSTM with TBPTT of 100/150?

The writing can be sometime a bit imprecise. For instance authors say that MARNN “learns better representations that many hierarchical RNN structure”. I agree that MARNN outperforms in term of accuracy, however, it is not clear what the author are referring to by “better representation” of the MARNN hidden state? Performance gain of MARNN could also be due to the external memory which allows  to retain more information of the input? In addition, it would be nice to precise which type of hierarchical RNN structure MARNN does (or doesn’t) outperform. Another claim is that MARNN can “easily learn long-term dependencies”. While this is reasonable, I am unsure that the empirical evaluation support this.  It would be nice to show how the gradients backpropagated through time behave in practice to support this claim? 


Memory-augmented network are a very important research directions and the MARNN architecture is interesting. However, it is not entirely clear to me what is the main advantage of MARNN relatively to other memory networks network such as TARDIS, NTM or Memory Network for training and/or inference. Although authors do compare with TARDIS, further comparison with the other networks and in term of computation time and memory could help clarify those points. 
",4
"This paper discusses applications of variants of RNNs and Gated CNN to acoustic modeling in embedded speech recognition systems, and the main focus of the paper is computational (memory) efficiency when we deploy the system. The paper well describes the problem of the current LSTM, especially focusing on the recurrent connection matrix operations, which is a bottle neck in this scenario, and introduces variants of RNNs (e.g., QRNN). Also these variants may not yield enough performance compared with LSTM, but 1-D convolution and/or deep structure helps to avoid the degradation. One of the biggest issues of this paper is that they use CTC as an acoustic model, while still many real speech recognition applications and major open source (Kaldi) use hybrid HMM/DNN(TDNN, LSTM, CNN, etc.) systems. Therefore, the paper's claim on CTC is not along with the current application trends. (It may be changed near future, but still hybrid systems are dominant). For example, the WSJ WER performance listed in Table 3 is easily obtained by a simple feed-forward DNN in the hybrid system. The latest Lattice free MMI with TDNN can achieve better performance (~2.X% WER), and this decoding is quite fast compared with LSTM. The authors should consider this current situation of state-of-the-art speech recognition. Also, the techniques described in the paper are all based on existing techniques, and the paper lacks the technical novelty.

Other comments:
- in Abstract and the first part of Introduction: as I mentioned above, CTC based character-prediction modeling is not a major acoustic model.
- The paper needs some discussions about TDNN, which is a major acoustic modeling (fast and accurate) in Kaldi
- p.4 first line ""and  represents element-wise multiplication"": The element-wise multiplication operation was first appeared in Eq. (1), and it should be explained there.
- Section 3.2: I actually don't fully understand the claims of this experiment based on TIMIT, as it is phoneme recognition, and not directly related to the real application, which is the main target of this paper I think. My suggestion is to place these TIMIT based experiments as a preliminary experiment to investigate the variants of RNN or gated CNN before the WSJ experiments. (I did not say that Section 3.2 is useless. This analysis is actually valuable, and this suggested change about the position of this TIMIT experiment can avoid some confusion of the main target of this paper.)


",4
"This paper present a study on efficient acoustic modeling using neural networks-based model. Four approaches are presented and evaluated: diag LSTM, QRNN, Gated ConvNet and adding a 1D convolution layer. The evaluation is done on ASR task using WSJ and in phoneme classification task using the TIMIT corpus. The study show that the inference speed is improved with comparable of better performance than the standard LSTM model.

The findings presented in this paper are interesting and quite useful when one wants to implement a LSTM-based acoustic model on mobile devices. The paper is well written and easy to ready. 

The main issue of this paper is the lack of novelty: the three evaluated approaches (Diag LSTM, QRNN and Gated ConvNet) are not novel, the only novelty is the addition of a 1D convolution, which is not enough for a conference like ICLR. 

Minor comments on the experiments:
* The network quantization approach has been shown to lead to efficient neural networks, could the authors provide a comparison between their approach and the quantization approach ?
* On the TIMIT experiment, the authors could add a decoder and use the PER metric instead of the frame accuracy, so they could provide comparison with the literature. 
* WSJ and TIMIT are quite small corpora compared to the available corpora, maybe the authors should consider using large corpora like Librispeech. It could be interesting to see the performance of the presented approaches.

Overall, this paper feels more like a technical report: the findings could be useful, but its novelty is too limited for ICLR. Hence I argue for rejection, and suggest that the authors consider submitting the paper to a speech conference like ICASSP.",4
"This paper investigates a number of techniques and neural network architectures for embedded acoustic modeling.  The goal is to reduce the memory access and make efficient computation, in the meantime, to sustain good ASR performance.  Overall, the paper is well motivated and well written.  However, I have following concerns.

1. It is not clear from the paper whether both the training and inference are conducted on embedded devices or only the inference?  I assume it is the latter but can't find it explicitly mentioned in the paper.  

2. The exploration carried out in the paper is more on the system level and the novelty is not overwhelmingly significant.

3. My major concern is that the reported WERs on WSJ and phoneme classification accuracy are quite off.  20%-30% WERs for WSJ  do not seem to be usable in real applications.  Honestly, I don't even think this performance is better than well-trained GMM-HMM acoustic models using a Viterbi decoder.  Furthermore, there is no clear winners across the investigated architectures  in terms of performance.  One question is if one wants to deploy such an on-device system, which architecture shall be chosen?  

4. A more general comment on the work explored  in the paper.  First of all, the on-device memory issue puts a heavy constraint on the capacity of acoustic models, which will significantly hurt the modeling capability for the DNN-based acoustic models.  Deep learning acoustic models can outperform GMM-HMM because they can use large model capacity with very deep and complex architectures when a large amount of training data is available.  Second, for CTC, when the training data is limited,  its performance is far worse than the hybrid DNN-HMM model, let alone a pure end-to-end fashion without using external LM and dictionary.  If WFST-based decoders (composition of WFSTs of LM, dictionary and deblank/repetition) are used, then the memory issue will surface again. 
",4
"This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck. Theoretical analysis shows that this bound is an lower bound on the VIB objective. The empirical analysis shows it outperforms VIB in some sense. 

I think this paper's contribution is rather theoretical than practical. The experiments section can be improved in the following aspect:
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines
- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting. In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off. 
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings. 
- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations.

Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:

- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016

It was kind of surprising that the authors did not cite this paper given the results are pretty much the same. It would also be helpful for the authors to do a comparison or connection section with this paper. 

I like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now.",5
"The paper presents a method of learning representations that is based on minimizing ""deficiency"" rather than optimizing for information sufficiency. While perfect optimization of the sufficiency term in IB is equivalent to minimizing deficiency, the thesis of the paper is that the variational upper bound on deficiency is easier to optimize, and when optimized produces
better (more compressed representations), while performing equally on test accuracy.



The paper is well written and easy to read. The idea behind the paper (optimizing for minimizing deficiency instead of sufficiency in IB) is interesting, especially because the variational formulation of DB is a generalization of VIB (in that VIB reduces to VDB for M=1). What takes away from the paper is that while perfect optimization of IB/sufficiency is equivalent to perfect optimization of DB, it is not clear what happens when perfection is not achieved. Further, the authors claim that DB is able to obtain more compressed representations (But is the goal a compressed representation, or an informative one?). The paper would also benefit from evaluation of the representation itself, and comparison to other non-information bottleneck based algorithms.
",7
"This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones. This problem has a natural variational form that can be easily implemented from VIB. Experiments show good performance comparing to VIB. 

This paper is well-written and easy to read. The idea using KL divergence creating a deficiency channel to learn data representation is very natural. It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9). 

My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem. However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function. For example, how does the method compare with (variants of) Variational Autoencoder? A discussion on this or some empirical evaluations would be nice. ",6
"
=== Post rebuttal update ===

Thanks to the addition of better baselines, I've increased my score for this paper. While I'm still not super convinced of its potential for application, I find the idea original and worth discussing at the conference.

=== Pre-rebuttal review ===
This paper presents an approach to compress a dataset into a much smaller number of synthetic samples that are optimized to yield as good performance as possible when a given model is trained on that smaller dataset. This is done by unrolling the gradient descent procedure of training such a model to allow for gradient-based optimization of synthetic samples themselves as well as the used learning rates.

In summary, my evaluation is as follow:

*Pros*
- Pretty original problem formulation
- Generally well written paper

*Cons*
- Lack of comparison with simple baselines in basic dataset distillation setting
- Use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated
- Possibly a mistake in the theoretical analysis of the linear case

Indeed, I found the paper to be generally quite clear and enjoyed reading it. One minor thing I struggled a bit with is the distinction between ""SG steps"" and ""Epochs"" (I believe the former corresponds to when the synthetic samples are different between GD steps, whereas the later corresponds to the number of times the method repeatedly cycles over these samples) so I would perhaps encourage the authors to emphasize that difference. 

I also find the problem statement that proposed to be interesting and thought provoking, and the solution that's proposed seems quite appropriate and well thought out.

That said, I'm worried about the following:

- Unless I misunderstood, in the basic dataset distillation setting a comparison is never provided with training on a randomly selected subset of the training set. Presumably the results are worse, but I think these results should be in the paper. I would also argue for having another baseline, which would try to (approximately) optimize the choice of which training examples are put in the subset. A very simple approach would be to take the 200 runs already performed for the random selection and select the subset providing the best accuracy on the full training set and only report the performance of that subset (instead of the mean and std of all 200 runs). In short, this would help determine to what extent there is value to synthesizing entirely new samples. Moreover, I think a simple alternative baseline for creating synthesized samples should be considered. Specifically, I'd personally would like to know the performance of using per-class k-means clustering and training on the cluster centroids as the distilled dataset.

- While I appreciate that the authors identify potential applications and report some results on them, I think they currently fall short of convincing the reader of the potential of dataset distillation for these applications. For domain adaptation, no actual domain adaptation baseline is compared against (a good candidate would be method from Daume III 2007, at the very least). For data poisoning, I find that the assumptions for attacks are pretty strong, i.e. that a) you have access to the parameters of the pre-trained models to attack and b) that the model is doing additional updates *only* on the synthesized data. If there are reasons to think that such assumptions are reasonable, I'd at least expect the paper to motivate why that is. 

- In the analysis of the simple linear case, in Equation 7, there appears to be a mistake, specifically some missing parentheses:

d^Td( (I-\eta/M d^Td)\theta_0 + \eta/M \tilde{d}^T\tilde{t}) = d^T t

i.e. there should be parentheses right after ""d^Td"" and right before ""="". This is from replacing \theta^* by the expression for \theta_1 in Equation 6, which is what I think Equation 7 is supposed to be doing. This possibly doesn't affect some of the conclusions taken from this section, but I'd like to see this potential mistake discussed/addressed.


That said, if the authors can sufficiently address the 3 points above, I'd be willing to increase my rating for this paper.

Finally, I have a few other more minor (nice-to-have) points:
- Having in the related work a discussion on the relationship with coreset methods would be nice
- Experiments showing how well the distilled datasets transfer to different network architectures than those used in training would be interesting? Even other ML algorithms would be quite interesting?
- ""We often find that the number of distilled images required to achieve good performance is an informative indicator of the dataset diversity"" => I'm not sure what in the paper actually justifies / demonstrates this statement.
- Figure 4 is presented as an ""Ablation study"", but an ablation study is where you remove certains parts of a model or algorithm and see what happens, which isn't the case here. I think it's better described as a hyper-parameter sensitivity study. 
- Some typos:
   * the below objective => the objective below
   * w.r.t. to => w.r.t
   * the discrete part rather => the discrete parts rather
   * necesary => necessary
",6
"The paper presents an algorithm for compressing the size of entire training data into a few synthetic training samples. The method is based on neural networks and is applied on image datsets. The authors comment two possible applications of their method domain adaptation and effective data poisoning attack.

The proposed technique seems to be limited to neural networks since it seems that is linked to the initialization of the networks. In this aspect, it could be interesting to have a more general method.

There are related works that are not commented, for instance :

Olvera-López, J. Arturo, et al. ""A review of instance selection methods."" Artificial Intelligence Review 34.2 (2010): 133-143.


Experimental section is weak. Few datasets are considered, other problems should be added. Additionally, related methods should be included to  compare the performance of the proposal. Some comments about the computational cost should be inserted. In this aspect, the experimental section should be improved following these recommendations.",5
"The paper addresses the interesting problem of generating a small number of synthetic examples that can be used to train a classifier, replacing a larger dataset. 

The paper is clearly written, the approach makes sense, and the experiments are interesting. 
My major concerns are regarding to previous literature, analysis of the algorithm, and details of the experiment. Overall, I expect an ICLR paper to go deeper (rather than wide). I recommend presenting strong convincing evidence on one front. 


Specific comments: 
(1)  I'm missing analysis of the proposed procedure. It wasn't fully clear which Loss it minimizes and if it indeed guaranteed to converge to the minimum of that loss. 

(2)  The topic of learning from few samples is presented as completely new. It is well known that for classical linear algorithms like the Perceptron and SVM, the weights are a weighted sum of (label-weighted) samples, hence by definition of these algorithm, there is a single sample that can be used to ""train"" the model in one step. I'd expect some discussion of how the proposed approach relate to these classical approaches. 
There is also existing literature on a related problem of selecting samples (Teaching dimension Goldman&Kearns) that could be somewhat relevant here. 

(3) Motivation. The paper provide several motivations for dataset distillation. I support the first motivation of scientific understanding what data is actually needed for a classifier, and this means that deeper analysis is needed. The practical motivations are less convincing, because (a) domain adaptation experiments are not compared with real baselines (b) robustness of poisoning with a single sample is not studied/discussed.

(4) experiments: The intro states that training with 10 images reaches 94% accuracy, but this does not seem consistent with the results in Table 1. The caption of figure 2 suggests that accuracy is between 12% and 94% which means the stated 94% is not representative or typical. Could you clarify? 
For domain adaptation. The baseline (random images) are very weak, and still perform almost   comparably to the proposed approach. More robust experiments are needed here: stronger baselines, decent hyper-parameter search etc.

(5) Writing and exposition: The paper addresses two issues: (a) learning with few synthetic samples, and (b) learning with few gradient steps. The intro tends to mix the two, and it is not clear why learning with a single gradient step is important. I recommend to separate the two topics more clearly. 
",5
"This paper proposes a novel architecture and regularization technique for RNN, where the hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata from an RNN is much simpler, and (2) forces RNN to operate like an automata and less like finite state machines. The authors make (1) immediately clear, and show (2) with empirical results.

Major comments:

(1) No experiments on widely used benchmarks for RNNs (e.g. language modeling, arithmetic tasks (for instance see Zaremba and Sutskever, 2015) ). Have you tried this by any chance?

(2) Theorems 3.1 and 3.2 are presented without proof. Will be good to at least include it in the appendix.

(3) IMDB experiments: you claim that SR-LSTM and SR-LSTM-p have ""superior"" extrapolation capabilities than vanilla LSTMs. However, as SR-LSTM and SR-LSTM-p give far lower train error rate, it's not strictly fair to claim that they extrapolate better to longer sequences than encountered during training time. 

Is the number of parameters held constant across 3 models? I'm struggling to understand why the training performance of the proposed models is significantly better than pure LSTM. For SR-LSTM-P I can see this being the case (the peephole connections effectively increase the hidden state size), but why does SR-LSTM (whose hidden states should be more constrained than pure LSTMS) perform better than LSTM during training? This makes me wonder whether SR-LSTM and SR-LSTM-P have higher capacity than LSTM somehow.

(4) MNIST experiments : please include results for SR-LSTM

Minor comments:

(1) page 8 : MNIST imagse -> images",6
"
Summary:

This paper is based on the observation that LSTMs use the hidden state to memorize information and the cell state (memory) is not fully utilized. To encourage the LSTM to utilize the cell state, authors constraint the hidden state to a set of centroid states and learn to transition between these centroids in a soft way. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks. The proposed model also has some interpretability of internal state transitions.

Major comments:

1.	The main claim of the paper is that SR-LSTM can extrapolate to longer sequences, unlike LSTM. However, the sequence lengths considered are too small. It would be interesting to train both models with specific sequence length and then keep testing them with longer sequence length and compare the performance. If SR-LSTM behaves like a DPDA, then with larger cell state, the performance should not drop as you increase the sequence length till the capacity of the cell state.

2.	Theorem 3.1 and 3.2 have no proofs. Please make them as notes rather than theorems.

3.	What do different colors in Figure 6 stands for?

4.	In the MNIST task authors claim that they have significant improvement when compared to LSTM. I am not sure if that is accurate. Also, why do you compare SR-LSTM-p only with LSTM? What is the performance of LSTM-p? Please report that as well.

5.	Even in table 3, can you please report the performance of LSTM-p?

Even though the paper does not show strong empirical performance in real-world tasks, I would still recommend for accepting this paper for its contributions in understanding RNNs better, provided authors answer to question 1, 4, and 5.


Minor comments:

1.	Fig 6 is not referred anywhere.

",6
"The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids.

In general, the paper is well written and the proposed model is theoretically grounded. Unfortunately, the proposed approach shines only on specifically designed benchmarks. It is not a surprise that a CF can be learned by an architecture very similar to DPDA (with addition of learnable parameters). There is a number of specifically designed tasks to test long-term memorization, such as copy/addition, etc. Furthermore, RNNs are mostly used for natural language processing tasks. This paper only conducts experiments on IMDB sentiment analysis ignoring better benchmarked tasks, such as language modelling.

It is not absolutely clear why authors claim that cell is playing the role of memory. It is always possible to rewrite LSTM formulas with h' which is concatenation of hidden state h and cell c. Results on ""peephole connection""-inspired SR-LSTM-p should be benchmarked against an LSTM with peephole connections.

The claim repeated several times that RNNs operate like DFAs, not DPDAs. This is an important point in the paper and should be verbalized more. Does it mean that it is easier to learn regular languages with RNNs?

While intuitive, theorems 3.1-3.2 are very vague to be theorems. Otherwise, they should be proven or provided a sketch of proof. For example, how do you formalize ""state dynamics""?

The quality of writing of the related work section is worse that the rest of the paper. Authors should explore more other hidden state regularization methods. And, perhaps, give less attention to stochastic RNNs since the final version of the proposed model is not stochastic.

To summarize, this paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown.

Minor points:
- Citation of Theano is missing
- Give a sentence explaining what is hidden state ""drifting""
- a-priori -> a priori",5
"This paper tackles a very important and practical problem in event stream planning. The problem is very interesting and the approach taken is standard.

The presentation of the paper is not clear enough. The notations and definitions and methods are presented in a complicated way. It's difficult to follows.

From the contribution point of view the paper looks like to be a combination of several existing and well developed approach: Neural Hawkes Process + particle smoothing + minimum bayes risk + alignment. It's not very surprising to see these elements together. It would have helped if the authors made it clear why each part is chosen and clearly state what is the novelty and contributed of the paper to the field.

The paper in its current format is not ready for publication. But it's a good paper and can be turned to a good paper for the next venue.",5
"The authors propose a particle smoothing approach with an approximate minimum Bayes risk decoder to impute missing events in the Neural Hawkes Process (NHP). The main goal is to address the missing events problem in continuous-time event analysis, which is an important problem in practice. The core idea is within the framework of particle smoothing. 

To formulate the posterior distribution of the missing event, the authors consider both the left-to-right past events and the right-to-left future events. The paper first applies the NHP to capture both the observed and inferred missing events to learn a representation of the past events, and then uses a similar NHP to learn the representation of the observed events from the future. Based on the two representations, it then formulates the intensity function of the missing events and uses the thinning algorithm to sample different particles. Based on the proposed distribution, the paper also considers to decode a single prediction achieving the Minimum Bayes Risk. Experiments on synthetic datasets with 10 different initializations and two real datasets show that the proposed smoothing approach is better than the filtering baseline. 

In general, this paper considers an important problem which is under active research in literature recently. However, there are a few weaknesses of the paper that should be addressed. 

1. The proposed technique is tightly connected to NHP, which could limit the applicability of the approach to other temporal point processes. The essential idea is similar to Bi-LSTM to learn the representation from both ends of a sequence of asynchronous temporal events. There are several different ways to represent the inter-event time to feed into the network other than NHP. Can the proposed method also be applied to other processes?

2. Within the particle filtering framework, each particle (hypothesis) is weighted by the likelihood of the sequence of observed events under that hypothesis. It turns out that the integral part of Equation 5 does not have an obvious analytical solution under NHP. Then, we first need a set of samples to approximate the likelihood evaluation. Later, we also need to sample particles. I am not quite convinced the computational efficiency of this approach in real applications of practice. Also, there is no analysis either empirically or analytically about the impact of the accumulative sampling errors on the inference performance. Furthermore, to learn the proposed distribution, the paper applies the REINFORCE algorithm under the proposed distribution q. But REINFORCE is known for large variance issue. Given that we already need lots of samples for the likelihood, it is unclear to me how stable the algorithm could be in practice.

3. The experimental evaluation is weak. For particle filtering and smoothing, it is known that the filtering techniques are candidates for solving the smoothing problem but perform poorly when T is large. That's why it is necessary to develop more sophisticated strategies for good smoothing
algorithms. As a result, it is unfair to only compare the smoothing approach with the filtering baseline. 

Actually, what people really care about is how different techniques can behave in real data to impute realistic missing events. From this perspective, I suggest to use the QQ-plot to evaluate the goodness of fitting on the synthetic dataset. For example, given a sequence of events generated from an independent temporal point process, we can randomly delete events, and then apply different techniques, including Linderman et al. (2017), Shelton et al.(2018), to impute missing events. Finally, we can compare the imputed sequence of events with the groundtruth. 

In addition, sequential monte carlo approach often suffers from skewed particle issue where one particle gradually dominates all the other particles with no diversity. It is unclear how the proposed approach is able to handle this. 

One missing related paper is ""Learning Hawkes Processes from Short Doubly-Censored Event Sequences""

Section 5.2 can be significantly strengthened if comparing with at least one of these approaches.

4. The paper is fairly written. I had some trouble reading back and forth for understanding Figure 1 since it has long caption that is not self-contained. The annotation of Section 2 is also too heavy to quickly skim through to memorize. ",4
"The paper presents an inference method (implicit distribution particle smoothing) for neural Hawkes processes that accounts for latent sequences of events that influence the observed trajectories.

Quality
+ The paper combines ideas from multiple areas of machine learning to tackle a challenging task of inference in multivariate continuous-time settings.
- The figures reported from the paper are comparative graphs with respect to particle filtering, and so the absolute level of performance of the methods is not characterized.  Reporting of distribution of sample weights and or run-times/complexity would strengthen the paper.

Clarity
- notation is complex replete with symbols ""@"" and text in math formulas
- It's not clear what p (""the data model"") and p_miss (""the missingness mechanism"") represent, and therefore why in equation 1: p(x,z) = p(xvz)p_miss(z| xvz) where v is the union symbol.  In addition, how it's related to MAR and MNAR is unclear. If e.g. following Murphy, one writes MAR as: p(r|x_u, x_o) = p(r|x_o), r is a missingness vector, x_u is x unobserved, and x_o is x observed, then r corresponds to observation or not, whereas in the manuscript p_miss is on the values themselves, i.e. on the space where z={k_{i,j}@t_{i,j}} resides.  We know, from the definition of MNAR that we can't use only the observed data to correctly infer the distributions of the missing values, and so while one can probabilistically predict in MNAR setting, their quality remains unknown.  If none of the experiments touch upon MNAR data, perhaps it is possible to omit this part.

Originality
+ the work is rich, complex, original, and uses leading methods from multiple areas of ML.

Significance
+ the significance of this work could be high, as it may provide a way to conduct difficult inference in an effective way to produce increasingly flexible modeling of trajectories amidst partial observation.
- however the exposition (particularly the experiments) does not fully demonstrate this.",5
"Sorry, I am not convinced by this paper.

I just don't believe that one can really gain any useful insight into neural networks by this kind of visualization.  In my opinion, all these kinds of visualization can give is the false believe that one understands what the network is doing.  (If you think about it, understanding itself is a rather vague and subjective term).  I guess my point is, these kinds of visualization don't seem to generate any actionable knowledge.  And how would one even meaningfully compare the outputs of competing methods of this general type?

",4
"Pros

1.	The paper is fairly clear.
2.	The problem is important: analyzing the internal computations of layered networks.
3.	The method seems to be a slight improvement on an existing method: the use of hierarchical clustering is nice.
4.	Figs. 3 and 5 superimposing the analyzed clusters on top of the network diagram are cool.

Cons

5.	The paper wastes valuable space writing out in detail the equations for backpropagation in a standard feed-forward MLP.
6.	The paper does not have an acceptable review of relevant prior work. This is particularly problematic as the proposal seems to be a rather small tweak to prior work of two 2018 papers by Watanabe et al. But there is extensive other literature attempting to address this problem, especially in the vision domain, where their main example - poor over-worked MNIST - resides.
7.	In my view, attempts to understand processing in NNs exclusively at the individual-unit level are essentially doomed at the outset. These networks crucially represent their information in distributed representations and it is joint action by multiple units rather than action by individual units that drives processing. Consider the “effect” variable analyzed in this paper, which is a simple correlation between the activity of a target hidden unit and the activity of a particular input or output unit. Suppose whenever hidden unit i is active, hidden unit j is also active, and vice versa. Now suppose j strongly drives output unit k via a connection with a large weight, while unit i has no connection at all to unit k. Then i will have a strong “effect” on k! The correlations between the activity of i and k is the same as the correlation between j and k, even though the causal interaction between i and k is nil, while the causal interaction between j and k is strong. In this especially transparent situation, it is the joint action of i and j that matters, and it so happens that this joint action has no contribution from i.
8.	So in addition to the problems arising from analyzing exclusively at the individual-unit level, there is the problem of defining “effect” by correlation instead of causation.
9.	I don’t myself gain any insight into how the MNIST network is working by looking at the clusters diagrammed in Fig. 4. There is no discussion of the fact that nearly all of their input-“effect” maps look like a slanted oval which is either on-center-off-surround or the reverse (no comment on the superficial, at least, connection to the receptive fields of neurons in the early mammalian visual system). Just how do these cluster maps explain anything? 
10.	The maps for the other example, time-series of prices of root vegetables, are even more baffling, but, superficially at least, the input maps suggest the hidden units are doing Fourier analysis; even this obvious observation is not made in the paper, however.
",3
"In this paper, the authors try to interpret the prediction mechanism of Layered Neural Networks (LNNs). The authors proposed to first define a feature vector that represents the roles of each hidden layer unit, via computing Pearson correlation coefficient. Then a hierarchical clustering method is applied to the generated feature vectors, such that tree-structured relationships among hidden layer units are revealed.

The purpose of the paper is to understand the prediction mechanism of Layered Neural Networks (LNNs). But based on the results in the experiments, I do not think the model achieves this purpose. Given the tree structure of LNN for the MNIST data set, I am still not able to understand how this LNN distinguishes the digit 0 from other digits. I am also not able to understand why a particular sample is classified as 0 rather than 6.

In Section 1, the authors mension that there are existing clustering-based methods that interpret LNN. The authors do not compare the proposed methods with these existing methods, either quantitatively or qualitatively. So I am also not sure the contribution of this paper, provided the existing methods.

In Section 3.1, the authors state that ""there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit"". I do not agree with this statement, because Ross et.al (2017) has proposed to measure it via gradient, although they are trying to solve a slightly different problem. Since the output of a hidden unit is a non-linear function of the input, I am not convinced that the proposed method that computes Pearson correlation coefficient is better choise than computing the gradient.

The proposed method provides a tree structure to describe the relationships between the hidden layer units. The authors also do not illustrate why learning the tree structure is particularly important. We can also run k-means with cosine similarity on the generated vector $v$, and learn the number of clusters via Bayesian information criterion (BIC). The authors do not explain why the tree-structured clustering results are more superior than the k-means clustering results.

In summary, I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.


References
Ross, Andrew Slavin, Michael C. Hughes, and Finale Doshi-Velez. ""Right for the right reasons: training differentiable models by constraining their explanations."" Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI). 2017.",3
"This paper introduces a relative entropy metric for evaluating hierarchical clustering. The paper is not well-written and its contributions are not clear to me. The abstracts and introduction promising for a novel hierarchical clustering algorithm, but I can only understand that they are using an agglomerative algorithm with the inter-cluster similarity of Eq. 10.
They show that their similarity metric reduces to the previous work by setting \pi = p or uniform. However, in the experiments, they only use these two cases, which does not support the importance of using relative entropy metric.  I guess picking the best \pi is an important part of this approach which has been left out. 
The authors violate the blindness of the paper by including a link to their github page, for which an anonymous repository should have been used.
It also worth noting that nowadays the graph representation term is often used for graph embedding, which makes the title very misleading. ",4
"This paper proposes a new formulation of hierarchical clustering for graphs.

Quality:
The proposed formulation has not been analyzed in detail and its advantage is not clear compared to existing approaches.
In addition, there are some existing measures for hierarchical clustering, for example, the dendrogram purity[1].
It would be interesting to analyze the relationship between the proposed method and such existing measures. 
[1] Heller, K. A. and Ghahramani, Z.: Bayesian Hierarchical Clustering, ICML2005

Clarity:
This paper is clearly written and easy to read.
The proposed criteria is carefully derived and well explained.
I feel that the title of the paper is not appropriate as it says the paper is about graph representation learning while a graph (representation) is already given as an input in the setting discussed in this paper. ""Learning hierarchical representation ..."" would be better.

Originality:
The originality is not high as most of theoretical discussion is based on the existing work and the resulting hierarchical clustering algorithm is a straightforward extension of the average linkage method.
Of course, it is quite interesting if a minor change makes a big difference in clustering performance (theoretically and/or empirically), but such result is not given.

Significance:
Significance of the contribution is not high as the advantage of the proposed formulation is not clear.
One of interesting questions is: how about higher order relationships between nodes?
The proposed method takes up to second order relationships between nodes, that is, edges into account.
Since the proposed formulation can naturally include higher order relationships, it would be interesting to analyze such relationships in hierarchical clustering.

Pros:
- The paper is clearly written.
- The proposed formulation of hierarchical clustering is interesting.
Cons:
- The advantage of the proposed formulation is not presented.
- Experiments are not thorough.",5
"This paper suggests a new metric for assessing the quality of hierarchical clustering. Dasgupta recently suggested the first such metric with interesting properties. This has encouraged a number of recent works about designing algorithms that work well for the metric and other similar metrics. This paper suggests a new metric for evaluating hierarchical clustering of given graph data. 

Here are the main comments about the paper:
- I am not convinced about the advantages of the new metric over the previously suggested metrics by Dasgupta and Cohen-Addad et al.
    - Theoretical analysis shows properties of the new metric that are similar to that of Dasgupta (since the metric itself has similarities). However, the advantage of the new metric is not very clear. 
    - Experimental analysis just shows that the new metric is different from Dasgupta’s but again there is no evidence to suggest why the new metric may be better. 

- In the abstract it is mentioned that “The best representation of the graph for this metric in turn yields a novel hierarchical clustering algorithm.” I do not understand this. Which novel algorithm is being referred to? 

- Again, it is mentioned in the abstract that “Experiments on both real and synthetic data illustrate the efficiency of the approach”. What efficiency is being referred to here and what is the approach? What I see is that known clustering algorithms are used to compare the new metric with the previous one by Dasgupta.

Overall, I think more work is needed in this paper. There are some non-trivial observations but unless the authors make the motivation for defining this new metric for evaluation more clear.

Other comments:
- Section 5: NP-hardness has not been shown and it is just mentioned that the authors believe that the problem is NP-hard just as the problem associated with the cost function of Dasgupta et al.",5
"Review for CO-MANIFOLD LEARNING WITH MISSING DATA
Summary:
This paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction.
The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. 
Novelty/Significance:
The overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn’t have a well established solution.
Questions/Clarity:
Smooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn’t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? 
 “Replacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties” – The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function’s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2.
Proposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1.
The authors write: “Missing values can sabotage efforts to learn the low dimensional manifold underlying the data. … As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.” However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue.
Why do the author’s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound.
Equation 5 is not clear that it is the first order taylor approximation. Omega’ is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ?
 “first-order Taylor approximation of a differentiable concave function provides a tight bound on the function” – Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won’t be anything close to tight.
The authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn’t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. 
It is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is.
Figure 4 appears before it is mentioned and is displayed as part of the previous section.
For the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise.
How big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)?
",7
"The manuscript proposes a co-manifold learning approach for missing data.  The problem is important, but the method is lack of novelty. 
Pros: important problem setting, Good experimental results.
Cons: the method is lack of novelty.

In detail, the method just simply combines a loss for competing missing values, which is not new,  and Laplacian losses for rows and columns, which are also not new. I don't see much novelty in the model.  
",4
"This paper presents a joint learning method for filling missing value and bi-clustering. The method extends (Chi et al. 2017), using a penalized matrix approximation. The proposed method is tested on three data sets, where two are synthetic and one small real-world data matrix. The presented method is claimed to be better than two classical approaches Nonlinear PCA and Diffusion Maps.

1) Filling missing values is not new. Even co-clustering with missing values also exists. It is insufficient to defeat two methods which are older than ten years. More extensive comparison is needed but lacking here. Why not first use a dedicated method such as MICE or collaborative filtering, and then run embedding method on rows and columns?

2) The purpose of the learning is unclear. The title does not give any hint about the learning goal. The objective function reads like filling missing values. The subsequent text claims that minimizing such a objective can achieve biclustering. However, in the experiment, the comparison is done via visualization and normal clustering (k-means).

3) The empirical results are not convincing. Two data sets are synthetic. The only real-world data set is very small. Why k-means was used? How to choose k in k-means?

4) The choice Omega function after Proposition 1 needs to be elaborated. A function curve plot could also help.

5) What is Omega' in Eq. 2?",4
"This paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue.  The writing needs quite a bit oof polish for the motivations to clearly stand out. Also, some of the notation makes things way more confusing that it should be. Is it possible to use something other than p() for the noise distribution, since the problem itself is to distinguish between p() and q(). I understand the notational overload, but it complicates the reading unnecessarily. I have the following questions if the authors could please address:

1) The inequality of Zhang et a. (2018) that this paper uses seems to be an easy corollary of the Data Processing Inequality :https://en.wikipedia.org/wiki/Data_processing_inequality Did I miss something? Can the authors specify if that is not the case?

2) In terms of relevance to ICLR, the applications of PCA, ICA and training of NNs is clearly important. There seems to be a significant overlap of Sec 5.3 with Zhang et al. Could the authors specify what the differences are in terms of training methodology vis-a-vis Zhang et al? It seems to me these are parallel submissions with this submissions focussing more on properties of Spread Divergences and its non deep learning applications, while the training of NNs and more empirical evidence is moved to Zhang et al. 

3) I am having a tough time understanding the derivation of Eq 25, it seems some steps were skipped. Can the authors please update the draft with more detail in the main text or appendix ?

4) Based on the results on PCA and ICA, I am wondering if the introduction of the spread is in some ways equivalent to assuming some sort of prior. In the PCA case, as an exercise to understand better, what happens if some other noise distribution is used ? 

5) I do not follow the purpose of including the discussion on Fourier transforms. In general sec 3 seems to be hastily written. Similarly, what is sec 3.2's goal ? 

6) The authors mention the analog to MMD for the condition \hat{D}(p,q)=0  \implies p =q. From sec 4, for the case of mercer spread divergence, it seems like the idea is that  ""the eigenmaps of the embedding should match on the transformed domain"" ? What is [a,b] exactly in context of the original problem? This is my main issue with this paper. They talk about the result without motivation/discussion to put things into context of the overall flow, making it harder than it should be for the reader. I have no doubt to the novelty, but the writing could definitely be improved.",5
"
Pros:

- interesting idea.

Cons:

- the paper forgets the state of the art for comparisons (optimal transport, data processing inequalities)
- the paper formally shows little as most results are in fact buried in the text and it is hard to tell the formal from the informal.
- experiments fall short of really using the setting proposed
- the paper focuses too much on keeping the identity of the indiscernibles and forgets the study of other properties (including downsides, such as variance increase)

Detail:

* The paper claims to propose a ""theory"" for spread divergences (conditioning a f-divergence by a ""third-party"" conditional distribution on supports which makes supports match) still keeping the identity of indiscernibles. 

* The paper recycles the notion of Spread f-divergences from Zhang et al. (which makes a circular reference to this paper for the introduction of these divergences).

* The paper motivates the notion of spread divergences by the fact that f-divergences impose matching supports (Section 1), not mentioning that optimal transport theory is a much natural fit for any such kind of setting (e.g. Wasserstein distances). This is a big omission and a missed occasion for a potentially interesting discussion.

* The paper then claims that ""spread noise makes distributions more similar"" (Section 2.1), not mentioning that equation (8), which it claims to have been shown by Zhang et al. paper (see below), is in fact a data processing inequality *long known*. They will find it, along with a huge number of other useful properties, in series of IEEE T. IT papers, among which Pardo and Vajda's ""About distances of discrete distributions satisfying the data processing theorem of information theory"",  Van Erven and Harremoes, ""Re ́nyi Divergence and Kullback-Leibler Divergence"" (for the KL / Rényi divergence, but you have more references inside), etc. .

* The paper then goes on ""showing"" (a word used often, even when there is not a single Theorem, Lemma or the like ever stated in the paper...) several properties (Section 2.2). The first states that (9) is equivalent to P being invertible. It is wrong because it is just in fact stating (literally) that P defines an injective mapping. The second states that (11) is equivalent to (12), without the beginning of a proof. I do need to see a proof, and in particular how you ""define"" an invertible transform ""p^-1"".

* The paper then gives two examples (Sections 3, 4). In Section 3, I am a bit confused because it seems that p and q must have supports in IR, which limits the scope of the example. The same limitation applies to Section 4, even when it is a bit more interesting. In all cases, the authors must properly state a Lemma in each Section that states and shows what is claimed before Section 3.

* The paper then makes several experiments. Unless I am mistaken, it seems that Section 5.1 relies on a trick that does not change the support from x to y. Therefore, what is the interest of the approach in this case ? In Section 5.2, isn’t the trick equivalent to considering ICA with a larger \gamma ?

* A concern is that the paper says little about the reason why we should pick one p(y|x) instead of another one. The focus is on the identity of indiscernibles. The paper also forgets some potential drawbacks of the technique, including the fact that variance increases — the increase can be important with bad choices, which is certainly not a good thing.",4
"Summary
=======
This paper introduces spread divergences. Spread divergences are obtained by taking the divergence between smoothed/noisy versions of two distributions. A spread divergence between two distributions of non-overlapping support can be defined even when the corresponding divergence is not. The authors discuss conditions under which the data generating process can be identified by minimizing spread divergences and apply spread divergences to the examples of PCA, ICA, and noiseless VAE.

Review
======
With a lot of papers focusing on generative modeling, divergence minimization is of great relevance to the ICLR community. Adding noise to distributions to ensure overlapping support is intuitive and has been used to stabilize training of GANs, but I am unaware of any work focusing on questions of identifiability and efficiency. I especially like the example of slowing EM in ICA with small noise. Here, some empirical results are lacking which analyze the speed/correctness of the identification of parameters for various choices of divergence/model noise. These would have greatly enhanced the paper. Instead, the available space was used to show model samples, which I find less helpful.

In Section 3.2 and Section 6 the authors argue that choosing noise which maximizes the spread divergence is optimal or at least preferable. This seems counterintuitive, given that the point of the noise was to make the distributions more similar. Please elaborate on why maximizing the divergence is a good strategy.

Minor
=====
The paper seems hastily written, with some grammar issues, typos, and sloppy use of LaTeX, e.g.:

– ""-\log"" instead of ""\log"" in definition of KL divergence in the introduction
– ""Section 2.2"" not ""section(2.2)"", ""Equation 24"" not ""equation(24)""
– ""model (Tipping & Bishop, 1999)"" instead of ""model Tipping & Bishop (1999)""
– ""\mid"" instead of ""|""
– ""x"" instead of ""y"" in Equation 28

Please provide a reference for the EM algorithm of ICA.",6
"# Summary
This work deals with few-shot learning and classification by means of similarity learning. The authors propose a method for generating a set of convolutional kernels, i.e. a mini-CNN, for a query image given a set of support samples (with samples from the same class and some other classes). Kernels are generated for each query and are adapted to the specific visual content found in the query image, thus a new embedding space is identified. The difficulty of the task is constrained by using a common base CNN for feature extraction, making the task resolution more feasible in the few shot regime. The method is evaluated on standard benchmarks Omniglot and miniImagenet with competitive results. 


# Paper strengths
- The paper has a good coverage of related work

- The proposed method is interesting and the results are encouraging

- The authors argue and study the influence of multiple elements over their contribution: number of sub-generators, distance metric, choice of architecture

# Paper weaknesses
- My main concern with this work is the incremental contribution with respect to the work by Han et al. (2018), ""Face recognition with contrastive convolution"". In that work the authors proposed a convolutional kernel generator for every pair of images to be compared/matched, while here the principle is simplified to re-use the same convolutional kernels for the a query image. The loss functions are nearly identical, both works use a classification loss and a loss ensuring kernels at different images with the same object should be similar. The visualizations of the feature maps are similar as well, though these would have been necessary any way for this type of contribution.

- The architecture of the kernel generator is not clear to the reader. Is it similar with the one from Han et al.?

- The related and relevant work from Gidaris and Komodakis (2018), Dynamic few-shot visual learning without forgetting, is not included as baseline in the evaluation. Their method is superior when using C4 and similar (while still keeping performance levels on previous tasks).

- Given that the evaluation for few-shot classification takes random samples of query and support samples and that we're dealing with stochastic models, it's common and encouraged to include error-bars/standard deviations in the results to get a better idea on the performances. I encourage the authors to do the same.

- The visualizations from Figure 3 would need some additional clarifications from the authors in the text. It's not clear what does the colormap refer to, red is for high activation and blue for low activation (as typical for jet colormap) or the other way around? If blue is highly active, it's worrying that most dogs (Q1,Q2,Q3) are active on the white dog in S3. As said, it would be useful to have some comments from the authors in the text to better explain the visualizations

- Minor remarks:
    + The evaluation protocol from Omniglot should be specified as there are 2 ways of doing it: 1) using characters from different alphabets at test time (easier); 2) using characters from the same alphabet (more difficult)
    + There are some other works dealing with weight generation or with adaptive embedding that would be worth mentioning: 
        * Y.X. Wang et al., Learning to model the tail, NIPS 2017
        * A. Veit and S. Belongie, Conditional similarity networks, CVPR 2017
    

# Conclusion
This paper advances an interesting idea for few-shot classification and gets competitive results. As mentioned in the section above, I'm worried about the incremental contribution on top of the work by Han et al.. In addition results are outperforming state of the art works, while requiring generating kernels and features for each query. My current rating is between Weak Reject and Borderline.",5
"Summary:
One of popular approach to few-shot classification is to learn an embedding function to a common feature space where the similarity between two examples is expected to be well determined. The current work claims that query-dependent feature space (referred to as individualized feature space) gains over the common feature space, in the task of few-shot classification. To this end, the paper employed a technique 'kernel generator' which has been recently proposed in [Han et al., 2018]. Few-shot classification is done using distance (e.g. Euclidean) in the query-dependent feature space. 
The paper evaluates this method using Omniglot and miniImagenet.

Strengths:
- Constructing individualized feature space tailored to each query is a novel idea.
- The paper shows strong quantitative results.

Weaknesses:
- The clarity is a big obstacle in this paper. Section 3 contains the main idea on 'kernel generator' which is the critical technique to map input images to individualized feature spaces. Unfortunately Section 3 is hard to follow. 
- Moreover, the idea of kernel generator is the one used in [Han et al., 2018], so the contribution of this paper is very limited.
- In a nutshell, the current work can be considered as a mix of matching network and kernel generator. 

Specific comments:
- Regarding terminology, authors state that ""there are three sets of examples in a few-shot classification task: a training set, a support set, and a testing set. The training set and the support set have disjoint label spaces with each other while the testing set shares the same label space with the support set."" I am very confused with what authors mean by support set. In general, each episode has a support set as well as queries in both meta-training and meta-test phases. Meta-training and meta-test has disjoint label spaces. 
- It is not clear to me what the problem setting is here. Queries in training and test phases have different label spaces. So, I am wondering feature space tailored to queries in the training phase can be well generalized to the test phase. Or you assumes that both cases have the same label space?
-Fig 2: The kernels and the conv features interact in a node which says “X”, making it seem like we are either pointwise multiplying or taking an outer product. The figure would be clearer if it somehow expressed that the two interact via convolution. (To add to this confusion, the kernels are thin which makes them look like vectors)
-eq(6): the index i is used to denote two things at once (g_i, c_ij). This notation should be different.
-eq(6): it says g_i is a fully-connected layer, but P_q^ij is a 3d tensor. Is g_i a 1x1 convolution, or do you flatten P?
-eq(9): what is H? Does it mean entropy? How is the set K_q a distribution of kernels? How does this loss relate to capturing the intrinsic characteristics of an object? This whole part should be clearer.

",5
"This paper proposes a new meta-learner for few-shot learning that conditions the parameters of the model on the given query image. The authors argue that this allows the model to focus on features particular to the query, thereby facilitating classification. The paper introduces a kernel generator as a meta-learner and report performance on two standard benchmarks, Omniglot and miniImagenet.

Several methods propose meta-learners that adapt the learner’s parameters to the task or each class in the task. This paper adapts to the query itself, which may provide other benefits, and provides a useful complement to prior work on parameter adaptation in few-shot classification.

While the core idea itself is clearly articulated, the reading is dense and many of the finer points are vaguely presented. This makes the paper hard to read and its contribution unclear. In particular, the meta objective itself is not defined, the second loss function contains an undefined (learnable?) functions whose role is not entirely clear. In the experimental section, the authors mention that they use Prototypical Networks (Snell et al., 2017) on top of their kernel generator. This puts their contribution in a different light, now as an extension of Snell et al., (2017). I’m also unclear about the novelty of kernel generator the authors supposedly introduce. The kernel generator appears identical to that of Han et at. (2018), in which case the contribution is its application to few-shot learning, not the kernel generator itself.

Since the main contribution of this paper is to condition the learner’s parameters on the query, as opposed to the task or the classes in the task, the relevant comparison is with respect to such alternative methods. Several such benchmarks are missing (below), and when considered, the reported results are relatively weak. 

For an up-to-date collection of benchmarks on miniImagenet, see Rusu et. al., (2018, https://arxiv.org/abs/1807.05960).

===

[1] Gidari and Komodakis. Dynamic few-shot visual learning without forgetting. 2018.
[2] Oreshkin et al.. TADAM: Task dependent adaptive metric for improved
few-shot learning. 2018.
[3] Qiao et al.. Few-shot image recognition by predicting
parameters from activations. 2017.
[4] Bauer et al.. Discriminative k-shot learning using probabilistic models. 2017.",3
"This paper proposed a differentiable metric for text generation tasks inspired by BLEU and a random training method by the Gumbel-softmax trick to utilize the proposed metric. Experiments showed that the proposed method improves BLEU compared with simple cross entropy training and policy gradient training.

Pros:
* The new metric provides a direct perspective on how good the conjunction of a generated sentence is, which has not been provided other metric historically used on language generation tasks, such as cross-entropy. 

Cons:
* Too many approximations that blur the relationship between the original metric (BLEU) and the derived metric.
* Not enough experiments and poor discussion. Authors should consume more space in the paper for experiments.

The formulation of the metric consists of many approximations and it looks no longer BLEU, although the new metric shares the same motivation: ""introducing accuracy of n-gram conjunction"" to evaluate outputs. Selecting BLEU as the starting point of this study seems not a reasonable idea. Most approximations look gratuitously introduced to force to modify BLEU to the final metric, but choosing an appropriate motivation first may conduct more straightforward metric for this purpose.

In experiments on machine translation, its setting looks problematic. The corpus size is relatively smaller than other standard tasks (e.g., WMT) but the size of the network layers is large. This may result in an over-fitting of the model easily, as shown in the results of cross-entropy training in Figure 3. Authors mentioned that this tendency is caused by the ""misalignment between cross entropy and BLEU,"" however they should first remove other trivial reasons before referring an additional hypothesis.
In addition, the paper proposed a training method based on Gumbel softmax and annealing which affect the training stability through additional hyperparameters and annealing settings. Since the paper provided only one training case of the proposed method, we couldn't discuss if the result can be generalized or just a lucky.

If the lengths of source and target are assumed as same, the BP factor becomes always 1. Why the final metric (Eq. 17) maintains this factor?",4
"Differentiable Expected BLEU for Text Generation

Paper Summary:

Neural translation systems optimizes training data likelihood, not the end metric of interest BLEU. This work proposes to approximate BLEU with a continuous, differentiable function that can be optimized during training.

Review:

The paper reads well. It has a few but crucial missing references. The motivation is easy to understand and a relevant problem to work on. The main weaknesses of the work lies in its very loose derivations, and its weak empirical results.

First on context/missing references: the author ignores approaches optimizing BLEU with log linear models (Franz Och 2003), and the structured prediction literature in general, both for exact (Tsochantaridis et al 2004) and approximate search (Daume and Marcu 2005). This type of approach has been applied to NMT recently (Edunov et al 2018). Your paper also misses important references addressing BLEU optimization with reinforcement strategies (Norouzi et al 2016) or (Bahdanau et al 2017). Although not targeting BLEU directly (Wiseman and Rush 16) is also a reference to cite wrt optimizing search quality directly. 

On empirical results, you chose to work IWSLT in the de-en direction while most of the literature worked on en-de. It prevents comparing your results to other papers. I would suggest to switch directions and/or to report results from other methods (Ranzato et al 2015; Wiseman and Rush 2016; Norouzi et al 2016; Edunov et al 2018). De-en is generally easier than en-de (generating German) and your BLEU scores are particularly low < 25 for de-en while other methods ranges in 26-33 BLEU for en-de (Edunov et al 2018).

On the method itself, approximating BLEU with a continuous function is not easy and the approach you take involves swapping function composition and expectation multiple times in a loose way. You acknowledge that (7) is unprincipled but (10) is also problematic since this equation does not acknowledge that successive ngrams overlap and cannot be considered independent. Also, the dependence of successive words is core to NMT/conditional language models and the independence hypothesis from the footnote on page 4 can be true only for a bag of word model. Overall, I feel that given the shortcuts you take, you need to justify that your approximation of BLEU is still correlated with BLEU. I would suggest to sample from a well trained NMT system to collect several hypotheses and to measure how well your BLEU approximation correlate with BLEU. How many times BLEU decides that hypA > hypB but your approximation invert this relation? is it true for large difference, small difference of BLEU score? at low BLEU score, high BLEU score?

Finally, you do not mention the distinction between expected BLEU  \sum_y P(y|x) BLEU(y, ref) and the BLEU obtained by beam search which only look at (an estimate of) the most likely sequence y* = argmax P(y|x) . Your approach and most reinforcement strategy targets optimizing expected BLEU, but this has no guarantee to make BLEU(y*, ref) any better. Could you report both an estimate of expected BLEU and beam BLEU for different methods? In particular, MERT (), beam optimization (Wiseman and Rush 2016) and  structured prediction (Edunov et al 2018) explicitly make this distinction. This is not a side issue as this discussion is in tension with your motivations.

Review Summary:

The paper misses important references. It chooses an empirical setup which prevents comparison with related work, and the report results on de-en seem weak. The proposed approach does not bound or estimate how far from BLEU is the proposed approximation. This means that the authors need to justify empirically that it preserves correlation with BLEU, which is not shown in the paper.

Missing references

An Actor-Critic Algorithm for Sequence Prediction (ICLR 2017)  Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio

Hal Daume III and Daniel Marcu. Learning as search optimization: Approximate large margin methods for structured prediction. ICML 2005.

Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato
Classical Structured Prediction Losses for Sequence to Sequence Learning, NAACL 18

Minimum Error Rate Training in Statistical Machine Translation Franz Josef Och. 2003 ACL

I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, Support Vector Machine Learning for Interdependent and Structured Output Spaces, ICML 2004.

Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Reward Augmented Maximum Likelihood for Neural Structured Prediction, 2016

Sequence-to-Sequence Learning as Beam-Search Optimization, Sam Wiseman and Alexander M. Rush., EMNLP 2016
",4
"The paper describes a differentiable expected BLEU objective which computes expected n-gram precision values by ignoring the brevity penalty. 

Clarity: 
Section 3 of the paper is very technical and hard to follow. Please rewrite this section to be more accessible to a wider audience by including diagrams and more explanation.

Originality/signifiance: the idea of making BLEU differentiable is a much researched topic and this paper provides a nice idea on how to make this work.

Evaluation: 
The evaluation is not very strong for the following reasons:

1) The IWSLT baselines are very weak. For example, current ICLR submissions, report cross-entropy baselines of >33 BLEU, whereas this paper starts from 23 BLEU on IWSTL14 de-en (e.g., https://openreview.net/pdf?id=r1gGpjActQ), even two years ago baselines were stronger: https://arxiv.org/abs/1606.02960

2) Why is policy gradient not better? You report a 0.26 BLEU improvement on IWSLT de-en, which is tiny compared to what other papers achieved, e.g., https://arxiv.org/abs/1606.02960, https://arxiv.org/abs/1711.04956

3) The experiments are on some of the smallest translation tasks. IWSLT is very small and given that the method is supposed to be lightweight, i.e., not much more costly than cross-entropy, it should be feasibile to run experiments on larger datasets.

This makes me wonder how significant any improvements would be with a good baseline and on a larger datasets.

Also, which test set are you using?

Finally, in Figure 3, why is cross-entropy getting worse after only ~2-4K updates? Are you overfitting? 
Please reference this figure in the text.",6
"This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks. The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward. This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers). The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.

Strengths:
+ The paper is generally clear and readable.
+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.

Major concern:
- My biggest concern is that the technical contributions of the paper are not clear at all. The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control). The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)). The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either. Overall, the paper does not make a compelling case for the novelty of the problem or approach.

Other concerns:
- For the cart-pole task, the paper states that the reward is modified ""to exclude any cost objective"". Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this). I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.
- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent. However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied). 

Typos:
- Pg. 5, Section 3.4: ""...this is would achieve...""
- Pg. 6: ...thedse value of 90...""",6
"This paper proposes a model free reinforcement learning algorithm with constraint on reward, with demonstration on cartpole and quadruped locomotion. 

strength: (1) challenging examples like the quadruped.
                (2) result seems to indicate the method is effective

There are several things I would like the authors to clarify:
(1) In section 3.2, why is solving (4) would give a ""exactly the desired trade-off between reward and cost""? First of all, how is the desired trade-off defined? And how is (4) solved exactly? If it is solved iteratively, i.e, alternating between the inner min and outer max, then during the inner loop, wouldn't the optimal value for \lambda be infinity when constrained is violated (which will be the case at the beginning)? And when the constrained is satisfied, wouldn't \lambda = 0? How do you make sure the constrained will still be satisfied during the outer loop since it will not incurred penalty(\lambda=0). Even if you have a lower bound on \lambda, this is introducing additional hyperparameter, while the purpose of the paper is to eliminate hyperparamter?
(2) In section 3.2, equation 6. This is clearly not a convex combination of Qr-Vr and Qc, since convex combination requires nonnegative coefficients. The subtitle is scale invariance, and I cannot find what is the invariance here (in fact, the word invariance\invariant only appears once in the paper). By changing the parametrization, you are no longer solving the original problem (equation 4), since in equation (4), the only thing that is related to \lambda is (Qr-Vr), and in (6), you introduce \lambda to Qc as well. How is this change justified?
(3)If I am not mistaken, the constrained can still be violated with your method. While from the result it seems your method outperforms manually selecting weights to do trade off,  I don't get an insight on why this automatic way to do tradeoff is better. And this goes back to ""exactly the desired trade-off between reward and cost"" in point(1), how is this defined?
(3) The comparison in the cartpole experiment doesn't seem fair at all, since the baseline controller is not optimized for energy, there is no reason why it would be comparable to one that is optimized for energy. And why would a controller "" switch between maximum and minimum actuation is indeed the optimal solution"" after swingup? Maybe it is ""a"" optimal solution, but wouldn't a controller that does nothing is more optimal(assuming there is no disturbance)?
(4)For Table I, the error column is misleading. If I understand correctly, exceeding the lower bound is not an error (If I am wrong, please clarify it in the paper). And it is interesting that for target=0.3, the energy consumption is actually the lowest.
(5)Another simple way to impose constrained would be to terminate the episode and give large penalty, it will be interesting to see such comparison.

minor points: 
* is usually used for optimal value, but is used in the paper as a bound.",7
"This paper uses constrained Markov decision processes to solve a multi-objective problem that aims to find the correct trade-off between cost and return in continuous control. The main technique is Lagrangian relaxation and experiments are focus on cart-pole and locomotion task.

Comments:

1) How to solve the constrained problem (8) is unclear. It is prefer to provide detailed description or pseudocode for this step.

2) In equation (8), lambda is a trade-off between cost and return. Optimization on lambda reduces burdensome hyperparameter selection, but a new hyperparameter beta is introduced. How do we choose a proper beta, and will the algorithm be sensitive to beta?

3) The paper only conducts comparison experiments with fixed-alpha baselines. The topic is similar to safe reinforcement learning. Including the comparison with safe reinforcement learning algorithms is more convincing.
",5
"
[Summary]
The main purpose of this paper is to propose an extension of relation networks.
The proposal consists of two parts: 1) to integrate constraints of dependency syntax to control which relations influence the representation, and 2) utilize a recurrent computation to capture higher-order relations.

[clarity]
This paper is basically well written.
Motivation and goal are clear.

[originality]
The idea of utilizing supervised or unsupervised dependency tree as constraints to control which relations influence the representation seems novel and interesting.
However, technically it consists of the combination of the previous methods, such as matrix-tree theorem for calculating conditional probabilities, and structured attention.
Therefore, the proposed method is incremental rather than innovative.

[significance]
Experiments on several varieties of datasets revealed that the proposed method consistently improved the performance from the baseline RN.
In contrast, it did not outperform the current best scores for all experiments comparing with the current published best methods.
Obviously, we have no reason that we must use RNs for such tasks.
Therefore, the actual effectiveness of the proposed method in terms of the actual task settings is unclear for me.
I concern about the actual calculation speed of the proposed method.
The proposed method seems to require much higher computational cost against the baseline RNs.

[Questions]
1, Regarding the approach in general, it would be nice to see how much it depends on the quality of the dependency parse. 
For example, we cannot always prepare a good parser for experiments on MT such as low-resource languages.
Do you have any comments for this?

2, Some experimental results showed that “RN intra-attn” was better than “Reccurent RNs”.
This implies for me that the higher-order dependency is useless for such tasks.
Are there any analyses why “Reccurent RNs” did not work well?

",5
"The main idea is to incorporate linguistic-based constrains in the form of dependency trees into different variations of relation networks.
In general, the paper is well written and organized, the presented problem is well motivated, and the approach is very strait forward. The experimental setting is comprehensive, and the results are indeed competitive in a wide range of tasks.
I think that using linguistic knowledge to improve Neural networks performance is very promising field, I think that you could get a much more substantial gains when applying your method in less resource-rich setups (maybe using some small subset of training for the SNLI and question duplication datasets).
It seems that your method relies heavily on previous works (RN, RNN-RN, latent dependency trees ,intra-sentence attention), can you please state clearly what your contribution is? does your model has any advantages over current state-of-the-art methods?   

edit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. ",5
"The paper presents an extension of relation networks (RNs) for natural language processing. RNs are designed to represent a set as a function of the representations of their elements. This paper treats a sentence as a set of words. Whereas regular RNs assume that the representation of a set is a uniform aggregation of the representation of the pairs of elements in the set, this paper proposes to weight the relevance of the pairs according to their tree-structured dependency relations between the words. The authors evaluate on a suite of NLP tasks, including SNLI, Quora duplicate question ID, and machine translation. They show marginal improvements over naive baselines, and no improvement over SOTA.

I am concerned about both the motivation for and the novelty of this work. My reading of this work is that the authors try to reverse engineer a TreeRNN in terms of RNs, but I am not sure what the reason is for wanting to use the RN framework in order to derive an architecture that, IIUC, essentially already exists. I can't find any fundamentally meaningful differences between the proposed architecture and the existing work on TreeRNNs, and the results suggest that there is nothing groundbreaking being proposed here. It is possible I am missing some key insight, but I do believe the burden is on the authors to highlight where the novelty is. The intro *and* related work sections should both be rewritten to answer the question: what is the insufficiency with current sentence encoding models that is addressed by this architecture? Currently, the intro addresses the tangential question: what is the insufficiency with RNs for NLP that is addressed by this architecture? If the latter is the question the authors want to answer, they need to first answer: why should we want to cast sentence encoders as RNs as opposed to any of the (many) other available architectures? Without a firmer understanding of what this paper contributes and why, I can't recommend acceptance. More detailed comments for the authors below. 

- You introduce a few naive baselines, but none of these is a TreeRNN. TreeRNNs are the obvious baseline, and you should be comparing on each and every evaluation task, even if there is no previously published result for using tree RNNs on that task. For the one result (SNLI, table 1) on which there is previous work using TreeRNNs, the table confirms my intuition that the proposed model is no improvement over the TreeRNN architecture. It seems very important to address this comparison across all of the evaluation tasks.
- I like the notion of marginalizing over latent tree structures, but the related work section needs to make clear what is being contributed here that is different from the cited past work on this problem
- On the MT eval, why are you missing values for zh-en on the NMT models that are actually competitive? I think many of these models are open-source or easy to reimplement? Its hard to draw conclusions when from such a gappy table.
- Only table 2 has significance values (over naive baseline that is) which implies that the other results are not significant? That is disconcerting. 
- I am disappointed in the analysis section. As is, you provide an ad-hoc inspection of some inferred trees. I find this odd since there is no evidence that the tree-ness of the architecture (as opposed to, e.g., recurrence or attention) is what leads to quantitative improvements (at least according to the experimental results in the tables), so there is no reason we should actually expect the trees to be good or interesting. My interpretation of these cherry-picked examples is that the learning is fighting the architecture a bit, basically ""learning a tree"" that reduces to being an attention mechanism that up-weights one or two salient words. 
- The analysis I *wanted* to see instead was why recursion helped for sentence classification, it did not for MT. You give an intuition for this result but no evidence. (That is assuming that, quantitatively, this trend actually holds. Which maybe is not the case if none of the results are significant.)
- In general, regarding evaluation, SNLI is overfit. You should use MNLI at least. I have trouble interpreting progress on SNLI as ""actual"" progress on language representation.
- The related work section as a whole is too short. If you need to cut space, move technical content to appendix, but don't compromise in related work. You listed many relevant citations, but you have no context to situate your contribution relative to this past work. What is the same/different about your method? You should provide an answer to that for each and every paper you cite. ",3
"Overview:

This paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.

Main strengths:

1. The paper is well written. The model description in Section 2 is very clear and precise.
2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.
3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.

Main weaknesses:

1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning are well-established). So this raises a concern about novelty (although the experimental results are new).

2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses ""far more resources."" The best ALL-CACO model also relies on several annotated but ""smaller"" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a ""similar"" amount of data from the specific task?

Overall feedback:

This is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.

Edit: I am changing my rating from 5 to 6 based on the authors' response.",6
"The paper proposes to transfer document classifiers between (closely) related languages by exploiting cross-lingual subword representations in a cross-lingual embedder jointly with word-based classifier: the embedder represents the words, while the classifier labels the document. The approach is reasonable, albeit somewhat unexciting, as the basic underlying ideas are in the vein of Pinter et al. (2017), even if applied on a different task.

The main concern I have with the paper is that it leaves much open in terms of exploring the dimension of (dis)similarity: How does the model perform when similarity decreases across language pairs in the transfer? The paper currently offers a rather biased view: the couplings French-Italian-Spanish, Danish-Swedish are all very closely related languages, and Amharic-Tigrinya are also significantly related. Outside these couplings, there's a paragraph to note that the method breaks down (Table 5 in the appendix). Sharing between Romance and Germanic languages is far from representative of ""loosely related languages"", for all the cross-cultural influences that the two groups share.

While the experiment is reasonably posed, in my view it lacks the cross-lingual breadth and an empirical account of similarity. What we do in cross-lingual processing is: port models from resource-rich to low-resource languages, and to port between very similar languages that already have resources is a purely academic exercise. This is not to say that evaluation by proxy should be banned, but rather that low-resource setups should be more extensively controlled for.

Thus, in summary, a rather straightforward contribution to computational modeling paired with sub-par experiment setup in my view amounts to a rejection. The paper can be improved by extending the experiment and controlling for similarity, rather than leaving it as implication.",4
"Summary: The authors address the task of cross language document classification when there is no training data available in the target language but data is available a closely related language. The authors propose forming character-based embeddings of words to make use of sub-word similarities in closely-related languages. The authors do an extensive evaluation using various combinations of related languages and show improved performance. In particular,  the performance is shown to be competitive with word-based models, which are tied to a requirement of resources involving the original language (such as MT systems, bilingual lexicons, etc). The authors show that their results are boosted when some additional resources (such as bilingual dictionaries of minimal size) are used in a multi-task learning setup.


- I would have liked to see some comparison where your model also uses all the resources available to CLWE based models (for example, larger dictionary, larger parallel corpus, etc)

- It is mentioned that you used parallel projection only for Amharic as for other languages you had enough RCV2 training data. However, it would be interesting to see if you still use parallel projection on top of this.

- I do not completely agree with the statement that CACO models are ""not far behind"" DAN models. IN Table 1, for most languages the difference is quite high. I understand that your model uses fewer resources but can it bridge the gap by using more resources? Is the model capable of doing so ?

- How did you tune the lambdas in Eqn 11? Any interesting insights from the values of these lambdas? Do these lambda values vary significantly across languages ?

- The argument about why the performance drops when you use language identifiers is not very convincing. Can you please elaborate on this ?

- Why would the performance be better in one directions as compared to another (North Germanic to Romance v/s ROmance to North Germanic). Some explanation is needed here.

- One recurring grievance that I have is that there are no insights/explanations for any results. Why are the gains better for some language pairs? Why is there asymmetry in the results w.r.t direction of transfer ? In what way do 2 languages help as compared to single source language? What is you use more that 2 source languages?

",6
"The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:

1) In model section, the task definition is not clear. It is expected to see what's the question generation task studied in this paper. An example or a model overview will definitly help.

2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf ""Seo, Minjoon, et al. ""Bidirectional attention flow for machine comprehension."" arXiv preprint arXiv:1611.01603 (2016).""  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.

3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?

4) In the training, how to deal the ground truth that are not in the vocab? The authors stated ""using a modified heuristic described below"", but no follow-ups in the paper.

5) The paper is not well written and organized. Small typos: in introduction, 'and and answer span', 'and output and output sequences'. In model, 'Glorot initialization', 'Bahdanau attention', it is not the common way to cite others' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder.

I have read the authors' detailed rebuttal. Thanks.",3
"This paper presents question generation models by designing variations of copying mechanism and reward functions. Experimental results show that different copying mechanism can improve upon basic seq2seq models, some of the reward functions also produce better results. I think the results are interesting, especially the ones compared with human evaluation (fig. 1), but it's might be better to explain on which aspect each of the feature contributes to the improvement. For instance, the authors can give some insights based on empirical results on what kind of questions will benefit from each type of copying. 


The authors should better organize table 1 and 2, and inform the readers on what is the consistent conclusion (if any). For table 2, there is no result for adding ""adversarial discriminator"" only. Also the item ""+226"" on the second row, is that an error?
",4
"In the paper, author investigate the use of copy mechanisms for the question generation task. It evaluates on the SQuAD dataset. The model is a popular seq2seq/encoder-decoder model with copy mechanisms using pointer networks. 

Pros:
It is well motivated. For the question generation task, a word to be predicted can be from either a global vocabulary list or copied from the given documents (location vocabulary).  There are some overlap between these two vocabulary lists.  This paper mainly investigates this issue.

It is well written and easy to follow.

Interesting analysis of human/automatic metrics.

Cons:
The tricks here are a bit of ad hoc. It is better to have a systemic study.

Baseline results are too low. E.g., officially QANet results (from the paper) on SQuAD v1 is around 82.7 (my implementation obtains 83.1). However in the paper, its best result is 72.6 in terms of F1 score. 

The authors only evaluated on one dataset. It is hard to convincing.

It is lack of comparison results of question generation in literature. 
",5
"This papers uses the label hierarchy to drive the search process over a set of labels using reinforcement learning. The approach offers clever and promising techniques to force the inference process in structured classification to converge, but experiments seem to lack apple-to-apple comparisons.

However, I think the authors should rather present this work as structured classification, as labels dependencies not modeled by the hierarchy are exploited, and as other graph structure could be exploited to drive the RL search.
I tend to see hierarchical classification as an approach to multi-label classification justified by a greedy decomposition that reduced both training and test time. This view has been outmoded for more than an decade, first as flat approaches became feasible, and now as end-to-end  structured classification is implementable with DNNs (see for instance David Belanger work with McCallum)

Compared to other structured classification approaches whose scope is limited by the complexity of the inference process, this approaches is very attractive. The authors open the optimization black box of the inference process by adding a few very clever tricks that facilitate convergence:
- Intermediate rewards based on the gain on F1 score
- Self critical training approach
- ""Clamped"" pre-training enabled by the use of state embeddings that are multiplied my a transition to any state in the free mode, and just the next states in the hierarchy in the clamped mode
- Addition of a flat loss to improve the quality of the document representation

While those tricks may have been used for other applications, they seem new in the context of hierarchical/multi-label/structured classification.

While the experiments appear thorough, they could be the major weakness of this paper. The results the authors quote as representative of other approaches seem in fact entirely reproduced on datasets that were not used on the original papers, and the authors do not try an apple-to-apple comparison to determine if this 'reproduction' is fair. None of the quoted work used the 2018 version of Yelp, and I could only find RCV1 Micro-F1 experiments in Johnson and Yang, who report a 84% micro-F1, far better than the 76.6% reported on their behalf here, and better than the 82.7% reported  by the authors. I read note 4 about the difference in the way the threshold is computed, but I doubt it can explain such a large difference. I did not check everything, but could not find and apple-to-apple comparison?

Have the network architecture been properly optimized in terms of hyper-parameters?
In particular, having tried Kim CNN on large label sets, I suspect the author settings using a single layer after the convolution is sub-optimal. I concur with the following paper than an additional hidden layer is essential: Liu et al ""Deep Learning for Extreme Multi-label Text Classification"". I also note the 32 batch size could be way too small for sparse label sets (I tend to use a batch size of 512 on this type of data).",5
"This work proposes an RL approach for hierarchical text classification by learning to navigating the hierarchy given a document. Experiments on 3 datasets show better performance. I'm happy to see that it was possible to 

1. ""we optimize the holistic metrics over the hierarchy by providing the policy network with holistic rewards""

I don't quite understand what are the ""holistic metrics"" and ""holistic rewards"". I would like the authors to answer ""what exactly does reinforcement learning get us ?""
 - Is it optimizing F1 metric or is it the ability to fix inconsistent labeling problem ? 
- If it is the latter, what is an example of inconsistent labeling, what fraction of errors (in table 2/3) are inconsistent errors. Are we really seeing the inconsistent errors drop ?
- If it is the former, how does this compare to existing approaches for optimizing F1 metric.

2. ""the F1 score of each sample xi""

a. F1 is a population metric, what does it mean to have F1 for a single sample ?
b. I'm not aware of any work that shows optimizing per-example f_1 minimizes f_1 metric over a sample.

3. with 10 roll-outs per training sample, imho, it seems unrealistic that the expected reward can be computed correctly. Would'nt most of the reward just be zero ? Or is it the case the model is initialized with an MLE pretrained parameters (which seems like it, but im not too sure).

Results analysis,
- imho, most of the rows in Table 2 does not seem comparable with each other due to pretrained word-embeddings and dataset filtering, e.g. SVM-variants, HLSTM.
- in addition to above, there is the standard issue of using different #parameters across models which increases/decreases model capacity. This is ok as long as all parameters were tuned on held out set, or using a common well established unfiltered test set - neither of which is clear to me.
- it is not clear how the F1 metric captures inconsistent labeling, which seems to be the main selling point for hi-lap. 

side comment
- reg textcnn performance, could it be that dropout is too high ? (the code was set to 0.5)
   ",4
"This paper presents an end to end rl approach for hierarchical text classification. The paper proposes a label assignment policy for determining the appropropriate positioning of a document in a hierarchy. It is based on capturing the global hierachical structure during training and prediction phases as against most methods which either exploit the local information or neural net approaches which ignore the hierarchical structure. It is demonstrated the method particularly works well compared to sota methods especially for macro-f1 measure which captures the label weighted performance. The approach seems original, and a detailed experimental analysis is carried out on various datasets. 

Some of the concerns that I have regarding this work are :
 - The problem of hierarchical text classification is too specific, and in this regard the impact of the work seems quite limited. 
 - The significance is further limited by the scale of the datasets of considered in this paper. The paper needs to evaluate against on much bigger datasets such as LSHTC datasets http://lshtc.iit.demokritos.gr/. For instance, the dataset available under LSHTC3 is in the raw format, and it would be really competitive to evaluate this method against other such as Flat SVM, and HRSVM[4] on this dataset, and those from the challenge.
- The experimental evaluation seems less convincing such as the results for HRSVM for RCV1 dataset are quite different in this paper, and that given HRSVM paper. It is 81.66/56.56 vs 72.8/38.6 reported in this paper. Given that  81.66/56.56 is not too far from that given by HiLAP, it remains a question if the extra computational complexity, and lack of scalability (?) of the proposed method is really a significant advantage over existing methods.
 - Some of the references related to taxonomy adaptation, such as [3] and reference therein,  which are also based on modifying the given taxonomy for better classification are missing.
 - Comparison with label embedding methods such as [1,2] are missing. For the scale of datasets discussed, where SVM based methods seem to be working well, it is possible that approaches [1,2] which can exploit label correlations can do even better.
[1] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, Sparse Local Embeddings for Extreme Multi-label Classification, in NIPS, 2015.
[2]  H. Yu, P. Jain, P. Kar, and I. Dhillon, Large-scale Multi-label Learning with Missing Labels, in ICML, 2014.
[3] Learning Taxonomy Adaptation in Large-scale Classification, JMLR 2016.
[4] Recursive regularization for large-scale classification with hierarchical and graphical dependencies, https://dl.acm.org/citation.cfm?id=2487644",4
"This paper describes a multi-task video classification and captioning model applied to a fine-grained object relationship video dataset, for a range of different classification and captioning tasks at different levels of granularity. This paper also creates a new video action dataset around kitchen objects and actions.  Finally, the paper includes an empirical study on both the multi-task performance and transfer learning performance between the two datasets considered.

Pros:
- This paper is clearly written and includes a thorough and well-laid out empirical component
- The contribution to the video action classification and captioning space seems like a worthwhile one

Cons:
- The novelty of this paper mainly seems to be with respect to video classification and captioning; other methodological aspects and empirical themes are interesting but fairly standard more generally.  The lack of experiments outside of one video action classification & captioning dataset (and one additional one for a transfer learning study) limit the empirical generality of the findings.

Overall take: This paper's contributions seem of interest to the video classification and captioning community, but less so to a broader or more methodologically-focused one such as ICLR.

Notes:
- The comments on insufficiency of existing video classification tasks in Sec. 3 are interesting, but seem pretty restricted to that specific domain
- The model used is a fairly standard CNN + LSTM video encoder, plus a basic MTL network approach with hard parameter sharing between tasks, as is commonly used today. Similarly, the transfer learning approach---pre-training on one task, then freezing layers and fine-tuning---is a standard approach.
- The empirical findings are interesting---for example, that training on fine-grained tasks improves coarse-grained accuracy, that MTL training is helpful, etc---but (a) seem in general like known themes, and (b) have limited generality either way beyond the specific types of tasks considered in the dataset examined.
- In general, much of the paper is focused on details specific to this application domain, rather than to general methods or themes potentially interesting to the broader ICLR community",5
"Summary
This paper studied video classification and video caption generation problem.
Especially, the paper tried few baseline architectures datasets used from pretraining features on recently proposed Something-something-V2 dataset and another newly proposed dataset.
This paper argues that fine-grained annotation helps learning good features and enhance transfer learning results

Strength
There are some interesting observations in terms of transfer learning.
Especially, comparison of fine-grained and coarse-grained dataset for transfer learning (Table 2), and effect of using caption and newly collected dataset for transfer learning (Figure 5) is interesting and the result is worth to be shared to the community.
In addition, a new dataset that are carefully collected for transfer learning might be useful to make progress on video classification and captioning.

Weakness
To many tables with different neural network parameter settings look distracting and does not provide much information. Instead, focusing more on effect of dataset for transfer learning and providing more analysis on this aspect would make the main argument of this paper stronger.
For example, effect of transfer could be studied on different dataset. If transfer learning with proposed dataset containing find-grained annotation / captions is useful, it might help boosting performance on other video recognition dataset as well.
Providing analysis on understanding the effect of fine-grained / captioning dataset for feature learning might help understanding as well.

Overall rating
This paper suggest interesting observations and useful dataset, but provides relatively less analysis on these observations. I believe providing more analysis on the dataset and effect of transfer would make the main argument of this paper stronger.",5
"Paper Summary - This paper presents an approach for fine-grained action recognition and video captioning. The authors train a model using both classification and captioning tasks and show that this improves performance on transfer learning tasks. The method is evaluated on the Something-Something v2 dataset as well as a new dataset (proposed in this paper). The authors also evaluate the benefit of using fine-grained action categories vs. coarse-grained action categories on transfer learning.

Paper Strengths
-  Comparing fine-grained vs. coarse-grained action categories for transfer learning is well motivated. Evaluating just this aspect in the context of video classification is helpful (Section 5.1). Establishing the baseline using linear classifiers for feature transfer makes the feature transfer result more robust. The authors have also done a good job of evaluating their method in the coarse-grained and fine-grained settings (Table 1, 2).
- The architectural and experimental design in this paper is well illustrated.
- The 20bn kitchen dataset has interesting categories about intention - pretending to use, using, and using & failing.
- The ablation in Table 1 is helpful in understanding the contribution of 3D vs. 2D convolutions.

Paper Weaknesses
- I believe this paper tries to do too much and as a result fails to show results convincingly. There are too many results and not much focus on analyzing them. In my opinion, the experimental setup in the paper is weak to fully support the authors' claims.
- I now analyze the main contributions of this paper as outlined by the authors in Section 1.
    - Label granularity and feature quality: To me this is the most interesting part of this paper and most related to its title. However, this is also the most under-analyzed aspect. The only result that the authors show is in Sec 5.1 and Fig 5. Apart from using the provided fine-grained vs. coarse-grained labels for evaluation, the authors do not perform many experiments in this domain and neither do they analyze these results. For example, the gain using fine-grained labels is not significant in Figure 5 (2Channel - CG vs. 2Channel - FG). The authors do not explain this aspect. Another missing baseline from Figure 5 is ""2Channel - Captions & CG actions"". This baseline is needed to understand the contribution of FG vs CG actions when also using captioning as additional supervision.
    - Baselines for captioning: The authors do not provide any details for this task. If the intent is to establish baselines there needs to be more effort on analyzing design decisions - e.g. decoding, layers in LSTM. Captioning metrics such as CIDER and SPICE are missing.
    - Captions as a source for transfer learning: This is poorly analyzed in this paper. 1) Can the captions be converted to ""tags"" and then used for supervision? What is the benefit of producing the full sequential text description over this simple approach? 2) Captions for transfer learning are only analyzed in Figure 5 without much explanation. It is hard to claim that captioning is the reason for performance gains without really analyzing it completely.
    - 20bn-kitchenware dataset - This dataset is explained in just one paragraph in Section 6. What is the motivation behind collecting this dataset as opposed to showing transfer learning on some other dataset?
- Missing references
        - There has been work in understanding the effect of fine-grained categories in ImageNet transfer learning - What makes ImageNet good for transfer learning? Huh et al. What is the insight provided over this work?
- Minor comments
    - Section 1: Figure 4 is referenced in points 1 & 3. I think you mean Figure 5.
",5
"A genetic algorithm is used to do an evolutionary architecture search to find better tree-like architectures with multiple memory cells and recurrent paths. To speed up search, an LSTM based seq2seq framework is also developed that can predict the final performance of the child model based on partial training results.

The algorithms and intuitions based on novelty search are interesting and there are improvements over baseline NAS model with the same architecture search space. 

Although, the experiments are not compared against latest architectures and best results. For example on PTB, there are new architectures such as those created by ENAS that result in much lower perplexity than best reported in Table 1, for the same parameter size. While you have mentioned ENAS in the related work, the lack of a comparison makes it hard to evaluate the true benefit if this work compared with existing literature. 

There is no clear abolition study for the Meta-LSTM idea. Figure 4 provides some insights but it'd be good if some experiments were done to show clear wins over baseline methods that do not employ performance prediction.

There are many typos and missing reference in the paper that needs to be fixed.",5
"This paper explores evolutionary optimization for LSTM architecture search. To better explore the search space, authors used tree-based encoding and Genetic Programing (GP) with homologous crossover, tree distance metric, etc.  The search process is pretty simple and fast. However, there is a lack of experiments and analysis to show the effectiveness of the search algorithm and of the architecture founded by the approach. 

Remarks:
The contents provided in this paper is not enough to be convinced that this is a better approach for RNN architecture search and for sequence modeling tasks. 
This paper requires more comparisons and analysis.

Experiments on Penn Tree Bank
 - The dataset on both experiments are pretty small to know the effect of the new architecture they found. More experiments on larger datasets e.g., wikitext-2 will be needed. 
 - In the paper ""On the state of the art of evaluation in neural language models"", Melis et al., 2018 reported improvement using classic LSTM over other variations of LSTM. They intensively compared the performance of classic LSTM, NAS, and RHN (Recurrent Highway Network) as authors did. Melis et al. reported LSTM (with depth 1) can already achieve a test perplexity of 59.6 with 10M parameters and 59.5 with 24M parameters.
- Could you analyze a new finding of the LSTM architecture compared to the classic LSTM and NAS? Figure 5 and 6 are not very clear how are their final architectures different and the important/useful nodes changes for different tasks?
- Recently, there are a number of architecture search algorithms introduced, but there is only one comparison in this direction (Zoph&Le16). It is important to compare this approach with other architecture search methods.",4
"The authors apply (tree-based) genetic programming (GP) to RNN search, or more specifically RNNs with memory cells, with the foremost example of this being the LSTM. GP provide a structured search that seems appropriate for designing NN modules, and has previously been applied successfully to evolving CNNs. However, the authors fail to mention that (tree-based) GP has been applied to evolving RNN topologies as far back as 2 decades ago, with even multiple cells in a single RNN unit [1]. The selection of more advanced techniques is good though - use of Modi for allowing multiple outputs, and neat-GP for more effective search (though a reference to the ""hall of fame"" [2] is lacking).

The authors claim that their method finds more complex, better performing structures than NAS, but allow their method to find architectures with more depth (max 15 vs. the max 10 of NAS), so this is an unfair comparison. It may be the case that GP scales better than the RL-based NAS method, but this is an unfair comparison as the max depth of NAS is not in principle limited to 10.

The second contribution of allowing heterogeneity in the layers of the network is rather minimal, but OK. Certainly, GP probably would have an advantage when searching at this level, as compared to other methods (like NAS). Performance prediction in architecture search has been done before, as noted by the authors (but see also [3]), so the particular form of training an LSTM on partial validation curves is also a minor contribution. Thirdly, concepts of archives have been in use for a long time [2], and the comparison to novelty search, which optimises for a hand-engineered novelty criteria, reaches beyond what is necessary. There are methods based on archives, such as MAP-Elites [4], which would make for a fairer comparison. However, I realise that novelty search is better known in the wider ML community, so from that perspective it is reasonable to keep this comparison in as well.

Finally, it is not surprising that GP applied to searching for an architecture for one task does not transfer well to another task - this is not specific to GP but ML methods in general, or more specifically any priors used and the training/testing scheme. That said, prior work has explicitly discussed problems with generalisation in GP [5].

[1] Esparcia-Alcazar, A. I., & Sharman, K. (1997). Evolving recurrent neural network architectures by genetic programming. Genetic Programming, 89-94.
[2] Rosin, C. D., & Belew, R. K. (1995, July). Methods for Competitive Co-Evolution: Finding Opponents Worth Beating. In ICGA (pp. 373-381).
[3] Zhou, Y., & Diamos, G. (2018). Neural Architect: A Multi-objective Neural Architecture Search with Performance Prediction. In SysML.
[4] Mouret, J. B., & Clune, J. (2015). Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909.
[5] Kushchu, I. (2002). An evaluation of evolutionary generalisation in genetic programming. Artificial Intelligence Review, 18(1), 3-14.",4
"The paper generalized the approach for safe RL by Chow et al, leveraging Lyapunov functions to solve constraint MDPs and integrating this approach into policy optimization algorithms that can work with continuous action spaces.
This work derives two classes of safe RL methods based on Lyapunov function, one based on constrained parameter policy optimization (called theta-projection), the other based on a safety layer. These methods are evaluated against Lagrangian-based counter-parts on 3 simple domains. 

The proposed Lyapunov-function based approach for policy optimization is certainly appealing and the derived algorithms make sense to me. This paper provides a solid contribution to the safe RL literature though there are two main reasons that dampen the excitement about the presented results:
First, the contribution is solid but seems to be of somewhat incremental nature to me, combining existing recent techniques by Chow et al (2018) [Lyapunov-function based RL in CMDP], Achiam et al (2017) [CPO with PPO is identical to SPPO] and Dalal et al (2018) [safety layer]. 
Second, the experimental results do not seem to show a drastic benefit over the Lagrangian baselines. It is for example unclear to me whether the jiggling of the Lagrange approach around the threshold is a problem is practice. Further, it seems that PPO-Lagrangian can achieve much higher performance in the Point and Gather task while approximately staying under the constraint threshold of 2. Also, as far as I understand, the experiments are based on extensive grid search over hyper-parameters including learning rates and regularization parameters. However, it is not clear why the initialization of the Lagrange multiplier for the Lagrangian baselines was chosen fixed. I would be curious to see the results w.r.t. the best initial multiplier or a comparison without hyper-parameter grid search at all. 

This is a light review based on a brief read.

Minor note:
In the figures: where is the magenta line? I assume magenta labels refer to teal lines?",6
"￼
To my understanding, this paper builds on prior work from Chow et al. to apply Lyapunov-based safe optimization to the policy-gradient setting. This seems is similar to work by Achiam 2017. While this work seems like an interesting framework for encompassing several classes of constrained policy optimization settings in the Lyapunov-based setting, I have some concerns about the evaluation methodology. 

It is claimed that the paper compares against “two baselines, CPO and the Lagrangian method, on several robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost.” Then it is stated in the experimental section “However since backtracking line-search in TRPO can be computationally expensive, and it may lead to conservative policy updates, without loss of generality we adopt the original construction of CPO to create a PPO counterpart of CPO (which coincides with SPPO) and use that as our baseline.” This seems directly to contrast to the earlier statement which states that it is unclear how to modify the CPO methodology to other RL algorithms. Moreover, is this really a fair comparison? The original method has been modified to form a new baseline and I’m not sure that it is “without loss of generality”. 

Also, it is unclear whether the results can be accepted at face value. Are these averaged across several random seeds and trials? Will performance hold across them? What would be the variance? Recent work has shown that taking 1 run especially in MuJoCo environments doesn’t necessarily provide statistically significant values. In fact the original CPO paper shows the standard deviations across several random seeds and compares directly against an earlier work in this way (PDO). Moreover, it is unclear why CPO was not directly compared against and neither was the ""equivalent"" baseline not compared on similar environments as in the original CPO paper. 

Comments:

Figure 3 is difficult to parse, the ends of the graphs are cut off. Maybe putting the y axis into log format would help with readability here or having the metrics be in a table.",5
"In this paper, authors compare different ways to enforce stability constraints on trajectories in dynamic RL problems. It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned ""CPO"") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method. While this approach is reasonable indeed, the novelty of the approach is questionable, not only in light of recent papers but older literature: inference of Markov Decision Processes under constraints is referred to and has been known a long time. Furthermore, the actual tasks chosen are quite simple and do not present complex instabilities. Also, actually creating a Lyapunov function and weighing the relative magnitude of its second derivative (steep/shallow) is not trivial and must influence the behavior of the optimizer. Also worth mentioning that complex nonlinearities might imply that instabilities in the observed dynamics are not seen and learned unless the space exploration is conservative. That is, comparison of CPO and Lagrangian constraint based RL with Lyapunov based method proposed depends on a lot of factors (such as those just mentioned) that are not systematically explored by the paper.  ",6
"In this paper, authors propose safe policy optimization algorithms based on the Lyapunov approach to constrained Markov decision processes.
The paper is very well written (a few typos here and there, please revise) and structured, and, to the best of my knowledge, it is technically sound and very detailed.
It provides incremental advances, mostly from Chow et al., 2018.
It fairly accounts for recent literature in the field.
Experimental settings and results are fairly convincing.

Minor issues:
Authors should not use not-previously-described acronyms (as in the abstract: DDPG, PPO, PG, CPO)",8
"This paper proposes an imitation learning algorithm framed as an off-policy RL scenario. They introduce all demonstrations into a replay buffer, with positive reward r=1. Subsequent states derived from the agent will be also introduced in the replay buffer but with reward r=0. There is no additional reward involved. The hope is that agents will learn to match the expert in states appearing in the replay buffer and not try to do much else.

Overall, I am not convinced that the paper accurately supports its claims.

1.	The authors support their method by saying that extending GAIL and other imitation learning algorithms to support pixel observations has failed. However, there are actually some papers showing quite successful applications of doing that: e.g. see Li et al. 2017 in challenging driving domain in TORCS simulator. 

2.	More problematic, I think there is a clear flaw with this algorithm: imagine having trained successfully and experiencing many trajectories that are accurately reproducing the behaviour of the expert. Given their method, all these new trajectories will be introduced into the replay buffer with a reward of 0, given they come from the agent. What will the gradients be when provided with state-action pairs with both r=0 and r=1? These situations will have high variance (even though they should be clear r=1 situations) and this will hinder learning, which will tend to  decay the behaviour as time goes on.
This actually seems to be happening, see Figure 1 at the end: both SQIL curves appear to slowly starting to decay.
This is why GAIL is training its discriminator further, you want to keep updating the distribution of “agent” vs “expert”, I’m not sure how this step can be bypassed?

3.	How would you make sure that the agent even starts encountering rewarding states? Do you need deterministic environments where this is more likely to happen? Do you need conditions on how much of the space is spanned by the expert demonstrations?

4.	Additionally, Figure 1 indicates BC and GAIL as regions, without learning curves, is that after training? 

5.	I am not convinced of the usefulness of the lengthy derivation, although I could not follow it deeply. Especially given that the lower bound they arrive at does not seem to accurately reflect the mismatch in distributions as explained above. 

6.	There are no details about the network architecture used, about the size of the replay buffer, about how to insert/delete experience into the replay buffer, how the baselines were set up, etc. There are so few details I cannot trust their comparison is fair. The only detail provided is in Footnote 5, indicating that they sample 50% expert and 50% agent in their mini-batches.

Overall, I think the current work does not offer enough evidence and details to support its claims, and I cannot recommend its publication in this current form

",5
"The paper introduces a relatively simple method for imitation learning that seems to be successful despite its simplicity. The method, SQIL, assigns a constant positive reward (r) to the demonstrations and zero reward to generated trajectories. While I like the connections between SQIL and SQL and the simplicity of the idea, I think there are several issues which connections with GAIL that are not discussed; some ""standard"" environments (such as Mujoco) that SQIL has not compared against the baselines. I believe the paper would have a bigger impact after some addressing some of the issues.

(
Update: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.

The Pong case is also quite interesting, although it seems slightly ""unfair"" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards.
)

1. The first issue is the similarity with GAIL in the ""realistic"" setting. Since we cannot have infinite expert demonstrations, there would always be some large enough network that could perfectly distinguish the demonstrations (assign reward to 1) and the generated policies (assign reward to 0). Therefore, it would seem to me that from this perspective SQIL is an instance of GAIL where the discriminator is powerful and expert demos are finite (and disjoint from generated trajectories, which is almost always the case for continuous control). In the finite capacity case, I am unsure whether the V and Q networks in SQIL does a similar job as the discriminator in GAIL / AIRL type algorithms, since both seem to extrapolate between demonstrations and generations?

2. Moreover, I don't think SQIL would always recover the expert policy even with infinite demonstrations. For example, lets think about the Reacher environment, where the agent controls a robotic arm to reach a target location. The expert demonstration is the fastest way of reaching the target (move minimum distance between joints). If we consider the MDP to have possibly very large / infinite horizon (only stops when it reaches the target), I could construct a hack policy that produces larger episodic reward compared to the expert. The policy would simply move back and forth between two expert demonstrated states, where it would receive 1 reward in the states for odd time and 0 reward for the states for even time. The reward would be something like 1 / (1 - \gamma^2) compared to the experts' reward which is \sum_{i=0..T} \gamma^{i} = (1 - \gamma^{T+1}) / (1 - \gamma). 

Some fix would be to set the reward for generated policies to be negative, or introduce some absorbing state where the expert will still receive the positive reward even after reaching the target (but that is not included in demonstrations). Nevertheless, a suitable reward prior seems to be crucial to the success of this SQIL, as with GAIL requiring reward augmentation.

3. Despite the above issues, I think this could be a very practical method due to its (perhaps surprising) simplicity compared to GAIL. However, the experiments only considered two environments that are not typically considered by GAIL; I believe SQIL would make a bigger impact if it is compared with GAIL in Mujoco environments -- seems not very implementation heavy because your code is based on OpenAI baselines anyway. Mujoco with image inputs would also be relevant (see ACKTR paper).

Minor points:
- What is the underlying RL algorithm for GAIL? It would seem weird if you use Q-learning for SQIL and TRPO for GAIL, which makes it impossible to identify whether Q-learning or SQIL contributed more to the performance. While GAIL used TRPO in the original paper, it would be relatively straightforward to come up with some version of GAIL that uses Q-learning. 
- Some more details in background for MaxEnt RL to make the paper more self contained.
- More details about the hyperparameters of SQIL in experiments -- e.g. what is \lambda?
- Did you pretrain the SQIL / GAIL policies? Either case, it would be important to mention that and be fair in terms of the comparison.
- Why does SQIL-11 perform worse than SQIL even though it is a tighter bound?
- wrt. math -- I think the anonymous comment addressed some of my concerns, but I have not read the updated version so cannot be sure.",6
"In this paper, the author derived a unified approach to utilize demonstration data and the data collected from the interaction between the current policy and the environment, inspired from soft Bellman equation. Different from previous methods such as DQfD (Hester et al., 2017), SQIL does not require the reward signal of the expert data, which is more general and natural for real-world applications such as demonstration from the human.  The author verified SQIL on a toy Lunar Lander environment and a high-dimension image based observation environment, which demonstrate its advantages over behavior cloning and GAIL. Besides the advantages, I have serval concern which may help the author to further improve the paper.

- The core algorithm is simple but I found the derivation is hard to read, which is a little messy from equation (5) to (7), and also the final loss (14) seems to be unrelated to the previous derivations( from equation (11) to (13)).  Also, can you add the gradient updates for $\theta$ with equation (11) (denoted by SQIL-11 in your paper)? I am looking forward to reading the revised version.

- To demonstrate the advantages of SQIL in high-dimension observations, the author only conducts one simple environment Car Racing, which is not enough to demonstrate its advantages. I wonder if it is possible to run more benchmark environments such as Atari game or Minecraft.

- In the related work, the author argues that methods such as DQfD require reward signal, but it would be great to demonstrate the advantages of SQIL over these methods (including DQfD and NAC (Gao et.al, 2018)).

- In previous imitation methods such as GAIL, they studied the effect of the amount of the demonstration data, which the paper should also conduct similar experiments to verify the advantage of SQIL.


Hester, Todd, et al. ""Deep Q-learning from Demonstrations."" arXiv preprint arXiv:1704.03732 (2017).
Gao, Yang, et al. ""Reinforcement learning from imperfect demonstrations."" arXiv preprint arXiv:1802.05313 (2018).",5
"This paper generalizes basic policy gradient methods by replacing the original Gaussian or Gaussian mixture policy with a normalizing flow policy, which is defined by a sequence of invertible transformations from a base policy.

Although the concept of normalizing flow is simple, and it has been applied to other models such as VAE, there seems no work on applying it for policy optimization. Thus I think this method is itself interesting.

However, I find the paper written in a way assuming readers very familiar with related concept and algorithms in reinforcement learning. Thus although one can get the general idea on how the method works, it might be difficult to get a deeper understanding on some details.

For example, normalizing flows are defined in Section 4, and then it is directly claimed that normalizing flows can be applied to policy optimization, without giving details on how it is actually applied, e.g., what is the objective function? and why one needs to compute gradients of the entropy (Section 4.1)?

Also, in the experiments, it is said that one can combing normalizing flows with TRPO without describing the details. I can't get how exactly normalizing flows + TRPO works.

The experiments also talk about 2D bandit problem, and again, without any descriptions. BTW, in the Section 4.3, what does [-1, 1]^2 mean? (I have seen {-1, 1}^2, but not [-1, 1]^2).

It seems that the authors only use the basic normalizing flow structures studied in Rezende&Mohamed (2015) and Dinh et al (2016). However, there are more powerful variants of normalizing flows such as the Multiplicative Normalizing Flows or the Glow. I wonder how good the results are if these more advanced versions are used. Maybe they can uniformly outperform Gaussian policy?

Update:
I feel the idea of this paper is straightforward, and the contribution is incremental. To improve the paper, stronger experiments need to be performed. ",4
"The authors in this work present an approach to policy optimization that relies on an alternative policy formulation based on normalizing flows. This is a relatively simple modification (this is no criticism) that essentially uses the same TRPO algorithm as previous approaches, but a different mechanism for generating the distribution over actions. The crux of the authors’ approach is detailed in equations (6) and (7), although it could have been useful to see more of the discussion of the architecture from appendix B in the actual text of the paper.

The authors then go on to analyze the properties and expressiveness of the resulting properties and show that it is more capable of capturing complex interactions than a simple Gaussian. It was somewhat unclear, however, in section 4.2 what the exact form of the policies being compared are. Is this a simple example with only the parameters of the Gaussian, or was the Gaussian parameterized by a multi-layer model? Further, one thing I would also have liked to see the authors question more is, for the problems they attack, whether this expressiveness is more useful “during exploration” or for the ultimate performance of the final policy.

The authors, finally, show that this approach is able to out-perform the alternative Gaussian policy. Ultimately this approach seems to be a simple modification (or replacement) of the standard policy formulation, and one that seems to lead to good performance gains. ",6
"The papers proposed to use normalizing flow policies instead of Gaussian policies to improve exploration and achieve better sample complexity in practice. While I believe this idea has not specifically been tried in previous literature and the vague intuition that NF leads to more exploration that helps learning a better policy, the novelty of combining these two seems limited, and the paper does not seem to provide enough justification to using NF policies instead of alternative policy distributions both in theory and in the experiments.

1. About Section 4.2. I believe that the normalizing flow in question would transform the volume of a Gaussian? So there would exist some parameter setting for a flow model that also shrinks volume, thereby resulting in lower variance policies? The arguments would thereby depend heavily on the specific architecture and initialization of the flow model, which is not discussed in detail. 

Also, why is finding a high variance policy better in terms of the trust region argument? Isn't the whole point of using trust region that the new policy should be closer to old policy to prevent performance degradation? I also think that a fair comparison would be compare KL between normalizing flow policies, instead of KL between NF and Gaussian.

2. The TRPO experiments seem wrong -- at least the results don't match what is reported in the ACKTR paper for Reacher and InverseDoublePendulum envs -- there the TRPO policy at least learns something. Also TRPO in general does not perform as bad as it may seem, see ""Deep RL that matters"" paper by Henderson et al. Maybe this is because of using OpenAI baselines code which seems to have worse TRPO performance.

There is also no experiments on ACKTR on the small Mujoco tasks (even in the Appendix), which seems to be a rather big oversight given the authors have already done even harder tasks for ACKTR + NF.

Moreover I think a fair comparison is to use almost the same architecture for implicit and gaussian, where the only difference is where you sample the noise. For Gaussian with flows, you can first use an MLP to produce deterministic outputs and then use flow to generate the mean actions. Otherwise it is impossible to say whether the architecture or the implicit distribution contributes more to the success.

One could also use truncated Gaussian distributions / Beta distributions / Gaussian + tanh, since Mujoco actions beyond (-1, 1) is treated as -1 or 1, so Gaussian should already be bad. It is unclear whether NF is able to outperform these settings. 

Minor points:

- Fix citations. Please use \citep throughout.
- Is Equation (6) correct? Seems like \Sigma_i should be the inverse of g_i(\epsilon)? Also this is the ""change of variables formula"" not ""chain rule"".
- Why is normalizing flow not part of the background?
- Add legends in Figure (1)
- Figure 2(c), I believe with max entropy you could already obtain diverse ant trajectories?
- I believe in the context of generative models, ""implicit"" typically means the case where likelihood is not tractable? Here the likelihood is perfectly tractable.",4
"Overview: 
This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at ICLR. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time.

Pros:
The paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.

Issues:

* (p.5) ""R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients.""

This statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian).

Control variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook ""Simulation.""

The fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s ""Mirage of Action-Dependent Baselines"", https://arxiv.org/abs/1802.10031.

Since the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale.

* The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: ""We choose the baseline ... to be a function of state ... it must be independent of the action ...."" and ""it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased."" In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited.

Cons
* Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. 
* I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.
* The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper.

*EDIT: 
I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:

(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that ""everyone knows"" what is meant when the actual claim is misleading.

(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.

(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.

The variance of the policy gradient estimator, subject to a baseline ""phi,"" is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from ""phi(a,s)"", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:
""We expect this to be the case when single actions have a large effect on the overall discounted
return (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward).""
Please see Sec. 3, ""Policy Gradient Variance Decomposition"" of the Mirage paper for further details.
The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. 

It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in ""A Better Second Order Baseline"" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.

(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.

(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation.",3
"This paper extends the ""infinitely differentiable Monte Carlo gradient estimator"" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates.

The paper is fairly clear and well written, and shows significant improvements on the tasks used in the DiCE paper.

I think the paper would be a much stronger submission with the following improvements:

- More explanation/intuition for how the authors came up with their new baseline (eq. (8)). As the paper currently reads, it feels as if it comes out of nowhere.
- Some analysis of the variance of the two terms in the second derivative in eq. (11). In particular, it would be nice to show the variance of the two terms separately (for both DiCE and this paper), to show that the reduction in variance is isolated to the second term (I get that this must be the case, given the math, but would be nice to see some verification of this). Also I do not have good intuition for which of these two terms dominates the variance. 
- I appreciate that the authors tested their estimator on the same tasks as in the DiCE paper, which makes it easy to compare them. However, I think the paper would have much more impact if the authors could demonstrate that their estimator allows them to solve new, more difficult problems. Some of these potential applications are discussed in the introduction, it would be nice if the authors could demonstrate improvements in those domains.

As is, the paper is still a nice contribution.",6
"Thank you for an interesting read.

This paper extends the recently published DiCE estimator for gradients of SCGs and proposed a control variate method for the second order gradient. The paper is well written. Experiments are a bit too toy, but the authors did show significant improvements over DiCE with no control variate.

Given that control variates are widely used in deep RL and Monte Carlo VI, the paper can be interesting to many people. I haven't read the DiCE paper, but my impression is that DiCE found a way to conveniently implement the REINFORCE rules applied infinite times. So if I were to derive a baseline control variate for the second or higher order derivatives, I would ""reverse engineer"" from the exact derivatives and figure out the corresponding DiCE formula. Therefore I would say the proposed idea is new, although fairly straightforward for people who knows REINFORCE and baseline methods.

For me, the biggest issue of the paper is the lack of explanation on the choice of the baseline. Why using the same baseline b_w for both control variates? Is this choice optimal for the second order control variate, even when b_w is selected to be optimal for the first order control variate? The paper has no explanation on this issue, and if the answer is no, then it's important to find out an (approximately) optimal baseline for this second order control variate. 

Also the evaluation seems quite toy. As the design choice of b_w is not rigorously explained, I am not sure the better performance of the variance-reduced derivatives generalises to more complicated tasks such as MAML for few-shot learning.

Minor:
1. In DiCE, given a set of stochastic nodes W, why did you use marginal distributions p(w, \theta) for a node w in W, instead of the joint distribution p(W, \theta)? I agree that there's no need to use p(S, \theta) that includes all stochastic nodes, but I can't see why using marginal distribution is valid when nodes in W are not independent.

2. For the choice of b_w discussed below eq (4), you probably need to cite [1][2].

3. In your experiments, what does ""correlation coefficient"" mean? Normalised dot product?

[1] Mnih and Rezende (2016). Variational inference for Monte Carlo objectives. ICML 2016.
[2] Titsias and Lázaro-Gredilla (2015). Local Expectation Gradients for Black Box Variational Inference. NIPS 2015.",5
"In this paper, the author proposed a better control variate formula for second-order Monte Carlo gradient estimators, based on a special version of DiCE (Foerster et al, 2018).  The motivation and the main method is easy to follow and the paper is well written.  The author followed the same experiments setting as DiCE, numerically verifying the advantages of the newly proposed baseline, which can estimate the Hession accurately. 

The work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and multi-agent reinforcement learnings.  However, the advantage of the proposed method is not verified thoroughly. The only real application demonstrated in the paper, can be achieved the same performance as the second-order baseline using a simple trick.  Since this work only focuses on second-order gradient estimations, I think it would be better to verify its advantages in various scenarios such as meta-learning or sparse reward RL  as the author suggested in the paper.

Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" ICML 2017.
Foerster, Jakob, et al. ""DiCE: The Infinitely Differentiable Monte-Carlo Estimator."" ICML 2018.
",6
"The paper proposes a model-based value-centric (MVC) deep RL algorithm for transfer learning. The algorithm optimizes neural networks to estimate the deterministic transitions and rewards, and uses the these models to learn a value function by minimizing the Bellman residual. Policy is represented implicitly as the action that greedily maximizes the return, expressed in terms of the learned models. The experiments show some improvement on transferability over DDPG and TRPO policies.

The paper has two relatively independent stories: The title and the introduction motivates the work by discussing the transferability of policies and value functions. However, instead of rigorously evaluating transferability, the paper proposes a model-based algorithm (MVC) for learning policies for continuous actions. Novelty of the new algorithm is quite limited, as it simply uses a learned dynamics model and reward function to learn a value function. Regarding transferability, introducing MVC seem quite orthogonal, and instead, it would be better to have a clear comparison of transferability using existing methods (e.g., DDPG). If having an explicit policy network hurts transferability, then existing algorithms can be modified by replacing the actor with greedy maximization, or alternatively other value based methods that do not involve actor network (NAF, SQL, QT-Opt) could be used.

Regarding the intuition why values transfer better, the examples given in the introduction and Section 3 are good and intuitive. However, from my experience, the limited information content of a policy is only a partial reason for poor transferability, and in practice I have seen policies to transfer, in fact, better than values. The chosen viewpoint based on information content is nice as it can be proven mathematically, but might not be the most insightful and important in practice. The experimental evaluation is not rigorous enough to allow drawing further conclusions. For example, one could compare the two approaches using a wider set of RL algorithms, include more realistic environments (ideally transfer to real-world), or have a heat map illustrating transferability w.r.t. selected parameters. Also, no comparison to the  state-of-the-art methods is provided (PPO, TD3, SAC).

Minor points:
- Please include the theorems in Section 5 (and proofs in the appendix). The intuition provided in the body is not very clear.
- Why is it necessary to assume a deterministic dynamics model? Why only the dynamics model can vary between the domains and not also the reward (second paragraph in Section 1)?
",5
"The paper considers the problem of transfer in continuous-action deep RL. In particular, the authors consider the setting where the dynamics of the task change slightly, but the effect on the policy is significant. They suggest that values are better suited for transfer and suggest learning a model to obtain these values.

Overall, there are interesting ideas here, but I am concerned about whether the proposed approach actually solves the problem the authors consider and its general applicability.

The point about value functions being better suited for transfer than policies is indeed true for greedy policies: it is well-known that they are discontinuous, and small differences in value can result in large differences in policy. This point is hence relevant in continuous control, where deterministic policies are considered.

But I am a bit confused as to why the proposed approach is better though. Eq. (4) still takes a max w.r.t. the estimated dynamics, etc. So even if the value function is continuous, by taking the max, we get a deterministic policy which has the same problem! That is probably why the performance is quite similar to DDPG. Considering a softer policy parameterization (a continuous softmax analogue) would be more in line with the authors’ motivation.

The proposed method itself doesn’t seem generally practical unfortunately, as it is suggested to learn the *model* of the environment for with a high-dimensional state space and a continuous action space, and do value iteration. In other words, if Property 2 was easy to satisfy, we wouldn’t be struggling with model-based methods as much as we are! However, I do appreciate that the authors illustrate the model loss curves in their considered domains. This raises a question of when are dynamics “easy”.

The theoretical justification is quite weak, since the bound in Proposition 2 is too loose to be meaningful (as the authors themselves acknowledge). One way to mitigate this would be to support it empirically, by considering a range of disturbances of the specified form, and showing the shape of the bound on a small domain. The same thing can be done for the parametric modifications considered in the experiments -- instead of considering a set of instances, consider the performance as a function of the range of disturbances to the same dynamics parameter.

Minor comments:
* The italicization of certain keywords in the intro is confusing, in particular precise, imprecise -- these aren’t well-defined terms, and don’t make sense to me in the mentioned context. The policy function isn’t more “precise” than the value.
* I suggest including the statements of the propositions in the main text",4
"This paper proposes a model-based value-centric (MVC) framework for transfer learning in continuous RL problems, and an algorithm within that framework. The paper attempts to answer two questions: (1) ""why are current RL algorithms so inefficient in transfer learning"" and (2) ""what kind of RL algorithms could be friendly to transfer learning by nature""? I think these are very interesting questions to investigate, and researchers that work on transfer learning could benefit from insights on them. However, I am not yet convinced that this paper answers these questions satisfyingly. It would be great to hear the author's thoughts on my questions below. 

The main insight I take away from the paper is that policy gradient methods are not suitable for transfer learning compared to model-based and value-centric methods for some assumptions (the reward function not changing and the transition dynamics being deterministic). This insight and the experiments in the paper are interesting, but I am unsure if the paper as it is presented now passes the bar for ICLR.

In general the paper has two contributions:
A) analysis of value-centric vs policy-centric methods
B) an algorithm that is more useful for transfer learning.

Regarding A)
The authors argue that policy-centric algorithms are less useful for transfer learning than value-centric methods. 

They first illustrate this with an example in Section 3. Since this is just one example, as a reader I wonder if it would not be possible to construct an example that shows the exact opposite, where value iteration fails but policy gradient doesn't. It feels like there are many assumptions that play into the given example (the reward function not changing; the transition dynamics being deterministic; the choice of using policy gradients and value iteration). 

In addition, the authors provide a theoretical justification in the Appendix (which I have briefly scanned) and the intuition behind it in Section 5. From what I understand, the main problem arises from the policy's output space being a Gaussian distribution, which causes the policy being able to get stuck in a local optimum. Further, the authors show (in the Appendix) that under some assumtions the value function always converges. Are there any guarantees on this when we don't have access to the true reward and transition functions (which themselves could get stuck in a local optimum)?

Would the authors say that the phenomenon is more a problem with the algorithm (policy gradient vs value iteration) than policy-centric and value-centric methods in general? Are there other methods that would be able to transfer policies better than policy gradient methods?

Regarding B)
The author's proposed method (MVC) has three components: the value function, the dynamics model and the reward model, all of which are learned by neural networks. It seems like the main advantage comes from using a model (since that's the aspect which changes when having to transfer to an altered MDP). Does the advantage of this method over DDPG and TRPO come from the fact that the dynamics model changes smoothly, and we have an approximation to it? Then it is not surprising that this outperforms a policy gradient method. 

Other comments:

- Could you explain what is meant by ""precise"" and ""imprecise"" when speaking about policies or value functions?
- Could you explain what is meant by the algorithm being ""accessible"" (e.g., Definition 1)?

- Section 2.1: In Property 1, what is f? Could you make explicit why we are interested in the two properties listed? By ""not rigorously"", do you mean that those properties are based on intuition? These properties are used later in the paper and the appendix, so I wonder how strong of an assumption this is.
- Section 2.2: Could you explain what is meant by ""task""? You say that within the MDP, the transition dynamics and reward functions change, but the task stays the same. However, earlier (in the introduction) you state that only the environment dynamics change. I find it confusing that ""the task"" is something hand-wavy and not part of the formal definition of the MDP. In what exact ways can the reward function be influenced by the change in the transition dynamics? 
- Section 3: Replace ""obviously"" with ""hence""; remove ""it is not hard to find that"". This might not be so trivial for some readers.
- Appendix B: Refer to Table 1 in the text.

Clarity: The paper is written well, but I think some assumptions and their affects should be stated more clearly and put into context. The paper misses a discussion / conclusion section. It would be great to see a discussion on some of the assumptions; e.g., what if the low dimensional assumtion breaks down? What if we assume that also the reward function can change? The authors are in a unique position to give insight into these things (even if the results from the paper do not hold after dropping some assumptions) and it would be very helpful to share these with the reader in a discussion section.",5
"The paper provides some bounds on the generalization performance of GANs for approximating distributions with discontinuous support. This work relies heavily on the results shown in [1] and [2] on the approximation power of Deep networks for non-smooth functions. The paper is globally well written and the proof seems sound. However, the experiments could be more convincing and the relevance of the result is questionable:

- By choosing the function class F to the be L_1-lipschitz, the resulting error bound loses it’s dependence on the smoothness beta and becomes slightly worse than the classical methods (equation 7 with kappa = 2+2D). Is this an artifact of the proof? if that is the case, it would be good to have a tighter bound: [3] might be a good starting point. 
- Neural networks used in practice are continuous usually, but it seems that all the analysis is all based on the fact that distributions with disjoint support require discontinuous networks. Can similar results be obtained in the more realistic case of continuous networks? Also what network architecture was used in the experiments?
- Although the bound in eq (5) clearly shows a tradeoff for S_g it only says that S_f should be as small as possible. Of course, if S_f =0 there is no discriminative power, but it’s unclear to me how the expression for S_f in eq (6)  can be obtained from (5) and why it would keep the discriminative power (in what sense?). Again, this tradeoff was discussed in prior work [3], so it might be worth looking into that direction.
- The discussion right after lemma 1 doesn't seem to be true: a distribution might have disjoint support and still have a density (i.e.: absolutely continuous with respect to the Lebesgue measure). It can even have a smooth density.
- The experiment doesn’t use the same metric to compare GANs method with other methods, so it is unclear how these methods compare. Moreover, figure 6 seems to show that other methods are also able to get the support right (Kernel E). Based on what could we claim that one method is better than the other?

Revision:
Thank you for your response. 
> In fact, our estimator in the theoretical and experimental analysis employs a continuous (ReLU) network. Though discontinuous networks are necessary for our setting (Lemma 2), we show that (continuous) ReLU networks can approximate the discontinuous network effectively (Lemma 3), hence the effectiveness of GANs is proved (Theorem 1).

-That clarifies things, however I find that the discussion after lemma 2 rather missleading, if in the end the result ends up using continuous generator:
""Because of the discontinuity, generative models with smooth functions, such as an
adversarial generative model with kernel generators (Sinn & Rawat, 2018), cannot work well with
disconnected supports.""  

- It is still unclear to me how the optimal value of S_f is obtained from eq (5). The author points out the work by Zhang+ (2018), but this should be clarified in the current version of the paper: What result in Zhang+(2018) do you use to get this value?

- I find the experiments  not very convincing. I understand that the point is not to show that GANs are better than  other methods but it is important to be make meaningful compairisons (use comparable scores) otherwise there is little scientific value in figure 5 especially.  

- As reviewer 1 mentions, lemma 3 is supposed to be one of  the main theoretical contributions of the paper, however, the proof seems very similar to the one in ([2], appendix B.1). Although the authors mention lemma 1 of [2] in the proof of lemma 3, it seems like the whole section in ([2] appendix B.1) is dedicated to show the very same result.

For all these reasons I still wouldn't recommend accepting this paper. 






[1]: Yarotsky. Error bounds for approximation with deep relu networks.
[2]: Massaki Imaizumi, Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively.
[3]: P. Zhang, Q. Liu, D. Zhou, T. Xu, and X. He. On the Discrimination-Generalization Tradeoff in GANs. 
",5
"This paper claims three contributions. 
1. We show that GANs perform better than other standard methods of estimating probability
measures when the measure satisfies the disconnected support property.

I prefer to state the contribution as ""We show that deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property."" This claim is essentially from Lemma 3, and it is a property of deep generative networks instead of GANs. If we have another way to train deep generative networks (say, variational auto-encode), we still get the same good approximation error, just linearly dependent of the number of disconnected pieces. The proof of Lemma 3 is mainly from the definition of locally smoothness and the results in Petersen & Voigtlaender 2017. It's a nice effort to leverage the result in Petersen & Voigtlaender 2017 to prove approximation properties of deep generative models. The flaw of this part is that the claims of other standard methods (Proposition 1) is very hand-wavy and floppy. The proof of Proposition 1 has lots of typos. For example, the definition of S_1 and S_2. And this sentence ""Then, by the proof of Lemma 1, p2(x) on S2 is a quadratic function with respect to z1 is 1-times differentiable but not twice-differentiable at the boundary ..."" I guess that the authors want to argue that traditional function approximation methods (like Kernel methods, polynomial approximation) all have rate n^{-1/(2+D)} approximation rate in the L^2 norm when the function to approximate has discontinuities... However, the authors fail to make this point clear. Moreover, if we use a mixture model of traditional approximations, the rate will not be deteriorated.  And we will get similar results in Lemma 3. Then, even the claim ""deep generative networks perform better than other standard methods of estimating probability measures when the measure satisfies the disconnected support property"" is not that grounded. 


2. We provide a new generalization error bound under a general formulation of GANs by analyzing an approximation error. The result is thus applicable to a wide range of variations of GANs.

This corresponding to the results in Theorem 1. The authors may want to write the assumption ""all the discriminators are L_1 Lipschitz continuous"" in the Theorem 1 explicitly, because this is an important assumption to get the results. The analysis in i & iii is standard. The analysis is the main contribution of this paper, and is from Lemma 3. 

3. Based on the generalization bound, we provide a theoretical guideline for selecting architectures
of generators and discriminators. 

I think the authors mean Equation (6) in this claim. However, this practical guidance is not practical, because (1) both \beta and \kappa are unknown in practice, especially \beta, (2) I can hardly image the number of connections (non-zero weights) will be my model design guidance instead of the model architecture. 

In the numerical experiment, ""We use d_F to evaluate errors by GANs, and a root of the expected squared errors with the L2-norm for the other methods."" With different metrics, is this a fair comparison?

Finally, there are lots of typos in the paper and appendix. ""refers to a probability measures"", ""The property makes a probability measure be complex"", ""with disconnected support"", ""Theorem 1, Corollary 1 and ??"", 

Among others, the original GAN(Goodfellow et al., 2014) is realized if F contains a logarithm of density ratio. The f-GAN (Nowozin et al., 2016) also belongs to this class. Equation (1) and (2) only includes Integral Probability Metrics, not divergences in Goodfellow et al., 2014 or Nowozin et al., 2016.

""smoothness and a dimension of data are sufficient to characterize an optimal convergence of generalization errors."" If we allow mixture models, that's a different story.

""A boundary of S is J combination of -smooth hyper surfaces"" Definition of S_{\alpha, J} is not clear. The exact definition in appendix is based on the definition of the horizon function, which include ""x_d \pm h"". Really confused about this \pm. The original definition in Petersen & Voigtlaender 2017 does not have this \pm. 

“an ordinary density function cannot be defined.” Can an ordinary density function be defined by setting the value outside its support to be 0?

“an empirical norm \| \|” What is the empirical norm?

In Proof outline of Theorem 1, we have the decomposition of i, ii and iii. Should P_0 in iii be P^{*}?

""We compare the numerical performance of GANs and the other methods with toy data with.""

More typos in the proofs, especially in proof of Lemma 3.
",6
"This paper provides a theoretical study of GANs in the following setting:

- The target distribution has a locally smooth density on a compact set [0, 1]^D. It might be supported only on M disjoint components, each of which has a smooth boundary, within that compact set.
- The latent noise dimension (inputs to the generator) is of the same dimension as the data.
- An IPM loss (1) is used.
- The discriminator functions of the IPM and the generator networks are both ReLU networks of at most L layers and at most S total nonzero weights, with all weights having magnitude at most B.
- The discriminator functions are Lipschitz continuous; this is implied by the previous assumption, but the bound is tighter if we have a tighter constraint here.
- We obtain the generator which exactly minimizes the IPM between the empirical distributions of m samples from the model and n samples from the target.
- Maybe: there is some g such that P* is produced by g. It's not clear in your statement of Theorem 1 whether this is necessary, or exactly what it means when you say that in Lemma 3, and I haven't fully checked what's used yet; it would help to explicitly say this is not assumed if it's not.

Pointing out that the kinds of distributions handled by GANs often have disjoint support, and analyzing this case, is certainly of interest.

The full-dimensional support assumption is not ideal, but of course just because the analysis doesn't apply to most practical GAN settings doesn't mean it's not an important step towards one that does.

Also note that the assumption about the structure of the networks eliminates the MMD GANs that you use in experiments -- which have a kernel function at the top of the critic network -- though it does allow for most GAN variants. Maybe the most interesting algorithm for this setting is the Coulomb GAN (ICLR 2018, https://arxiv.org/abs/1708.08819 ), which uses a neural network critic of the kind you study but estimates a distance which (unlike the Wasserstein and most other GAN objective functions) has good statistical convergence properties (kappa=2 as you mention).

My biggest concern by far, though, is Proposition 1. You present it as if it's a lower bound: establishing that there is some class of distributions for which Theorem 1 shows that GANs can account for local smoothness and standard methods are shown not to be able to. This isn't what you do; instead you exhibit a class of distributions for which Theorem 1 shows that GANs can account for local smoothness, and previous analyses of standard methods do not show that they are able to take advantage of it. This is not the same thing at all! Although the previous upper bounds have matching lower bounds, you don't demonstrate (and it is likely not the case) that the distribution you show fits into the class of distributions used by the previous lower bounds, and so it remains very possible that other methods are able to take advantage of local smoothness as well as GANs do.

Given that Proposition 1 is hence a very weak statement, your main contribution in terms of disconnected support becomes ""we can show that GANs can adapt to disconnected supports (under these other assumptions) that has not previously been shown for other methods."" Not only is this a weaker result, but the degree to which you show GANs can take advantage of local smoothness is somewhat limited: at least with the parameter choices in Corollary 1, the smoothness only improves m dependence, not n dependence.

But in the GAN setting, m is essentially a question of how long you optimize for (and the relative rates between generator and discriminator updates, and various other questions like that out of scope for this paper), not any kind of externally fixed limitation like n. It's perhaps not too surprising that you don't show that optimizing a WGAN is statistically easier than estimating Wasserstein the distance, but given that estimating Wasserstein is so hard in high dimensions, it's a little disappointing. (Maybe it's easier, so that kappa is smaller, with locally smooth densities as here, though I don't know of any results like that offhand.)

(It's interesting, then, that in Corollary 1 the generator complexity depends only on m, basically the amount of optimization you're willing to do, while the discriminator complexity depends only on the available number of target samples n.)

Another concern is that to me, it is not very clear exactly what the statements of the assumptions mean. For example, does Theorem 1 apply only if I search over all generators in the class \mathcal G = \Xi(S_g, B_g, L_g)? In particular, does this mean that I have to consider all possible architectures matching those constraints, including allowing for all possible depths up to L_g, and all possible ways of allocating widths of the various interior layers / which weight entries are fixed at zero? It seems so, but this could be more explicit in the statements, even just by replacing ""an existing \mathcal G"" with ""\mathcal G = \Xi(S_g, B_g, L_g)"" in the statement of (e.g.) Theorem 1.

In your numerical experiments: you don't make it at all clear enough that you're plotting *different loss functions* for the GANs and the other methods! (You say this, but only in the text where it should definitely also be in the figure caption.) What happens if you plot the L2 difference for all methods, and the MMD/Wasserstein for all methods? Looking at Figure 6, it's not obvious that the GAN would do so substantially better. (It does seem to perhaps have the overall scale of the two components better than the other methods, but it doesn't look like as enormous a difference as it seems from Figure 5.)

Overall: Theorem 1 is of interest, but the results and especially the comparison to classic methods are not as resounding as they're presented here.

(Note that I have not (yet) verified or even really read most of the proofs; I might come back and do that later.)

Smaller points:

- I don't think that f-GAN actually fits in the framework (1) as you claim, since it needs to use the conjugate of f on the samples from Q. Also, the original GAN does fit into (1) but not your assumptions about the network form of f, since it needs a log in its activations.

- Another class of generative models where disconnected supports are really important is normalizing flows, which often build ""bridges"" between separated modes because (like your Lemma 2) their generators are constrained to be smooth and invertible. See e.g. Figure 2 of https://arxiv.org/abs/1810.01367 (who propose a new normalizing flow less susceptible to these problems). 

- Remark 1 seems so obvious that it need not even be stated, since beta-smooth implies beta'-smooth for beta' < beta. It would only be interesting if you could actually take advantage of the smoother components somehow.

- Many papers in your bibliography are cited only as arXiv preprints when they were actually published in various places. For example, the first four papers were published at ICLR 2017, ICML 2017, ICML 2017, and ICLR 2018, respectively.

- There are many small typos and grammatical errors in the draft, including some that would be caught by a spell-checker (""methdos"" on page 8), and an undefined LaTeX reference at the top of page 2. It would benefit from a thorough proofread.
",6
"
I have mixed feelings about this paper. On one hand, it’s a thorough and well-written experimental paper, something which is really important but is also clearly underappreciated in the machine learning community. On the other, it was not really obvious to me why some of objectives tested here are interesting: LM objectives like ELMo have seen a lot of uptake in the NLP community (and this is definitely an NLP paper), but most of the others—like skip-thought, MT, and autoencoders—have not. So the basic research question doesn’t seem like an especially burning one. The trends in Fig. 2 show that these alternatives underperform an LM objective, which suggests that the NLP community can keep using that objective without worry—and everything else in the figure seems as we would expect. 

In short, I think the paper is a well-done study on a hypothesis of perhaps minor interest. The results are sensible but confirm what we already strongly suspected, and they seem unlikely to strongly influence other research, since they confirm that everyone has been the right thing all along. I’m not entirely sure what I learned from this.

To me, the most interesting experiment is the final one in Section 6. This experiment seems like it could be the germ for a far more interesting paper getting at how these pretraining objectives help with downstream tasks. As it stands, it feels like an interesting nugget tacked on to an otherwise complete (and much less interesting) paper.

Presentational comments:

Fig.1: really nitpicky, but the typography of the POS tags and CCG categories is all wrong. These aren’t mathematical symbols!

Fig 2. Slightly confused why these are broken up into two separate plots.

Fig 4. is hard to read due to the lurid colors and patterns, which require a lot of cross-referencing with the legend. I wonder if this would be better as simply a table. I also found it very confusing at first since the y-axes are out of sync between the two figures—initially it looked as if the legend was overlaid on a set of bars in the left figure that had the same baseline as the right figure. 
",6
"This paper tests various pretraining objectives (language modeling, machine translation, skip-thought, and autoencoding) on two syntactic tasks: POS tagging and CCG tagging. It finds that language modeling outperforms the other pretraining objectives; additionally, randomly-initializing an encoder achieves decent performance when given a large amount of labeled data for the tagging task. The experiments in this paper are very thorough and explained well. By controlling for pretraining data size, the authors are able to reasonably claim that language modeling is superior to translation as a syntactic transfer learning task. On the other hand, I have some concerns regarding the significance of the paper's contributions, and as such I am borderline on its acceptance. 

comments:
- the experiments in the paper feel biased towards language modeling. Language modeling is the only token-level prediction task of the four objectives here, but both of the two downstream tasks are at the token level. It is perhaps unsurprising then that language modeling performs best; perhaps the authors could have considered some sentence-level downstream tasks as well to properly control for this? Or added some more word-level pretraining objectives? 

- sort of relatedly, the authors do not provide any explanations as to *why* language modeling is a better pretraining objective than translation. What kinds of examples do the tagging models using LM pretraining get right that the translation models do not? Such an analysis could help provide more concrete insights into what kind of information each objective is encoding.

- the claim that LMs > translation is not a new finding. The authors cite Blevins et al, who find the same result on the task of dependency arc prediction. Similarly, the surprisingly good performance of random encoders was also found in Conneau et al., ACL 2018. As the main contribution of this paper seems to be a more controlled study of Blevins et al on different syntactic tasks, I don't think there is enough here for an ICLR submission. 

- what is the effect of the specific dataset and architecture on the results? Here we just look at a couple translation datasets (all news data) and LSTM models. Do things change when we move to transformers or more diverse domains? ",5
"This is nicely written paper analyzing the effect of various pre-training methods and shows that language models are very effective on sequence tagging tasks (POS, CCG). The experiments are well motivated and well described.

Regarding Table 1: which one of the ""LM forward"" models was used in the subsequent experiments? 

Are the input embeddings for the random init LSTM pre-trained or are they also randomly initialized?",7
"The goal of the paper clearly motivated and well described. However, the notations and figures are more complicated than necessary; hence, it is a bit hard to follow the paper in detail. There are also some missing related works about domain adaptation for object detectors. For instance,
Chen et al. ""Domain Adaptive Faster R-CNN for Object Detection in the Wild"" In CVPR 2018.
Inoue et al. ""Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation"" In CVPR 2018.
The authors should cite these papers and compare with their performance.
Finally, the proposed method doesn't consistently improve the detection accuracy. 
The proposed method also slows down the frame rate of the detector due to multiple iterations of feedforward/feedback inferences.",4
"The paper proposes a method called cautious inference to improve inference accuracy for object detection models. The main idea is inspired by the previous work of Guided Perturbations, which is applied to fully convolutional networks to improve the segmentation/accuracy accuracy purely during inference time.  The original idea is to use the predicted labels of the network as pseudo ground truths (after making the predictions to be a one-hot vector), and then back propagate the error signals to the network input to get the gradients. And finally the gradients are added back to the original inputs to perform another round of prediction. Here the inputs can be either the original image, or some intermediate feature maps. Experiments are shown for both 2D and 6D object detections. 

Comments:

- I think overall it is an interesting idea to directly alter the input of the network in order to fit to the testing distribution. However, the motivation and story told in the introduction is a bit of an oversell compared to the experiment validation section. Most of the results shown are just doing training and testing of images drawn from the *same* distribution. Like coco train and test, or VOC train and test. It would be great to see if the cautious inference would work when the distribution is different. For example ""elephant in the room"" case, or new object categories are added during testing.

- I am actually curious to see this method can be used to improve the AP on the *training* set as well, just to understand it better -- is it trying to recover the generalization error of the network, or it is doing some implicit context reasoning inference that can help training as well. 

- It might be better to compare/combine the method to other inference-only improvements for object detection. For example there is soft-NMS, 
Bodla, Navaneeth, et al. ""Soft-nms—improving object detection with one line of code."" Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.

 - I am not sure I fully understand B-box part: I think it is easy to have B-obj and B-cls as one can just take the max of the class prediction and then use the inferred class label for one-hot vector construction, but I am confused about the box part as no ground-truth is given during testing. In Table 2 I also cannot find BP improving performance by itself in anyway.

- For COCO, please report results on test-dev set, the minival set images are used only for validation. 

",6
"The paper proposes a iterative approach at inference time to improve object detections. The work relies on updating the feature activations and perform new feed forward passes to obtain improved results. 

Pros:
(+) The idea of iterative inference is potentially effective
(+) The paper is well written and clear
(+) The authors show results on compelling benchmarks
Cons:
(-) Reported improvements are very small
(-) Important baselines are missing


First, while the authors state correctly that their updates have no memory cost and no new parameters are added, they do require more FLOPs at test time. For N-stages, the approach requires xN more operations for forward passes  and xN for backward passes. This is a serious shortcoming as it adds compute time per image for the inference stage and cannot be parallelized. 

The authors show small improvements for AP on COCO. From their analysis, it seems that the biggest gains come from N=1 stages, while the improvement added for N>1 are miniscule (Table 1). Note that the authors show results on COCO minival (5k images) and from my experience there, it's expected to see a +/- 0.2% AP between different trained models of the same architecture. The authors report a +0.46% gain. 

In addition, the authors do not provide results for other baseline approaches that have similar FLOPs at test time, such as iterative bounding box regression and input scale augmentation. Note that both these approaches do not add any parameters and require no additional memory, but add to the FLOPs at test time. From my personal experience, test time augmentations can add +1.5% to the final performance. Concretely, look at Mask R-CNN arXiv Table 8 last two rows. Test time augmentations add 1.5% on top of an already enhanced model. Empirically, the better the model the harder it is to get gains from inference tricks! And still test time augmentations boost performance significantly.

Given the small gains and the lack of competing baselines, it is hard to make a case for accepting the paper. ",4
"The paper proposes to reparameterize the discriminator to be an explicit function of two densities so that one could inject domain specific knowledge easily. As the authors say, that one way to inject domain specific information is by learning an energy function. Making use of this intuition, authors proposed to  regularize the discriminator in  
(GANs) framework by leveraging structured Gibbs distributions.

I found the introduction a bit hard to read. Otherwise paper is written in a readable way. 

Something which I like about this paper, is authors use the proposed method for actual RL problems as compared to just image generation. I think this is important as well as interesting.  As a community we should be moving towards evaluating generative models for the problems where we actually want to use generative models for.

Some questions:

- I'm not sure if the paper is really novel as the authors themselves point out that it corresponds to adding adversarial component in R2P2. 
- I also did not find results very convincing. As I said, its important to evaluate on RL problems ONLY if it makes sense on toy problems first. Like in the paper, authors made a big claim about reducing spurious modes, but it has not been demonstrated any where per se. May be authors can construct a toy problem in which they can show that the spurious mode issue, and how the proposed method kills these spurious modes.  This also reminds me of the literature in Boltzmann machines and more recently in Variational Walkback [1]. This could also be cited, and could be interesting to authors.

[1] Variational Walkback, https://arxiv.org/abs/1711.02282. The authors in Variational walkback also make the assumption p == q. 

What would make the paper stronger ?
- Constructing toy problems in order to illustrate the mode coverage and spurious modes issue would be interesting.",5
"Summary: The paper tries to answer the problems of regularizing GANS. They reparametrize the discriminator to be an explicit function of two densities: the generator probability density function q and a structured Gibbs distribution v.  

Comments:
1: This paper focuses on mode coverage problems, where spurious modes of learned model(q) not supported by target model(p) are pruned off.  It is not clear why this is a significant problem.  GAN trained models typically suffer from mode collapsing, requiring additional noise injection to support generation of diverse data.  This work seems to argue that the opposite is worth paying attention to, focusing on removal of modes.

2: The implementation of the architecture is similar to R2P2, except for the introduction of a new adversarial component. But according the evaluation in table 1 and table 2, we see that baseline model R2P2 performs better in -H(p,q) and for -H(q, pKDE) the value is near equal to their model. 

3: They assume the generator is invertible, which enables the analytic evaluation of the q. But no supporting evidence or design architecture for the statement above is provided.

4: The explanation of imposing structure on the model distribution is not clear. In the introduction they first claim “we cannot impose structure directly on the joint distribution of a GAN’s outputs.” But after they claim “we submit that regularizing the structure of a GAN’s generator and discriminator is generally more difficult than imposing meaningful structure directly on the model distribution, which we will refer to as q. These two statements conflict because the model distribution is a joint distribution of GAN’s outputs.


6 Typos: 
1)	in equation(1) we should minimize q for all the terms. 
2)	in equation(1) first term is unrelated with v. 
3)	in equation(1) the sup is for the last two terms. 
4)	in equation(2) in RHS of equation the first term q_ |  should be q_ .

7: Writing could be improved.

8: In table 2 what’s the meaning of evaluation metric Road%
",5
"This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It’s difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. 

It appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper.

1.    Typo in the training objective (Eq .1):  the second (or the first) ""sup"" should be removed? 

2.    Section 2.3 is very confusing. Particularly, how is the parameter \phi introduced? What’s the detailed update of \phi? 
- ""We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p."" If v_\phi(x) is a distribution, what’s the parametric form of v?  
- ""Generally, this is achieved by structuring the energy function V_\phi:=\log v_\phi."" It seems that V_\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\phi(x), however, why the distribution is self-normalized? Specifically, why \int \exp(V_\phi) dx = 1? Otherwise, how the authors deal with the partition function \int \exp(V_\phi(x)).  
- It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs?

3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode.

If the authors can fully address the concerns above, I will consider changing the scores.  

Other comments:
1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. 

2. Duplicate citations – R2P2 [35] [36]

3. Other related papers:
 - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17
- Tu et al., Learning Approximate Inference Networks for Structured Prediction, ICLR 18",4
"In this paper, the authors propose a dynamic convolution model by exploiting the inter-scene similarity. The computation cost is reduced significantly by reusing the feature map. In general, the paper is present clearly, but the technical contribution is rather incremental. I have several concerns:
1. The authors should further clarify their advantages over the popular framework of CNN+LSTM. Actually, I did not see it. 
2.  What is the difference between the proposed method and applying incremental learning on CNN?
3. The proposed method reduced the computation in which phase, training or tesing?
4. The experimental section is rather weak. The authors should make more comprehensive evaluation on the larger dataset. Currently, the authors only use some small dataset with short videos, which makes the acceleration unnecessary. 
",3
"Summary - This paper proposes a technique to reduce the compute cost when applying recognition models in surveillance models. The core idea is to analytically compute the pixels that changed across frames and only apply the convolution operation to those pixels. The authors term this as dynamic convolution and evaluate this method on the SSD architecture across datasets like PETS, AVSS, VIRAT.

Paper strengths
- The problem of reducing computational requirements when using CNNs for video analysis is well motivated. 
- The authors analyze a standard model on benchmark datasets which makes it easier to understand and place their results in context.

Paper weaknesses
- A simple baseline that only processes a frame if \sum_{ij} D_{ij} exceeds a threshold is never mentioned or compared against. In general, the paper does not compare against any other existing work which reduces compute for video analysis, e.g., tracking. This makes it harder to appreciate the contribution or practical benefit of using this method.
- The paper has many spelling and grammar mistakes - ""siliarlity"", ""critiria"" etc.
- Continuous convolutions - It is not clear to me what is meant by this term. It is used many times and there is an entire section of results on it (Table 6), but without clearly understanding this concept, I cannot fully appreciate the results.
- Section 5.2 - what criteria or metric is used to compute scene similarity?
- Overall, I think this paper can be substantially improved in terms of providing details on the proposed approach and comparing against baselines to demonstrate that Dynamic-Convolutions are helpful.
- Design decisions such as cell-based convolution (Figure 3) are never evaluated empirically.",4
"The paper addresses the problem of computational inefficiency in video surveillance understanding approaches. It suggests an approach called Dynamic Convolution consists of Frame differencing, Prediction, and Dyn-Convolution steps. The idea is to reuse some of the convolutional feature maps, and frame features particularly when there is a significant similarity among the frames. The paper evaluates the results on 4 public datasets. However, it just compares the approach to a baseline, which is indeed applying convnet on all frames. 

- State of the art is not well-studied in the paper. Video understanding approaches usually are not just applying convnet on all frames. Many of the approaches on video analysis, select a random set of frames (or just a single frame) [5], and extract the features for them. There is another set of work on attention, that try to extracts the most important spatio-temporal [1-4] information to solve a certain task. These approaches are usually computationally less expensive than applying convnet on all video frames. I suggest the authors compare their model with these approaches. 

[1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al. 
[2] Recurrent Models of Visual Attention, Mnih et al.
 [3] Action recognition using visual attention, Sharma et al.
 [4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al.
 [5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al. 

- In addition, car and pedestrian detection performance is part of the evaluation process. In this case, the approach should be also compared to the state-of-the-art tracking approaches (that are cheaper to acquire) in terms of computational efficiency and performance. 
- The writing of the paper should also improve to make the paper more understandable and easier to follow. Some examples: 1. Unnecessary information can be summarized. For example, many details on the computational costs in abstract and the introduction can just simply be replaced by stating that “these approaches are computationally costly”.  2. Using present tense for the SoTA approaches is more common.“ShuffleNet (Zhang et al. (2017)) proposed two new strategies”.  3. Long sentences are difficult to follow: “In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”
  + The problem of large-scale video understanding is an important and interesting problem to tackle.  ",4
"Pros:
Overall, this is a nice empirical paper with a reasonably extensive set of experiments. It is interesting that, among networks that train to ~100% with Layca, the best generalizing ones tend to have balanced training between layers (Fig. 2), and that tuned SGD does not generalize as well as Layca (Fig. 4). I think this paper’s focus on discrepancies in training & generalization originating from layers of a deep network is an interesting and important topic of study that warrants further empirical and theoretical investigation from the community. I think the work already has some interesting results and will encourage further investigation.

Cons:
--Would appreciate greater discussion of the originality of the results; in particular, a more upfront discussion (which is currently concisely presented in the supplementary) regarding algorithms that are similar to Layca when less crucial steps are dropped, e.g. Yu et al 2017 and Ginsburg et al 2018.
--After reading the paper, I don’t feel especially convinced that rotation (of the flattened weight matrix) is the best quantity to analyze training dynamics of a single layer. Could there be greater discussion & motivation for this, and in particular, relationship to work where weights are parameterized using orthogonal matrices, or even orthogonal initialization?

Some minor comments:
--Would have appreciated a discussion of the learning rate schedule (as well as other experimental details, e.g. loss function used and what role this plays) and whether networks with lower learning rates would need to be trained longer.
--Greater discussion of why the first and last layer(s) do not experience the same rotation rate as other layers and if there would be better generalization if they did.
",6
"Paper summary: The authors propose layer rotation speed as a measure of layer-wise training speed and introduce the Layca optimizer as a means of enforcing uniform layer rotation speed throughout the network. They show empirically that layer rotation speed is linked to the generalization performance of deep neural networks and that weight decay induces uniform layer rotation speeds.


Detailed comments:

Overall, I felt that the paper introduced some interesting ideas but I was not left convinced that layer rotation speed is the correct measure of layer training speed. I hope that the authors can clarify this based on my questions and comments below.

1) In the introduction you refer to input and feedback signals to a layer, I assume this refers to the forward and backward pass. As I understand it, this intuition and the findings of Figure 1 do not immediately relate to the notion of layer rotation speed during training. Could you clarify what you mean by ""the input and feedback signals that a layer receives could also influence the generalization ability induced by the layer's training"", to me this statement seems obvious as input+feedback signals contains training entirely.

2) In Figure 1 you show that when training one chosen layer and keeping the others fixed, if the chosen layer is deeper into the network the test accuracy is worse. I wonder to what extent this might be remedied by initialization. For example, one might expect that when sampling random square matrices there are some very small eigenvalues which ""kill"" information in the forward pass. If we train a layer deep in the network it may have access to less information from the data than one earlier on. Whereas training an earlier layer could allow this layer to shift mass into the parts of the eigenspace which are well represented (so-to-speak) in the future layers. Have you thought about this at all? One simple way to evaluate this would be to initialize the weights to be random orthogonal matrices, ensuring that the eigenvalues are equal. With that said, I thought that this was an interesting experiment with a fairly surprising outcome!

3) In related work you discuss the vanishing and exploding gradients problems in terms of layer-level training speed. I think that another relevant research direction may be dynamical isometry [1] which solves this problem by restricting all singular values of the Jacobian matrix to be close to 1. These ideas may also be relevant when discussing Layca and layer-rotation.

4) I found section 3.1. a little unconvincing. It is not obvious to me that layer rotation speed is necessarily a good measure of training speed. In fact, there are many updates which have large cosine distance (as you define it) but do not change the network function (for example, permuting the weight matrices in fully-connected networks). Why is the rotation defined through a vectorization of the weight matrix as opposed to e.g. the polar decomposition? Is this a computational issue? Similarly, in section 3.2 you liken Layca to optimization on a manifold but I am not convinced that this makes sense for matrices which inherently have some structure (e.g, perhaps the Stiefel manifold would be more meaningful).

6) Figure 2 shows that uniform rotation leads to improved test accuracy. But could it be the case that controlling the effective learning rate is sufficient (and layer rotation is one way to achieve this)? For example, we might take the sign of the update and use this to ensure that each weight matrix has the same effective learning rate (something like [2]). Do you expect this would have a similar effect? If not, what is unique about layer rotation that provides good test accuracy?

7) You claim that SGD and adaptive methods with weight decay works without taking extra care to control the layer-rotation rate, as weight decay provides a similar effect. Firstly, you use weight decay and L2 regularization interchangeable, could you be explicit about exactly which you mean (see e.g. [3]). Assuming you mean weight decay (and not L2 regularization), then this could also be due to the effective learning rate ([4,5,6]) which may have some interaction with layer rotation rate (i.e. Figure 4). In summary, I would have liked to see an explanation for why weight decay leads to uniform rotation speeds.

8) If I understand correctly, Figure 5 shows 5 tasks and reportedly 5 optimization schemes - each on a different task? It seems more reasonable to compare these on the same task.

Overall I felt that the paper had some interesting contributions and a fairly comprehensive empirical study. However, I do not feel that the paper gives adequate attention to the notion of effective learning rate induced by weight decay and I was not totally convinced that the way layer rotation speed is defined is the correct way.

Minor comments:

- A lot of white space and a large caption for Figure 1.
- Section 4 opens with ""monitor and control"", but I think the latter is really presented in section 4 and not section 3.
- I think a diagram of the projection step of the Layca algorithm would be informative (for 2D weight vector).
- Why does `5` appear in equation 1? Is this an arbitrary choice?
- Some of the lighter colors in e.g. Figure 2(b) made some lines hard to read when printed. I do not believe that this affected the image significantly.

Clarity: The paper is well written and is easy to understand. Some of the figures in the experiments are a little cluttered and the lighter colors can be hard to see (e.g. Fig 2(b)), but this is minor.

Significance: The paper presents an interesting view point but I am not convinced that it offers as strong an explanation for these phenomena as other approaches. I believe with some more clarification the results could become more significant. My review score hinges mostly on the interaction between layer rotation speed and the effective layer-wise learning rate.

Originality: To my knowledge, the ideas are presented in the paper are original. In particular, this is a novel way to characterize layer-wise training speed.


References:

[1] Pennington et al. ""Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"" https://arxiv.org/abs/1711.04735
[2] Bernstein et al. ""signSGD: Compressed Optimisation for Non-Convex Problems"" https://arxiv.org/abs/1802.04434
[3] Loshchilov et al. ""Fixing Weight Decay Regularization in Adam"", https://arxiv.org/pdf/1711.05101.pdf
[4] Laarhoven, ""L2 Regularization versus Batch and Weight Normalization"" https://arxiv.org/abs/1706.05350
[5] Hoffer et al. ""Norm matters: efficient and accurate normalization schemes in deep networks"" https://arxiv.org/abs/1803.01814
[6] Anonymous, ""Three Mechanisms of Weight Decay Regularization"" https://openreview.net/forum?id=B1lz-3Rct7   (Another ICLR 2019 submission)",5
"This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by  angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca.

This paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference).

In addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.",5
"This paper is addressing several research challenges as a method to generate objects with desired functionalities, a method to extract form-to-function mapping, a method to operationally support a functionality arithmetic. The work illustrated in this paper is really interesting and is addressing relevant and open problems in the domain of product design.

Nevertheless the manuscript has a couple of weaknesses, one concerned with the presentation and another related to the design of the study. 

The lack of a consistent choice for the lexicon is sometimes misleading. It is not always clear whether the use of different terms is addressing synonyms or to discriminate between two distinct concepts. For example let consider the following pairs: functionality versus affordance, function versus functional, class versus category, feature versus shape. 

The study addresses several questions. Not always is clear what is the purpose or better the research questions that are driving the design of the experiments. While in the manuscript the are many repetition of the objectives of the study, less attention is devoted to explain what are the working hypothesis underlying the proposed methods. For example, one of the objective is a method to generate objects with desired functionalities. Only in the final Section there is a brief mention of the dichotomy between meash-based versus voxel-based. As reported in Section 2 there are in literature other works but there is not a claim on what is the specific purpose of the present study. The contrast of voxel versus mesh looks like a motivation but it only a speculation. A similar comment might address the dichotomy deterministic (ontology) versus probabilistic (autoencoder). In this case the experiment design should provide some empirical evidence about this contrast.

A minor comment. Figure 7a is illustrating the functional essence of table. According to the caption Figure 5a is illustrating the same functional essence for the same category/class table. Should the pictures look the same? 
",5
"
This paper proposed a 3D shape generation model. The model is essentially an auto-encoder. The authors explored a new way of interpolation among encoded latent vectors, and drew connections to object functionality.

The paper is, unfortunately, clearly below the bar of ICLR in many ways. It’s technically incremental: the paper doesn’t propose a new model; it instead suggests new way of interpolating the latent vectors for shape generation. The incremental technical innovation is not well-motivated or justified, either: the definitions of new concepts such as ‘functional essence’ and ‘importance vector’ are ad-hoc. The results are poor, much worse compared with the state-of-the-art shape synthesis methods. The writing and organization can also be improved. For example, the main idea should be emphasized first in the method section, and the detailed network architecture can be saved for a separate subsection or supplementary material. 

It’s good that the authors are looking into the direction of modeling shape functionality. This is an importance area that is currently less explored. I suggest the authors look into the rich literature of geometry modeling in the computer graphics and vision community, and improve the paper by drawing inspiration from the latest progress there. 
",3
"This paper presents a method for generating 3D objects. They train a VAE to generate voxel occupancy grids. Then, they allow a user to generate novel shapes using the learned model by combining latent codes from existing examples. 

Pros:
- The idea of linking affordances to 3D object generation is interesting, and relevant to the machine learning and computer vision communities.

- They propose to evaluate the quality of the shape based on a physical simulation (Section 4.4.3), which is an interesting idea.

Cons:
- This paper is not well written. The method is described in too much detail, and the extra length (10 pages) is unnecessary. Cross entropy, VAEs, and many of the CNN details can usually just be cited, instead of being described to the reader.

- The paper uses suggestive terminology, like ""functional essence"" and ""functional arithmetic"" for concepts that are fairly mundane (see Lipton and Steinhardt, 2018 for an extended discussion of this issue). For example, the ""functional essence"" of a class is essentially an average of the VAE latent vectors (Section 3.3.1). The paper claims, without sufficient explanation, that this is computation is motivated by the idea that ""form follows function"".

- The results are not very impressive. There is no rigorous evaluation. They propose several nice metrics to use (eg. affordance simulation), but the results they present for each metric are quite limited. The qualitative results are also not particularly compelling.

- The paper should more thoroughly evaluate the importance weighting that is described in Section 3.3.2.

 - The technical approach (combining VAE vectors to make new shapes) is not particularly novel[

Overall:

The paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.
",3
"This paper proposes a combination of Evolutionary methods and variational representation learning to improve the sample efficiency of RL methods.
They train a VAE on environment frames, as well as an action-conditioned Dynamics model to predict the next frames, and these form the representations fed into a policy network which is trained through ES.

Overall, I find the problem setting interesting, and they try to tackle Atari games instead of simpler domains.
The use of CMA-ES instead of NES is a good improvement, and the way they motivate using VAE representations to obtain manageable representation sizes is well put forward.
[Edit: as mentioned by the other reviewers, this extension isn't as novel, given Ha et al's work, hence that reduces my confidence about accepting this paper further...]

However, this paper suffers from several issues in its current state:
1.	Its presentation is overly detailed about known literature. Section 2 goes in low-level details which are not necessary. It covers ES methods even though a citation to Salimans et al. 2017 would have been sufficient. Section 2.1 is a really complete coverage of CMA-ES, which should really just be a citation of the actual paper again or should be in the Appendix, this doesn’t warrant 1.5 pages of the main text.
2.	The actual model presentation is too succinct and split into Section 2.2 and Section 3 (network architectures and parameters). It is never clear how many parameters are optimized by CMA-ES (I counted ~8200 parameters if the MLP of size 256 x 32 x n_a is used). Algorithm 3 however was extremely clear and helpful to fully understand the method.
3.	There is no clear evaluation of the performance of the VAE representations and of the RNN dynamics model. Did they actually learn to represent anything at all? Figure 4 is not sufficient in providing evidence supporting this.  Compare this to Higgins et al. 2017, which used VAEs which represented enough information to perform at the same performance as non-variational representations.
4.	This feeds into the biggest issue with the current results: The proposed method works rather badly, obtaining worst performance than ES on 35 out of 51 games (68%). On most of these games, the proposed method does not seem to be able to get off the ground at all.  Why is that the case? Obviously if the VAE+RNN do not represent the games well enough, the performance will be bad. Did the policy learning with CMA-ES converge well? (seeing learning curves might help) The fact that no gradients are passed back from the Policy to the VAE/RNN clearly emphasises that issue (The policy only affect the data on which the representations are periodically retrained on).

In conclusion, even though I feel this paper tries to tackle an interesting problem, the results are not sufficient to support them as of now.

Typos:
-	“Donates” instead of “denotes” in a few places.

References:
-	Higgins et al., 2017: https://arxiv.org/abs/1707.08475 
",4
"Summary: They propose ""Sample Efficient Deep Neuroevolution"" (SEDN) model and experiment on Atari games. In this model, they use a Variational Encoder (VAE) to encode state frames into a latent vector, and use an LSTM to encode the current latent vector and action to predict the next latent space. A policy network (trained using CMA-ES) takes the latent space, and hidden state of the RNN as an input, and outputs an action to execute.

The strengths of this paper is that it is clearly written. They even explain details of RL, background of evolution strategies, motivation of using CMA-ES (and also the algorithm itself, which is no small ordeal), so it might a good background review paper of the literature. The experimental setup is relatively easy to understand. I suggest putting Algorithm 3 before Table 1 since presenting the algorithm before results may be more natural.

That being said, there are issues with this work that needs to be addressed before publication. I will list the issues and some suggestions I have, in order to help make this work better, hopefully good enough for acceptance:

1) The authors cited [1] a few times in the paper, but actually their approach of using a VAE to compress frames into a latent, an LSTM to predict the next latent, and a CMA-ES trained network for the policy is precisely what is proposed in [1] (which had experiments that trained on the actual environment, like in this work, and also the generated environment). This paper reads like they have proposed the setup, and lacks clarity as to which parts are their contribution, and which parts are prior work, which I believe is important for a paper submitted to an academic venue. Not to say at all that there's no contribution or originally in this work - there are many, but I feel they should list out which bits are their contribution, and which bits are prior work more clearly. Doing so will make this paper and their contributions stronger.

In my opinion, their contributions are: Expanding on the approach of [1] to study on a larger set of environments (the Atari suite), where they also incorporated an iterative training loop (described Algorithm 3) that was not used in [1]. Also, unlike [1], they used a multi-layer policy network, and also explained and rationalized the intuition behind the choice of CMA-ES. I think by listing out the contributions, and separating them from previous work, the paper will be much stronger.

2) The results are not terribly strong. They achieved good results on 7 games out of 50 using 10M frames. To me, that's actually not a deal breaker, since research is not a SOTA game, but I would like to see a more detailed analysis of why the algorithm works, and when it fails so people know what future work needs to be done to address this. I'm also not convinced that using CMA-ES would have an advantage over A3C (with the same latent / hidden features going into A3C as inputs), so perhaps the author might achieve better results if A3C was used to train the policy network (or not, but would be nice to see this experiment). It would offer more insight if we know what kind of terminal scores can be achieved using this algorithm, if it were allowed to train for 1B frames like the other 2 setup. Finally, if the author was able to show that training inside a generated environment, even for pre-training before going back to the actual environment, helps sample efficiency, that would be a very interesting result to me.

I'm assigning a preliminary score of 5 for this work for now, but if the author address point (1) to my satisfaction I will revise the score to +1 points, and if the author is able to achieve much better results, or address items in point (2) to my satisfaction, I will revise the score by +1 or potentially +2 points, so the final score of this work may lie in the range of 5 -> 8. I feel the author should be able to improve the paper to get a score of 6-7 in the end, at least from me. Good luck!

[1] https://arxiv.org/abs/1803.10122",5
"The main difficulty of neuroevolution---requiring a huge number of simulations for high dimensional problem---is addressed in this paper by introducing VAE to reduce the state space dimensionality and using a rather shallow controller network. This idea itself is very promising, however, it has been introduced in (Ha and Schmidhuber, 2018).  Still, there seems to be differences in how to gather histories and how to use them. Nevertheless, the differences are not well described in the text. The effect of the modification is not evaluated on experiments.",4
"Summary: This work demonstrates that, although the Boltzmann softmax operator is not a non-expansion, a proposed dynamic Boltzmann operator (DBS) can be used in conjunction with value iteration and Q-learning to achieve convergence to V* and Q*, respectively. This time-varying operator replaces the traditional max operator. The authors show empirical performance gains of DBS+Q-learning over Q-learning in a gridworld and DBS+DQN over DQN on Atari games.

Novelty: (1) The error bound of value iteration with the Boltzmann softmax operator and convergence & convergence rate results in this setting seem novel. (2) The novelty of the dynamic Boltzmann operator is somewhat thin, as (Singh et al. 2000) show that a dynamic weighting of the Boltzmann operator achieves convergence to the optimal value function in SARSA(0). In that work, the weighting is state-dependent, so the main algorithmic novelty in this paper is removing the dependence on state visitation for the beta parameter by making it solely dependent on time. A question for the authors: How does the proof in this work relate to / differ from the convergence proofs in (Singh et al. 2000)?

Clarity: In the DBS Q-learning algorithm, it is unclear under which policy actions are selected, e.g. using epsilon-greedy/epsilon-Boltzmann versus using the Boltzmann distribution applied to the Q(s, a) values. If the Boltzmann distribution is used then the algorithm that is presented is in fact expected SARSA and not Q-learning. The paper would benefit from making this clear.

Soundness: (1) The proof of Theorem 4 implicitly assumes that all states are visited infinitely often, which is not necessarily true with the given algorithm (if the policy used to select actions is the Boltzmann policy). (2) The proof of Theorem 1 uses the fact that |L(Q) - max(Q)| <= log(|A|) / beta, which is not immediately clear from the result cited in McKay (2003). (3) The paper claims in the introduction that “the non-expansive property is vital to guarantee … the convergence of the learning algorithm.” This is not necessarily the case -- see Bellemare et al., Increasing the Action Gap: New Operators for Reinforcement Learning, 2016. 

Quality: (1) I appreciate that the authors evaluated their method on the suite of 49 Atari games. This said, the increase in median performance is relatively small, the delta being about half that of the increase due to double DQN. The improvement in mean score in great part stems from a large improvement occurs on Atlantis.

There are also a number of experimental details that are missing. Is the only change from DQN the change in update rule, while keeping the epsilon-greedy rule? In this case, I find a disconnect between the stated goal (to trade off exploration and exploitation) and the results. Why would we expect the Boltzmann softmax to work better when combined to epsilon-greedy? If not, can you give more details e.g. how beta was annealed over time, etc.?

Finally, can you briefly compare your algorithm to the temperature scheduling method described in Fox et al., Taming the Noise in Reinforcement Learning via Soft Updates, 2016?

Additional Comments:
(1) It would be helpful to have Atari results provided in raw game scores in addition to the human-normalized scores (Figure 5). (2) The human normalized scores listed in Figure 5 for DQN are different than the ones listed in the Double DQN paper (Van Hasselt et al, 2016). (3) For the DBS-DQN algorithm, the authors set beta_t = ct^2 - how is the value of c determined? (4) Text in legends and axes of Figure 1 and Figure 2 plots is very small. (5) Typo: citation for MacKay - Information Theory, Inference and Learning Algorithms - author name listed twice.

Similarly, if the main contribution is DBS, it would be interesting to have a more in-depth empirical analysis of the method -- how does performance (in Atari or otherwise) vary with the temperature schedule, how exploration is affected, etc.?

After reading the other reviews and responses, I still think the paper needs further improvement before it can published.",4
"The writing and organization of the paper are clear.  Theorem 1 seems fine but is straightforward to anyone who has studied this topic and knows the literature.  Corollary one may be technically wrong (or at least it doesn't follow from the theorem), though this can be fixed by replacing the lim with a limsup.  Theorem 4 seems to be the main result all the work is leading up to, but I think this is wrong.  Stronger conditions are required on the sequence \beta_t, along the lines discussed in the paragraph on Boltzmann exploration in Section 2.2 of Singh et al 2000.  The proof provided by the authors relies on a ""Lemma 2"" which I can't find in the paper.  The computational results are potentially interesting but call for further scrutiny.  Given the issues with the theoretical results, I think its hard to justify accepting the paper.",4
"I liked this paper overall, though I feel that the way it is pitched to the reader is misguided. The looseness with which this paper uses 'exploration-exploitation tradeoff' is worrying. This paper does not attack that tradeoff at all really, since the tradeoff in RL concerns exploitation of understood knowledge vs deep-directed exploration, rather than just annealing between the max action and the mean over all actions (which does not incorporate any notion of uncertainty). Though I do recognize that the field overall is loose in this respect,  I do think this paper needs to rewrite its claims significantly. In fact it can be shown that Boltzmann exploration that incorporates a particular annealing schedule (but no notion of uncertainty) can be forced to suffer essentially linear regret even in the simple bandit case (O(T^(1-eps)) for any eps > 0) which of course means that it doesn't explore efficiently at all (see Singh 2000, Cesa-Bianchi 2017). Theorem 4 does not imply efficient exploration, since it requires very strong conditions on the alphas, and note that the same proof applies to vanilla Q-learning, which we know does not explore well.

I presume the title of this paper is a homage to the recent 'Boltzmann Exploration Done Right' paper, however, though the paper is cited, it is not discussed at all. That paper proved a strong regret bound for Boltzmann-like exploration in the bandit case, which this paper actually does not for the RL case, so in some sense the homage is misplaced. Another recent paper that actually does prove a regret bound for a Boltzmann policy for RL is 'Variational Bayesian Reinforcement Learning with Regret Bounds', which also anneals the temperature, this should be mentioned.

All this is not to say that the paper is without merit, just that the main claims about exploration are not valid and consequently it needs to be repositioned. If the authors do that then I can revise my review.

Algorithm 2 has two typos related to s' and a'.",5
"The authors study the fundamental problems with GAN training. By performing a gradient analysis of the value surface of the optimal discriminator, the authors identify several key issues. 

In particular, for a fixed GAN objective they consider the optimal discriminator f* and analyze the gradients of f* at points x ~ P_g and x~P_d. The gradient decouples into the magnitude and direction terms. In previous work, the gradient vanishing issue was identified and the authors show that it is fundamentally only controlling the magnitude. Furthermore, controlling the magnitude doesn’t suffice as the gradient direction itself might be non-informative to move P_g to P_d. The authors proceed to analyze two cases: (1) No overlap between P_g and P_d where they show that the original GAN formulation, as well as the Wasserstein GAN will suffer from this issue, unless Lipschitzness is enforced. (2) For the case where P_g and P_d have overlap, the gradients will be locally useful which the authors identify as the fundamental source of mode collapse. 

The main theoretical result suggests that (1) penalizing the discriminator proportionally to the square of the Lipschitz constant is the key -- the choice of divergence is not. This readily implies that pure Wasserstein divergence may fail to provide useful gradients, as well as that other divergences combined with Lipschitz penalties (precise technical details in the paper) might succeed. Furthermore, it also implies that one can mix and match the components of the objective function for the discriminator, as long as the penalty is present, giving rise to many objectives which are not necessarily proper divergences. Finally, one can explain the recent success of many methods in practice: While the degenerate examples showing deficiencies of current methods can be derived, in practice we implement discriminators as some deep neural networks which induce relatively smooth value surfaces which in turn make the gradients more meaningful.

Pro:
- Clear setup and analysis of the considered cases. Interesting discussion from the perspective of the optimal discriminator and divergence minimization. The experiments on the toy data are definitely interesting and confirm some of the theoretical results. 
- A convincing discussion of why Wasserstein distance is not the key, but rather it is the Lipschitz constant. This brings some light on why the gradient penalty or spectral normalization help even for the non-saturating loss [2]. 
- Discussion on why 1-Lip is sufficient, but might be too strong. The authors suggest that instead of requiring 1-Lip on the entire space, it suffices to require Lipschitz continuity in the blending region of the marginal distributions. 

Con:
- Practical considerations: I appreciate the theoretical implications of this work. However, how can we exploit this knowledge in practice? As stated by the authors, many of these issues are sidestepped by our current inductive biases in neural architectures.
-  Can you provide more detail on your main theorem, in particular property (d). Doesn't it imply that the discriminator is constant?
- Which currently known objectives do not satisfy the assumptions of the theorem?
- The work would benefit from a polishing pass.

========
Thank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score.",6
"The authors try to claim that Lipschitz continuity of the discriminator is a fundamental solution of GANs, and that current methods do not satisfy this approach in principle.

There are several false statements in this paper. In particular, sections 2.3 and 4.4 are wrong (and most of the paper is based on statements made there). The necessary constraint for the Wasserstein distance is NOT f(x) - f(y) <= d(x, y) for all x ~ Pr, y ~ Pg. It has to actually be 1-Lipschitz in the entire space. See Chapters 5 and 6 of [1], for example remark 6.4 or particular cases 5.16 and 5.4. Indeed, this is how it is written in all of the literature this reviewer is aware off, and it's a fact well used in the literature. Indeed, all the smoothness results for optimal transport in [1] heavily exploit the fact that the gradient of the critic is in the direction of the optimal transport map, which wouldn't be the case in the situation the authors try to claim of 'f not being defined outside of the support of Pr or Pg'.

Furthermore, the relationship between Lipschitz continuity and having a gradient is elaborated in [2] https://arxiv.org/abs/1701.07875 , for example figure 2 clearly show this. Furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold *without* the Wasserstein distance, but with Lipschitz continuity are already elaborated in the wgan paper. See in fact, appendix G.1 [3], where this is described in detail.

[1]: http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf
[2]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf
[3]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf",4
"
[pros]
- It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases.
- It also proposes use of the penalty term in terms of the Lipschitz constant  of the discriminative function.

[cons]
- Some of the arguments on the Wasserstein distance and on WGAN are not sound.
- Theorem 3 does not make sense.
- The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).

[Quality]
I found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below.

[Clarity]
The main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions.

[Originality]
Despite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\phi,\varphi,\psi$ and the form of the gradient penalty, $\max \|\nabla f(x)\|_2^2$ in this paper versus $E[(\|\nabla f(x)\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal.

[Significance]
This paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning.

Detailed comments:

In Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation.

It is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the ""critic"" $f$ via a multilayer neural network with weight clipping.

One can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading.

On optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\sim P_g}[f(x)]-E_{x\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. 

I do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes ""$\forall x \not= y$"", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\|x-y\|$ to obtain $f(x)-f(y)=k\|y-x\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \not= y$ such that $f(y)-f(x)=k\|x-y\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\|x-y\|$ under the Lipschitz condition.

Appendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\partial J_D/\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as ""arg min"" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function.

Page 5, line 36: $J_D(x)$ appears without explicit definition.

Page 23, lines 34 and 38: Cluttered expression $\frac{\partial [}{\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.",5
"This paper proposed a model-free HRL method, which is combined with unsupervised learning methods, including abnormality discovery and clustering for subgoal discovery. In all, this paper studies a very important problem in RL and is easy to follow. The technique is sound. Although the novelty is not that significant (combining existing techniques), it showed good results on Montezuma’ revenge, which is considered as a very challenging  problem for primitive action based RL.

Although the results are impressive, I still have some doubt about the generalizability of the method. It might be helpful to improve its significance if more diversified domains can be tested.

The paper can be strengthen by providing some ablation test, for example, is performance under different K for Kmeans? 

Also some important details seems missing, for example, the data used for kmeans, it is mentioned that the input to the controller is four consecutive frame of size 84x84, so the input data dimension is more than 10k, I guess some further dimensionality reduction technique has to be applied in order to run kmeans effectively.

Regarding the comparisons, the proposed method is only compared with one primitive action based method. It might be better to include results from other HRL methods, such as Kulkarni et al.

Is the curve based on the mean of different runs? It might be useful to include an errorbar to show the statistical significance.
",5
"Summary:
The authors propose an HRL system which learns subgoals based on unsupervised analysis of recent trajectories. The subgoals are found via anomaly/outlier detection (in this case states with a very high reward) and the clustering together of states that are very similar. The system is evaluated on the 4-rooms task and on the atari game Montezuma’s Revenge.

The paper cites relevant work and provides a nice explanation of subgoal-based HRL. The paper is for the most part well-written and easy to follow. 

The experiments are unfortunately not making a very convincing case for the general applicability of the the methods. While the system does not employ a model of the environment, k-means clustering based on distances seems to be particularly well-suited for the two environments investigated in the paper. It is known that the 4-rooms experiment is much easier to solve with subgoals that correspond to the rooms themselves. I can only conclude from this experiment that k-means can find those subgoals given the right number (4) of clusters and injecting the knowledge that distances in grid-worlds correlate well with transition probabilities. Similarly, the use of distance-based clustering seems well-suited for games with different rooms like Montezuma’s Revenge but that might not generalize to many other games. 

The anomaly detection subgoal discovery is interesting as a method to speed-up learning but it still requires these (potentially sparse) high reward states to be found first. For tasks with sparse rewards it does make sense to set high reward states as potential subgoals instead of waiting for value to propagate. That said, the reward for the lower level policy is only less sparse in the sense that wasting time gets punished with a negative reward. Subgoal discovery based on rewards should probably also take the ability of the current policy to obtain those rewards into account like some other methods for subgoal discovery do (see for example Florensa et al., 2018). The authors mention that the subgoals were manually chosen by Kulkarni et al. (2016) instead of learned in an unsupervised way but I don’t think that the visual object detection method employed there is that much more problem specific. 

Like Kulkarni et al. (2016), the authors compare their method with DQN (Mnih et al. 2015) but it was already known that that baseline cannot solve the task at all and a lot more results on Montezuma’s Revenge have been published since then. A more insightful baseline would have been to compare with at least some other HRL methods that are able to learn the task to some extend like perhaps Feudal Networks (Vezhnevets et al., 2017). Looking at the graph in the Feudal Networks paper for comparison, the results in this paper seem to be on par with the LSTM baseline there but it is hard to compare this on the basis of the number of episodes. Did the reward go up further after running the experiment longer? 

Since the results are not that spectacular and a comparison with prior work is lacking, the main contributions of the paper are more conceptual. I think that it is interesting to think more carefully about how sparse reward states and state similarities can be used more efficiently but the ideas in the paper are not original or theoretically founded enough to have a lot of impact without the company of stronger empirical results.

Extra reference:
Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel. (2017). Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366.

",4
"This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of “interesting” states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.

Clarity:
I found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? It’s not clear from the algorithm box. Some important details are also left out. The section on Montezuma’s Revenge mentioned that the goal set was initialized using a “custom edge detection algorithm”. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I can’t find it anywhere in the paper. Omissions like this make the method completely unreproducible. 

Novelty:
The idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of “Dynamic Abstraction in Reinforcement Learning via Clustering” by Mannor et al. or of “Learning Options in Reinforcement Learning” by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.

Significance:
I was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezuma’s Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, it’s not clear how this method could address scalability issues. Repeated clustering could become expensive and it’s not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.

Overall quality:
The proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) I’m not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.

Minor comments:
- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.
- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I don’t understand what it means for a problem to have scaling issues.
- Please check your usage of \cite and \citep. Some citations are in the wrong format.
- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\gamma max Q) are held fixed in Q-learning.",3
"Summary:
This paper tries to verify a hypothesis that language grounding DO help to overcome language drift when two agents creating their own protocol in order to communicate with each other. There are several constraints to enforce: 1) naturalness, say ""Englishness"", 2) grounded in visual semantics. The experiments prove that both constraints help the most (say, BLUE score). 1) w/o 2) restricts the vocabulary into a small set with the most frequent words, while 1) with 2) can resemble the original distribution. 

Strength:
- How to make the protocol automatically created by two agents much explainable/meaningful is a very interesting topic. This paper explores plausible constraints to reach this goal. 

Weakness:
- Visual grounding task brings more data there. To fairly compare, I hope to add one more baseline PG+LM+G_text, where G_text simply means to use text data (captions) alone, i.e., without visual signals.
",6
"This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. They demonstrate that the jointly optimized agents perform best when regularized in this manner to prevent language drift.

1. Framing: I’m uncertain about the framing of this paper. The authors pose the problem of “language drift,” which is indeed a frequent problem in multi-agent communication tasks where the principle supervision involves non-linguistic inputs and outputs. They then design a three-language MT task as a test case, where the inputs and outputs are both linguistic. Why attack this particular task and grounding solution? I can imagine some potential goals of the paper, but also see more direct ways to address each of the potential goals than what the authors have chosen:
1a. Study how to counter language drift in general — why not choose a more intuitive two-agent communication task, e.g. navigation, game playing, etc.?
1b. Study how to counter language drift in the MT task — aren’t there simpler solutions to prevent language drift in this particular task? e.g. require “cycle-consistency” – that it be possible to reconstruct the French input using the French output? Why pick multimodal grounding, given that it imposes substantial additional data requirements?
1c. Build a better/more data-efficient machine translation system — this could be an interesting goal and suitable for the paper, but this doesn’t seem to be the framing that the authors intend.

2. Interpretation of first results:
2a. Thanks for including standard deviation estimates! I think it’s also important that you do some sort of significance testing on the comparison between PG+LM+G and PG+LM performance for Fr->En->De — these numbers look pretty close to me. You could run e.g. a simple sign test on examples within each corpus between the two conditions.
2b. It would also be good to know how robust your results are to hyperparameter settings (especially the entropy regularization hyperparameter).

3. Token frequency results: These are intriguing but quite confusing to me!
3a. How sensitive are these results to your entropy regularization setup? How does PG behave without entropy regularization?
3b. Table 6 shows that the PG model has very different drift for different POS categories. Does this explain away the change in the token frequency distribution? What do the token frequency effects look like for PG within the open-class / content word categories (i.e., controlling for the huge difference in closed-class behavior)?

4. Minor comments:
4a. There’s a related problem in unsupervised representation learning for language. Work on VAEs for language, for example, has shown that the encoder often collapses meaning differences in the latent representation, and leans on an overly powerful decoder in order to pick up all of the lost information. It would be good to reference this work in your framing (see e.g. Bowman et al. (2015)).
4b. In sec. 3.1 you overload notation for R. Can you subscript these so that it’s especially clear in your results which systems are following which reward function?
4c. Great to show some qualitative examples in Table 7 — can you explicitly state where these are from (dev set vs. test set?) and whether they are randomly sampled?

References:
Bowman et al. (2015). Generating sentences from a continuous space. https://arxiv.org/abs/1511.06349
",6
"The paper presents an approach to refining a translation system with grounding (in addition to LM scores) in the loop to manage linguistic drift.  The intuition is straightforward and results are clearly presented, but the gains are unfortunately much weaker than I would have hoped for.  

The results for both Fr-En and Fr-En-De only show very small gains for adding grounding, often with PG+LM results being within 1 std-dev of the PG+LM+G results.  Otherwise, the results are quite nice with interesting increases in linguistic diversity.  This leads me to wonder if this approach would show more gains with a human evaluation rather than BLEU score. 

What is the performance of PG+G without the +LM?  

Minor -- In Fig 2, should the green line (PG+LM) have continued climbing to >21 BLEU?",6
"
Summary

From a theoretical point of view, one might be tempted to believe that deep CNNs are translation equivariant and their predictions are translation invariant. In practice, this is not necessarily true. The authors propose to augment standard deep CNNs with low-pass filters to reduce this problem. The results seem promising for an older VGG architecture.

Quality

The paper is very verbose, the figures and captions are tedious to read, the mathematical notation seems strange as well, making the writing more concise is highly encouraged. The main ideas are easy to follow and the choice of experiments seems fine. 

Significance

This is the first empirical work trying to fix the issue of non-translation equivariance in convolutional neural networks. The conclusions of this work are potentially relevant for a wide audience of CNN practitioners.

Main Concerns

To show that all claims of the paper do indeed hold, the authors should attack their augmented network with the translation attack of [1]. As robustness to this type of transformations is one of the main goals, it should be tested if it was achieved. The attack can be found in some open source frameworks [2] and should be easy to apply.

Wall-clock times need to be reported for the various blurring kernels and compared to the baselines.

Extend results to a cutting-edge architecture, e.g. DenseNets or Wide ResNets. If this result is not provided the significance of the work is not clear.

Despite being more expensive, do dilations fix the issue of missing translation equivariance provably and not just approximately like the low-pass filtering approach proposed here? This should be discussed and a comparison in terms of wall-clock time would be great as well.

Minor

- Strange notation e.g. in equation 1. Why not write: x+\delta x in the argument of the function instead of ""Shift"". The current notation seems unnecessarily informal.
- Figure 4: show scale and color bar.

[1] Engstrom et al., ""A rotation and a translation suffice: Fooling cnns with simple transformations.""
[2] https://foolbox.readthedocs.io/en/latest/modules/attacks/decision.html#foolbox.attacks.SpatialAttack",6
"This paper analyzed on the core factor that make CNNs fail to hold shift-invariance, the naive downsampling in pooling. And based on that the paper proposed the modified pooling operation by introducing a low-pass filter which endows a shift-equivariance in the convolution features and consequently the shift-invariance of CNNs.

Pros:
1.	The paper proposed a simple but novel approach to make CNNs shift-invariant following the traditional signal processing principle.
2.	This work gave convincing analysis (from both theoretical illustrations and experimental visualizations) on the problem of original pooling and the effectiveness of the proposed blur kernels.
3.	The experiment gave some promising results. Without augmentation, the proposed method shows higher consistency to the random shifts.

Cons:
1.	When cooperating with augmentation, the test accuracy on random shifted images of proposed method did not exceed the baseline. Although the consistency is higher, it is secondary to the test accuracy of random shifted data. And it is confused to do average on consistency and test accuracy, which are in different scales, and then compare the overall performance on the averages. 
2.	It seems to be more convincing if the ‘random’ test accuracy is acquired by averaging several random shifts on a single image and then do average among images, as well as to show how accuracy various on shifting distance.
3.	Some other spatial transforming/shifting adaptive approaches should be taken into consideration to compare the performance.
4.	There are some minor typos, such as line 3 in Section 3.1 and line 15 in Section 3.2
",5
"This work shows that adding a simple blurring into max pooling layers can address issues of image classification instability under small image shifts. In general this work presents a simple and easy to implement solution to a common problem of CNNs and even though it lacks more thorough theoretical analysis of this problem from the signal processing perspective (such as minimal size of the blurring kernel for fulfilling the Nyquist-Shannon sampling theorem), it seems to provide ample empirical evidence.

Pros:
+ The introduction and motivation is really well written and Figure 3 provides a clear visualisation main max pooling operator issues.
+ The proposed method is really simple and shows promising results on the CIFAR dataset. With random shifts, authors had to tackle cropping with circular shifts. As it can cause artifacts in the data, authors also provide baseline performances on the original data (used for both training and testing).
+ Authors provide a thorough evaluation, ranging from comparing hidden representations to defining consistency metrics of the classified classes.

This work is lacking in the experimental section due to some missing details and few inconsistencies. I believe the most of my concerns can be relatively easily fixed/clarified in an update of this submission.

Major issues, which if fixed would improve the rating:
- It is not correct to average test accuracy and test consistency as both measures are different quantities, especially when using them for ranking. The difference between accuracy of different methods are considerably smaller than differences in the classification consistency. 
- It is not clear how many shifts are used for computing the ""Random Test Accuracy"" and the ""Classification Accuracy"". Also whether the random shifts are kept constant between evaluated networks and evaluation metrics.
- Authors do not address the question what is the correct order of operations for the blurring. E.g. would the method empirically work if blurring was applied before max pooling? Do the operations commute?
- The selection of the filters is rather arbitrary, especially regarding the 1D FIR filters. The separability of these filters should be discussed.
- I believe authors should address how this work differs to [1], as it also tests different windowing functions for pooling operators, even though in different tasks.

Minor issues, which would be nice to fix however which do not influence my rating:
* Section 3.1 - And L-Layer deep *CNN*, H_l x W x C_l -> H_l x W_l x C_l
* Section 3.1. Last paragraph - I would not agree with the statement that in CNNs the shift invariance must necessarily emerge upon shift equivariance. If anything, this may hold only for the last layer of a network without fully connected layers and with average pooling of the classifier output (ResNet/GoogleNet like networks).
* Explicitly provide the network architecture as [Simonyan14] does not test on CIFAR and cannot use Batch normalisation.
* It would be useful to add citation for the selected FIR filters.
* The flow of section 4.2. can be improved to help readability. The three metrics should be first motivated before their introduction. Metric 2. paragraph - the metric is defined below, not above. 
* It would be interesting to see what would be the performance if the blurring filters were trained as well (given some sensible initialisation).
* One future direction would be to verify that this approach generalises to larger networks as well. It might be worth to discuss this in the conclusions.

[1] Scherer, Dominik, Andreas Müller, and Sven Behnke. ""Evaluation of pooling operations in convolutional architectures for object recognition."" Artificial Neural Networks–ICANN 2010. Springer, Berlin, Heidelberg, 2010. 92-101.",5
"I have read the discussion from the authors. my evaluation stays the same.
--------
this paper studies an interesting question of how to learn causal effects from observational data generated from reinforcement learning. they work with a very challenging setting where an unobserved confounder exists at each time step that affects actions, rewards and the confounder at next time step.

the authors fit latent variables models to the observational data and perform experiments.

the major concern is on the causal inference side, where it is not easy to claim anything causal in such a complicated system with unobserved confounders. causal inference with unobserved confounders cannot be simply solved by fitting a latent variable model. there exists negative examples even in the simplest setting that two distinct causal structure can lead to the same observational distribution. for example here, https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/

it could be helpful if the authors can lay out the identification assumptions for causal effects. before claiming anything causal and justifying experimental results.",4
"The paper addresses an important and often overlooked issue in off-policy reinforcement learning - the possibility of confounding between the agent's actions and the rewards. This is a subject which has been exhaustively explored in the causal inference literature, and the authors are very correct in suggesting that it should be incorporated into the world of reinforcement learning.  Specifically they propose a generative model with a global latent confounder that is inferred using a variational autoencoder architecture.  

The paper is generally well-written, though some points could be made clearer in my opinion, as detailed below. The experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but I am not entirely sure whether the given architecture is necessary, see comments below. 

High-level comments:
(1) Classic RL deals with confounders all the time. The state is a confounder between the action and the reward. The issue of confounding becomes less trivial when one is performing off-policy RL when the original policy is *unknown*. This is exactly the case that the authors mention when they cite the recent work by Gottesman et al. (2018) who deal with using RL to learn from the actions of physicians in a hospital.  While I am sure the authors are aware of these distinctions, I think the paper would be better if this is spelled out very explicitly. This includes explaining why this issue doesn't come up in classic RL.

(2) Assuming the case above - off-policy RL with unknown confounders - one would usually assume ""no unmeasured confounding"", i.e. that the observed actions are an unknown but learnable function of the observed states. That is basically the scenario of most off-policy RL.

(3) However, the authors strive to go one step beyond the case (2), to a situation where there is an *unmeasured* confounder affecting both observed actions and rewards. If nothing is known about this unmeasured confounder, then it is generally impossible to learn effective policies, as the causal effects of actions are not identifiable from the observed data. In this paper, the authors make an implicit assumption that while the confounder is unmeasured, it can still be inferred from the data. This is an intermediate step between ""no unmeasured confounding"" and ""complete unmeasured confounding"". This is related to work on using proxy variables e.g. Kuroki & Pearl (2014) and even more closely related to the work cited by Louizos et al. (2017).
Again, I think the paper would be much improved if all this is addressed explicitly. 

(4) An important consequence of point (3) above is that in fact adding the single global latent-confounder U is not, in itself, very important from a causal perspective. The sequence of variables Z_1... Z_T are already latent confounders that are assumed to be inferrable from data. It is true that the addition of the global U might change the statistical and optimization properties of the model. This leads to a very important conclusion: the authors should test their model with and without U. I think this specific ablation experiment is crucial. In many cases I am sure that the assumption of a global latent confounder is a good one and is especially useful in the VAE case where it will make optimization more stable. However, in principle, all of U's roles could be taken within the sequence of Z's, and I am curious to see in practice how big of an effect it has.

(5) I wish to add that even if the U variable turns out to not add much empirically, this work is still valid since the sequence of Z's can themselves be considered inferred latent confounders.

Specific comments:
(1) 2.3: there are more than 2 ways of computing the do-operator. RCTs and backdoor are the best known approaches, but not the only ones, e.g. there is frontdoor adjustment. 

(2) I think the paper would be easier to follow if there was one concrete example used throughout. This will make it easier to understand and possibly verify/criticize the assumptions of the generative model.

(3) Related to ""higher-level point (4)"" above, in eqs. 17 & 18 note that Z_t is unknown, same as U. Both are inferred. This also leads to the question which Z_t is actually used in practice? Is it the mean, or is it also sampled from the approximate posterior q?

(4) Below eq. 19, it would be very useful for the readers if you could explain exactly when would there be a difference between the two versions p(r_{t+1}|z_t,a_t) and p(r_{t+1}|z_t, do(a_t=a))

(5) In the description of all the experiments I was missing a crucial point: how does the introduced confounder affect the reward? Is it only through the different actions? The way it is currently explained, it seems like the added variable introduces lack of *overlap*, but not strictly confounding.

(6) The description of the experiment in 4.3 could be more detailed. What exactly was the training and test? What RL method was used? What did the baseline optimize for? I would like to see an ablation experiment where U is not included in the model. 

(7) In 4.5, what is the ""vanilla"" method? And as mentioned above, I would like to see an ablation experiment where U is not included in the model.  


",4
"This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. 

First, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn’t make sense: in RL you get to choose the policy so it doesn’t make sense to assume that the choice of action is confounded while you’re doing RL. To get around this, the authors assume that they’re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don’t explain why you would want to do RL at all when you could just do causal inference directly. If you can’t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). 

# Method
The authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to  believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. 

Quibbles:
 - Page 3: the authors claim the model is “without loss of generality” but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc.
  - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn’t all be factorized normal distributions. Real data isn’t made up of factorized normals.

# Experiments

The authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I’ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. “Confounding” is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn’t affected directly by the latent variable. Because of this, there isn’t actually a confounding problem - the “confounder” simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). 

The RL evaluations aren’t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions.

# Summary
This work studies a setting in which the correct baselines would be causal inference algorithms (but they aren’t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. 
",2
"Summary:
Proposes BCRL for learning from a fixed collection of off-policy experience (I'll call this the ""training data""). BCRL attempts to avoid backing up values from states that are not present in the training data, on the assumption that the current estimates of these values are likely to be inaccurate. In the continuous state-action case, this is accomplished by training a generative model to propose, given a state `s`, an action `a` such that a transition similar to `(s, a)` is in the training data. A secondary policy is then trained to perturb the proposed action within a constrained region to maximize value. BCRL outperforms DDPG and DQN when learning from fixed data, but BCRL is slightly worse than behavior cloning at learning to reproduce an expert policy that does not take exploration actions.

Review:
The overall approach is sound. The problem of extrapolation is intuitively obvious, but not something I had thought about before. I think typically exploration would correct the problem since states with over-estimated values would become more likely to be reached, giving an opportunity to get a better estimate. 

The learning setting is closer to imitation learning than to what I would call RL, since the BCRL approach essentially avoids extrapolation error by ignoring the parts of the problem that are not represented in the training data. The well-known problem with behavior cloning is compounding errors once the agent strays into areas of the state space that are far from the training data. To me ""off-policy RL"" implies that the goal is to learn a complete policy from off-policy data. I think the ""competitors"" to which BCRL should be compared are imitation learning algorithms address noisy demonstrations, and not so much off-policy RL algorithms. It would also be interesting to see the generalization performance of BCRL outside of its training data.

The BCRL idea might be applicable in a conventional RL setting as well, since the initial stages of learning could be subject to a similar extrapolation error until there has been enough exploration. A comparison to something like TRPO in this setting would be interesting.

The paper is well-written with good coverage of related literature. There are a few points where the technical content is imprecise, which I note below. 

Comments / Questions:
* Could one obtain a similar effect to BCRL by simply initializing the value estimates pessimistically?
* Sec 4.1: Since B is a set of (s, a, s', r) tuples, what does it mean for a state s' to be ""in B""? Similar question for state-action tuples (s, a).
* As you note in the appendix, the construction in Sec 4.1 is essentially creating a new MDP that contains only the transitions that occur in the training data. I'd suggest stating as much in the main paper for intuition.
* Sec 4.2 / 5: The perturbation constraint \Phi is set to 0.05 in the experiments. Since the actions in these control problems are vectors, what does a scalar constraint correspond to? How is the constraint enforced during learning?
* What are the distance functions D_S and D_A?

Pros:
* A good approach to applying RL methods in the ""imitation-like"" setting. I've seen similar things attempted before, but this method makes more sense. 

Cons:
* The learning setting is more like ""fuzzy"" behavior cloning from noisy data than off-policy RL. Experimental comparison against more-sophisticated imitation learning approaches is missing.",7
"Authors consider a problem of off-policy reinforcement learning in a setting explicitly constrained to a fixed batch of transitions. 
The argument is that popular RL methods underperform significantly in this setting because they fail to account for extrapolation error caused by inevitably sparse sampling of the possible action-state space.
To address this problem, authors introduce the notion of batch-constrained RL which studies policies and associated value functions only on the state-space covered by the available training data.
For practical applications a deep RL method is introduced which enables generalisation to the unseen states and actions by the means of function approximation.

I find the problem studied in the paper very important. It is indeed strongly connected to the idea of imitation learning which has been studied previously, but I like the explicit point from which authors see the problem.
The experimental results seem quite appealing to justify use of the proposed approach.

However, on the clarity side the paper should be improved before publication.

The interplay between action generating VAE G_w(s) and \pi is unclear to me.
First, what does it mean that G(s) is trained to minimise the distance D_A?

If G(s) is a VAE, then it is trained to minimise the corresponding variational lower bound, how is minimisation of the distance over actions is incorporated here? And what exactly is this distance?
Similarly, what does “D_S will be defined by the implicit distance induced by the function approximation” exactly mean?

Other comments / questions:

Page 6: “Theorem 1 implies with access to only
a subset of state-action pairs in the MDP, the value function… This suggests batch-constrained policies
are a necessary tool for combating extrapolation bias.”
This might be true, but it does not follow from the Theorem 1 as it only applies to the batch Bellman operator and not the standard one used in most methods.

Corollary 1 and 2: What is Q^* here?

Page 7, first sentence: should there be if A_s, e != \emptyset? 

Epsion-Batch-constrained policy iteration: would the beam search actually maximize Q function? This needs to be proven or at least discussed.

I don’t see how is epsilon used in the iteration scheme.  This needs to be clarified.

Equation 11: the subscript of the max operator looks weird, should there be just a_i?

Figure 4: where is “True value” curve on the plots?

The notation \pi(s, a; \Phi) used throughout the paper is confusing and can be interpreted as a joint distribution over states and actions.

As I said, currently the paper does not appear to be easy to follow to me and even if it does contain important ideas, I believe they must be communicated in a clearer way.
I am eager to revise my evaluation if authors make substantial effort to improve the paper.",5
"This paper studies extrapolation error in off-policy batch reinforcement learning (RL), where the extrapolation error refers to the overestimation of the value for the state-action pairs that are not in the training data.

The authors propose batch-constrained RL, where the policy is optimize under the constraint that, at each state, only those actions that have been taken in that state in the training data are allowed.  This is then extended to continuous space, where it allows only the state-action pairs that are close to a state-action pair in the training data.  When there is no such action for a given state, the action that is closet to a feasible action at that state is selected.

It makes intuitive sense that the proposed approach works well as long as we only encounter state-action pairs that are closed to one of the state-action pairs in the batch.  However, I do not expect that this is always the case.  The proposed method is to simply choose the closest action in the batch.  Then why does the proposed approach perform well?  Is it because the experiments are performed under rather deterministic settings?  How often are no state-action pairs found in the neighbor?  Is there any mechanism for recovering from ""not in the batch""?

The paper would be much stronger if it study this challenge of ""not in the batch"" more in depth.  Technical contributions in the present paper are rather limited.

A key assumption in the discrete case is that whole episodes are in the batch.  This is rather restricting, because in many applications, it is infeasible to collect a whole episode, and parts of many episodes are collected from many agents.  Although this assumption is stated, it would be nice to emphasize by also stating that the theorems do not hold when this assumption does not hold.  The assumption becomes less important for continuous case, because of approximation.  It might be interesting to study the performance of the proposed approach when the assumption does not hold in the continuous case.
",5
"This paper proposes a new approach to construct model-X knockoffs based on VAE, which can be used for controlling the false discovery rate. Both numerical simulations and real-data experiments are provided to corroborate the proposed method.  

Although the problem of generating knockoffs based on VAE is novel, the paper presentation is not easy to follow and the notation seems confusing. Moreover, the main idea of this paper seems not entirely novel. The proposed method is based on combining the analysis in ''Robust inference with knockoffs'' by Barber et. al. and the VAE.  

Detailed comments:

1. The presentation of the main results is a bit short. Section 2, the proposed method, only takes 2 pages. It would be better to present the main results with more details. 

2. The method works under the assumption that there exists a random variable $Z$ such that $X_j$'s are mutually independent conditioning on $Z$. Is this a strong assumption? It seems better to illustrate when this assumption holds and fails.

3. The notation of this paper seems confusing. For example, the authors did not introduce what $(X_j, X_{-j}, \tilde X_j, \tilde X_{-j} )$ means. Moreover, in Algorithm 1, what is $\hat \theta$ and $\hat f$. 

4. I think there might be a typo in the proof of Theorem 2.1. In the main equation, why $\tilde Z$ and $\tilde X$ did not appear? They should show up somewhere in the probabilities.

5. In Theorem 2.2, how strong is the assumption that $\sup_{z,x} | log (density ratio)| $ is smaller than $\alpha_n$? Usually, we might only achieve nonparametric rate for estimating the likelihood ratios. But here you also take a supremum, which might sacrifice the rate. The paper suggested that $\alpha_n$ can be o( (n \log p)^{-1/2}). When can we achieve such a rate?

6. Novelty. Theorem 2.2 seems to be an application of the result in Barber et. al. Compared with that work, this paper seems to use VAE to construct the distribution $ P_{\tilde X| X}$ and its analysis seems hinges on the assumptions in Theorem 2.2 that might be stringent.

7. In Figure 1 and 2, what is the $x$-axis?

8. A typo: Page 2, last paragraph. ""In this paper, we relaxes the ...""",3
"This manuscript tackles an important problem, namely, generating the knockoff procedure for FDR-controlled feature selection so that it can work on any data, rather than only for data generated from a Gaussian (as in the original work) or several specific other cases (mixtures of Gaussians or HMMs).  The basic idea is to exploit ideas from variational autoencoders to create a generic knockoff generation mechanism. Specifically, the main idea of the paper is to map the input covariates X into latent variable Z using a variational auto-encoder, generate the knockoffs \tilde{Z} in the latent space, and then map \tilde{Z} back to the input space to get the knockoffs \tilde{X}. The authors claim that their contribution in threefold: 

(1) Given the assumption is valid that X is mutually independent conditional on Z, the generated knockoffs \tilde{X} is proved to be valid in terms of satisfying the necessary swap condition. 

(2) Given (1) holds, and given that the discrepancy (measured by KL-divergence) between the true conditional probability Q(Z|X) and its estimate using variational auto-encoder is bounded by o((nlogp)^{-1/2}), the FDR is also bounded.

(3) The proposed knockoffs generating procedure can achieve controlled FDR and better power. 

In agreement with the above intuition, I have major concerns about the correctness of the paper.

The cornerstone of the proof in contribution (1) relies on the assumption that X is mutually independent conditional on Z. However, this assumption is invalid if there are dependencies between x_i and x_j. Therefore, the proposed knockoffs \tilde{X} cannot be proved valid by Theorem 2.1.

The erroneous proof in contribution (1) leads to the failure of contribution (2) stated in Theorem 2.2. The FDR is no longer controlled. Intuitively, according to algorithm 1, \tilde{Z} and \tilde{X} are expected to have the same distribution as Z and X, respectively; therefore, the FDR cannot be controlled.

The experimental results are suspicious. In general, it seems fishy that the proposed VAE approach outperforms Model X in the fully Gaussian case.  In this setting, Model X should have an advantage, since it is specifically designed for Gaussian generated data.  Related to this, the text is confusing: ""Since the data were not Gaussian, the second-order matching method has the lowest power. The assumptions of the Fixed knockoff generations holds for the Gaussian cases, …"" In the figure, the second-order matching method has low power even in the Gaussian case.

Minor comments:

p. 2: The exposition should explain the motivation for Knockoff+.

The manuscript contains numerous grammatical errors, a few examples of which are below:

p. 1: ""biological linked"" -> ""biologically linked""

p. 1: ""associated certain"" -> ""associated with a certain""

p. 1: ""showed light"" -> ""shed light""

p. 1: ""which has very limited"" -> ""which has limited""

p. 1: ""leveraging on the power of of"" -> ""leveraging the power of""


",4
"In the paper , the authors proposed the use of autoencoder for Model-X knockoffs. The authors proved that, if there exists latent factors, and if the encoders and the decoders can approximate conditional distributions well, the autoencoder can be used for approximating Model-X knockoff random variables: one can find relevant features while controlling FDR (Theorem 2.2).

I think the theoretical part is good, and the experimental results seem to be promising.

My concern is the gap between theory and practice. In the manuscript, the authors used VAE for approximating conditional distributions. The question is how we can confirm that the trained encoder and decoder satisfy the assumptions in Theorem 2.2. If the trained models violate the assumptions, the control of FDR is no longer guaranteed, which may lead to false discoveries. As long as this gap remains unfilled, we cannot use the procedure reliably: we always need to doubt whether the encoders and decoders are trained appropriately or not. I think this gap is unfavorable for scientific discovery where only rigid procedures are accepted.
How we can use the proposed procedure reliably, e.g. for scientific discovery? Is there any way to confirm that the encoders and decoders are appropriate? Or, is there any way to bypass the gap so that we can guarantee the FDR control even for inappropriate models?",6
"This paper presents an approach to address the task on zero-shot learning for speech recognition, which consist of learning an acoustic model without any resources for a given language. The universal phonetic model is proposed, which learns phone attributes (instead of phone label), which allows to do prediction on any phone set, i.e. on any language. The model is evaluated on 20 languages and is shown to improve over a baseline trained only on English.

The proposed UPM approach is novel and significant: being able to learn a more abstract representation for phones which is language-independent is a very promising lead to handle the problem of ASR on languages with low or no resources available. 

However, the results are the weak point of the paper. While the results demonstrate the viability of the approach, the gain between the baseline performance and the UPM model is quite small, and it's still far from being usable in practice. 

To improve the paper, the authors should discuss the future work, i.e. what are the next steps to improve the model.

Overall, the paper is significant and can pave the way for a new category of approaches to tackle zero-shot learning for speech recognition. Even if the results are not great, as a first step they are completely acceptable, so I recommend to accept the paper.

Revision:
The approach of using robust features is interesting and promising, as well as the idea of training on multiple languages. Overall, the authors response addressed most of the issues, therefore I am not changing my rating.",7
"Overview:

This paper proposed an approach for zero-shot phoneme recognition, where it is possible to recognise phonemes in a target language which has never been seen before. Rather than just training a phoneme recogniser directly on background data and then applying it to unseen data, phonetic features are first predicted, allowing phonemes not in the source language set to be predicted.

Main strengths:

The paper's main strength lies in that this is a very unexplored area that could assist in the development of speech technology where it is currently not possible. The proposed model (Section 2) has also not been considered in prior work.

Main weaknesses:

The paper's main weakness is in some of its claims and that it misses some very relevant literature. Detailed comments together with a minimal list of references are given below (but I would encourage the authors to also read a bit more broadly). But in short I do not think it is that easy to claim that this is the first paper to do zero-shot learning on speech; many of the zero-resource studies where unlabelled audio is used could be seen as doing some for of zero-shot matching. Specifically [5] is able to predict unseen phoneme targets.  Multilingual bottleneck features can be applied to languages that have never been seen before [2], and the output of phoneme recognisers trained on one language have long been applied to get output on another unseen language. The first one-shot learning speech paper [4] (to my knowledge) is also not mentioned at all. The approach in the paper also still relies on some text data from the target language; if this then can be described as ""zero-shot"" learning, then I think many of these previous studies c also make this claim.

Overall feedback:

There is definitely value in this work, but it should be much better situated within the broader literature. Below I give some editorial suggestions and also outline some suggestions for further experiments.

Detailed comments, suggestions and questions:

- Abstract: It would be useful to have some details of the ""baseline model"" here already, especially since it is such a new task.
- Introduction: ""... but they can hardly predict phones or words directly due to their unsupervised nature."" This is a strong statement that maybe requires more justification. On the one hand, the statement is true, and the high word error rates in e.g. [3] can be cited. On the other hand, it has been shown that at the phone-distinction level, these models perform quite well and sometimes outperform supervised models [1]. Since this paper also considers phone error rate as a metric, I think care should be taken with such statements.
- Introduction: ""While zero-shot learning has attracted a lot of attention in *the* computer vision community, this setup has hardly been studied in speech recognition research especially in acoustic modeling."" Definitely look at some of the studies mentioned below, and also [4] specifically.
- ""However, we note that our model can be combined with a well-resourced language model to recognize words."" How would this be done, since I think this is actually quite a challenging task.
- Section 2: ""... useful the original ESZSL architecture ..."" -> ""... useful in the original ESZSL architecture ...""
- Section 2.2: I assume the small text corpus is at the phone level (and not characters directly)? This should be clarified, and it could raise the question of whether this approach is truly ""zero-shot"".
- Section 3.2: ""We used EESEN framework ..."" -> ""We used the EESEN framework ...""
- Section 4: You could look at the recent work in [2], which uses multilingual bottleneck features trained on 10 languages and applied to multiple unseen languages. It would be interesting to also train your approach on multiple languages instead of only English.

Missing references:

1. M. Heck, S. Sakti, and S. Nakamura, ""Feature Optimized DPGMM Clustering for Unsupervised Subword Modeling: A Contribution to Zerospeech 2017,"" in Proc. ASRU, 2017.
2. E. Hermann and S. J. Goldwater, ""Multilingual bottleneck features for subword modeling in zero-resource languages,"" in Proc. Interspeech, 2018.
3. H. Kamper, K. Livescu, and S. Goldwater, An embedded segmental k-means model for unsupervised segmentation and clustering of speech,"" in Proc. ASRU, 2017.
4. B. M. Lake, C.-Y. Lee, J. R. Glass, and J. B. Tenenbaum, ""One-shot learning of generative speech concepts,"" in Proc. CogSci, 2014.
5. O. Scharenborg, F. Ciannella, S. Palaskar, A. Black, F. Metze, L. Ondel, and M. Hasegawa-Johnson, ""Building an ASR system for a low-resource language through the adaptation of a high-resource language asr system: Preliminary results,""in Proc. ICNLSSP, 2017.

Edit: Based on the rebuttal I've changed my rating from 4 to 5.",5
"This paper proposes to train a Universal Phonetic Model for building speech recognition for new languages without any training data. It suggests to use X-SAMPA to map phones from all the languages into a single phonetic space. The prediction models are designed to first predict the phonetic features and then the phones depending on the target language.
Overall , the paper is quite clear written. 
- Strengthens:
+ It observed overall improvements for all the target languages.

- Weaknesses:
+ The idea and the proposed model are not novel. 
+ All the baseline systems have relative high phone error rates.
+ The authors claimed to have a universal phonetic model but actually the model was trained only with English data. Therefore, experimental setup could be improved. In my opinion, it makes more sense to define a bunch of resource-rich languages as source and then train a real universal phonetic model. 
+ Overall, this paper lacks an analysis what are exactly improved and why the improvements for some target languages are larger than for the others.
 ",4
"
The paper presents a model to perform audio super resolution. The proposed model trains a neural network to produce a high-resolution audio sample given a low resolution input. It uses three losses: sample reconstructon, adversarialy loss and feature matching on a representation learned on an unsupervised way.

From a technical perspective, I do not find the proposed approach very novel. It uses architectures following closely what has been done for Image supre-resolution. I am not aware of an effective use of GANs in the audio processing domain. This would be a good point for the paper. However, the evidence presented does not seem very convincing in my view. While this is an audio processing paper, it lacks domain insights (even the terminology feels borrowed from the image domain). Again, most of the modeling decisions seem to follow what has been done for images. The empirical results seem good, but the generated audio does not match the quality of the state-of-the-art.

The presentation of the paper is correct. It would be good to list or summarize the contributions of this work.

Recent works have shown the amazing power of auto-regressive generative models (WaveNet)  in producing audio signals. This is, as far as I know, the state-of-the-art in audio generation. The authors should motivate why the proposed model is better or worth studying in light of those approaches. In particular, a recent work [A] has shown very high quality results in the problem of speech conversion (which seems harder than bandwidth extension). It would seem to me that applying such models to the bandwith extension task should also lead to very high quality results as well. What is the advantage of the proposed approach? Would a WaveNet decoder also be improved by including these auxiliary losses?

While the audio samples seem to be good, they are also a bit noisy even compared with the baseline. This is not the case in the samples generated by [A] (which is of course a different problem). 

The qualitative results are evaluated using PESQ. While this is a good proxy it is much better to perform blind tests with listeners. That would certainly improve the paper. 

Feature spaces are used in super resolution to provide a space in which the an L2 loss is perceptually more relevant. There are many such representations for audio signals. Specifically the magnitude of time-frequency representations (like spectrograms) or more sophisticated features such as scattering coefficients. In my view, the paper would be much stronger if these features would be evaluated as alternative to the features provided by the proposed autoencoder. 

One of the motivations for defining the loss in the feature space is the lack (or difficulty to train) auxiliary classifiers on large amounts of data.  However, speech recognition models using neural networks are quite common. It would be good to also test features obtained from an off-the-shelf speech recognition system. How would this compare to the proposed model?

The L2 ""pixel"" loss seems a bit strange in my view. Particularly in audio processing, the recovered high frequency components can be synthesized with an arbitrary phase. This means that imposing an exact match seems like a constraint as the phase cannot be predicted from the low resolution signal (which is what a GAN loss could achieve). 

The paper should present ablations on the use of the different losses. In particular, one of the main contributions is the inclusion of the loss measured in the learned feature space. The authors mention that not including it leads to audible artifacts. I think that more studies should be presented (including quantitative evaluations and audio samples).

How where the hyper parameters chosen? is there a lot of sensitivity to their values?


[A] van den Oord, Aaron, and Oriol Vinyals. ""Neural discrete representation learning."" Advances in Neural Information Processing Systems. 2017.
",4
"This paper presents a GAN-based method to perform audio super-resolution. In contrast to previous work, this work uses auto-encoder to obtain feature losses derived from unlabeled data. 

Comments:
(1) Redundant comma: “filters with very large receptive fields are required to create high quality, raw audio”.

(2) There are some state-of-the-art non-autoregressive generative models for audio waveform e.g., parallel wavenet, clarinet. One may properly discuss them in related work section. Although GAN performs very well for images, it hasn't obtained any compelling results for raw audios. Still, it’s very interesting to explore that. Any nontrivial insight would be highly appreciated.

(3) In multiscale convolutional layers, it seems only larger filter plays a significant role. What if we omit small filter, e.g., 3X1?

(4) It seems the proposed MU-GAN introduces noticeable noise in the upsampled audios. 

Pros:
- Interesting idea and fascinating problem. 
Cons:
- The results are fair. I didn’t see big improvement over previous work (Kuleshov et al., 2017).

I'd like to reconsider my rating after the rebuttal.
",5
"PRO’s:
+well-written
+nice overall system: GAN framework for super-sampling audio incorporating features from an autoencoder
+some good-sounding examples

CON’s:
-some confusing/weakly-presented parts (admittedly covering lots of material in short space)
-I am confused about the evaluation; would like additional qualitative/observational understanding of what works, including more on how the results differ from baseline

SUMMARY: The task addressed in this work is: given a low-resolution audio signal, generate corresponding high-quality audio. The approach is a generative neural network that operates on raw audio and train within a GAN framework. 
Working in raw sample-space (e.g. pixels) is known to be challenging, so a stabilizing solution is to incorporate a feature loss. Feature loss, however, usually requires a network trained on a related task, and if such a net one does not already exist, then building one can have its own (possibly significant) challenges. In this work, the authors avoid this auxiliary challenge by using unsupervised feature losses, taking advantage of the fact that any audio signal can be downsampled, and therefore one has the corresponding upsampled signal as well.

The training framework is basically that of a GAN, but where, rather than providing the generator with a low-dimensional noise signal input, they provide the generator with the subsampled audio signal. The architecture includes a generator ( G(lo-fidelity)=high-fidelity ), a discriminator ( D(high-fidelity) = real or by super-sampled ? ), and an autoencoder ( \phi( signal x) = features of signal x at AE’s bottleneck). 

COMMENTS:

The generator network appears to be nearly identical to that of Kuleshov et al (2017)-- which becomes the baseline-- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term. This is overall a nice problem and a nice approach! In that light, I believe that there is a new focus in this work on the perceptual quality of the outputs, as compared to (Kuleshov et al 2017). I would therefore ideally like to see (a) some attempts at perceptually evaluating the resulting output (beyond PESQ, e.g. with human subjects and with the understanding that, e.g. not all AMT workers have the same aural discriminative abilities themselves), and/or (b) more detailed associated qualitative descriptions/visualization of the super-sampled signal, perhaps with a few more samples if that would help. That said, I understand that there are page/space limitations. (more on this next)

Given the similarity of the U-net architectures to (Kuleshov et al 2017), why not move some of those descriptions to the appendix? 

For example, I found the description and figure illustrating the “superpixel layers” to be fairly uninformative: I see that the figure shows interleaving and de-interleaving, resulting in trading-off dimensionalities/ranks/etc, and we are told that this helps with well-known checkerboard artifacts, but I was confused about what the white elements represent, and the caption just reiterated that resolution was being increased and decreased. Overall, I didn’t really understand exactly the role that this plays in the system; I wondered if it either needed a lot more clarification (in an appendix?), or just less space spent on it, but keeping the pointers to the relevant references.  It seems that the subpixel layer was already implemented in Kuleshov 2017, with some explanation, yet in the present work a large table (Table 1(b)) is presented showing that there is no difference in quality metrics, and the text also mentions that there is no significant perceptual difference in audio. If the subpixel layer were explained in detail, and with justification, then I would potentially be OK with the negative results, but in this case it’s not clear why spend this time on it here. It’s possible that there is something simple about it that I am not understanding. I’m open to being convinced. Otherwise, why not just write: “Following (Kuleshov et al 2017), we use subpixel layers (Shi et al) [instead of ...] to speed up training, although we found that they make no significant perceptual effects.” or something along those lines, and leave it at that? 

I did appreciate the descriptions of models’ sensitivity to size/structure of the conv filters, importance of the res connections, etc.

My biggest confusion was with the evaluation & results:

Since the most directly related work was (Kuleshov 2017), I compared the super resolution (U-net) samples on that website (https://kuleshov.github.io/audio-super-res/ ) to the samples provided for the present work ( https://sites.google.com/view/unsupervised-audiosr/home ) and I was a bit confused, because the quality of the U-net samples in (Kuleshov 2017) seemed to be perceptually significantly better than the quality of the Deep CNN (U-net) baseline in the present work. Perhaps I am in error about this, but as far as I can tell, the superresolution in (Kuleshov et al 2017) is significantly better than the Deep CNN examples here. Is this a result of careful selection of examples? I do believe what I hear, e.g. that the MU-GAN8 is clearly better on some examples than the U-net8. But then for non-identical samples, how come U-net4 actually generally sounds better than U-net8? That doesn’t make immediate sense either (assuming no overfitting etc). Is the benefit in moving from U-net4 to U-net8 within a GAN context but then stabilizing  it with the feature-based loss? If so, then how does MU-GAN8 compare to U-net4? Would there be any info for the reader by doing an ablation removing the feature loss from the GAN framework? etc. I guess I would like to get a better understanding of what is actually going on, even if qualitative. Is there any qualitative or anecdotal observation about which “types” of samples one system works better on than another? For example, in the provided examples for the present paper, it seemed to be the case that perhaps the MU-GAN8 was more helpful for supersampling female voices, which might have more high-frequency components that seem to get lost when downsampling, but maybe I’m overgeneralizing from the few examples I heard. 

Some spectrograms might be helpful, since they do after all convey some useful information despite not telling much of the perceptual story. For example, are there visible but inaudible artifacts? Are such artifacts systematic?

Were individual audio samples represented as a one-hot encoding, or as floats? (I assume floats since there was no mention of sampling from a distribution to select the value).

A couple of typos:

descriminator → discriminator 

pg 6 “Impact of superpixel layers” -- last sentence of 2nd par is actually not a sentence. “the reduction in convolutional kernels prior to the superpixel operation.”

Overall, interesting work, and I enjoyed reading it. If some of my questions around evaluation could be addressed-- either in a revision, or in a rebuttal (e.g. if I completely misunderstood something)-- I would gladly consider revising my rating (which is currently somewhere between 6 and 7).
",6
"This manuscript applies transfer learning for protein surface prediction. The problem is important and  the idea is novel and interesting. However, the  transfer learning model is unclear. 
Pros:  interesting and novel idea
Cons:  unclear transfer learning model, insufficient experiments. 

Detail: section 4 describes the transfer learning model used in the work, but the description is unclear. It is unknown the used model is a new model or existing model. Besides, in the experiments, the proposed method is not compared to other transfer learning methods.  Thus, the evidence of the experiments is not enough. ",5
"Summary:
This paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces. They show improvements in predictive performance over some other recent deep learning methods. 
The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.

Novelty:
The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN

Clarity:
- The paper is well written, with ample background into the problem.

Significance:
- Their method improves over prior deep learning approaches to this problem. However, the results are a bit misleading in their reporting of the std error. They should try different train/test splits and report the performance.
- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community
- Protein conformation information is not required by their method

Comments:
- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)

-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred

- The authors use a balanced ratio of positive and negative examples. The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones. Can they show performance at various ratios of positive:negative examples? In case there is a consistent improvement over prior methods, then this would be a clear winner
",5
"For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins’ native structure.
The authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.

My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario. Also, the compared methods don’t really use the validation set from the complex data for training at all. Thus the experiment comparison is not really fair. 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling. Specifically, the testing dataset is fixed. A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.

Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis. Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can. Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods. Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.

Overall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.


Some minor issues:
1)	on page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?

2)	It is not clear what the “replicates” refer to in the experiments.

3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?
",5
"The paper proposes the inclusive neural random field model. Compared the existing work, the model is different because of the use of the inclusive-divergence minimization for the generative model and the use of stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo  (SGHMC) for sampling. Experimental results are reported for unsupervised, semi-supervised, and supervised learning problems on both synthetic and real-world datasets. Specific comments follow:

1. A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.

2. A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.

Other points:
3. the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.

4. more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.

5. what makes some of the statistics in the tables unobtainable or unreported?


============= After Reading Response from Authors ====================

The reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors’ argument. 

“The target NRF model, the generator and the sampler are all different.”
It is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.

As for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2’s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.

The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.

In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.

A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.

",5
"This paper addresses an important problem of learning the random field using neural networks by using a inclusive auxiliary generator. Comparing to existing state-of-the-art methods for learning neural random fields, this paper used a the inclusive-divergence (KL divergence of the density approximate and the auxiliary generator) which avoids the intractable entropy term. SGLD/SGHMC are used to revise samples drawn from the auxiliary generator and these two sampling methods are examined theoretically.  

In generally, the paper is well motivated and well written. Experiments are sufficient and convincing, especially the synthetic data experiments with GMM distributions. 

However, I am a little bit concerned that the theoretical contribution seems weak. As discussed in the related work, the idea of using neural network to learn the random field is not new. Using inclusive-divergence is also not new, e.g. Xie et al (2016) and Wang & Ou (2017) already proposed to use the inclusive-divergence. If I understand it correctly, the only contribution here is to apply the SGLD/SGHMC to revise the samples and authors provided some theoretical analysis of SGLD/SGHMC.

The overall technical quality of the paper is sound but I am not 100% sure about the equations, e.g. the second line in Eq. 4. 

In summary, this paper is well written and authors have done a good job. But I will appreciate if authors can elaborate
more on the novelty and innovation of the paper. ",6
"This paper describes a method of training NRFs with auxiliary generator networks, using an error that minimizes KL(NRF || generator).  This formulation enables the use of iterative gradient-based stochastic sampling of image samples from the model distribution using SGLD/SGHMC.  Applications to both unsupervised sample generation and semi-supervised classification are evaluated.

I'm not very familiar with these types of NRFs or random sampling techniques, but the approach appears sound and is evaluated rather well.  I would have liked some more background and explicit description and contrast compared to the explicit NRF.  While this is described already, I think the contrasts could potentially be spelled out even more explicitly, particularly in the descriptions of the sampling algorithms.

The toy example with mixture of gaussians is convincing showing the contrast in results between the exclusive NRF, inclusive, and sampling gradient revision steps.

Experimental evaluations on MNIST, SVHN and CIFAR show that the system obtains performance similar to SOA generative systems, in both semi-supervised classification and sample generation.


Questions and comments:

- While the paper claims the results show classification and generation performance are complementary, Table 3 appears to validate the opposite claim, that these are to a large degree a trade-off.  The fact that this system performs well at both is good, but to me it looks like it may be on the ""shoulder"" of a frontier curve if one were to plot the classification vs generation performance of the different current systems.

- Table 4 and sec 4.4:  I think these could be clearer.  The first observation states that revision improves IS.  But using more iterations (increasing L) does not appear to increase IS.  There does appear to be a consistent increase from the first column (generation) to second (revision), though -- is this what this observation refers to?  In addition, I'm not entirely clear what the ""Generation IS"" vs ""Revision IS"" column refers to --- I believe ""generation"" is the initial sampling of x=g(h) (i.e. h followed by q(x | h)), and ""revision"" is the application of gradient revision.  But then how does the generation IS results change from row to row (which only modify the revision step)?

",6
"The authors address the problem of representation learning in which data-generative factors of variation are separated, or disentangled, from each other. Pointing out that unsupervised disentangling is hard despite recent breakthroughs, and that supervised disentangling needs a large number of carefully labeled data, they propose a “weakly supervised” approach that does not require explicit factor labels, but instead divides the training data in to two subsets. One set, the “reference set” is known to the learning algorithm to leave a set of generative “target factors” fixed at one specific value per factor, while the other set is known to the learning algorithm to vary across all generative factors. The problem setup posed by the authors is to separate the corresponding two sets of factors into two non-overlapping sets of latents. 

Pros:

To address this problem, the authors propose an architecture that includes a reverse KL-term in the loss, and they show convincingly that this approach is indeed successful in separating the two sets of generative factors from each other. This is demonstrated in two different ways. First, quantitatively on an a modified MNIST dataset, showing that the information about the target factors is indeed (mostly) in the set of latents that are meant to capture them. Second, qualitatively on the modified MNIST and on a further dataset, AffectNet, which has been carefully curated by the authors to improve the quality of the reference set. The qualitative results are impressive and show that this approach can be used to transfer the target factors from one image, onto another image.

Technically, this work combines and extends a set of interesting techniques into a novel framework, applied to a new way of disentangling two sets of factors of variation with a VAE approach. 

Cons:

The problem that this work solves seems somewhat artificial, and the training data, while less burdensome than having explicit labels, is still difficult to obtain in practice. More importantly, though, both the title and the start of the both the abstract and the introduction are somewhat misleading. That’s because this work does not actually address disentangling in the sense of “Learning disentangled representations from visual data, where high-level generative factors correspond to independent dimensions of feature vectors…” What it really addresses is separating two sets of factors into different parts of the representation, within each of which the factors can be, are very likely are, entangled with each other.

Related to the point that this work is not really about disentangling, the quantitative comparisons with completely unsupervised baselines are not really that meaningful, at least not in terms of what this work sets out to do. All it shows is whether information about the target factors is easily (linearly) decodable from the latents, which, while related to disentangling, says little about the quality of it. On the positive side, this kind of quantitative comparison (where the authors approach has to show that the information exists in the correct part of the space) is not pitted unfairly against the unsupervised baselines.

===
Update: 
The authors have made a good effort to address the concerns raised, and I believe the paper should be accepted in its current form. I have increased my rating from 6 to 7, accordingly. ",7
"Summary: Given two sets of data, where one is unlabelled and the other is a reference data set with a particular factor of variation that is fixed, the approach disentangles this factor of variation from the others. The approach uses a VAE whose latents are split into e that represents the factor of variation and z that represents the remaining factors. A symmetric KL loss that is approximated using the density-ratio trick is optimised for the learning, and the method is applied to MNIST digit style disentangling and AffectNet facial expression disentangling.

Pros:
- Clearly written
- Results look promising, both quantitative and qualitative.

Cons:
- Mathieu et al disentangle a specific factor from others without explicit labels but by drawing two images with the same value of the specified factor (i.e. drawing from the reference set) and also drawing a third image with a any value of the specified factor (i.e. drawing from the unlabelled set). Hence their approach is directly applicable to the problem at hand in the paper. Although Mathieu et al use digit/face identity as the shared factor, their method is directly applicable to the case where the shared factor is digit style/facial expression. Hence it appears to me that it should be compared against.
- missing reference - Bouchacourt - explicit labels aren’t given and data is grouped where each group shares a factor of var. But here the data is assumed to be partitioned into groups, so there is no equivalent to the unlablled set, hence difficult to compare against for the outlined tasks.
- Regarding comparison against unsupervised disentangling methods, there have been more recent approaches since betaVAE and DIP-VAE (e.g. FactorVAE (Kim et al) TCVAE (Chen et al)). It would be nice to compare against these methods, not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers.

Other Qs/comments
- the KL terms in (5) are intractable due to the densities p^u(x) and p^r(x), hence two separate discriminators need to be used to approximate two separate density ratios, making the model rather large and complicated with many moving parts. What would happen if these KL terms in (5) are dropped and one simply uses SGVB to optimise the resulting loss without the need for discriminators? Usually discriminators tend to heavily underestimate density ratios (See e.g. Rosca et al), especially densities defined on high dimensions, so it might be best to avoid them whenever possible. The requirement of adding reconstruction terms to the loss in (10) is perhaps evidence of this, because these reconstruction terms are already present in the loss (3) & (5) that the discriminator should be approximating. So the necessity of extra regularisation of these reconstruction terms suggests that the discriminator is giving poor estimates of them. The reconstruction terms for z,e in (5) appear sufficient to force the model to use e (which is the motivation given in the paper for using the symmetric KL), akin to how InfoGAN forces the model to use the latents, so the necessity of the KL terms in (5) is questionable and appears to need further justification and/or ablation studies.
- (minor) why not learn the likelihood variance lambda?

************* Revision *************
I am convinced by the rebuttal of the authors, hence have modified my score accordingly.",6
"The paper proposes reference based VAEs, which considers learning semantically meaningful feature with weak supervision. The latent variable contains two parts, one related to the reference set and the other irrelevant. To prevent degenerate solutions, the paper proposed to use reverse KL resulting in a ALICE-style objective. The paper demonstrates interesting empirical results on feature prediction, conditional image generation and image synthesis.

I don’t really see how Equation (5) in symmetric KL prevents learning redundant z (i.e. z contains all information of e). It seems one could have both KL terms near zero but also have p(x|z, e) = p(x|z)? One scenario would be the case where z contains all the information about e (which learns the reference latent features), so we have redundant information in z. In this case, the learned features e are informative but the decoder does not use e anyways. To ensure that z does not contain information about e, one could add an adversarial predictor that tries to predict e from z. Note that this cannot be detected by the feature learning metric because it ignores z for RbVAE during training.

The experiments on conditional image generation look interesting, but I wonder if the ground truth transformation for MNIST can be simply described as in some linear transformation on the original image. I wonder if the proposed method works on SVHN, where you can use label information as reference supervision. Moreover, I wonder if it is possible to use multiple types of reference images, but fewer images in each type, to reach comparable or even better performance.

Minor points:
- Why assume that the reference distribution is delta distribution whose support has measure zero, instead of a regular Gaussian?
- (6), (8), (10) seems over complicated due to the semi-supervised nature of the objective. I wonder if having an additional figure would make things clearer. 
- Maybe it is helpful to cite the ALICE paper (Li et al) for Equation (10).
- Table 1, maybe add the word “respectively” so it is clearer which metric you use for which dataset.
- I wonder if it is fair enough to compare feature prediction with VAE and other models since they do not use any “weak supervision”; a fairer baseline could consider learning with the weak supervision labels (containing the information that some images have the same label). The improvement on AffectNet compared to regular VAE does not look amazing given the additional weak supervision.
",6
"UPDATE (after author response):

Thank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.

There's one point that the reviewers didn't clearly address:  ""It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions."" Please consider adding such an experiment.

The current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that ""Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd."" Showing that the method outperforms other methods would definitely strengthen the paper.

Section 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5.

---------

The paper proposes a new loss for training deep latent variable models. The novelty seems a bit limited, and the proposed method does not consistently seem to outperform existing methods in the experiments. I'd encourage the authors to add more experiments (see below for suggestions) and resubmit to a different venue.

Section 4:
- q(z) seems to be undefined. Is it the aggregated posterior?
- How is equation (1) related to ELBO that is used for training VAEs?

Some relevant references are missing: I’d love to see a discussion of how this loss relates to other VAE-GAN hybrids.

VEEGAN: Reducing mode collapse in GANs using implicit variational learning
https://arxiv.org/pdf/1705.07761.pdf

Distribution Matching in Variational Inference
https://arxiv.org/pdf/1802.06847.pdf


Section 5.1:
- The quantitative comparison measures MSE in pixel space and inception score, neither of which are particularly good measures for measuring the quality of how well the conditionals match. I’d encourage the authors to consider other metrics such as log-likelihood.

- It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions.

Section 5.4: 
- The error bars seem quite high. Is there a reason why the method cannot reliably reduce mode collapse?

Minor issues:
- CIFAT-10 -> CIFAR-10
",6
"The goal this is work is to develop a generative model that enjoys the strengths of both GAN and VAE without their inherent weaknesses. The paper proposes a learning framework, in which a generating process p is modeled by a neural network called generator, and an inference process q by another neural network encoder. The ultimate goal is to match the joint distributions, p(x, z) and q(x, z), and this is done by attempting to match the priors  p(z) and q(z) and matching the conditionals p(x|z) and q(x|z). As both q(z) and q(x|z) are impossible to sample from, the authors mathematically expand this objective criterion and rewrite to be dependent only on p(x|z), q(x) and q(z|x), that can be easily sampled from. In the main part of the work, the authors use the f-divergence theory (Nowozin et al., 2016) to present the optimization problem as minmax optimization problem, that is learned using an adversarial game, using training and inference algorithms that are proposed by the authors. In experiments, the authors consider both reconstruction and generation tasks using the MNIST, CIFAR10 and CelebA datasets. Results show that the proposed method yields better MSE reconstruction error as better as a higher inception scores for the generated examples, compared to a standard GAN and a few other methods. 

This work establishes an important bridge between the VAE and GAN framework, and has a a good combination of theoretical and experimental aspects. Experiments results are encouraging, even though only relatively simple and small datasets were used. Overall, I would recommend accepting the paper for presentation in the conference. ",7
"This paper presents a variant of the adversarial generative modeling
framework, allowing it to incorporate an inference mechanism. As such it is
very much in the same spirit as existing methods such as ALI/BiGAN. The
authors go through an information theoretic motivation but end up with the
standard GAN objective function plus a latent space (z) reconstruction
term. The z-space reconstruction is accomplished by first sampling z from
its standard normal prior and pushing that sample through the generator to
get sample in the data space (x), then x is propagated through an encoder
to get a new latent-space sample z'. Reconstruction is done to reduce the
error between z' and z.

Novelty: The space of adversarially trained latent variable models has
grown quite crowded in recent years. In light of the existing literature,
this paper's contribution can be seen as incremental, with relatively low novelty. 

In the end, the training paradigm is basically the same as InfoGAN, with
the difference being that, in the proposed model,  all the latent
variables are inferred (in InfoGAN, only a subset of the latent
variables are inferred) . This difference was a design decision on the part of the InfoGAN
authors and, in my opinion, does not represent a significantly novel
contribution on the part of this paper.  

Experiments: The experiments show that the proposed method is
better able to reconstruct examples than does ALI -- a result is not
necessarily surprising, but is interesting and worth further
investigation. I would like to understand better why it is that latent
variable (z) reconstruction gives rise to better x-space reconstruction.

I did not find the claims of better sample quality of AIM over ALI to be
well supported by the data. In this context, it is not entirely clear what
the significant difference in inception scores represents, though on this, the
results are consistent with those previously published

I really liked the experiment shown in Figure 4 (esp. 4b), it makes the
differences between AIM and ALI very clear. It shows that relative to ALI,
AIM sacrifices coherence between the ""marginal"" posterior (the distribution
of latent variables encoded from data samples) and the latent space
prior, in favor of superior reconstructions. AIM's choice of trade-off is
one that, in many contexts, one would happy to take as it ensures that
information about x is not lost -- as discussed elsewhere in the paper.
I view this aspect of the paper by far the most interesting. 

Summary,
Overall, the proposed AIM model is interesting and shows promise, but I'm
not sure how much impact it will have in light of the existing literature
in this area. Perhaps more ambitious applications would really show off the
power of the model and make it standout from the existing crowd. 
",4
"This paper presents a new method for learning diverse policies that can potentially transfer better to new environments. The proposed method aims to find simulation configurations that lead to diverse behaviors using “submodular optimizations” technique; an idea stemmed from past data summarization methods.  

Pros:

-The paper deals with an important problem in RL which is learning robust policies that can transfer better

- Simple idea based on prior information theory literature is proposed. 

- Good incorporation of past methods for improving robustness in RL.

Cons:

>>The paper has provided weak evidence and to support the effectiveness and significance of the proposed approach. The current analysis and experimental evaluations are not convincing.

Relevant baselines are missed and limited tasks are explored:
- A relevant prior work of Pinto et al. (2017) is not considered in the baselines for comparison.  
- Only two tasks are considered in experiments which are not representative of the effectiveness of the proposed approach and does not provide convincing evidence that the proposed method performs better than prior works.
-In the HalfCheetah task only the performance of random baseline is shown and comparison with EpOpt is not reported
-Results of the base DDPG policy is reported in the experiments (Fig 3 and Table 1). For a thorough comparison these missing results are necessary. 

>>Correct citation to the prior work of “domain randomization” is necessary:
- The second paragraph of related work section (first paragraph of page 2) explains that the idea of domain randomization has been first used in computer vision by Tobin et al. 2017. The idea of domain randomization was first proposed and deployed on a real robot platform in Sadeghi and Levine 2016 (Sadeghi, Fereshteh, and Sergey Levine. ""CAD2RL: Real single-image flight without a single real image."" arXiv preprint arXiv:1611.04201 (2016).) and was later used in Tobin et al. 2017 (this is also explained in the Tobin et al. 2017). Adding proper citation to Sadeghi and Levine 2016 is necessary. 

>> Relevant prior works are missed:
- The proposed idea in the paper is relevant to multi-task RL (e.g. Teh Y, Bapst V, Czarnecki WM, Quan J, Kirkpatrick J, Hadsell R, Heess N, Pascanu R. Distral: Robust multitask reinforcement learning. InAdvances in Neural Information Processing Systems 2017 (pp. 4496-4506). Adding related discussion about prior multi-task RL methods is highly recommended for improving the paper.
- Citating several relevant prior RL methods that deal with transfer learning is missed. A few examples are:

Rusu AA, Vecerik M, Rothörl T, Heess N, Pascanu R, Hadsell R. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286. 2016 Oct 13.

Rusu AA, Rabinowitz NC, Desjardins G, Soyer H, Kirkpatrick J, Kavukcuoglu K, Pascanu R, Hadsell R. Progressive neural networks. arXiv preprint arXiv:1606.04671. 2016 Jun 15.

Rusu AA, Colmenarejo SG, Gulcehre C, Desjardins G, Kirkpatrick J, Pascanu R, Mnih V, Kavukcuoglu K, Hadsell R. Policy distillation. arXiv preprint arXiv:1511.06295. 2015 Nov 19.


>> The format of the paper needs to be improved. 
- The introduction section is rather incomplete and does not properly motivate the problem and the proposed solution.
-The organization of the experimental section should be improved. It is hard to follow the experiments and find the key experimental results in the current version of the paper.
",4
"This paper addresses the problem of finding a policy that will perform well in a real environment when training in a simulator that may have errors.  It takes the now standard approach of trying to find a policy that performs well in an ensemble of simulated environments that are perturbations of the basic simulator.   The question is:  how can we construct an ensemble that represents the uncertainty about the real world well while being small enough for computational efficiency.  The idea is to construct a diverse set of samples that represents the whole space of important variations in the simulation;  the particular novelty here is to ensure that the sample set attains coverage over the *behaviors* of the simulator rather than the parameters of the simulation.  This problem is made difficult by the fact that there is no finite space of samples to choose from and the fact that we don't have a natural distance metric on the simulator behavior.

The main positive contributions of the paper are:
- The view of the problem of selecting from an infinite set as one of streaming sub-modular optimization.  This is a nice idea that is new to me and seemed appropriate for the problem.
- The idea that we want diversity in behavior, and then the technical approach of defining a kernel on simulator parameter sets that depends on the trajectories that those parameters induce.

I do have a set of questions and concerns:
- Might it not be better (more robust) to use not just trajectories from the current policy, but from other policies as well, to compute the kernel on parameter sets?  
- How do you get the length-scale parameters for the kernel?
- The confidence intervals in table 1 are too big to really justify firm conclusions;  it would be better to run the algorithms several more times, until the intervals pull apart.
- You say: ""For ease of implementation  and since, in higher dimensional system, the variance of the policy gradients becomes  a significant factor, we train the  robot  on both the environment summaries and the N_s random rollouts.""   This seems like it might be an important point that should be addressed earlier.  And, why does this ease implementation?
- I didn't understand:  ""Diverse summaries are more consistent than pure random sampling.""  What do you mean by consistent here?
- The metrics used in the empirical comparisons don't seem exactly right to me.  The goal of this work is to learn a policy that is robust in some sense (so  that,  e.g., it will do sim-2-real well).   We  really want  it to  work well in all  possible cases, not just in expectation or according to the sampling distribution you create, (I guess---since the paper said  the minimax criterion was desirable but difficult to work with).  So, then, it seems like  the best performance criterion would be to sample a whole lot of  domain parameters and report performance  on the worst  (rather than reporting performance on a distribution that's like the one you  trained it on or on an easy random one).

Overall, my view is that the idea is good, but somewhat small, and it hasn't really yet been proven to make a big difference.",6
"This paper studies the problem of robust policy optimization, motivated by the fact that policies that work in simulations do not transfer well to real world. The authors propose to use the diversity measure on the roll out trajectories to select a diverse set of simulator configurations and train policies that can work well for all of those selected configurations. 

Overall, I think the idea is interesting but it is not entirely clear why adding diverse configurations should result in good performance, and the experiments are very limited and not convincing enough. 

Pros:
- The paper is easy to follow.
- The idea of using a diverse summary to do robust policy optimization is interesting. 
- The diversity measure on the trajectories instead of the space of configuration parameters also intuitively makes sense since it takes into account that the similarity between two configuration parameters does not typically mean the similarity between their corresponding policies. 

Cons:
- The setting of this paper seems to only work for the fully observable case with state space being in R^d, deterministic dynamics and deterministic policy (otherwise the diversity measure would be stochastic?). It would be good to clarify these in Sec. 2. 
- For the example in Fig 2 and the first experiment, what I don't understand is why the initial state is not part of the policy.
- It is not clear if the reason that EP-OPT performed worse than the proposed approach is only because there are not enough rollouts for EP-OPT. This could be an unfair comparison.
- It would be good to show the comparison to EP-OPT for the second experiment as well. 
- Two experiments may not be enough to verify valid performance since there could be a lot of randomness in the results.
- In page 6, it would be good to clarify that the summary being optimal is only with respect to f(M_s), but not the original problem of finding optimal policy. ",5
"* I have revised my score upwards due to the authors response to my concerns --- particularly the addition of new results on graph classification. The original review remains here, and I respond to the author's response below. 

The authors propose a new technique to add “pooling” and “unpooling” layers to a graph neural network (GNN). To deal with the lack of spatial locality in graphs, the downsampling operation relies on a learned scalar projection vector (which gives the “scores” for selecting different nodes). During upsampling, the model simple relies on storing the un-sampled adjacency matrix. Thorough experimental results on Cora, Citeseer, and Pubmed highlight the utility of the approach, with ablation studies isolating the importance of the pool/unpool operations.

Overall, this is an interesting paper with the possibility of having a moderate impact within the area of GNNs/GCNs, and the method is clearly described. While there are a number of minor modifications made to the standard GCN model, which could potentially confound the results, the authors do provide a sensible ablation study to isolate the importance of their pool/unpool operations. The overall results on the three node classification datasets are also quite strong. 

The primary shortcoming of this paper is that it only evaluates the model on three citation network datasets (Cora, Citseer, and Pubmed). While these datasets are now standard in the GCN/GNN community, they are very small, have few labeled examples, and it would greatly strengthen the paper to use a different dataset or two, e.g., the Reddit or PPI datasets from Hamilton et al. 2017 or the BlogCatolog dataset used in Grover et al. 2016 could be used for node classification. Or the authors could apply the proposed technique to graph classification or link prediction. In this reviewers opinion, it is very hard to judge the general utility of a method when results are only provided on these three very-specific datasets, where the performance differences between methods are now very marginal. 

In a related point, while this work cites other approaches that apply pooling operations in graph neural networks (e.g., Ying et al. 2018, Simonovsky and Komodakis 2018), no comparisons are made against these approaches. One would suppose that these comparisons are not made because this paper only tests the graph U-net for node classification, but it would greatly strengthen this paper to add comparisons to these other pooling operations, e.g., for graph classification. Moreover, it is possible to define analogous unpooling operations for Ying et al. 2018 and Simonovsky and Komodakis 2018, similar to the unpooling operation used in this work (e.g., for Ying et al.’s DiffPool you can just “unpool” to the previous graph and assign each node a feature corresponding to the weighted sum of the features of the assigned clusters). Of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in Ying et al. 2018 and Simonovsky and Komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting. 

As another minor point, whereas unpooling operations can be used to define a generative model in the image setting, this is not the case here, as the unpooling operation relies on knowledge about the input graph (i.e., the model always unpools to the same connectivity structure). This is not necessarily a bad thing, but it could improve the paper to clarify this issue. ",7
"This paper proposes pooling and upsampling operations for graph structured data, to be interleaved with graph convolutions, following the spirit of fully convolutional networks for image pixel-wise prediction. Experiments are performed on node classification benchmarks, showing an improvement w.r.t. architectures that do not perform any downsampling/upsampling operations.

Given that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling, such as graph classification / regression. Moreover, authors should compare to other graph pooling methods.

Authors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions to see if they have the same effect on the performance when dealing with graphs. 

Some choices in the method seem rather arbitrary, such as the tanh non-linearity in \tilde y. Could the authors elaborate on that? How important is the gating?

It would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?

The proposed unpooling operation seems to be the same as unpooling performed to upsample images, that is using skip connections to track indices, by recovering the position where the max value comes from and setting the rest to 0. Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?

When skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.

What is the influence of the first embedding layer to reduce the dimensionality of the features?

How do the models in Table 2 compare in terms of number of parameters?
 What's the influence of imposing larger weights on self loop in the graph?

What about experiments in inductive settings?

Please add references for the following claim ""U-Net models with depth 3 or 4 are commonly used...""

Please double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures.

The literature review could be significantly improved, missing relevant papers to discuss include:
- Gori et al. A new model for learning in graph domains, 2005.
- Scarselli et al. The graph neural network model, 2009.
- Bruna et al. Spectral networks and locally connected networks on graphs, 2014.
- Henaff et al. Deep convolutional networks on graph-structured data, 2015.
- Niepert et al. Learning convolutional neural networks for graphs, 2016.
- Atwood and Towsley. Diffusion-convolutional neural networks, 2016.
- Bronstein et al. Geometric deep learning: going beyond Euclidean data, 2016.
- Monti et al. Geometric deep learning on graphs and manifolds using mixture model cnns, 2017.
- Fey et al. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels, 2017.
- Gama et al. Convolutional Neural Networks Architectures for Signals Supported on Graphs, 2018.
As well as other pixel-wise architecture for image-based tasks such as:
- Long et al. Fully Convolutional Networks for Semantic Segmentation, 2015.
- Jegou et al. The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation, 2016.
- Isola et al. Image-to-image translation with conditional adversarial networks, 2016.
- Zhao et al. Stacked What-Where auto-encoders, 2015.",4
"Summary:
This paper introduces an encoder-decoder neural net architecture for arbitrary graphs. The core contribution is pooling and un-pooling operations for respectively graph down and up sampling.

Pros:
+ U-Net like architectures indeed are very successful in vision applications, and having a model that was similar properties on graphs would be very useful.
+ The paper is clearly written. 
+ I really liked the idea behind the pooling operation: it is simple, seems easy to implement efficiently, and generally makes sense (although see concerns below). 
+ The choice of the baselines is reasonable, and experimental results seem convincing. Ablation studies are also there.

Cons:
- It is not clear why the evaluation seem to only be done for the transductive learning settings. I understand that some of the previous work might have done that, but this application scenario is quite limited.
- One concern about the g-pool operation is that it is not local: unlike e.g. max pool on 2D which produces local maxima, here the selection is done globally, which could lead to situations where the entire parts of the graph are completely ignored. 
- Another concern, which has been partially addressed in section 3.4 is that the connectivity is not really taken into account when downsampling the adjacency matrix. The solution which introduces previously non-existing edges and thus kind of modifies the original graph is not very satisfying. 
",7
"The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. 

1. Doesn't explain how exactly the mutation action is learned, and missing the explanation of how RL acts on its modification on NAS (Evo-NAS). 
2. Very poor explanation on LEARN TO COUNT experiment. The experiment contains difficult setups on a toy data, which makes it difficult to repeat. In figure 3, the paper says that the sample efficiency of the Evo-NAS strongly outperforms both the evolutionary and the neural agent. However, where the strength comes from is not discussed in detail. In figure 2, the paper claims that PQT outperforms Reinforce for both the Neural and the Evo-NAS agent. For the Evo-NAS agent, the gain is especially pronounced at the beginning of the experiment. Thus, the paper concludes that PQT can provide a stronger training signal than Reinforce. However, how much stronger training signal can obtain of the proposed method is not discussed. Because the experiments of 5.1 is setup on a toy data with complicated parameters. The conclusions based on this data set is not convincing. It would be better to add comparative results on the CIFAR and Imagenet data for convenient comparisons with state-of-the-art. 
3. Confusing notation and experimental setup. In 5.1, the sequence a is first defined as <a1, a2, .., an>. Then, after eq.2, the sequence a is given as a=<1, 2, ..., n>. It would be better to use different symbols here. ",4
"Review: 
The paper introduces a novel way to do architecture search that uses an RNN to guide the mutation operation. The method and the motivation of the idea as long with the related work are all clearly described. However, the experiments section does not show a big uplift of the method versus the baselines and the number of types of tasks is relatively small (artificial and text). 

Cons:
- No image task
- No large scale task to show the scalability
- No baselines that are not coming from AUTO-ML to show the relative performance of a classical method",5
"Summary:

The paper proposes a hybrid approach which combines evolution and RL. The key idea is to conduct tournament selection over a population of architectures with learned mutations. The mutations are defined as the output of an RNN controller which either reuses or alters the sequence descriptor of the parent at each step. The proposed hybrid architect is evaluated on both synthetic and text classification tasks and then compared against pure evolutionary and RL-based agents.

Pros:

* The method can be viewed as a generalization of conventional evolution by replacing the handcrafted (uniform) distribution of mutations with a learned one. On the one hand, this should hopefully improve the sample efficiency of pure genetic methods since the population can evolve towards more meaningful directions, assuming useful patterns can be learned by the mutation controller. On the other hand, mutating existing architectures seem a easier task than sampling the entire architecture from scratch.
* The synthetic experiment is interesting, though it's hard to draw any conclusions based a single task.

Cons:

* To my knowledge, all text classification tasks used in 5.2 are quite small. There is no evidence that the method can scale to and work well on large-scale tasks, where improving the sample efficiency becomes truly crucial and challenging. 
* It is good to see comparisons against pure evo and RL within the authors' own search space. However, the advantage of the proposed evo-NAS, especially when evaluated on real-world text classification tasks, does not seem significant enough. In particular, there is a clear overlap between the performance of architectures found by NAS, evo and evo-NAS (Figure 4). The advantage of evo-NAS is even smaller if we compare the very best model (as can be read from Figure 4) instead of the average among the top 10 (as reported in Table 2). In my option, performance of the strongest model is arguably more interesting than the averaged one in practice.
* Since no results on CIFAR or ImageNet are provided as in most prior works in the literature, it is impossible to empirically compare the method with the state-of-the-art. The experiments would be more convincing if a comparison can be provided on those benchmarks. Otherwise, it is possible that the current search space & hyperparameters are tailored towards evo-NAS and it remains unclear whether the method can generalize well to other domains and/or search spaces.
",4
"Summary:
The authors look at the problem of exploration in deep RL. They propose a “curiosity grid” which is a virtual grid laid out on top of the current level/area that an Atari agent is in. Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game. The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).

The authors argue that this method enables better exploration and they obtain an impressive score on Montezuma’s Revenge (MR). 

Review:
The paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm. The algorithm itself seems to work well and some of the results are convincing. I am a bit worried about the fact that the agents have access to their history of locations (“the grid”). The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.

The authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly. Only removing the grid access made results on MR very unstable. However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there. 

I was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.

The future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage. 

Nits/writing feedback:
- There is no need for such repetitive citing (esp paragraph 2 on page 2). Sometimes the same paper is cited 4 times within a few lines. While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting

####
Revision:

The rebuttal does little to clarify open questions:
1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.
2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.
3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm. This seems contradictory.",5
"The reinforcement learning tasks with sparse rewards are very important and challenging. The main idea of this work is to encourage intra-life novelty. The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode. 

However, the results are not enough to be accepted to ICLR having a very high standard. In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C. There are some RL algorithms reported to be better than A2C. For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017. 

=================================================================================================
I've read the rebuttal. I updated my score but still not vote for accept. 

This paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments.
",5
"This paper proposes use of intra-life coverage (an agent must visit all locations within each episode) for effective exploration in Atari games. This is in contrast of approaches that use inter-life coverage or curiosity metrics to incentivize exploration. The paper shows detailed results and analysis on 2 Atari games: Montezuma’s Revenge and Seaquest, and reports results on other games as well.

Strengths
1. Intuitively, the idea of intra-life curiosity is reasonable. The paper pursues this idea and provides experimental evidence towards it on 2 Atari games. It is able to show compelling improvements on the challenging Montezuma’s Revenge game.

Weaknesses
1. The two primary comparison points are missing:
1a. Comparison to other exploration methods. A number of methods that use state visitation counts (also referred to as diversity, eg. [A,B]), or prediction error (also referred to as curiosity, eg [C]) have been proposed in recent years. It is important to place the contributions in this paper in context of these other works. A number of these references are missing and no experimental comparison to these methods has been made. 

1b. Comparison between inter and intra life curiosity. One of the central motivation is the utility of intra-life curiosity vs inter-life curiosity, yet no comparisons to this effect have been provided.

2. Additionally, the paper employs a custom way of computing coverage (or diversity). It is in terms of location of agent on the screen, as opposed to featurization of the full game screen as used in prior works. It is possible that a large part of the gain comes from the clever design of the space for computing intrinsic exploration reward. The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward). More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for. The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.

3. I will encourage investigation on a more varied set of tasks. Perhaps, also using some MuJoCo environments, or 3D navigation environments. Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid. Additionally, all of these are still on Atari.

[A] Diversity is All You Need: Learning Skills without a Reward Function Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine

[B] EX2: Exploration with Exemplar Models for Deep Reinforcement Learning Justin Fu, John D. Co-Reyes, Sergey Levine

[C] Curiosity-driven Exploration by Self-supervised Prediction Deepak Pathak, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell International Conference on Machine Learning (ICML), 2017

",5
"This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong.

The paper is quite well written and clear. However there is several unsupported claims (see below).

A lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator.

Pros:
- The experimental section is quite thorough and the results seem overall good.
- The paper is quite clear.

Cons:
- There is a major mistake in the derivation of the proposed method. In eq. (6) & (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 <=> JSD(p||q) = 0 <=> p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either.

- The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator.

- I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator.

- The authors claim that their method doesn't ""just memorize the true data distribution"". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. 

- The authors states that ""adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training"", can the author give some references to support that statement ?

- According to the author: ""Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime"". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean.

Others:
- It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ?

Conclusion:
The idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved.

References:
[1] Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS",5
"This paper modifies the GAN objective by defining the TRUE and FAKE labels in terms of both the training sample, and a newly introduced random variable s. The intuition is that by progressively changing the definition of s, and its effect on the label, we can prevent the discriminator network from immediately learning to separate the two classes. 

The paper doesn't give any strong theoretical support for this intuition. And it I found it a bit surprising that the discriminator doesn't immediately learn the one extra bit of information introduced by every new level of augmentation. However, the results do seem to show that this augmentation has a beneficial effect on two different architectures in different data scenarios, although the increase is not uniform over all settings.

The approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated. Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs. There is also no direct comparison to other methods of weakening the discriminator, although these are mentioned in the related work. I think the paper would be much improved by a thorough investigation of the method's effect on training stability, to go along with the current set of evaluations.",4
"Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.

Pro:
- A simple idea to make the problem progressively more difficult.
- The writing is relatively easy to follow.
- Standardized experimental setup.

Con:
- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.
- Clarity can be improved: several pages of theory can really be summarized into “learning the joint distribution implies that the marginals are also correctly learned’ (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. 
- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a ""lens"" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.
- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?

[1] http://proceedings.mlr.press/v80/sajjadi18a.html
[2] https://arxiv.org/abs/1706.08500",5
"The authors formalize the feature attribution problem as a feature selection problem and they demonstrate that several existing feature attribution methods can be interpreted as approximation algorithms for Exclusive Feature Selection and Inclusive Feature Selection.

- The authors claim that IFS is better suited as the formalization for the feature attribution problem and EFS has several unfavourable properties. Although they did exhaustive experiments to show this, it is not clear to the reviewer. 

- Also, it is more interesting if the authors can show how we use IFS in real applications.
",4
"In this paper, the authors study the feature attribution problem as feature selection (exclusive and inclusive).  The authors go through previous work, provide definitions and attempt to answer questions that are relevant to this task.  The authors provide several experiments in order to empirically evaluate which of the two feature selection approaches is better suited for feature attibution.  Although this is a good review, and the motivation is sound, I think that much more ellaboration, and experiments on more than 200 images, would be required to reach definitive conclusions.",4
"This paper formulates feature attribution from a feature selection perspective, and compares EFS (Exclusive Feature Selection) and IFS (Exclusive Feature Selection), which shows IFS is a better fit for feature attribution.

[+] The paper is well-structured and the proposed approach is clearly presented.
[-] It would helpful if the author could discuss the time complexity of proposed methods and compare the running time with baseline methods in evaluation.
[-] My major concern on this paper is the significance, as the contribution of the paper seems to be very limited.
    1) Formalizing the feature attribution problem as a feature selection problem is straightforward. IFS and EFS are just Forward and Backward stepwise feature selection, which are classic feature selection schemes. Applying them to feature attribution/saliency map does not seem to have much technical contribution.
    2) One claimed contribution of this paper is that existing feature attribution methods can be viewed as approximation of IFS and EFS. However, this contribution also seems to be minor. As many feature selection methods are known to be approximation of backward or forward stepwise feature selection, it is straightforward to show the connection between other feature attribution methods and IFS/EFS.

In conclusion, I would recommend to reject this paper due to the limited novelty and technical contribution.
",3
"This paper presents a GAN construction which encourages the latent space to mode-match the data distribution.  This allows for unsupervised class-label inference (experimentally verified for two tasks).

I think the paper is of low significance, but the approach outlined is interesting.

Unfortunately, I think the work is slightly let down by the presentation (there are many typos, and the first couple of sections could do with a rewrite), as well as a lack of rigorous experimentation.  I believe that the paper is also missing references to the conditional VAE literature, which shares many similarities (at least in application) with the described approach.

Pros:
- Some theoretical justification for the approach taken.
- Early evidence that the method allows for latent space class separation, given a prior on number of classes.

Cons:
- A little more experimental evidence would be welcome.  E.g. why is the result for CIFAR 10 not shown---hard to understand how helpful the inductive bias is for a general problem.
- No discussion of conditional VAEs (which were designed with a very similar goal in mind).
- No discussion of why decomposing h in the manner in which they did was appropriate.
- Would be nice to see a more detailed study of how adding supervision + varying the strength of the inductive bias affects performance.

",5
"This paper is concerned with the so-called conditional generation, which was descried as the task of sampling from an unknown distribution conditioned on semantics of the data. This proposed method aims at achieving this by using a latent distribution engineered according to a certain data prior. Experimental results showed that the proposed method seems to produce good results.

I have several questions about the motivation and the method in the paper. First, it is not clear to me how the ""semantics"" of the data was defined. Is it given by visual inspection? Is it possible to find it with some automated method? Second, the authors seem to advocate the idea that data of a k-mode distribution should be generated from a k-mode latent distribution. It might be useful in certain scenarios; however, it is not clear why the transformation from the latent to the observed does not change the number of modes or why keeping the same number of modes would endow the latent distribution a ""semantics"" meaning. We know that a k-mode distribution can be obtained by applying a smooth nonlinear transformation to a Gaussian or uniform distribution and, similarly, a k-mode distribution can be transformed to a single-mode distribution with a smooth mapping. So I am not sure why engineering the latent distribution this way can give it a ""semantics"" meaning. Should we try to enforce a kind of smoothness of the transformation, by, say, penalizing high derivative values? Third, the experimental results seem nice, but the lack of comparisons blurs the advantage of the proposed method. How is the result produced by GAN compared to the reported one? How did the original GAN with the engineering latent distribution work? It would be appreciated if the authors could address these issues more clearly.",5
"The paper proposes simple modifications to GAN architecture for unsupervised conditional image generation. The authors achieve this by making the distribution of noise z dependent on variable y that can depend on the label distribution when available. This involves learning to predict the input noise z as well as y from the generated image. The qualitative results shown for unsupervised conditional image generations using the approach are convincing. 

Pros:
	- The paper is well written and easy to follow.
	- The simple modification to the noise distribution leads to good results on unsupervised conditional image generation.
	- Minimizes loss terms exactly instead of lower bound as is commonly done in other similar unsupervised approaches.
	- Theoretical justifications for the approach are convincing.

Cons:
	- The paper can be strengthened by including ablation studies on the loss term for the z reconstruction.
	- How will InfoGAN and VEEGAN compare with similar loss terms for the z reconstruction added to their objective?
	- It will be useful to show FID and other similar scores to better evaluate the learned generative model. Including mode counting, experiments will strengthen the paper.
	- ACGAN can suffer from the issue of generating images where it is easy to infer y [1]. This leads to mode collapse within each category/class. Will the proposed approach suffer from similar issues?

[1] Shu, Rui, Hung Bui, and Stefano Ermon. ""AC-GAN Learns a Biased Distribution.""",6
"The authors propose to combine BadGAN framework and VAT to accelerate learning in the semi-supervised setting. The paper shows that the VAT approach is actively pushing the decision boundary away from the high-density regions. While the BadGAN approach pulls the decision boundary to low-density regions. This simultaneous push and pull lead to after convergence in testing accuracy. The authors also report competitive results on standard datasets used for SSL such as SVHN and CIFAR10.

Positives:
The approach overcomes some of the difficulties with BadGAN which arise from training a GAN and density estimation network for generating “bad samples” useful for SSL. Instead of using a GAN, the proposed approach uses adversarial samples using VAT that are sufficiently confusing to the current estimate of the classifier. 
The theoretical justifications for the VAT interpretation are interesting and convincing. The visualizations of the bad samples show qualitatively that the bad samples from the BadGAN and proposed approach differ. Several other visualization aids in understanding the behavior of the algorithm.

Negatives:
Requires additional hyperparmeter tuning in tau and rho. Tuning these with large validation sets can lead to an overoptimistic estimate of the generalization. How sensitive is the performance to these parameters? 
As the authors point out, the method has limitations when the number of labeled samples is much smaller. It will be nice to see some results in this aspect.
Please include more details to clarify what is meant by ‘the role of the second term of (1) and (2) are overlapped’.",7
"This paper makes the interesting observation that the generative procedure proposed by Bad GAN paper can be replaced by a slightly modified VAT procedure. The reasoning is sound and leverages the intuition that adversarial examples (subject to a sufficiently small perturbation radius) are likely to be closer to a decision boundary than the original sample. 

The paper is generally easy to follow but the presentation could be improved. In particular more could be done to describe the terms in Equation 5. I’m also curious about the behavior of L^true, which is equivalently the fourth term in Eq 1. Even when reading Bad GAN paper, I did not quite understand their claim that this can be correctly interpreted as a conditional entropy term (if they really wanted conditional entropy, they should probably have either done H(p(k|x)) or H(p(k|x, k <= K))). I agree with the authors that the roles of the second and fourth terms overlapped, and I think this is sufficiently interesting to warrant some further elaboration in the paper. I also liked the reminder that power iteration selects a non-unique sign for the first eigenvector (subject to the random vector initialization); I encourage the authors to do an ablation test to convince the reader that “this modification helps to improve convergence speed of the test accuracy.”

The propositions in this paper were, in my opinion, not particularly insightful. While I think it is nice that the authors went through the effort of providing some formalism to the intuition that VAT has a “push decision boundary away from high-density regions”, I’m less sure if propositions 1 and 2 really provides any additional insight the behavior of VAT. Proposition 1 is pretty weak in that it only covers a 2-class logistic regression; it seems obvious that the adversarial perturbation points in the direction toward the decision hyperplane. If the authors could extend this to more general non-linear classifiers (perhaps subject to some assumptions), that would be more interesting. I don’t think Proposition 2 has any real value and recommend its relegation to the appendix.

I think the biggest weakness of this paper is the experiments. Taking Table 1 at face value, the conclusion that FAT is simply competitive with existing approaches suggests that the additional machinery isn’t particularly useful, providing little more than a vanilla VAT. I also think MNIST/SVHN has run its course as good semi-supervised learning benchmarks and would prefer to see such algorithms being scaled to more complex data. The main argument for why FAT should be prefered over VAT comes from Section 6.2. Figure 4 is more interesting, but is complicated by the fact that FAT checks both possible eigenvectors (+/- u) during training, which requires two forward passes in the classifier; did the authors give a similar treatment to VAT? Please show wall-clock time too. Unfortunately the computational efficiency gain seems to only hold true for MNIST/SVHN, but not for CIFAR. I worry that the observed gains will not sustain once we move to more complicated datasets.


Pros:
+ Simple and clean proposal
+ Easy to read
Cons:
- Limited insight
- Weak experiments",5
"The paper proposes to use the technique in VAT to generate adversarial complementary examples in the (K+1)-class semi-supervised learning framework described by the Bad GAN paper. This leads to a formulation that combines the VAT loss and the (K+1)-class classification loss. The paper also provides analysis regarding why VAT is useful for semi-supervised learning.

Pros
1. It is interesting to bridge two state-of-the-art semi-supervise learning methods in a meaningful.
2. Some positive results have been presented in Table 1 and Figure 4.

Cons and questions
1. I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here?
2. Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy.
3. If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method?
4. The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak.
5. There is not ablation study of different terms in the objective function.
6. In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison.",5
"In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness.
The paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network.
The authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach.
I enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well.
The proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn’t understand very well its implications on the expressiveness of proposed method against classical low-rank approach.
",7
"The paper proposes an input-dependent low rank approximations of weight matrices in neural nets with a goal of accelerating inference. Instead of decomposing a matrix (a layer) to a low rank space like previous work did, the paper proposes to use a linear combination/mixture of multiple low rank spaces. The linear combination is dynamic and data dependent, making it some sense of non-linear combination. The paper is interesting, however,
I doubt its significance at three aspects:
(1) computation efficiency: the primary motivation of this paper is to accelerate inference stage; however, it might not be wise to break computation in a single low-rank space to segments in multiple low-rank spaces. In the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together. Because of this, the primary motivation in this paper wasn't successfully supported by wall-clock time;
(2) low-rank approximation: low-rank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?). Because of this, low-rank neural nets [1][2] start from trained models, approximate it and fine-tune it, while this method trains from scratch without an approximation target. Although, we can fit the method to approximate trained matrices, then decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent). Therefore, I view this paper more in a research line of designing compact neural nets, which brings me to a concern in (3).
(3) efficient architecture design: essentially, the paper proposes a class of compact neural nets, at each layer of which there are K ""low-rank"" branches with a gating mechanism to select those branches. However, branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. 
 
Clarity:
How FLOPs reduction are exactly calculated? I am not convinced by FLOPs reduction in the LSTM experiments, since in LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper.

Improvement:
(1) Accuracy improvement in Table 1 is not statistically significant, but used more parameters. For example, an improvement of 93.01% over 92.92% is within an effect of training noise;
(2) It is a little hacking to conclude that a random matrix  P_random has a small storage size because we can storage a seed for recovery. When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation;
(3) sparse gating of low-rank branches may make this method more computation efficient.

[1] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014.
[2] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014.
[3] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. ""Going deeper with convolutions."" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9. 2015.
[4] Huang, Gao, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. ""Multi-scale dense networks for resource efficient image classification."" arXiv preprint arXiv:1703.09844 (2017).",4
"Some suggested improvements follow below

1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected.
2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN.
3. In (1), the dimensions of U^k and V^k should be mentioned explicitly.
4. The choice of “k” in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN?
5. The paper addresses low rank factorization for “MLP”, RNN/LSTM and “pointwise” convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward.
6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer?
7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?",6
"The paper discusses on a rate distortion interpretation of adversarial examples by building the equivalence of DNN and a noisy channel. The proposed topic is very interesting. However, it is quite disappointing after reading the paper, that it does not deliver. In a sense, the reader has an impression that the paper is a collection of fractions of small thoughts and empirical observation pieces that are yet to be stringed up coherently.
*To start with, the contributions are not clear. The major equations (1-4) are all pre-existing. The main Figure (fig.1) is also not new. Sec.3 is on implications, while it is more a discussion section centered on existing works about capacity and adversarial examples. Although it is claimed in the beginning of the paper 3 theoretical and empirical contributions, they are not clearly presented in the follow-up text.
*Empirical evaluation, in Fig.2, a legend should be in place to introduce the colored curves. Currently it is unclear what it is for each of the curves. 
*Fig.3: it is unclear why the MI plots are of piecewise straight lines. Does it imply that the two MIs are linearly related?
*Table 1&2: Not clear how this observation has to do with the RD theory.

Seems no response from the authors. ",4
"This paper considers the trade-off between the prediction accuracy of deep neural networks (DNNs) and sensitivity to adversarial examples.
Reviewing the (Gaussian) channel capacity and rate-distortion theory, i.e., the information bottleneck, the authors discuss their implications on the generalization performance of DNNs. The experiments demonstrate the SNR of gradients, information plane, the generalization gap, and fault tolerance against adversarial examples.

While the interpretations of DNN learning by the information theoretic concepts are interesting, most of them are already known results, and hence provide little novel theoretical knowledge.

The discussions in Section 3 are superficial. It is not clear how they are related to the main arguments of this paper.

While the experiment of fault tolerance is interesting, the implications obtained from experiments are somewhat trivial.

minor comments:
p.2, l.15: h, w, and c are undefined.   
Section 2: Rate-distortion theory is usually explained by the sphere covering argument instead of sphere packing. 
Section 4.3.1: It is not explained what zero and one-shot transfer learning is.

Pros:
The experiment of fault tolerance is interesting. 
Cons:
Theoretical parts are basic results of information theory.
The implications of experiments are somewhat trivial. 


",3
"This paper tries to draw connections between rate distortion theory and DNNs and use some intuitions from that domain to draw conclusions about robustness and generalization of the DNNs. 

The paper is mostly written in a storytelling narrative with very little rigor. In my opinion, this lack of rigor is problematic for a conference paper that has to be concise and rigorous. Moreover, the story is not told in a cohesive way. In most parts of the paper, there is not much relationship between the consecutive paragraphs. And even within most of the paragraphs, I was lost in understanding what the authors meant. I wish the paper would have been self-contained and made concrete definitions and statements instead of very high-level ideas that are difficult to judge. In the current state, it is very difficult for me to say what exactly is the contribution of the paper in terms of the story other than some loosely related high-level ideas. I feel like most parts the story that the authors are telling is already told by many other papers in other forms(papers that authors have cited and many other ones).


",2
"I think this paper conducts several interesting analysis about MT hallucinations and also proposes several different ways of reducing this effect. My questions are as follows:

* I am very curious about how do you decide the chosen noisy words. I am also wondering what is the difference if you do choose different noisy words. Another thing, if the noisy words are unseen in the training set, will it be treated as ""UNK""?
* Can you highlight what is changed in the upper right side of fig.4? It would be great if you include gloss in the figure as well.",6
"	My major concern about the work is that the studied model is quite weak. 
	""All models we present are well trained with a BLEU score of at least 20.0 on the test set, a reasonable score for 2-layer models with 256 hidden units."" 
	""We then used the WMT De!En 2016 test set (2,999 examples) to compute the hallucination percentage for each model.""
	I checked the WMT official website http://matrix.statmt.org/matrix. It shows that the best result was a BLEU score of 40.2, which was obtained at 2016. The models used in this work are about 20.0, which are much less than the WMT results reported two years ago. Note that neural machine translation has made remarkable progress in recent two years, not to mention that production systems like Google translator perform much better than research systems. Therefore, the discoveries reported in this work are questionable. I strongly suggest the authors to conduct the studies base on the latest NMT architecture, i.e., Transformer.
	
	Furthermore, I checked the examples given in  introduction in Google translator and found no hallucination. So I'm not sure whether such hallucinations are really critical to today's NMT systems. I'd like to see that the study on some production translation systems, e.g., applying Algo 1 to Google translator and check its outputs, which can better motivate this work.
	
	For the analysis in Section 6.1, if attention is the root cause of hallucinations, some existing methods should have already address this issue. Can you check whether the model trained by the following work still suffers from hallucinations?
Modeling Coverage for Neural Machine Translation, ACL 16.",4
"The authors introduce hallucinations in NMT and propose some algorithms to avoid them. 
The paper is clear (except section 6.2, which could have been more clearly described) and the work is original. 
The paper points out hallucination problems in NMT which looks like adversarial examples in the paper ""Explaining and Harnessing Adversarial Examples"". So, the authors might want to compare the perturbed sources to the adversarial examples.
If analysis is provided for each hallucination patten, that would be better. 
",7
